[
  {
    "objectID": "slides/slides_2.html#outline",
    "href": "slides/slides_2.html#outline",
    "title": "Lecture 1",
    "section": "Outline",
    "text": "Outline\n\nWhat are INLA and inlabru?\nWhy the Bayesian framework?\nWhich model are inlabru-friendly?\nWhat are Latent Gaussian Models?\nHow are they implemented in inlabru?"
  },
  {
    "objectID": "slides/slides_2.html#what-is-inla-what-is-inlabru",
    "href": "slides/slides_2.html#what-is-inla-what-is-inlabru",
    "title": "Lecture 1",
    "section": "What is INLA? What is inlabru?",
    "text": "What is INLA? What is inlabru?\nThe short answer:\n\nINLA is a fast method to do Bayesian inference with latent Gaussian models and inlabru is an R-package that implements this method with a flexible and simple interface.\n\nThe (much) longer answer:\n\nRue, H., Martino, S. and Chopin, N. (2009), Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71: 319-392.\nVan Niekerk, J., Krainski, E., Rustand, D., & Rue, H. (2023). A new avenue for Bayesian inference with INLA. Computational Statistics & Data Analysis, 181, 107692.\nLindgren, F., Bachl, F., Illian, J., Suen, M. H., Rue, H., & Seaton, A. E. (2024). inlabru: software for fitting latent Gaussian models with non-linear predictors. arXiv preprint arXiv:2407.00791.\nLindgren, F., Bolin, D., & Rue, H. (2022). The SPDE approach for Gaussian and non-Gaussian fields: 10 years and still running. Spatial Statistics, 50, 100599."
  },
  {
    "objectID": "slides/slides_2.html#where",
    "href": "slides/slides_2.html#where",
    "title": "Lecture 1",
    "section": "Where?",
    "text": "Where?\n\n\n\n Website-tutorials\n\n\ninlabru https://inlabru-org.github.io/inlabru/\nR-INLA https://www.r-inla.org/home\n\n\n\n\n\n\n Discussion forums\n\n\ninlabru https://github.com/inlabru-org/inlabru/discussions\nR-INLA https://groups.google.com/g/r-inla-discussion-group\n\n\n\n\n\n\n Books\n\n\n\nBlangiardo, M., & Cameletti, M. (2015). Spatial and spatio-temporal Bayesian models with R-INLA. John Wiley & Sons.\nGómez-Rubio, V. (2020). Bayesian inference with INLA. Chapman and Hall/CRC.\nKrainski, E., Gómez-Rubio, V., Bakka, H., Lenzi, A., Castro-Camilo, D., Simpson, D., … & Rue, H. (2018). Advanced spatial modeling with stochastic partial differential equations using R and INLA. Chapman and Hall/CRC.\nWang, X., Yue, Y. R., & Faraway, J. J. (2018). Bayesian regression modeling with INLA. Chapman and Hall/CRC."
  },
  {
    "objectID": "slides/slides_2.html#so-why-should-you-use-inlabru",
    "href": "slides/slides_2.html#so-why-should-you-use-inlabru",
    "title": "Lecture 1",
    "section": "So… Why should you use inlabru?",
    "text": "So… Why should you use inlabru?\n\n\nWhat type of problems can we solve?\nWhat type of models can we use?\nWhen can we use it?"
  },
  {
    "objectID": "slides/slides_2.html#so-why-should-you-use-inlabru-1",
    "href": "slides/slides_2.html#so-why-should-you-use-inlabru-1",
    "title": "Lecture 1",
    "section": "So… Why should you use inlabru?",
    "text": "So… Why should you use inlabru?\n\nWhat type of problems can we solve?\nWhat type of models can we use?\nWhen can we use it?\n\n\nTo give proper answers to these questions, we need to start at the very beginning .."
  },
  {
    "objectID": "slides/slides_2.html#the-core",
    "href": "slides/slides_2.html#the-core",
    "title": "Lecture 1",
    "section": "The core",
    "text": "The core\n\nWe have observed something."
  },
  {
    "objectID": "slides/slides_2.html#the-core-1",
    "href": "slides/slides_2.html#the-core-1",
    "title": "Lecture 1",
    "section": "The core",
    "text": "The core\n\nWe have observed something.\nWe have questions."
  },
  {
    "objectID": "slides/slides_2.html#the-core-2",
    "href": "slides/slides_2.html#the-core-2",
    "title": "Lecture 1",
    "section": "The core",
    "text": "The core\n\nWe have observed something.\nWe have questions.\nWe want answers!"
  },
  {
    "objectID": "slides/slides_2.html#how-do-we-find-answers",
    "href": "slides/slides_2.html#how-do-we-find-answers",
    "title": "Lecture 1",
    "section": "How do we find answers?",
    "text": "How do we find answers?\nWe need to make choices:\n\n\n\nBayesian or frequentist?\nHow do we model the data?\nHow do we compute the answer?"
  },
  {
    "objectID": "slides/slides_2.html#how-do-we-find-answers-1",
    "href": "slides/slides_2.html#how-do-we-find-answers-1",
    "title": "Lecture 1",
    "section": "How do we find answers?",
    "text": "How do we find answers?\nWe need to make choices:\n\nBayesian or frequentist?\nHow do we model the data?\nHow do we compute the answer?\n\nThese questions are not independent."
  },
  {
    "objectID": "slides/slides_2.html#bayesian-or-frequentist",
    "href": "slides/slides_2.html#bayesian-or-frequentist",
    "title": "Lecture 1",
    "section": "Bayesian or frequentist?",
    "text": "Bayesian or frequentist?\nIn this course we embrace the Bayesian perspective\n\nThere are no “true but unknown” parameters !"
  },
  {
    "objectID": "slides/slides_2.html#bayesian-or-frequentist-1",
    "href": "slides/slides_2.html#bayesian-or-frequentist-1",
    "title": "Lecture 1",
    "section": "Bayesian or frequentist?",
    "text": "Bayesian or frequentist?\nIn this course we embrace the Bayesian perspective\n\nThere are no “true but unknown” parameters !\nEvery parameter is described by a probability distribution!"
  },
  {
    "objectID": "slides/slides_2.html#bayesian-or-frequentist-2",
    "href": "slides/slides_2.html#bayesian-or-frequentist-2",
    "title": "Lecture 1",
    "section": "Bayesian or frequentist?",
    "text": "Bayesian or frequentist?\nIn this course we embrace the Bayesian perspective\n\nThere are no “true but unknown” parameters !\nEvery parameter is described by a probability distribution!\nEvidence from the data is used to update the belief we had before observing the data!"
  },
  {
    "objectID": "slides/slides_2.html#some-more-details-i",
    "href": "slides/slides_2.html#some-more-details-i",
    "title": "Lecture 1",
    "section": "Some more details I",
    "text": "Some more details I\nWe define linear predictor the mean (or a function of the mean) of our observations given the model components.\n\nIn this case \\(E(y_i|\\beta_0, \\beta_i) = \\eta_i = \\beta_0 + \\beta_1 x_i\\)"
  },
  {
    "objectID": "slides/slides_2.html#some-more-details-i-1",
    "href": "slides/slides_2.html#some-more-details-i-1",
    "title": "Lecture 1",
    "section": "Some more details I",
    "text": "Some more details I\nWe define linear predictor the mean (or a function of the mean) of our observations given the model components.\n\nIn this case \\(E(y_i|\\beta_0, \\beta_i) =\\eta_i =  \\color{red}{\\boxed{\\beta_0}} +  \\color{red}{\\boxed{\\beta_1 x_i}}\\)\nThis model as two components !"
  },
  {
    "objectID": "slides/slides_2.html#some-more-details-ii",
    "href": "slides/slides_2.html#some-more-details-ii",
    "title": "Lecture 1",
    "section": "Some more details II",
    "text": "Some more details II\nGiven the linear predictor \\(\\eta\\) the observations a independent on each other!\n\nThis means that all dependencies in the observations are accounted for by the components!"
  },
  {
    "objectID": "slides/slides_2.html#some-more-details-ii-1",
    "href": "slides/slides_2.html#some-more-details-ii-1",
    "title": "Lecture 1",
    "section": "Some more details II",
    "text": "Some more details II\nGiven the linear predictor \\(\\eta\\) the observations a independent on each other!\n\nThe observation model (likelihood) can be written as: \\[\n\\pi(\\mathbf{y}|\\eta,\\sigma^2) = \\prod_{i = 1}^n\\pi(y_i|\\eta_i,\\sigma^2)\n\\]"
  },
  {
    "objectID": "slides/slides_2.html#lets-formalize-this-a-bit",
    "href": "slides/slides_2.html#lets-formalize-this-a-bit",
    "title": "Lecture 1",
    "section": "Let’s formalize this a bit…",
    "text": "Let’s formalize this a bit…\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\\\\nE(y_i|\\eta_i, \\sigma^2) & = \\eta_i\n\\end{aligned}\n\\] Note: We assume that, given the linear predictor \\(\\eta\\), the data are independent on each other! Data dependence is expressed through the components if the linear predictor."
  },
  {
    "objectID": "slides/slides_2.html#lets-formalize-this-a-bit-1",
    "href": "slides/slides_2.html#lets-formalize-this-a-bit-1",
    "title": "Lecture 1",
    "section": "Let’s formalize this a bit…",
    "text": "Let’s formalize this a bit…\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\(y_i|\\eta_i,\\sigma^2\\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\)\nA model for the linear predictor \\[\nE(y_i|\\eta_i,\\sigma^2) = \\eta_i = \\beta_0 + \\beta_1x_i\n\\]"
  },
  {
    "objectID": "slides/slides_2.html#lets-formalize-this-a-bit-2",
    "href": "slides/slides_2.html#lets-formalize-this-a-bit-2",
    "title": "Lecture 1",
    "section": "Let’s formalize this a bit…",
    "text": "Let’s formalize this a bit…\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\(y_i|\\eta_i,\\sigma^2\\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\)\nA model for the linear predictor\n\n\\[\nE(y_i|\\eta_i,\\sigma^2) = \\eta_i = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_1x_i} }\n\\]\nNote 1: These are the components of our model! These explain the dependence structure of the data."
  },
  {
    "objectID": "slides/slides_2.html#lets-formalize-this-a-bit-3",
    "href": "slides/slides_2.html#lets-formalize-this-a-bit-3",
    "title": "Lecture 1",
    "section": "Let’s formalize this a bit…",
    "text": "Let’s formalize this a bit…\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\(y_i|\\eta_i,\\sigma^2\\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\)\nA model for the linear predictor \\(\\eta_i = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_1x_i} }\\)\nA prior for the model components \\(\\textbf{u}\\) \\[\n\\mathbf{u} = \\{\\beta_0, \\beta_1\\}\\sim\\mathcal{N}(0,\\mathbf{Q}^{-1})\n\\] Note: These have always a Gaussian prior and are use to explain the dependencies among data!"
  },
  {
    "objectID": "slides/slides_2.html#lets-formalize-this-a-bit-4",
    "href": "slides/slides_2.html#lets-formalize-this-a-bit-4",
    "title": "Lecture 1",
    "section": "Let’s formalize this a bit…",
    "text": "Let’s formalize this a bit…\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\(y_i|\\eta_i,\\sigma^2\\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\)\nA model for the linear predictor \\(\\eta_i = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_1x_i} }\\)\nA prior for the model components \\(\\mathbf{u} = \\{\\beta_0, \\beta_1\\}\\sim\\mathcal{N}(0,\\mathbf{Q}^{-1})\\)\nA prior for the non-gaussian parameters \\(\\theta\\) \\[\n\\theta = \\sigma^2\n\\]"
  },
  {
    "objectID": "slides/slides_2.html#latent-gaussian-models-lgm",
    "href": "slides/slides_2.html#latent-gaussian-models-lgm",
    "title": "Lecture 1",
    "section": "Latent Gaussian Models (LGM)",
    "text": "Latent Gaussian Models (LGM)\n\n\n\nThe observation model: \\[\n\\pi(\\mathbf{y}|\\eta,\\theta) = \\prod_{i=1}^{n}\\pi(y_i|\\eta_i,\\theta)\n\\]\nLinear predictor \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\)\nLatent Gaussian field \\(\\pi(\\mathbf{u}|\\theta)\\)\nThe hyperparameters: \\(\\pi(\\theta)\\)\n\n\n\nStage 1 The data generating process"
  },
  {
    "objectID": "slides/slides_2.html#latent-gaussian-models-lgm-1",
    "href": "slides/slides_2.html#latent-gaussian-models-lgm-1",
    "title": "Lecture 1",
    "section": "Latent Gaussian Models (LGM)",
    "text": "Latent Gaussian Models (LGM)\n\n\n\nThe observation model: \\[\n\\pi(\\mathbf{y}|\\eta,\\theta) = \\prod_{i=1}^{n}\\pi(y_i|\\eta_i,\\theta)\n\\]\nLinear predictor \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\)\nLatent Gaussian field \\(\\pi(\\mathbf{u}|\\theta)\\)\nThe hyperparameters: \\(\\pi(\\theta)\\)\n\n\n\nStage 1 The data generating process\nStage 2 The dependence structure"
  },
  {
    "objectID": "slides/slides_2.html#latent-gaussian-models-lgm-2",
    "href": "slides/slides_2.html#latent-gaussian-models-lgm-2",
    "title": "Lecture 1",
    "section": "Latent Gaussian Models (LGM)",
    "text": "Latent Gaussian Models (LGM)\n\n\n\nThe observation model: \\[\n\\pi(\\mathbf{y}|\\eta,\\theta) = \\prod_{i=1}^{n}\\pi(y_i|\\eta_i,\\theta)\n\\]\nLinear predictor \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\)\nLatent Gaussian field \\(\\pi(\\mathbf{u}|\\theta)\\)\nThe hyperparameters: \\(\\pi(\\theta)\\)\n\n\n\nStage 1 The data generating process\nStage 2 The dependence structure\nStage 3 The hyperparameters"
  },
  {
    "objectID": "slides/slides_2.html#latent-gaussian-models-lgm-3",
    "href": "slides/slides_2.html#latent-gaussian-models-lgm-3",
    "title": "Lecture 1",
    "section": "Latent Gaussian Models (LGM)",
    "text": "Latent Gaussian Models (LGM)\n\n\n\n\nThe observation model: \\[\n\\pi(\\mathbf{y}|\\eta,\\theta) = \\prod_{i=1}^{n}\\pi(y_i|\\eta_i,\\theta)\n\\]\nLinear predictor \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\)\nLatent Gaussian field \\(\\pi(\\mathbf{u}|\\theta)\\)\nThe hyperparameters: \\(\\pi(\\theta)\\)\n\n\n\n\n\nStage 1 The data generating process\nStage 2 The dependence structure\nStage 3 The hyperparameters\n\n\n\n\nQ: What are we interested in?"
  },
  {
    "objectID": "slides/slides_2.html#the-posterior-distribution",
    "href": "slides/slides_2.html#the-posterior-distribution",
    "title": "Lecture 1",
    "section": "The posterior distribution",
    "text": "The posterior distribution\n\n\n\n\n\n\n\nposterior\n\n\n\nA\n\nPrior\n belief\n\n\n\nC\n\nBayes Theorem\n &\n Bayesian Computations\n\n\n\nA-&gt;C\n\n\n\n\n\nB\n\nObservation\n model\n\n\n\nB-&gt;C\n\n\n\n\n\nD\n\nPosterior\n distribution\n\n\n\nC-&gt;D\n\n\n\n\n\n\n\n\n\n\n\\[\n\\color{red}{\\pi(\\mathbf{u},\\theta|\\mathbf{y})}\\propto \\color{blue}{\\pi(\\mathbf{y}|\\mathbf{u},\\theta)}\\color{green}{\\pi(\\mathbf{u}|\\theta)\\pi(\\theta)}\n\\]"
  },
  {
    "objectID": "slides/slides_2.html#the-posterior-distribution-1",
    "href": "slides/slides_2.html#the-posterior-distribution-1",
    "title": "Lecture 1",
    "section": "The posterior distribution",
    "text": "The posterior distribution\n\n\n\n\n\n\n\nposterior\n\n\n\nC\n\nBayes Theorem\n &\n Bayesian Computations\n\n\n\nD\n\nPosterior\n distribution\n\n\n\nC-&gt;D\n\n\n\n\n\nE\n\nBayesian Computation are hard!!\n Here is where\n INLA\n comes in!!!\n\n\n\nE-&gt;C\n\n\n\n\n\nA\n\nPrior\n belief\n\n\n\nA-&gt;C\n\n\n\n\n\nB\n\nObservation\n model\n\n\n\nB-&gt;C"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression",
    "href": "slides/slides_2.html#inlabru-for-linear-regression",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\beta_0 + \\beta_i x_i\n\\end{aligned}\n\\]\n\n\nlibrary(inlabru)\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression-1",
    "href": "slides/slides_2.html#inlabru-for-linear-regression-1",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_i x_i}}\n\\end{aligned}\n\\]\n\n\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression-2",
    "href": "slides/slides_2.html#inlabru-for-linear-regression-2",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\color{red}{\\boxed{\\beta_0 + \\beta_i x_i}}\n\\end{aligned}\n\\]\n\n\nlibrary(inlabru)\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression-3",
    "href": "slides/slides_2.html#inlabru-for-linear-regression-3",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\n\\color{red}{\\boxed{y_i|\\eta_i, \\sigma^2}} & \\color{red}{\\boxed{\\sim \\mathcal{N}(\\eta_i,\\sigma^2)}}\\\\\n\\eta_i & = \\beta_0 + \\beta_i x_i\n\\end{aligned}\n\\]\n\n\nlibrary(inlabru)\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression-4",
    "href": "slides/slides_2.html#inlabru-for-linear-regression-4",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\beta_0 + \\beta_i x_i\n\\end{aligned}\n\\]\n\n\nlibrary(inlabru)\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression-5",
    "href": "slides/slides_2.html#inlabru-for-linear-regression-5",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression"
  },
  {
    "objectID": "slides/slides_2.html#real-datasets-are-more-complicated",
    "href": "slides/slides_2.html#real-datasets-are-more-complicated",
    "title": "Lecture 1",
    "section": "Real datasets are more complicated!",
    "text": "Real datasets are more complicated!\nData can have several dependence structures: temporal, spatial,…\nUsing a Bayesian framework:\n\nBuild (hierarchical) models to account for potentially complicated dependency structures in the data.\nAttribute uncertainty to model parameters and latent variables using priors.\n\nTwo main challenges:\n\nNeed computationally efficient methods to calculate posteriors (this is where INLA helps!).\nSelect priors in a sensible way (we’ll talk about this)"
  },
  {
    "objectID": "slides/slides_2.html#the-good-news",
    "href": "slides/slides_2.html#the-good-news",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! 😃\n\nStage 1: What is the distribution of the responses?\nStage 2: What are the model components? and what is their distribution?\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?"
  },
  {
    "objectID": "slides/slides_2.html#the-good-news-1",
    "href": "slides/slides_2.html#the-good-news-1",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! 😃\n\nStage 1: What is the distribution of the responses?\n\nGaussian response? (temperature, rainfall, fish weight …)\nCount data? (people infected with a disease in each area)\nPoint pattern? (locations of trees in a forest)\nBinary data? (yes/no response, binary image)\nSurvival data? (recovery time, time to death)\n… (many more examples!!)\n\nStage 2: What are the model components? and what is their distribution?\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?"
  },
  {
    "objectID": "slides/slides_2.html#the-good-news-2",
    "href": "slides/slides_2.html#the-good-news-2",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! 😃\n\nStage 1: What is the distribution of the responses?\n\nWe assume data to be conditionally independent given the model components and some hyperparameters\nThis means that all dependencies in data are explained in Stage\n\n\n\n\nStage 2: What are the model components? and what is their distribution?\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?"
  },
  {
    "objectID": "slides/slides_2.html#the-good-news-3",
    "href": "slides/slides_2.html#the-good-news-3",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! 😃\n\nStage 1: What is the distribution of the responses?\nStage 2: What are the model components? and what is their distribution?\n\nHere we can have:\n\nFixed effects for covariates\nUnstructured random effects (individual effects, group effects)\nStructured random effects (AR(1), regional effects, )\n…\n\nThese are linked to the responses in the likelihood through linear predictors.\n\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?"
  },
  {
    "objectID": "slides/slides_2.html#the-good-news-4",
    "href": "slides/slides_2.html#the-good-news-4",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! 😃\n\nStage 1: What is the distribution of the responses?\nStage 2: What are the model components? and what is their distribution?\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?\nThe likelihood and the latent model typically have hyperparameters that control their behavior.\nThey can include:\n\nVariance of observation noise\nDispersion parameter in the negative binomial model\nVariance of unstructured effects\n…"
  },
  {
    "objectID": "slides/slides_2.html#the-second-good-news",
    "href": "slides/slides_2.html#the-second-good-news",
    "title": "Lecture 1",
    "section": "The second good news!",
    "text": "The second good news!\nNo matter how complicated is your model, the inlabru workflow is always the same 😃\n\n# Define model components\ncomps &lt;- component_1(...) + \n  component_2(...) + ...\n\n# Define the model predictor\npred &lt;- linear_function(component_1, \n                            component_2, ...)\n\n# Build the observation model\nlik &lt;- bru_obs(formula = pred,\n               family = ... ,\n               data = ... ,\n                ...)\n\n# Fit the model\nfit &lt;- bru(comps, lik, ...)"
  },
  {
    "objectID": "slides/slides_2.html#the-second-good-news-1",
    "href": "slides/slides_2.html#the-second-good-news-1",
    "title": "Lecture 1",
    "section": "The second good news!",
    "text": "The second good news!\nNo matter how complicated is your model, the inlabru workflow is always the same 😃\n\n# Define model components\ncomps &lt;- component_1(...) + \n  component_2(...) + ...\n\n# Define the model predictor\npred &lt;- linear_function(component_1, \n                            component_2, ...)\n\n# Build the observation model\nlik &lt;- bru_obs(formula = pred,\n               family = ... ,\n               data = ... ,\n                ...)\n\n# Fit the model\nfit &lt;- bru(comps, lik, ...)\n\nNOTE we will see later that this function can also be non-linear….😁"
  },
  {
    "objectID": "slides/slides_2.html#the-tokyo-rainfall-data",
    "href": "slides/slides_2.html#the-tokyo-rainfall-data",
    "title": "Lecture 1",
    "section": "The Tokyo rainfall data",
    "text": "The Tokyo rainfall data\nOne example with time series: Rainfall over 1 mm in the Tokyo area for each calendar day during two years (1983-84) are registered."
  },
  {
    "objectID": "slides/slides_2.html#the-model",
    "href": "slides/slides_2.html#the-model",
    "title": "Lecture 1",
    "section": "The model",
    "text": "The model\nStage 1 The observation model\n\\[\ny_t|\\eta_t\\sim\\text{Bin}(n_t, p_t),\\qquad \\eta_t = \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\] \\[\nn_t = \\left\\{\n\\begin{array}{lr}\n1, & \\text{for}\\; 29\\; \\text{February}\\\\\n2, & \\text{other days}\n\\end{array}\\right.\n\\] \\[\ny_t =\n\\begin{cases}\n\\{0,1\\}, & \\text{for}\\; 29\\; \\text{February}\\\\\n\\{0,1,2\\}, & \\text{other days}\n\\end{cases}\n\\]\n\nthe likelihood has no hyperparameters"
  },
  {
    "objectID": "slides/slides_2.html#the-model-1",
    "href": "slides/slides_2.html#the-model-1",
    "title": "Lecture 1",
    "section": "The model",
    "text": "The model\nStage 1 The observation model\n\\[\ny_t|\\eta_t\\sim\\text{Bin}(n_t, p_t),\\qquad \\eta_t = \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\]\nStage 2 The latent field \\[\n\\eta_t = \\beta_0 + f(\\text{time}_t)\n\\]\n\nprobability of rain depends on on the day of the year \\(t\\)\n\\(\\beta_0\\) is an intercept\n\\(f(\\text{time}_t)\\) is a RW2 model (this is just a smoother). The smoothness is controlled by a hyperparameter \\(\\tau_f\\)"
  },
  {
    "objectID": "slides/slides_2.html#the-model-2",
    "href": "slides/slides_2.html#the-model-2",
    "title": "Lecture 1",
    "section": "The model",
    "text": "The model\nStage 1 The observation model\n\\[\ny_t|\\eta_t\\sim\\text{Bin}(n_t, p_t),\\qquad \\eta_t = \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\]\nStage 2 The latent field \\[\n\\eta_t = \\beta_0 + f(\\text{time}_t)\n\\]\nStage 3 The hyperparameters\n\nThe structured time effect is controlled by one parameter \\(\\tau_f\\).\nWe assign a prior to \\(\\tau_f\\) to finalize the model."
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-time-series",
    "href": "slides/slides_2.html#inlabru-for-time-series",
    "title": "Lecture 1",
    "section": "inlabru for time series",
    "text": "inlabru for time series\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim \\text{Binomial}(n_t,p_t)\\\\\n\\text{logit}(p_t) = \\eta_i & = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{f(\\text{time}_t)}}\n\\end{aligned}\n\\]\n\n\nTokyo[1:3,]\n\n  y n time\n1 0 2    1\n2 0 2    2\n3 1 2    3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-time-series-1",
    "href": "slides/slides_2.html#inlabru-for-time-series-1",
    "title": "Lecture 1",
    "section": "inlabru for time series",
    "text": "inlabru for time series\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim \\text{Binomial}(n_t,p_t)\\\\\n\\text{logit}(p_t) = \\color{red}{\\boxed{\\eta_i}} & = \\color{red}{\\boxed{\\beta_0 + f(\\text{time}_t)}}\n\\end{aligned}\n\\]\n\n\nTokyo[1:3,]\n\n  y n time\n1 0 2    1\n2 0 2    2\n3 1 2    3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-time-series-2",
    "href": "slides/slides_2.html#inlabru-for-time-series-2",
    "title": "Lecture 1",
    "section": "inlabru for time series",
    "text": "inlabru for time series\n\n\nThe Model\n\\[\n\\begin{aligned}\n\\color{red}{\\boxed{y_t|\\eta_t}} & \\color{red}{\\boxed{\\sim \\text{Binomial}(n_t,p_t)}}\\\\\n\\text{logit}(p_t) = \\eta_i & = \\beta_0 + f(\\text{time}_t)\n\\end{aligned}\n\\]\n\n\nTokyo[1:3,]\n\n  y n time\n1 0 2    1\n2 0 2    2\n3 1 2    3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-time-series-3",
    "href": "slides/slides_2.html#inlabru-for-time-series-3",
    "title": "Lecture 1",
    "section": "inlabru for time series",
    "text": "inlabru for time series\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim \\text{Binomial}(n_t,p_t)\\\\\n\\text{logit}(p_t) = \\eta_i & = \\beta_0 + f(\\text{time}_t)\n\\end{aligned}\n\\]\n\n\nTokyo[1:3,]\n\n  y n time\n1 0 2    1\n2 0 2    2\n3 1 2    3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#example-disease-mapping",
    "href": "slides/slides_2.html#example-disease-mapping",
    "title": "Lecture 1",
    "section": "Example: disease mapping",
    "text": "Example: disease mapping\nWe observed larynx cancer mortality counts for males in 544 district of Germany from 1986 to 1990 and want to make a model.\n\n\n\n\\(y_i\\): The count at location \\(i\\).\n\\(E_i\\): An offset; expected number of cases in district \\(i\\).\n\\(c_i\\): A covariate (level of smoking consumption) at \\(i\\)\n\\(\\boldsymbol{s}_i\\): spatial location \\(i\\) ."
  },
  {
    "objectID": "slides/slides_2.html#bayesian-disease-mapping",
    "href": "slides/slides_2.html#bayesian-disease-mapping",
    "title": "Lecture 1",
    "section": "Bayesian disease mapping",
    "text": "Bayesian disease mapping\n\n\n\nStage 1: We assume the responses are Poisson distributed: \\[          \ny_i \\mid \\eta_i \\sim \\text{Poisson}(E_i\\exp(\\eta_i)))\n\\]\n\n\n\n\n\n\nStage 2: \\(\\eta_i\\) is a linear function of three components: an intercept, a covariate \\(c_i\\), a spatially structured effect \\(\\omega\\) likelihood by \\[\n\\eta_i = \\beta_0 + \\beta_1\\ c_i + \\omega_i\n\\]\n\n\n\n\n\n\nStage 3:\n\n\\(\\tau_{\\omega}\\): Precisions parameter for the random effects\n\n\n\n\n\nThe latent field is \\(\\boldsymbol{u} = (\\beta_0, \\beta_1, \\omega_1, \\omega_2,\\ldots, \\omega_n)\\), the hyperparameters are \\(\\boldsymbol{\\theta} = (\\tau_{\\omega})\\), and must be given a prior."
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-disease-mapping",
    "href": "slides/slides_2.html#inlabru-for-disease-mapping",
    "title": "Lecture 1",
    "section": "inlabru for disease mapping",
    "text": "inlabru for disease mapping\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_i|\\eta_t & \\sim \\text{Poisson}(E_i\\lambda_i)\\\\\n\\text{log}(\\lambda_i) = \\eta_i & = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_1\\ c_i}} + \\color{red}{\\boxed{\\omega_i}}\n\\end{aligned}\n\\]\n\n\ng = system.file(\"demodata/germany.graph\",\n                package=\"INLA\")\nGermany[1:3,]\n\n  region         E  Y  x region.struct\n1      1  7.965008  8 56             1\n2      2 22.836219 22 65             2\n3      3 22.094716 19 50             3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + beta1(x, model = \"linear\") +\n  space(region, model = \"besag\", graph = g)\n\n# define model predictor\neta = Y ~ beta0 + beta1 + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"poisson\",\n              E = E,\n              data = Germany)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-disease-mapping-1",
    "href": "slides/slides_2.html#inlabru-for-disease-mapping-1",
    "title": "Lecture 1",
    "section": "inlabru for disease mapping",
    "text": "inlabru for disease mapping\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_i|\\eta_t & \\sim \\text{Poisson}(E_i\\lambda_i)\\\\\n\\text{log}(\\lambda_i) = \\color{red}{\\boxed{\\eta_i}} & = \\color{red}{\\boxed{\\beta_0 + \\beta_1\\ c_i + \\omega_i}}\n\\end{aligned}\n\\]\n\n\ng = system.file(\"demodata/germany.graph\",\n                package=\"INLA\")\nGermany[1:3,]\n\n  region         E  Y  x region.struct\n1      1  7.965008  8 56             1\n2      2 22.836219 22 65             2\n3      3 22.094716 19 50             3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + beta1(x, model = \"linear\") +\n  space(region, model = \"bym2\", graph = g)\n\n# define model predictor\neta = Y ~ beta0 + beta1 + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"poisson\",\n              E = E,\n              data = Germany)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-disease-mapping-2",
    "href": "slides/slides_2.html#inlabru-for-disease-mapping-2",
    "title": "Lecture 1",
    "section": "inlabru for disease mapping",
    "text": "inlabru for disease mapping\n\n\nThe Model\n\\[\n\\begin{aligned}\n\\color{red}{\\boxed{y_i|\\eta_t}} & \\color{red}{\\boxed{\\sim \\text{Poisson}(E_i\\lambda_i)}}\\\\\n\\text{log}(\\lambda_i) = \\eta_i & = \\beta_0 + \\beta_1\\ c_i + \\omega_i\n\\end{aligned}\n\\]\n\n\ng = system.file(\"demodata/germany.graph\",\n                package=\"INLA\")\nGermany[1:3,]\n\n  region         E  Y  x region.struct\n1      1  7.965008  8 56             1\n2      2 22.836219 22 65             2\n3      3 22.094716 19 50             3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + beta1(x, model = \"linear\") +\n  space(region, model = \"bym2\", graph = g)\n\n# define model predictor\neta = Y ~ beta0 + beta1 + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"poisson\",\n              E = E,\n              data = Germany)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-disease-mapping-3",
    "href": "slides/slides_2.html#inlabru-for-disease-mapping-3",
    "title": "Lecture 1",
    "section": "inlabru for disease mapping",
    "text": "inlabru for disease mapping"
  },
  {
    "objectID": "slides/slides_2.html#bayesian-geostatistics",
    "href": "slides/slides_2.html#bayesian-geostatistics",
    "title": "Lecture 1",
    "section": "Bayesian Geostatistics",
    "text": "Bayesian Geostatistics\nEncounter probability of Pacific Cod (Gadus macrocephalus) from a trawl survey.\n\n\n\n\n\n\n\n\\(y(s)\\) Presence of absence in location \\(s\\)"
  },
  {
    "objectID": "slides/slides_2.html#bayesian-geostatistics-1",
    "href": "slides/slides_2.html#bayesian-geostatistics-1",
    "title": "Lecture 1",
    "section": "Bayesian Geostatistics",
    "text": "Bayesian Geostatistics\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim\\text{Binom}(1, p(s))\n\\]\nStage 2 Latent field model \\[\n\\eta(s) = \\text{logit}(p(s)) = \\beta_0 + f( x(s)) + \\omega(s)\n\\]\nStage 3 Hyperparameters"
  },
  {
    "objectID": "slides/slides_2.html#bayesian-geostatistics-2",
    "href": "slides/slides_2.html#bayesian-geostatistics-2",
    "title": "Lecture 1",
    "section": "Bayesian Geostatistics",
    "text": "Bayesian Geostatistics\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim\\text{Binom}(1, p(s))\n\\]\nStage 2 Latent field model \\[\n\\eta(s) = \\text{logit}(p(s)) = \\beta_0 + f( x(s)) + \\omega(s)\n\\]\n\nA global intercept \\(\\beta_0\\)\nA smooth effect of covariate \\(x(s)\\) (depth)\nA Gaussian field \\(\\omega(s)\\) (will discuss this later..)\n\nStage 3 Hyperparameters"
  },
  {
    "objectID": "slides/slides_2.html#bayesian-geostatistics-3",
    "href": "slides/slides_2.html#bayesian-geostatistics-3",
    "title": "Lecture 1",
    "section": "Bayesian Geostatistics",
    "text": "Bayesian Geostatistics\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim\\text{Binom}(1, p(s))\n\\]\nStage 2 Latent field model \\[\n\\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\beta_1 x(s) + \\omega(s)\n\\]\nStage 3 Hyperparameters\n\nPrecision for the smooth function \\(f(\\cdot)\\)\nRange and sd in the Gaussian field \\(\\sigma_{\\omega}, \\tau_{\\omega}\\)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-geostatistics",
    "href": "slides/slides_2.html#inlabru-for-geostatistics",
    "title": "Lecture 1",
    "section": "inlabru for geostatistics",
    "text": "inlabru for geostatistics\n\n\nThe Model\n$$\n\\[\\begin{aligned}\ny(s)|\\eta(s) & \\sim\\text{Binom}(1, p(s))\\\\\n\\eta(s) &  = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{ f(x(s))}} + \\color{red}{\\boxed{ \\omega(s)}}\\\\\n\n\\end{aligned}\\]\n$$\n\n\ndf %&gt;% select(depth, present) %&gt;% print(n = 3)\n\nSimple feature collection with 2143 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 343.0617 ymin: 5635.893 xmax: 579.3681 ymax: 5839.019\nProjected CRS: +proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\n# A tibble: 2,143 × 3\n  depth present            geometry\n  &lt;dbl&gt;   &lt;dbl&gt;        &lt;POINT [km]&gt;\n1   201       1 (446.4752 5793.426)\n2   212       1 (446.4594 5800.136)\n3   220       0 (448.5987 5801.687)\n# ℹ 2,140 more rows\n\n\n\nThe code\n\n# define model component\ncmp = ~ -1 + Intercept(1) +  depth_smooth(log(depth), model='rw2') + \n  space(geometry, model = spde_model)\n\n# define model predictor\neta = present ~ Intercept + depth_smooth + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              data = df,\n              family = \"binomial\")\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-geostatistics-1",
    "href": "slides/slides_2.html#inlabru-for-geostatistics-1",
    "title": "Lecture 1",
    "section": "inlabru for geostatistics",
    "text": "inlabru for geostatistics\n\n\nThe Model\n$$\n\\[\\begin{aligned}\ny(s)|\\eta(s) & \\sim\\text{Binom}(1, p(s))\\\\\n\\color{red}{\\boxed{\\eta(s)}} &  = \\color{red}{\\boxed{\\beta_0 +  f(x(s)) +  \\omega(s)}}\\\\\n\n\\end{aligned}\\]\n$$\n\n\ndf %&gt;% select(depth, present) %&gt;% print(n = 3)\n\nSimple feature collection with 2143 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 343.0617 ymin: 5635.893 xmax: 579.3681 ymax: 5839.019\nProjected CRS: +proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\n# A tibble: 2,143 × 3\n  depth present            geometry\n  &lt;dbl&gt;   &lt;dbl&gt;        &lt;POINT [km]&gt;\n1   201       1 (446.4752 5793.426)\n2   212       1 (446.4594 5800.136)\n3   220       0 (448.5987 5801.687)\n# ℹ 2,140 more rows\n\n\n\nThe code\n\n# define model component\ncmp = ~ -1 + Intercept(1) +  depth_smooth(log(depth), model='rw2') + \n  space(geometry, model = spde_model)\n\n# define model predictor\neta = present ~ Intercept + depth_smooth + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              data = df,\n              family = \"binomial\")\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-geostatistics-2",
    "href": "slides/slides_2.html#inlabru-for-geostatistics-2",
    "title": "Lecture 1",
    "section": "inlabru for geostatistics",
    "text": "inlabru for geostatistics\n\n\nThe Model\n$$\n\\[\\begin{aligned}\n\\color{red}{\\boxed{y(s)|\\eta(s)}} & \\sim \\color{red}{\\boxed{\\text{Binom}(1, p(s))}}\\\\\n\\eta(s) &  = \\beta_0 +  f(x(s)) +  \\omega(s)\\\\\n\n\\end{aligned}\\]\n$$\n\n\ndf %&gt;% select(depth, present) %&gt;% print(n = 3)\n\nSimple feature collection with 2143 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 343.0617 ymin: 5635.893 xmax: 579.3681 ymax: 5839.019\nProjected CRS: +proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\n# A tibble: 2,143 × 3\n  depth present            geometry\n  &lt;dbl&gt;   &lt;dbl&gt;        &lt;POINT [km]&gt;\n1   201       1 (446.4752 5793.426)\n2   212       1 (446.4594 5800.136)\n3   220       0 (448.5987 5801.687)\n# ℹ 2,140 more rows\n\n\n\nThe code\n\n# define model component\ncmp = ~ -1 + Intercept(1) +  depth_smooth(log(depth), model='rw2') + \n  space(geometry, model = spde_model)\n\n# define model predictor\neta = present ~ Intercept + depth_smooth + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              data = df,\n              family = \"binomial\")\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-geostatistics-3",
    "href": "slides/slides_2.html#inlabru-for-geostatistics-3",
    "title": "Lecture 1",
    "section": "inlabru for geostatistics",
    "text": "inlabru for geostatistics"
  },
  {
    "objectID": "slides/slides_2.html#take-home-message",
    "href": "slides/slides_2.html#take-home-message",
    "title": "Lecture 1",
    "section": "Take home message!",
    "text": "Take home message!\n\nMany of the model you have used (and some you have never used but will learn about) are just special cases of the large class of Latent Gaussian models\ninlabru provides an efficient and unified way to fit all these models!"
  },
  {
    "objectID": "day4_practical_6.html",
    "href": "day4_practical_6.html",
    "title": "Practical 6",
    "section": "",
    "text": "Aim of this practical:\nIn this practical we are going to look at some model comparison and validation techniques.\nDownload Practical 6 R script",
    "crumbs": [
      "Home",
      "Practical 6"
    ]
  },
  {
    "objectID": "day4_practical_6.html#model-checking-for-linear-models",
    "href": "day4_practical_6.html#model-checking-for-linear-models",
    "title": "Practical 6",
    "section": "1 Model Checking for Linear Models",
    "text": "1 Model Checking for Linear Models\nIn this exercise we will:\n\nLearn about some model assessments techniques available in INLA\nConduct posterior predictive model checking\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nRecall a simple linear regression model with Gaussian observations\n\\[\ny_i\\sim\\mathcal{N}(\\mu_i, \\sigma^2), \\qquad i = 1,\\dots,N\n\\]\nwhere \\(\\sigma^2\\) is the observation error, and the mean parameter \\(\\mu_i\\) is linked to the linear predictor through an identity function:\n\\[\n\\eta_i = \\mu_i = \\beta_0 + \\beta_1 x_i\n\\] where \\(x_i\\) is a covariate and \\(\\beta_0, \\beta_1\\) are parameters to be estimated.\n\n1.1 Simulate example data\nWe simulate data from a simple linear regression model\n\n\nCode\nbeta = c(2,0.5)\nsd_error = 0.1\n\nn = 100\nx = rnorm(n)\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error)\n\ndf = data.frame(y = y, x = x)  \n\n\n\n\n1.2 Fitting the linear regression model with inlabru\nNow we fit a simple linear regression model in inalbru by defining (1) the model components, (2) the linear predictor and (3) the likelihood.\n\n# Model components\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n# Linear predictor\nformula = y ~ Intercept + beta_1\n# Observational model likelihood\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n# Fit the Model\nfit.lm = bru(cmp, lik)\n\n\n\n1.3 Residuals analysis\nA common way for model diagnostics in regression analysis is by checking residual plots. In a Bayesian setting residuals can be defined in multiple ways depending on how you account for posterior uncertainty. Here, we will adopt a Bayesian approach by generating samples from the posterior distribution of the model parameters and then draw samples from the residuals defined as:\n\\[\nr_i = y_i - x_i^T\\beta\n\\]\nWe can use the predict function to achieve this:\n\nres_samples &lt;- predict(\n  fit.lm,         # the fitted model\n  df,             # the original data set\n  ~ data.frame(   \n    res = y-(beta_0 + beta_1)  # compute the residuals\n  ),\n  n.samples = 1000   # draw 1000 samples\n)\n\nThe resulting data frame contains the posterior draw of the residuals mean for which we can produce some diagnostics plots , e.g.\n\n\nResiduals checks for Linear Model\nggplot(res_samples,aes(y=mean,x=1:100))+geom_point() +\nggplot(res_samples,aes(y=mean,x=x))+geom_point()\n\n\n\n\n\nBayesian residual plots: the left panel is the residual index plot; the right panel is the plot of the residual versus the covariate x\n\n\n\n\nWe can also compare these against the theoretical quantiles of the Normal distribution as follows:\n\n\nQQPlot for Linear Model\narrange(res_samples, mean) %&gt;%\n  mutate(theortical_quantiles = qnorm(1:100 / (1+100))) %&gt;%\n  ggplot(aes(x=theortical_quantiles,y= mean)) + \n  geom_ribbon(aes(ymin = q0.025, ymax = q0.975), fill = \"grey70\")+\n  geom_abline(intercept = mean(res_samples$mean),\n              slope = sd(res_samples$mean)) +\n  geom_point() +\n  labs(x = \"Theoretical Quantiles (Normal)\",\n       y= \"Sample Quantiles (Residuals)\") \n\n\n\n\n\n\n\n\n\n\n\n1.4 Posterior Predictive Checks\nNow, instead of generating samples from the mean, we will account for the observational process uncertainty by:\n\nSampling \\(y^{1k}_i\\sim\\pi(y_i|\\mathbf{y})\\) \\(k = 1,\\dots,M;~i = 1,\\ldots,100\\) using generate() (here we will draw \\(M=500\\) samples)\n\n\nsamples =  generate(fit.lm, df,\n  formula = ~ {\n    mu &lt;- (beta_0 + beta_1)\n    sd &lt;- sqrt(1 / Precision_for_the_Gaussian_observations)\n    rnorm(100, mean = mu, sd = sd)\n  },\n  n.samples = 500\n) \n\n\nComparing some summaries of the simulated data with the one of the observed one\n\nHere we compare (i) the estimated posterior densities \\(\\hat{\\pi}^k(y|\\mathbf{y})\\) with the estimated data density and (ii) the samples means and 95% credible intervals against the observations.\n\n# Tidy format for plotting\nsamples_long = data.frame(samples) %&gt;% \n  mutate(id = 1:100) %&gt;% # i-th observation\n  pivot_longer(-id)\n\n# compute the mean and quantiles for the samples\ndraws_summaries = data.frame(mean_samples = apply(samples,1,mean),\nq25 = apply(samples,1,function(x)quantile(x,0.025)),  \nq975 = apply(samples,1,function(x)quantile(x,0.975)),\nobservations = df$y)  \n\np1 = ggplot() + geom_density(data = samples_long, \n                        aes(value, group = name),  color = \"#E69F00\") +\n  geom_density(data = df, aes(y))  +\n  xlab(\"\") + ylab(\"\") \n\np2 = ggplot(draws_summaries,aes(y=mean_samples,x=observations))+\n  geom_errorbar(aes(ymin = q25,\n                   ymax = q975), \n               alpha = 0.5, color = \"grey50\")+\ngeom_point()+geom_abline(slope = 1,intercept = 0,lty=2)+labs()\n\np1 +p2",
    "crumbs": [
      "Home",
      "Practical 6"
    ]
  },
  {
    "objectID": "day4_practical_6.html#sec-linmodel",
    "href": "day4_practical_6.html#sec-linmodel",
    "title": "Practical 6",
    "section": "2 GLM model checking",
    "text": "2 GLM model checking\nIn this exercise we will:\n\nLearn about some model assessments techniques available in INLA\nConduct posterior predictive model checking using CPO and PIT\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nIn this exercise, we will use data on horseshoe crabs (Limulus polyphemus) where the number of satellites males surrounding a breeding female are counted along with the female’s color and carapace width.\n Download data set \nA possible model to study the factors that affect the number of satellites for female crabs is\n\\[\n\\begin{aligned}\ny_i&\\sim\\mathrm{Poisson}(\\mu_i), \\qquad i = 1,\\dots,N \\\\\n\\eta_i &= \\mu_i = \\beta_0 + \\beta_1 x_i + \\ldots\n\\end{aligned}\n\\]\nWe can explore the conditional means and variances given the female’s color:\n\ncrabs &lt;- read.csv(\"datasets/crabs.csv\")\n\n# conditional means and variances\ncrabs %&gt;%\n  summarise( Mean = mean(satell ),\n             Variance = var(satell),\n                     .by = color)\n\n   color     Mean  Variance\n1 medium 3.294737 10.273908\n2   dark 2.227273  6.737844\n3  light 4.083333  9.719697\n4 darker 2.045455 13.093074\n\n\nThe mean of the number of satellites vary by color which gives a good indication that color might be useful for predicting satellites numbers. However, notice that the mean is lower than its variance suggesting that overdispersion might be present and that a negative binomial model would be more appropriate for the data (we will cover this later).\nFitting the model\nFirst, lets begin fitting the Poisson model above using the carapace’s color and width as predictors. Since, color is a categorical variable in our model we need to create a dummy variable for it. We can use the model.matrix function to help us constructing the design matrix and then append this to our data:\n\ncrabs_df = model.matrix( ~  color , crabs) %&gt;%\n  as.data.frame() %&gt;%\n  select(-1) %&gt;%        # drop intercept\n  bind_cols(crabs) %&gt;%  # append to original data\n  select(-color)        # remove original color categorical variable\n\nThe new data set crabs_df contains a dummy variable for the different color categories (dark being the reference category). Then we can fit the model in inlabru as follows:\n\ncmp =  ~ -1 + beta0(1) +  colordarker +\n       colorlight + colormedium +\n       w(weight, model = \"linear\")\n\nlik =  bru_obs(formula = satell ~.,\n            family = \"poisson\",\n            data = crabs_df)\n\nfit_pois = bru(cmp, lik)\n\nsummary(fit_pois)\n\ninlabru version: 2.13.0\nINLA version: 25.08.21-1\nComponents:\nbeta0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\ncolordarker: main = linear(colordarker), group = exchangeable(1L), replicate = iid(1L), NULL\ncolorlight: main = linear(colorlight), group = exchangeable(1L), replicate = iid(1L), NULL\ncolormedium: main = linear(colormedium), group = exchangeable(1L), replicate = iid(1L), NULL\nw: main = linear(weight), group = exchangeable(1L), replicate = iid(1L), NULL\nObservation models:\n  Family: 'poisson'\n    Tag: &lt;No tag&gt;\n    Data class: 'data.frame'\n    Response class: 'integer'\n    Predictor: satell ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[beta0, colordarker, colorlight, colormedium, w], latent[]\nTime used:\n    Pre = 0.367, Running = 0.311, Post = 0.0646, Total = 0.743 \nFixed effects:\n              mean    sd 0.025quant 0.5quant 0.975quant   mode kld\nbeta0       -0.501 0.196     -0.885   -0.501     -0.117 -0.501   0\ncolordarker -0.008 0.180     -0.362   -0.008      0.345 -0.008   0\ncolorlight   0.445 0.176      0.101    0.445      0.790  0.445   0\ncolormedium  0.248 0.118      0.017    0.248      0.479  0.248   0\nw            0.001 0.000      0.000    0.001      0.001  0.001   0\n\nMarginal log-Likelihood:  -489.43 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\n\n2.1 Model assessment and model choice\nNow that we have fitted the model we would like to carry some model assessments. In a Bayesian setting, this is often based on posterior predictive checks. To do so, we will use the CPO and PIT - two commonly used Bayesian model assessment criteria based on the posterior predictive distribution.\n\n\n\n\n\n\nPosterior predictive model checking\n\n\n\nThe posterior predictive distribution for a predicted value \\(\\hat{y}\\) is\n\\[\n\\pi(\\hat{y}|\\mathbf{y}) = \\int_\\theta \\pi(\\hat{y}|\\theta)\\pi(\\theta|\\mathbf{y})d\\theta.\n\\]\nThe probability integral transform (PIT) introduced by Dawid (1984) is defined for each observation as:\n\\[\n\\mathrm{PIT}_i = \\pi(\\hat{y}_i \\leq y_i |\\mathbf{y}{-i})\n\\]\nThe PIT evaluates how well a model’s predicted values match the observed data distribution. It is computed as the cumulative distribution function (CDF) of the observed data evaluated at each predicted value. If the model is well-calibrated, the PIT values should be approximately uniformly distributed. Deviations from this uniform distribution may indicate issues with model calibration or overfitting.\nAnother metric we could used to asses the model fit is the conditional predictive ordinate (CPO) introduced by Pettit (1990), and deﬁned as:\n\\[\n\\text{CPO}_i = \\pi(y_i| \\mathbf{y}{-i})\n\\]\nThe CPO measures the density of the observed value of \\(y_i\\) when model is fit using all data but \\(y_i\\). CPO provides a measure of how well the model predicts each individual observation while taking into account the rest of the data and the model. Large values indicate a better fit of the model to the data, while small values indicate a bad fitting of the model\n\n\nTo compute PIT and CPO we can either:\n\nask inlabru to compute them by set options = list(control.compute = list(cpo = TRUE)) in the bru() function arguments.\nset this as default in inlabru global option using the bru_options_set function.\n\nHere we will do the later and re-run the model\n\nbru_options_set(control.compute = list(cpo = TRUE))\n\nfit_pois = bru(cmp, lik)\n\nNow we can produce histograms and QQ plots to assess for uniformity in the PIT values which can be accessed through inlabru_model$cpo$pit :\n\nPlotR Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfit_pois$cpo$pit %&gt;%\n  hist(main = \"Histogram of PIT values\")\n\nqqplot(qunif(ppoints(length(fit_pois$cpo$pit))),\n       fit_pois$cpo$pit,\n       main = \"Q-Q plot for Unif(0,1)\",\n       xlab = \"Theoretical Quantiles\",\n       ylab = \"Sample Quantiles\")\n\nqqline(fit_pois$cpo$pit,\n       distribution = function(p) qunif(p),\n       prob = c(0.1, 0.9))\n\n\n\n\nBoth Q-Q plots and histogram of the PIT values suggest a not so great model fit. For the CPO values, usually the following summary of the CPO is often used:\n\\[\n-\\sum_{i=1}^n \\log (\\text{CPO}\\_i)\n\\]\nThis quantities is useful when comparing different models - a smaller values indicate a better model fit. CPO values can be accessed by typing inlabru_model$cpo$cpo.\n\n\n\n\n\n\n Task\n\n\n\nThe model assessment above suggests that a Poisson model might not be the most appropriate model, likely due to the overdispersion we detected previously. Fit a Negative binomial to relax the Poisson model assumption that the conditional mean and variance are equal. Then, compute the CPO summary statistic and PIT QQ plot to decide which model gives the better fit.\n\n\nTake hint\n\nTo specify a negative binomial model you only need to change the family distribution to family =  \"nbinomial\".\n\n\n\n\nClick here to see the solution\n\npar(mfrow=c(1,2))\n\n# Fit the negative binomial model\n\nlik_nbinom =  bru_obs(formula = satell ~.,\n            family = \"nbinomial\",\n            data = crabs_df)\n\nfit_nbinom = bru(cmp, lik_nbinom)\n\n# PIT checks\n\nfit_nbinom$cpo$pit %&gt;%\n  hist(main = \"Histogram of PIT values\")\n\nqqplot(qunif(ppoints(length(fit_nbinom$cpo$pit))),\n       fit_nbinom$cpo$pit,\n       main = \"Q-Q plot for Unif(0,1)\",\n       xlab = \"Theoretical Quantiles\",\n       ylab = \"Sample Quantiles\")\n\nqqline(fit_nbinom$cpo$pit,\n       distribution = function(p) qunif(p),\n       prob = c(0.1, 0.9))\n\n\n\n\n\n\n\n# CPO comparison\n\ndata.frame( CPO = c(-sum(log(fit_pois$cpo$cpo)),\n                    -sum(log(fit_nbinom$cpo$cpo))),\n          Model = c(\"Poisson\",\"Negative Binomial\"))\n\n       CPO             Model\n1 465.4061           Poisson\n2 379.3340 Negative Binomial\n\n# Overall, we can see that the negative binomial model provides a better fit to the data.",
    "crumbs": [
      "Home",
      "Practical 6"
    ]
  },
  {
    "objectID": "day4_practical_6.html#leave-group-out-cross-validation",
    "href": "day4_practical_6.html#leave-group-out-cross-validation",
    "title": "Practical 6",
    "section": "3 Leave-Group-Out Cross validation",
    "text": "3 Leave-Group-Out Cross validation\nIn this practical we are going to fit a geostatistical model. We will:\n\nLearn how to to compute model comparison in inlabru using LGOCV\n\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(inlabru) \nlibrary(sf)\nlibrary(terra)\n\n\n# load some libraries to generate nice map plots\nlibrary(scico)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(mapview)\nlibrary(tidyterra)\n\n\n3.1 The data\nIn this practical, we will revisit the data on the Pacific Cod (Gadus macrocephalus) from a trawl survey in Queen Charlotte Sound. The pcod dataset is available from the sdmTMB package and contains the presence/absence records of the Pacific Cod during each surveys along with the biomass density of Pacific cod in the area swept (kg/Km\\(^2\\)). The qcs_grid data contain the depth values stored as \\(2\\times 2\\) km grid for Queen Charlotte Sound.\nThe dataset contains presence/absence data from 2003 to 2017. Lets consider year 2003 for now.\nWe first load the dataset and select the year of interest\n\nlibrary(sdmTMB)\n\npcod_df = sdmTMB::pcod %&gt;% filter(year==2003)\nqcs_grid = sdmTMB::qcs_grid\n\nThen, we create ab sf object and assign the rough coordinate reference to it:\n\npcod_sf =   st_as_sf(pcod_df, coords = c(\"lon\",\"lat\"), crs = 4326)\npcod_sf = st_transform(pcod_sf,\n          crs = \"+proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\" )\n\nWe convert the covariate into a raster and assign the same coordinate reference:\n\ndepth_r &lt;- rast(qcs_grid, type = \"xyz\")\ncrs(depth_r) &lt;- crs(pcod_sf)\n\n\n\n\n\n\n\n\n\n\n\n\n3.2 The models\nHere, we will model the binary presence-absence data as a Binomial model of the form:\n\\[\n\\begin{aligned}\ny(s)|\\eta(s)&\\sim\\text{Binom}(1, p(s))\\\\\n\\eta(s) &= \\text{logit}(p(s)) \\\\\n\\omega(s) &\\sim \\text{  GF with range } \\rho\\  \\text{ and maginal variance }\\ \\sigma^2\n\\end{aligned}\n\\]\nWe will fit three models. One where we consider the observation as Bernoulli and where the linear predictor contains only one intercept and the GR field \\(\\omega(s)\\) defined through the SPDE approach. The other two models will also include depth as a linear and non-linear smoothed covariate in the linear predictor.\nConstruct the mesh and the SPDE model\n\nmesh = fm_mesh_2d(loc = pcod_sf,         \n                  max.edge = c(10,20),     \n                  offset = c(5,50))   \n\nspde_model =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n\n\n\n\n\n\n\n\n\n\nModel 1 - spatial only\n\nModel 1 - spatial only This is a model with that includes a structured spatial effect only. Here the linear predictor is defined as:\n\n\\[\n  \\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s)\n\\]\n\n# Model 1 components\ncmp_1 = ~ Intercept(1) + space(geometry, model = spde_model)\n\n# Model 1 linear predictor\nformula_1 = present ~ Intercept  + space\n\n# Model 1 observational model\nlik_1 = bru_obs(formula = formula_1, \n              data = pcod_sf, \n              family = \"binomial\")\n# fit Model 1\nfit_spatial = bru(cmp_1,lik_1)\n\n\nModel 2 - Depth covariate The depth enters the model in a linear way. The linear predictor is then defined as:\n\n\\[\n  \\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s) + \\beta_1\\ \\text{depth}(s)\n\\]\n\n# Model 2 components\ncmp_2 = ~ Intercept(1) + \n  space(geometry, model = spde_model)+\n  covariate(depth_r$depth_scaled, model = \"linear\") \n\n# Model 2 linear predictor\nformula_2 = present ~ Intercept  + covariate + space\n\n# Model 2 observational model\nlik_2 = bru_obs(formula = formula_2, \n              data = pcod_sf, \n              family = \"binomial\")\n\n# Fit Model 2\nfit_depth_linear = bru(cmp_2,lik_2)\n\n\nModel 2 - Smooth depth covariate The depth enters the model in a non linear way. The linear predictor is then defined as:\n\n\\[\n  \\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s) +  f(\\text{depth}(s))\n\\]\n\n# create the grouped variable\ndepth_r$depth_group = inla.group(values(depth_r$depth_scaled))\n\n# Model 3 components\ncmp_3 = ~ Intercept(1) + \n  space(geometry, model = spde_model)+\n  covariate(depth_r$depth_group, model = \"rw2\")\n\n# Model 3 linear predictor\nformula_3 = present ~ Intercept  + covariate + space\n\n# Model 2 observational model\nlik_3 = bru_obs(formula = formula_2, \n              data = pcod_sf, \n              family = \"binomial\")\n\n# Fit Model 2\nfit_depth_smooth = bru(cmp_3,lik_3)\n\n\n\n3.3 Model Comparison through LGOCV\nNext we illustrate how to implement modelling comparison using leave-out group cross validation (LGOCV). The underlying idea is that of a Bayesian prediction setting where we approximate the posterior predictive density \\(\\pi(\\mathbf{\\tilde{Y}}|\\mathbf{y})\\) defined as the integral over the posterior distribution of the parameters, i.e.\n\\[\n\\pi(\\mathbf{\\tilde{Y}}|\\mathbf{y}) = \\int_\\theta \\pi(\\mathbf{\\tilde{Y}}|\\theta,\\mathbf{y}) \\pi(\\theta|\\mathbf{y})d\\theta\n\\]\nthe LGOCV selects a fixed test point \\(i\\) and remove a certain group of data \\(\\mathbb{I}_i\\) according to a specific prediction task. Thus, we are interested in the posterior predictive density\n\\[\n\\pi(Y_i|\\mathbf{y}{-\\mathcal{I}i}) = \\int\\theta \\pi(Y_i|\\theta,\\mathbf{y}{-\\mathbb{I}_i}) \\pi(\\theta|\\mathbf{y})d\\theta\n\\]\nWith this, a point estimate \\(\\tilde{Y_i}\\) can be computed based on \\(\\pi(Y_i|\\mathbf{y}_{-\\mathbb{I}_i})\\) and the predictive performance be assessed using an appropriate scoring function \\(U(\\tilde{Y}_i,Y_i)\\), for example, the log-score function\n\\[\n\\frac{1}{n}\\sum_{i=1}^n \\mathrm{log}~ \\pi(\\mathbf{\\tilde{y}}|\\mathbf{y}).\n\\]\nIn this example, the LGOCV strategy will be used to compare the previous fitted spatially explicit models. Here, the leave-out group \\(\\mathbb{I}_i\\) is manually defined for the \\(i\\)th row of the data based on a buffer of size \\(b=25\\) centered at each data point:\n\n# create buffer of size 25 centred at each site\nbuffer &lt;- st_buffer(pcod_sf, dist = 25)\n\n# Lists of the indexes of the leave-out-group for each observation i\nIi &lt;- st_intersects(pcod_sf,buffer)\n\nFigure 1 illustrate the manual construction of the leave-out-group for the 2nd observation in our data.\n\n\n\n\n\n\n\n\nFigure 1: Example of the CV strategy for the 2nd testing point.\n\n\n\n\n\nWe then can use the inla.group.cv function to compute the log-scores for Model 3 as follows:\n\nlgocv_depth_smooth = inla.group.cv(result = fit_depth_smooth,\n                                   groups = Ii)\nlog_depth_smooth = mean(log(lgocv_depth_smooth$cv),\n                        na.rm=T)\n\nWhen we do model comparison, we want to have the same groups for different models. We can easily do this by passing an inla.group.cv class object to inla.group.cvfunction. If we want to use the groups constructed by model 3 to compute LGOCV, we have the following code:\n\nlgocv_spatial = inla.group.cv(result = fit_spatial, \n                              group.cv = lgocv_depth_smooth)\n\nlgocv_depth_linear = inla.group.cv(result = fit_depth_linear,\n                                   group.cv = lgocv_depth_smooth)\n\nlog_score_spatial&lt;- mean(log(lgocv_spatial$cv),na.rm=T)\nlog_depth_linear &lt;-mean(log(lgocv_depth_linear$cv),na.rm=T)\nlog_depth_smooth &lt;-mean(log(lgocv_depth_smooth$cv),na.rm=T)\n\nIn this example, the model that includes the smoothed covariate has the highest log-score and thus it is preferred over the other two “simpler” models.\n\ndata.frame(logspat= log_score_spatial,\n           logdepthl = log_depth_linear,\n           logdepths = log_depth_smooth )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog-Score LGOCV\n\n\nContinuous spatial\nGaussian field\nLinear depth\ncovariate effect\nSmooth depth\ncovariate effect\n\n\n\n\n−0.71\n−0.64\n−0.51\n\n\n\n\n\n\n\n\n\n3.4 Spatio-temporal LGOCV comparison\n\n3.4.1 Model fitting\nNow lets compare two different space-time models using LGOCV and some information criteria metrics. The general model structure is given by:\n\\[\n\\begin{aligned}\ny(s,t)|\\eta(s,t)&\\sim\\text{Binom}(1, p(s,t))\\\\\n\\eta(s,t) &= \\text{logit}(p(s,t)) \\\\\n\\end{aligned}\n\\] We begin by loading the data between 2003 and 2011 and convert it to an sf object (we will also create a time index for modelling):\n\npcod_spat = sdmTMB::pcod %&gt;%\n  filter(year %in% 2003:2011) %&gt;%\n  st_as_sf( coords = c(\"lon\",\"lat\"), crs = 4326) %&gt;%\n  st_transform(., crs = \"+proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\" ) %&gt;%\n   mutate(time_idx = match(year, c(2003, 2004, 2005, 2007, 2009, 2011, 2013, 2015, 2017)),\n         id = 1:nrow(.)) # Observation id for CV\n\nNow we tell inlabru that we want to compute WAIC, DIC and model likelihood as follows:\n\nbru_options_set(control.compute = list(waic = TRUE,dic= TRUE,mlik = TRUE))\n\n\nModel 1 - time iid effect We consider a separable space-time model with a linear predictors given by:\n\n\\[\n\\eta(s,t) = \\beta_0 + f_1(\\text{depth}(s)) + f_2(t) + \\omega(s)\n\\]\n\n\\(f_1(\\text{depth}(s))\\) is a smooth covariate effect of depth\n\\(f_2(t)\\) is an IID effect of time\n\\(\\omega(s)\\) is Matérn random field.\n\n\n# Model components\ncmp_spat = ~ Intercept(1) + \n  covariate(depth_r$depth_group, model = \"rw2\")+\n  trend(time_idx, model = \"iid\")+\n  space(geometry, model = spde_model)\n\n# Linear predictor\nformula_spat = present ~ Intercept  + covariate  + trend + space\n\n# Observational model\nlik_spat = bru_obs(formula = formula_spat, \n              data = pcod_spat, \n              family = \"binomial\")\n\n# Fit Model \nfit_spat = bru(cmp_spat,lik_spat)\n\n\n\n\n\n\n\nNote\n\n\n\nNote that there are some survey locations in certain years that fall outside the depth raster region. inlabru will input these missing covariate values using the nearest available value. This can be computationally expensive, but you can avoid it by supplying a raster layer that encompasses all of your data points (e.g., by pre-imputing these missing values with your preferred method of choice).\n\n\n\nModel 2 - spatiotemporal field We consider a separable space time model with a linear predictor given by:\n\n\\[\n\\eta(s,t) = \\beta_0 + f_1(\\text{depth}(s)) + \\omega(s,t)\n\\]\n\n\\(f_1(\\text{depth}(s))\\) is a smooth covariate effect of depth\n\\(\\omega(s,t)\\) is a space-time Matérn spatial field with AR1 time component\n\nNow we fit the model as follows (this might take a couple of minutes to run):\n\n# PC prior for AR(1) correlation parameter\nh.spec &lt;- list(rho = list(prior = 'pc.cor0', param = c(0.5, 0.1)))\n\n# Model components\ncmp_spat_ar1 = ~ Intercept(1) + \n  covariate(depth_r$depth_group, model = \"rw2\")+\n  space_time(geometry,\n        group = time_idx ,\n        model = spde_model,\n        control.group = list(model = 'ar1',hyper = h.spec))\n\n# Linear predictor\nformula_spat_ar1 = present ~ Intercept  + covariate  + space_time\n\n# Observational model\nlik_spat_ar1 = bru_obs(formula = formula_spat_ar1, \n              data = pcod_spat, \n              family = \"binomial\")\n\n# Fit Model \nfit_spat_ar1 = bru(cmp_spat_ar1,lik_spat_ar1)\n\n\n\n3.4.2 Model comparison\nTo manually define the leave-out groups we can apply the buffer approach as before while considering a time window of \\(t \\pm q\\). Here we set \\(q = 2\\) as we believe time dependency is weaker after a 2 year period (Figure 2 illustrate this strategy for the 750th observation). To compute this we can create a buffer surrounding each location and then loop through every observation to identify the leave-out groups based on those location that are inside the buffer within a 2 year span:\n\n# create buffer of size 25  centred at each site\n\nbuffer_25 &lt;- st_buffer(pcod_spat, dist = 25) \n\n\n# empty lists to include the indexes of the leave-out-group for each observation i\nI_i &lt;- list()\n\n# loop though each observation and store the leave-out-group based on the buffer\nfor( i in 1:nrow(pcod_spat)){\n  \n  # Temporal filtering of data within a 2 years of span of  observation i\n  df_sf_subset &lt;- pcod_spat %&gt;% \n    filter( between(time_idx,left = pcod_spat$time_idx[i]-2, \n                    right = pcod_spat$time_idx[i]+2)) \n  # Spatial filtering of the observations that are within the buffer of the ith observation\n  Buffer_i &lt;-df_sf_subset %&gt;% st_intersects(buffer_25[i,],sparse = FALSE) %&gt;% # identify \n    unlist()\n  \n  # obtain the indexes of the leave out group\n  I_i[[i]] &lt;-  df_sf_subset[Buffer_i,] %&gt;%  pull(id)\n  \n}\n\n\n\n\n\n\n\n\n\nFigure 2: Example of the spatiotemporal CV strategy for the 100nd testing point.\n\n\n\n\n\nNow that we have the leave-out group indexes for each observation, we can compute the log-score using the inla.group.cv function as before:\n\nlgocv_spat_ar1 &lt;- inla.group.cv(result = fit_spat_ar1, groups  = I_i)\nlogscore_spat_ar1 = mean(log(lgocv_spat_ar1$cv),na.rm=T)\n\n\nlgocv_spat &lt;- inla.group.cv(result = fit_spat, group.cv  = lgocv_spat_ar1)\nlogscore_spat = mean(log(lgocv_spat$cv),na.rm=T)\n\nThe following table summarises different model comparison metrics, which favors the model with a spatio-temporal field over the simples iid model.\n\ntable = data.frame( DIC = c(fit_spat_ar1$dic$dic, fit_spat$dic$dic),\n                    WAIC = c(fit_spat_ar1$waic$waic, fit_spat$waic$waic),\n                    mlik = c(fit_spat_ar1$mlik[1,1],fit_spat$mlik[1,1]),\n                    LGOCV = c(logscore_spat_ar1,logscore_spat))\n\n rownames(table) = c(\"Model 2 - spatiotemporal field\",\n                     \"Model 1 - time iid effect\")\n\n\n\n\n\n\n\n\n\n\n\n\nDIC\nWAIC\nmlik\nLGOCV\n\n\n\n\nModel 1 - time iid effect\n1356.179\n1351.832\n-763.2828\n-0.5001042\n\n\nModel 2 - spatiotemporal field\n1338.464\n1330.124\n-758.5321\n-0.4985709\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nModify Model 1 so that instead of an \\(iid\\) yearly trend, the \\(f_2(t)\\) trend follow an AR(1) process. Compare this again the the other two models using the LGOCV scores.\n\n\nTake hint\n\nYou only need to change the model component: trend(time_idx, model = ???). You may also use the PC prior we set for the spatiotemporal field in Model 2.\n\n\n\n\nClick here to see the solution\n\n# Model components\ncmp_spat_2 = ~ Intercept(1) + \n  covariate(depth_r$depth_group, model = \"rw2\")+\n  trend(time_idx, model = \"ar1\",\n        control.group = list(model = 'ar1',hyper = h.spec))+\n  space(geometry, model = spde_model)\n\n# Linear predictor\nformula_spat_2 = present ~ Intercept  + covariate  + trend + space\n\n# Observational model\nlik_spat_2 = bru_obs(formula = formula_spat, \n              data = pcod_spat, \n              family = \"binomial\")\n\n# Fit Model \nfit_spat_2 = bru(cmp_spat_2,lik_spat_2)\n\n# Compute log-score\n\nlgocv_spat_2 &lt;- inla.group.cv(result = fit_spat_2, group.cv  = lgocv_spat_ar1)\nmean(log(lgocv_spat_2$cv),na.rm=T)",
    "crumbs": [
      "Home",
      "Practical 6"
    ]
  },
  {
    "objectID": "day2_practical_4.html",
    "href": "day2_practical_4.html",
    "title": "Practical 4",
    "section": "",
    "text": "Aim of this practical:",
    "crumbs": [
      "Home",
      "Practical 4"
    ]
  },
  {
    "objectID": "day2_practical_4.html#sec-areal_data",
    "href": "day2_practical_4.html#sec-areal_data",
    "title": "Practical 4",
    "section": "Areal (lattice) data",
    "text": "Areal (lattice) data\nAreal data our measurements are summarised across a set of discrete, non-overlapping spatial units such as postcode areas, health board or pixels on a satellite image. In consequence, the spatial domain is a countable collection of (regular or irregular) areal units at which variables are observed. Many public health studies use data aggregated over groups rather than data on individuals - often this is for privacy reasons, but it may also be for convenience.\nIn the next example we are going to explore data on respiratory hospitalisations for Greater Glasgow and Clyde between 2007 and 2011. The data are available from the CARBayesdata R Package:\n\nlibrary(CARBayesdata)\n\ndata(pollutionhealthdata)\ndata(GGHB.IZ)\n\nThe pollutionhealthdata contains the spatiotemporal data on respiratory hospitalisations, air pollution concentrations and socio-economic deprivation covariates for the 271 Intermediate Zones (IZ) that make up the Greater Glasgow and Clyde health board in Scotland. Data are provided by the Scottish Government and the available variables are:\n\nIZ: unique identifier for each IZ.\nyear: the year were the measruments were taken\nobserved: observed numbers of hospitalisations due to respiratory disease.\nexpected: expected numbers of hospitalisations due to respiratory disease computed using indirect standardisation from Scotland-wide respiratory hospitalisation rates.\npm10: Average particulate matter (less than 10 microns) concentrations.\njsa: The percentage of working age people who are in receipt of Job Seekers Allowance\nprice: Average property price (divided by 100,000).\n\nThe GGHB.IZ data is a Simple Features (sf) object containing the spatial polygon information for the set of 271 Intermediate Zones (IZ), that make up of the Greater Glasgow and Clyde health board in Scotland ( Figure 1 ).\n\n\n\n\n\n\n\n\nFigure 1: Greater Glasgow and Clyde health board represented by 271 Intermediate Zones\n\n\n\n\nLet’s start by loading useful libraries:\n\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(scico)\n\nThe sf package allows us to work with vector data which is used to represent points, lines, and polygons. It can also be used to read vector data stored as a shapefiles.\nFirst, lets combine both data sets based on the Intermediate Zones (IZ) variable using the merge function from base R:\n\nresp_cases &lt;- merge(GGHB.IZ, pollutionhealthdata, by = \"IZ\")\n\nIn epidemiology, disease risk is usually estimated using Standardized Mortality Ratios (SMR). The SMR for a given spatial areal unit \\(i\\) is defined as the ratio between the observed ( \\(Y_i\\) ) and expected ( \\(E_i\\) ) number of cases:\n\\[\nSMR_i = \\dfrac{Y_i}{E_i}\n\\]\nA value \\(SMR &gt; 1\\) indicates that there are more observed cases than expected which corresponds to a high risk area. On the other hand, if \\(SMR&lt;1\\) then there are fewer observed cases than expected, suggesting a low risk area.\nWe can manipulate sf objects the same way we manipulate standard data frame objects via the dplyr package. Lets use the pipeline command %&gt;% and the mutate function to calculate the yearly SMR values for each IZ:\n\nlibrary(dplyr)\nresp_cases &lt;- resp_cases %&gt;% \n  mutate(SMR = observed/expected, .by = year )\n\nNow we use ggplot to visualize our data by adding a geom_sf layer and coloring it according to our variable of interest (i.e., SMR). We can further use facet_wrap to create a layer per year and chose an appropriate color palette using the scale_fill_scico from the scico package:\n\nggplot()+\n  geom_sf(data=resp_cases,aes(fill=SMR))+\n  facet_wrap(~year)+scale_fill_scico(palette = \"roma\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nProduce a map that shows the spatial distribution of each of the following variables for the year 2011:\n\nAverage particulate matter pm10\nAverage property price price\nPercentage of working age people who are in receipt of Job Seekers Allowance jsa\n\n\n\nhint\n\nYou can use the filter function from dplyr to subset the data according to the year of interest.\n\n\n\n\nClick here to see the solution\n\n# Library for plotting multiple maps together\n\nlibrary(patchwork)\n\n# subset data set for 2011\n\nresp_cases_2011 &lt;- resp_cases %&gt;% filter(year ==2011)\n\n# pm10 plot\n\npm10_plot &lt;- ggplot()+\n  geom_sf(data=resp_cases_2011,aes(fill=pm10))+\n  scale_fill_scico(palette = \"navia\")\n\n# property price\n\nprice_plot &lt;- ggplot()+\n  geom_sf(data=resp_cases_2011,aes(fill=price))+\n  facet_wrap(~year)+scale_fill_scico(palette = \"bilbao\")\n\n#  percentage jsa\n\njsa_plot &lt;- ggplot()+\n  geom_sf(data=resp_cases_2011,aes(fill=jsa))+\n  facet_wrap(~year)+scale_fill_scico(palette = \"lapaz\") \n\n# plot maps together\n\npm10_plot + price_plot + jsa_plot + plot_layout(ncol=3)\n\n\n\n\n\n\n\n\n\n\n\nAs with the other types of spatial modelling, our goal is to observe and explain spatial variation in our data. Generally, we are aiming to produce a smoothed map which summarises the spatial patterns we observe in our data.\nA key aspect of any spatial analysis is that observations closer together in space are likely to have more in common than those further apart. This can lead us towards approaches similar to those used in time series, where we consider the spatial closeness of our regions in terms of a neighbourhood structure.\nThe function poly2nb() of the spdep package can be used to construct a list of neighbors based on areas with contiguous boundaries (e.g., using Queen contiguity).\n\nlibrary(spdep)\n\nW.nb &lt;- poly2nb(GGHB.IZ,queen = TRUE)\nW.nb\n\nNeighbour list object:\nNumber of regions: 271 \nNumber of nonzero links: 1424 \nPercentage nonzero weights: 1.938971 \nAverage number of links: 5.254613 \n2 disjoint connected subgraphs\n\n\nThe warning tell us that the neighbourhood is comprised of two interconnected regions. By looking at the neighbourhood graph below, we can see that these are the North and South Glasgow regions which are separated by the River Clyde.\n\nplot(st_geometry(GGHB.IZ), border = \"lightgray\")\nplot.nb(W.nb, st_geometry(GGHB.IZ), add = TRUE)\n\n\n\n\n\n\n\n\nYou could use the snap argument within poly2nb to set a distance at which the different regions centroids are consider neighbours. To do so we first need to be aware about the spatial units of the spatial coordinate reference system (CRS). We can check this as follows:\n\nst_crs(GGHB.IZ)$units\n\n[1] \"m\"\n\n\nThen, we could set a distance of 250m to join the IZ centroids that are are less than 250m apart.\n\nW.nb250 &lt;- poly2nb(GGHB.IZ,snap=250)\nW.nb250\n\nNeighbour list object:\nNumber of regions: 271 \nNumber of nonzero links: 1758 \nPercentage nonzero weights: 2.393758 \nAverage number of links: 6.487085 \n\n\n\nplot(st_geometry(GGHB.IZ), border = \"lightgray\")\nplot.nb(W.nb250, st_geometry(GGHB.IZ), add = TRUE)\n\n\n\n\n\n\n\n\nOnce we have identified a set of neighbours using our chosen method, we can use this to account for correlation.\nMoran’s \\(I\\) is a measure of global spatial autocorrelation, and can be considered an extension of the Pearson correlation coefficient. For a set of data \\(Z_1, \\ldots, Z_m\\) measured on regions \\(B_1, \\ldots B_m\\), with neighbourhood matrix \\(W\\), we can compute Moran’s I as:\n\\[\nI = \\frac{m}{\\sum_{i=1}^m \\sum_{j=1}^m w_{ij}}\\frac{\\sum_{i=1}^m \\sum_{j=1}^m w_{ij} (Z_i - \\bar{Z})(Z_j - \\bar{Z})}{\\sum_{i=1}^m (Z_i - \\bar{Z})^2}\n\\]\nThis is basically a function of differences in values between neighbouring areas. By far the most common approach is to use a binary neighbourhood matrix, \\(W\\), denoted by\n\\[ w_{ij} = \\begin{cases} 1 & \\text{if areas } (B_i, B_j) \\text{ are neighbours.}\\\\ 0 & \\text{otherwise.} \\end{cases} \\]\nBinary matrices are used for their simplicity. Fitting spatial models often requires us to invert \\(W\\), and this is less computationally intensive for sparse matrices.\nMoran’s \\(I\\) ranges between -1 and 1, and can be interpreted in a similar way to a standard correlation coefficient.\n\n\\(I=1\\) implies that we have perfect spatial correlation.\n\\(I=0\\) implies that we have complete spatial randomness.\n\\(I=-1\\) implies that we have perfect dispersion (negative correlation).\n\nOur observed \\(I\\) is a point estimate, and we may also wish to assess whether it is significantly different from zero. We can test for a statistically significant spatial correlation using a permutation test, with hypotheses:\n\\[\n\\begin{aligned}\nH_0&: \\text{ negative or no spatial association } (I \\leq 0)\\\\\nH_1&: \\text{ positive spatial association } (I &gt; 0)\n\\end{aligned}\n\\]\nWe can use moran.test() to test this hypothesis by setting alternative = \"greater\". To do so, we need to supply list containing the neighbors via the nb2listw() function from the spdep package. Lets assess now the spatial autocorrelation of the SMR in 2011:\n\n# subset the data\nresp_cases_2011 &lt;- resp_cases %&gt;% filter(year ==2011)\n\n# neighbors list \nnbw &lt;- nb2listw(W.nb, style = \"W\")\n\n# Global Moran's I\ngmoran &lt;- moran.test(resp_cases_2011$SMR, nbw,\n                     alternative = \"greater\")\ngmoran\n\n\n    Moran I test under randomisation\n\ndata:  resp_cases_2011$SMR  \nweights: nbw    \n\nMoran I statistic standard deviate = 11.42, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.439899780      -0.003703704       0.001508809 \n\n\n\n\n\n\n\n\n Question\n\n\n\nWhat do we conclude from the Moran’s I test?\n\n\nAnswer\n\nSince have set the alternative hypothesis to be $ I &gt; 0 $ and have a p-value \\(&lt;0.05\\), we then reject the null hypothesis and conclude there is evidence for positive spatial autocorrelation.\n\n\n\nA local version of Moran’s I can also be used to measure the similarity between each IZ via the localmoran function:\n\nlmoran &lt;- localmoran(resp_cases_2011$SMR, nbw, alternative = \"two.sided\")\n\nIn this case we set alternative = \"two.sided\" to test whether there is any evidence of spatial autocorrelation in our data:\n\\[\n\\begin{aligned}\nH_0&: \\text{ no spatial association } (I=0)\\\\\nH_1&: \\text{ some spatial association } (I \\neq 0)\n\\end{aligned}\n\\]\nWe can obtain the \\(Z\\)-scores from the test (Z.Ii) where any values smaller than –1.96 indicate negative spatial autocorrelation, and z-score values greater than 1.96 indicate positive spatial autocorrelation:\n\nresp_cases_2011_m &lt;- resp_cases_2011 %&gt;% mutate(Zscores = lmoran[,\"Z.Ii\"])\n\nresp_cases_2011_m &lt;- resp_cases_2011_m %&gt;% \n  mutate (SAC = case_when(\n  Zscores &gt; qnorm(0.975) ~ \" M &gt; 1\" ,\n  Zscores &lt; -1*qnorm(0.975) ~ \" M &lt; 1\",\n  .default = \"M = 0\"),\n  SAC=as.factor(SAC)\n)\n\nWe can visualize these results using either ggplot as we did before, or via the mapview library which contains interactive features:\n\nlibrary(mapview)\nmapview(resp_cases_2011_m, \n        zcol = \"SAC\",\n        layer.name = \"SAC\",\n        col.regions=c(\"turquoise\",\"orange\",\"grey40\"))\n\n\n\n\n\nIn this practical we will:\n\nExplore tools for geostatistical spatial data wrangling and visualization.\nCompute a variogram to assess for spatial autocorrelation in our data.\n\nFirst, lets load some useful libraries for data wrangling and visualization\n\n# For plotting\nlibrary(mapview)\nlibrary(ggplot2)\nlibrary(scico) # for colouring palettes\n\n# Data manipulation\nlibrary(dplyr)",
    "crumbs": [
      "Home",
      "Practical 4"
    ]
  },
  {
    "objectID": "day2_practical_4.html#georeferenced-data",
    "href": "day2_practical_4.html#georeferenced-data",
    "title": "Practical 4",
    "section": "Georeferenced data",
    "text": "Georeferenced data\nTobler’s first law of geography states that:\n“Everything is related to everything else, but near things are more related than distant things”\nSpatial patterns are fundamental in environmental and ecological data. In many ecological and environmental settings, measurements from fixed sampling units, aiming to quantify spatial variation and interpolate values at unobserved sites.\nGeorefernced data are the most common form of spatial data found in environmental setting. In these data we regularly take measurements of a spatial ecological or environmental process at a set of fixed locations. This could be data from transects (e.g, where the height of trees is recorded), samples taken across a region (e.g., water depth in a lake) or from monitoring stations as part of a network (e.g., air pollution). In each of these cases, our goal is to estimate the value of our variable across the entire space.\nLet \\(D\\) be our two-dimensional region of interest. In principle, there is aninfinite number of locations within \\(D\\), each of which can be represented by mathematical coordinates (e.g., latitude and longitude). We then can identify any individual location as \\(s_i = (x_i, y_i)\\), where \\(x_i\\) and \\(y_i\\) are their coordinates.\nWe can treat our variable of interest as a random variable, \\(Z\\) which can be observed at any location as \\(Z(\\mathbf{s}_i)\\).\nOur geostatistical process can therefore be written as: \\[\\{Z(\\mathbf{s}); \\mathbf{s} \\in D\\}\\]\nIn practice, our data are observed in a finite number of locations, \\(m\\), and can be denoted as:\n\\[z = \\{z(\\mathbf{s}_1), \\ldots z(\\mathbf{s}_m) \\}\\]\nIn the next example, we will explore data on the Pacific Cod (Gadus macrocephalus) from a trawl survey in Queen Charlotte Sound. The pcod dataset is available from the sdmTMB package and contains the presence/absence records of the Pacific Cod during each surveys along with the biomass density of Pacific cod in the area swept (kg/Km\\(^2\\)). The qcs_grid data contain the depth values stored as \\(2\\times 2\\) km grid for Queen Charlotte Sound.\n\nlibrary(sdmTMB)\n\npcod_df = sdmTMB::pcod \nqcs_grid = sdmTMB::qcs_grid\n\n\nGeoreferrenced data\nLet’s create an initial sf spatial object using the standard geographic coordinate system (EPSG:4326). This correctly defines the point locations based on latitude and longitude.\n\nlibrary(sf)\npcod_sf =   st_as_sf(pcod_df, coords = c(\"lon\",\"lat\"), crs = 4326)\n\nNow we can transform to the standard UTM Zone 9N projection (EPSG:32609) which uses meters:\n\npcod_sf_proj &lt;- st_transform(pcod_sf, crs = 32609)\nst_crs(pcod_sf_proj)$units\n\n[1] \"m\"\n\n\nWe can change the spatial units to km to better reflect the scale of our ecological study and to make resulting distance/area values more intuitive to interpret:\n\npcod_sf_proj = st_transform(pcod_sf_proj,\n                            gsub(\"units=m\",\"units=km\",\n                                 st_crs(pcod_sf_proj)$proj4string)) \nst_crs(pcod_sf_proj)$units\n\n[1] \"km\"\n\n\nInstead of first setting an EPSG code and then transforming, we can define the target Coordinate Reference System (CRS) directly using a proj4string. This allows us to customize non-standard parameters in a single step, in this case, explicitly setting the projection units to kilometers (+units=km).\n\npcod_sf = st_transform(pcod_sf,\n                       crs = \"+proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\" )\nst_crs(pcod_sf)$units\n\n[1] \"km\"\n\n\nSpatial sf objects can be manipulated the same way we manipulate standard data frame objects via the dplyr package. For example, you can select a specific year using the filter function from dplyr. Let’s map the present/absence of the Pacific Cod in 2017 using the mapview function:\n\npcod_sf %&gt;% \n  filter(year== 2017) %&gt;%\n  mutate(present = as.factor(present)) %&gt;%\nmapview(zcol = \"present\",\n        layer.name = \"Occupancy status of Pacific Cod in 2017\")\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nUse ggplot and the sf library to map the biomass density of the pacific cod across years.\n\n\nhint\n\nYou can plot ansf object by adding a geom_sf layer to a ggplot object. You can also use the facet_wrap argument to plot an arrange of plots according to a grouping variable.\n\n\n\n\nClick here to see the solution\n\nggplot()+ \n  geom_sf(data=pcod_sf,aes(color=density))+ \n  facet_wrap(~year)+\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nRaster Data\nEnvironmental data are typically stored in raster format, which represents spatially continuous phenomena by dividing a region into a grid of equally-sized cells, each storing a value for the variable of interest. In R, the terra package is a modern and powerful tool for efficiently working with raster data. The function rast(), can be used both to read raster files from standard formats (e.g., .tif or .tiff) and to create a new raster object from a data frame. For instance, the following code creates a raster from the qcs_grid grid data for Queen Charlotte Sound.\n\nlibrary(terra)\ndepth_r &lt;- rast(qcs_grid, type = \"xyz\")\ndepth_r\n\nclass       : SpatRaster \nsize        : 102, 121, 3  (nrow, ncol, nlyr)\nresolution  : 2, 2  (x, y)\nextent      : 341, 583, 5635, 5839  (xmin, xmax, ymin, ymax)\ncoord. ref. :  \nsource(s)   : memory\nnames       :    depth, depth_scaled, depth_scaled2 \nmin values  :  12.0120,    -6.000040,  4.892624e-08 \nmax values  : 805.7514,     3.453937,  3.600048e+01 \n\n\nThe raster object contains three layers corresponding to the (i) depth values, (ii) the scaled depth values and (iii) the squared depth values.\nNotice that there are no CRS associated with the raster. Thus, we can assign appropriate CRS using the crs function. Additionally, we also want the raster CRS to match the CRS in the survey data (recall that we have previously reprojected our data to utm coordinates). We can assign an appropiate CRS that matches the CRS of the sf object as follows:\n\ncrs(depth_r) &lt;- crs(pcod_sf)\n\nWe can use the tidyterra package to plot raster data using ggplot by adding a geom_spatraster function and then select an appropriate fill and color palettes:\n\nlibrary(tidyterra)\n\n\nAttaching package: 'tidyterra'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nggplot()+ \n  geom_spatraster(data=depth_r$depth)+\n  geom_sf(data=pcod_sf,aes(color=factor(present))) +\n  facet_wrap(~year)+\n    scale_color_manual(name=\"Occupancy status for the Pacific Cod\",\n                     values = c(\"black\",\"orange\"),\n                     labels= c(\"Absence\",\"Presence\"))+\n  scale_fill_scico(name = \"Depth\",\n                   palette = \"nuuk\",\n                   na.value = \"transparent\" )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nMap the scaled depth and the presence/absence records of the Pacific cod for 2003 to 2005 only.\n\n\nhint\n\nThe different layers of a raster can be accessed using the $ symbol.\n\n\n\n\nClick here to see the solution\n\nggplot()+ \n  geom_spatraster(data=depth_r$depth_scaled)+\n  geom_sf(data=pcod_sf %&gt;% filter(year %in% 2003:2005),\n          aes(color=factor(present)))+ \n  facet_wrap(~year)+\n  scale_color_manual(name=\"Occupancy status for the Pacific Cod\",\n                     values = c(\"black\",\"orange\"),\n                     labels= c(\"Absence\",\"Presence\"))+\n    scale_fill_scico(name = \"Scaled Depth\",\n                     palette = \"davos\",\n                     na.value = \"transparent\" )\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutocorrelation and Variograms\nSpatial statistics quantifies the fundamental principle that nearby things are closely related. This spatial dependence means that observation are not independent, as assumed by most classical statistical models, but are instead correlated relative to their proximity. While this correlation can be a valuable source of information, it must be explicitly accounted for to avoid wrong inference and incorrect conclusions.\nThe first step is to assess whether there is any evidence of spatial dependency in our data. Spatial dependence in georeferenced data can be explored by a function known as a variogram \\(2\\gamma(\\cdot)\\) (or semivariogram \\(\\gamma(\\cdot)\\)). The variogram is similar in many ways to the autocorrelation function used in time series modelling. In simple terms, it is a function which measures the difference in the spatial process between a pair of locations a fixed distance apart.\nThe variogram measures the variance of the difference in the process \\(Z(\\cdot)\\) at two spatial locations \\(\\mathbf{s}\\) and \\(\\mathbf{s+h}\\) and is defined as :\n\\[\\mathrm{Var}[Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h})] = E[(Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h}))^2] = 2\\gamma_z(\\mathbf{h}).\\]\nHere, \\(2\\gamma_z(\\mathbf{h})\\) is the variogram, but in practice we use the semi-variogram, \\(\\gamma_z(\\mathbf{h})\\). We use the semi-variogram because our points come in pairs, and the semi-variance is equivalent to the variance per point at a given lag.\n\nWhen the variance of the difference \\(Z(\\mathbf{s}) - Z(\\mathbf{t})\\) is relatively small, then \\(Z(\\mathbf{s})\\) and \\(Z(\\mathbf{t})\\) are similar (spatially correlated).\nWhen the variance of the difference \\(Z(\\mathbf{s}) - Z(\\mathbf{t})\\) is relatively large, then \\(Z(\\mathbf{s})\\) and \\(Z(\\mathbf{t})\\) are less similar (closer to independence).\n\nThe variogram is a function of the underlying geostatistical process \\(Z\\). In practice, we only have access to \\(m\\) realisations of this process, and therefore we have to estimate the variogram. This is known as the empirical variogram.\nWe obtain this by computing the semi-variance for all possible pairs of observations: \\(\\gamma(\\mathbf{s}, \\mathbf{t}) = 0.5(Z(\\mathbf{s}) - Z(\\mathbf{t}))^2\\).\n\n\n\n\n\n\n Example\n\n\n\nTo illustrate how an empirical variogram is computed, consider the biomass density of the Pacific Cod in 2017 for the two highlighted locations below.\n\n\n\n\n\n\n\n\n\n\nWe can first compute the distance between the two locations using the standard Euclidean distance formula as\n\n\\[h = \\sqrt{(441.6 -481)^2 + (5743.4-5748.2)^2} \\approx  39 ~\\text{Km}\\]\n\nNext, we compute the semi-variance between the points using their observed values as \\[\\gamma(\\mathbf{s}, \\mathbf{t}) = 0.5(Z(\\mathbf{s}) - Z(\\mathbf{t}))^2 = 0.5(149.5 - 40.64)^2 = 5925.25\\]\nWe repeat this process for every possible pair of points, and plot \\(h\\) against \\(\\gamma(\\mathbf{s}, \\mathbf{t})\\) for each.\n\n\n\nTo make the variogram easier to use and interpret, we divide the distances into a set of discrete bins, and compute the average semi-variance in each. We compute this binned empirical variogram as:\n\\[\\gamma(\\mathbf{h}) = \\frac{1}{2N(h_k)}\\sum_{(\\mathbf{s},\\mathbf{t}) \\in N(h_k)}[z(\\mathbf{s}) - z(\\mathbf{t})]^2\\]\nWe can calculate the binned- empirical variogram for the data using variogram function from the gstat library. This plot shows the semi-variances for each pair of points. Lets compute a variogram for the biomass density of the Pacific Cod in 2017:\n\nlibrary(gstat)\n\npcod_sf_subset &lt;- pcod_sf %&gt;% filter(year ==2017)\n\nvgm1 &lt;- variogram(density~1, pcod_sf_subset)\nplot(vgm1)\n\n\n\n\n\n\n\n\nAssessing spatial dependence\nWe can construct null envelope based on permutations of the data values across the locations, i.e. envelopes built under the assumption of no spatial correlation. By overlapping these envelopes with the empirical variograms we can determine whether there is some spatial dependence in our data ,e.g. if our observed variograms falls outside of the envelopes constructed under spatial randomness.\nWe can construct permutation envelopes on the gstat empirical variogram using the envelope function from the variosig R package. Then we can visualize the results using the envplot function:\n\nlibrary(variosig)\n\nvarioEnv &lt;- envelope(vgm1,\n                     data = pcod_sf_subset,\n                     locations = st_coordinates(pcod_sf_subset),\n                     formula = density ~ 1,\n                     nsim = 499)\n\nenvplot(varioEnv)\n\n\n\n\n\n\n\n\n[1] \"There are 1 out of 15 variogram estimates outside the 95% envelope.\"\n\n\nIn this practical we will:\n\nExplore tools for spatial point pattern data wrangling and visualization.\n\nFirst, lets load some useful libraries:\n\n# For plotting\nlibrary(ggplot2)\nlibrary(scico) # for colouring palettes\n\n# Data manipulation\nlibrary(dplyr)",
    "crumbs": [
      "Home",
      "Practical 4"
    ]
  },
  {
    "objectID": "day2_practical_4.html#spatial-point-processes-data",
    "href": "day2_practical_4.html#spatial-point-processes-data",
    "title": "Practical 4",
    "section": "Spatial Point processes data",
    "text": "Spatial Point processes data\nIn point processes we measure the locations where events occur and the coordinates of such occurrences are our data.\nPoint process models are probabilistic models that describe the likelihood of patterns of points that represent the random location of some event. A spatial point process is a set of locations that have been generated by some form of stochastic (random) mechanism. In other words, the point process is a random variable operating in continuous space, and we observe realisations of this variable as point patterns across space (and/or time).\nConsider a fixed geographical region \\(A\\). The set of locations at which events occur are denoted \\(\\mathbf{s} = s_1,\\ldots,s_n\\). We let \\(N(A)\\) be the random variable which represents the number of events in region \\(A\\).\nOur primary interest is in measuring where events occur, so the locations are our data. We typically assume that a spatial point pattern is generated by an unique point process over the whole study area. This means that the delimitation of the study area will affect the observed point patters.\nThe observed distribution of points can be described based on the intensity of points within a delimited region. We can define the (first order) intensity of a point process as the expected number of events per unit area. This can also be thought of as a measure of the density of our points. In some cases, the intensity will be constant over space (homogeneous), while in other cases it can vary by location (inhomogeneous or heterogenous).\nIn the next example, we will explore tools for visualizing spatial point patterns. Specifically, we will map the spatial distribution of the Ringlet butterfly in Scotland’s Cairngorms National Park (CNP).\n\nBNM citizen science program\nCitizen science initiatives have become an important source of information in ecological research, offering large volumes of species distribution data collected by volunters for multiple taxonomic groups across wide spatial and temporal scales\nButterflies for the New Millennium (BNM) is a large-scale monitoring scheme launched in the earlies 70’s to keep track of butterflies’ populations in the UK. With over 12 million butterflies sighting and more than 10,000 volunteers, this recording scheme has proven to be a successful program that has been used to assess long-term changes in the distributions of UK butterfly species.\nHere we will focus on the distribution of the Ringlet butterfly species, which holds particular significance in environmental studies as one of the Habitat specialists species (UK Government, 2024). The data set consists of Ringlet butterfly presence-only records collected by volunteers in Scotland’s Cairngorms National Park (CNP).\nReading shapefiles into R\nFirst, we load the geographical region of interest which can be downloaded here (i.e., CNP boundaries). We can use thre st_read function from the sf library to load the .shp file by specifying the directory where you downloaded the files:\n\nlibrary(sf)\nshp_SGC &lt;-  st_read(\"datasets/SG_CairngormsNationalPark/SG_CairngormsNationalPark_2010.shp\",quiet =T)\n\nThen, we can use appropriate CRS for the UK (i.e., EPSG code: 27700) :\n\nshp_SGC &lt;- shp_SGC %&gt;% st_transform(crs = 27700)\nst_crs(shp_SGC)$units\n\n[1] \"m\"\n\n\nNotice that the spatial resolution is in meters. Let’s change the spatial units to km to make resulting distance/area values more intuitive to interpret:\n\nshp_SGC &lt;- st_transform(shp_SGC,gsub(\"units=m\",\"units=km\",st_crs(shp_SGC)$proj4string)) \nst_crs(shp_SGC)$units\n\n[1] \"km\"\n\n\nWe can then plot the CNP boundary as follows:\n\nggplot()+\n  geom_sf(data=shp_SGC)\n\n\n\n\n\n\n\n\nCreating sf spatial objects in R\nNow we will read the Ringlet butterfly records which can be downloaded below:\n Download data set \n\nringlett &lt;- read.csv(\"datasets/bnm_ringlett.csv\")\nhead(ringlett)\n\n         y         x\n1 57.58752 -2.712498\n2 54.97742 -3.274879\n3 54.89929 -3.771451\n4 55.40323 -5.737059\n5 54.91438 -3.959336\n6 55.87255 -4.167174\n\n\nThe data set contains the longitude latitude where an observation was made. We can convert this into a spatial sf object using the st_as_sf function by declaring the columns in our data that contain the spatial coordinates:\n\nringlett_sf &lt;- ringlett %&gt;% st_as_sf(coords = c(\"x\",\"y\"),crs = \"+proj=longlat +datum=WGS84\") \n\n\n\n\n\n\n\n Task\n\n\n\nWe have set standard WGS84 coordinates for the Ringlet butterfly occurrence records. Set the CRS to match the CRS used in shapefile. Then, produce a map of the CNP region with the projected observations overlayed.\n\n\nTake hint\n\nYou can use the st_transform() function to change the coordinates of an sf object (type ?st_transform for more details)/\n\n\n\n\nClick here to see the solution\n\n\nCode\nringlett_sf &lt;- ringlett_sf %&gt;%\n  st_transform(st_crs(shp_SGC))\n\nggplot()+\n  geom_sf(data=shp_SGC)+\n  geom_sf(data=ringlett_sf)\n\n\n\n\n\n\n\n\n\n\n\n\nWe can subset two sf objects with the same CRS in the same way as we subset a data frame in R. For example, if we want to subset the Ringlet butterfly occurrence records to those contained only within the CNP, we can type the following:\n\nringlett_CNP &lt;- ringlett_sf[shp_SGC,] # crop to mainland\n\nIf we plot the ringlett_CNP object along with the CNP boundary, we should then obtain a map of the occurrence records within the park:\n\nggplot()+\n  geom_sf(data=shp_SGC)+\n  geom_sf(data=ringlett_CNP)\n\n\n\n\n\n\n\n\nReading Raster Data\nWe can use the terra R package to read raster files. The Scotland_elev.tiff raster contains the output of a digital elevation model for Scotland:\n Download raster data \nOnce you download the raster you can read it using the rast function after specifying the path where the file has been stored. Then, we assign the same CRS as our data.\n\nlibrary(terra)\nelevation_r &lt;- rast(\"datasets/Scotland_elev.tiff\")\ncrs(elevation_r) = crs(shp_SGC)\nplot(elevation_r)\n\n\n\n\n\n\n\n\nWe can apply different R functions to our rasters. For example, we can scale the elevation values as follows:\n\nelevation_r &lt;- elevation_r %&gt;% scale()\n\nLastly, we can crop the raster to the boundaries of our region of interest. Let’s crop the elevation raster to the CNP area using the crop function:\n\nelev_CNP &lt;- terra::crop(elevation_r,shp_SGC,mask=T)\nplot(elev_CNP)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nUsing tidyterra and ggplot, produce a map of the elevation profile in the CNP and overlay the spatial point pattern of the Ringlet butterfly occurrence records. Use an appropriate colouring scheme for the elevation values. Do you see any pattern?\n\n\nTake hint\n\nYou can use the geom_spatraster() to add a raster layer to a ggplot object. Furthermore the scico library contains a nice range of coloring palettes you can choose, type scico_palette_show() to see the color palettes that are available.\n\n\n\n\nClick here to see the solution\n\n\nCode\nlibrary(scico)\n\nggplot()+ \n  tidyterra::geom_spatraster(data=elev_CNP)+\n  geom_sf(data=ringlett_CNP)+\n  scale_fill_scico(name = \"Elevation scaled\",\n                   palette = \"devon\",\n                   na.value = \"transparent\" )",
    "crumbs": [
      "Home",
      "Practical 4"
    ]
  },
  {
    "objectID": "day1_practical_2.html",
    "href": "day1_practical_2.html",
    "title": "Practical 2",
    "section": "",
    "text": "Aim of this practical:\n\nSet priors for different linear models\nCompute and visualize posterior densities and summaries for marginal effects\nFit hierarchical flexible models\n\nwe are going to learn:\n\nHow to change some of the R default priors in inlabru\nHow to explore and visualize model parameters\nFit different flexible models\n\n Download Practical 2 R script \n{r child=“practicals\\LM_ex_priors.qmd”}\n{r child=“practicals\\LMM_fish_example.qmd”}\n{r child=“practicals\\HGAMM_ex.qmd”}",
    "crumbs": [
      "Home",
      "Practical 2"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "inlabru Workshop",
    "section": "",
    "text": "Welcome to the course!\n\nWelcome to the inlabru workshop!\nThe goal for the workshop is to …\nThe workshop is intended for … No knowledge of R-INLA is required.\nWorkshop materials in the github repository inlabru-workshop\n\n\n\nLearning Objectives for the workshop\nAt the end of the workshop, participants will be able to:\n\nILO1\nILO2\nILO3\n\nIntended audience and level: The tutorial is intended for … No knowledge of R-INLA is required.\n\n\nSchedule Program\n\nDay 1Day 2Day 3Day 4Day 5\n\n\n\n\n\nTime\nTopic\n\n\n\n\n10:00 - 10:30\nICES informative session\n\n\n10:30 - 11:30\nSession 1: Introduction to inlabru\n\n\n11:30 - 13:00\nPractical Session 1\n\n\n13:00 - 14:30\nLunch break 🍽️\n\n\n14:30 - 15:30\nSession 2: Latent Gaussian Models and INLA\n\n\n15:30 - 15:45\nCoffee Break ☕\n\n\n15:45 - 16:45\nPractical Session 2\n\n\n16:45 - 17:00\nwrap-up and outlook\n\n\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 10:00\nSession 3: Temporal modelling and smoothing part 1\n\n\n10:00 - 10:30\nSnack 🥙\n\n\n10:30 - 11:30\nSession 4: Temporal modelling and smoothing part 2\n\n\n11:30 - 13:00\nPractical Session 3\n\n\n13:00 - 14:30\nLunch break 🍽️\n\n\n14:30 - 15:30\nSession 5: Introduction to Spatial Statistics\n\n\n15:35 - 15:45\nCoffee Break ☕\n\n\n15:45 - 16:45\nPractical Session 4\n\n\n16:45 - 17:00\nwrap-up and outlook\n\n\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 10:00\nSession 6: Areal Processes\n\n\n10:00 - 10:30\nSnack 🥙\n\n\n10:30 - 11:30\nSession 7: Geostatistics\n\n\n11:30 - 13:00\nPractical Session 5\n\n\n13:00 - 14:30\nLunch break 🍽️\n\n\n14:30 - 15:30\nSession 8: Spatial Point processes\n\n\n15:35 - 15:45\nCoffee Break ☕\n\n\n15:45 - 16:45\nPractical Session 5 continued\n\n\n16:45 - 17:00\nwrap-up and outlook\n\n\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 10:00\nSession 9: Spatiotemporal models\n\n\n10:00 - 10:30\nSnack 🥙\n\n\n10:30 - 11:30\nSession 10: Model comparison and evaluation\n\n\n11:30 - 13:00\nPractical Session 6\n\n\n13:00 - 14:30\nLunch break 🍽️\n\n\n14:30 - 15:30\nSession 11: Multilikelihoods/joint likelihood\n\n\n15:35 - 15:45\nCoffee Break ☕\n\n\n15:45 - 16:45\nPractical Session 7\n\n\n16:45 - 17:00\nwrap-up and outlook\n\n\n\n\n\n\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 10:00\nSession 12: Zero inflated models\n\n\n10:00 - 10:30\nSnack 🥙\n\n\n10:30 - 11:30\nSession 13: Complex observational processes: Distance Sampling\n\n\n11:30 - 13:00\nPractical Session 8\n\n\n13:00 - 13:15\nCoffee Break ☕\n\n\n13:15 - 14:00\nClosing session\n\n\n\n\n\n\n\n\nIn preparation for the workshop\nParticipants are required to follow the next steps before the day of the workshop:\n\nInstall R-INLA\nInstall inlabru (available from CRAN)\n\n\n# Enable universe(s) by inlabru-org\noptions(repos = c(\n  inlabruorg = \"https://inlabru-org.r-universe.dev\",\n  INLA = \"https://inla.r-inla-download.org/R/testing\",\n  CRAN = \"https://cloud.r-project.org\"\n))\n\n# Install some packages\ninstall.packages(\"inlabru\")\n\n\nMake sure you have the latest R-INLA, inlabru and R versions installed.\nInstall the following libraries:\n\n\n\ninstall.packages(c(\n  \"CARBayesdata\",\n  \"DAAG\",\n  \"dplyr\",\n  \"FSAdata\",\n  \"ggplot2\",\n  \"gstat\",\n  \"gt\",\n  \"lubridate\",\n  \"mapview\",\n  \"patchwork\",\n  \"scico\",\n  \"sdmTMB\",\n  \"sf\",\n  \"spatstat\",\n  \"spdep\",\n  \"terra\",\n  \"tidyr\",\n  \"tidyterra\",\n  \"tidyverse\",\n  \"variosig\"\n))",
    "crumbs": [
      "Home",
      "`inlabru` Workshop"
    ]
  },
  {
    "objectID": "day1_practical.html",
    "href": "day1_practical.html",
    "title": "Practical 1",
    "section": "",
    "text": "Aim of this practical:  In this first practical we are going to look at some simple models\nwe are going to learn:\nDownload Practical 1 R script",
    "crumbs": [
      "Home",
      "Practical 1"
    ]
  },
  {
    "objectID": "day1_practical.html#sec-linmodel",
    "href": "day1_practical.html#sec-linmodel",
    "title": "Practical 1",
    "section": "Linear Model",
    "text": "Linear Model\nIn this practical we will:\n\nSimulate Gaussian data\nLearn how to fit a linear model with inlabru\nGenerate predictions from the model\n\nStart by loading useful libraries:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n# load some libraries to generate nice plots\nlibrary(scico)\n\nAs our first example we consider a simple linear regression model with Gaussian observations \\[\ny_i\\sim\\mathcal{N}(\\mu_i, \\sigma^2), \\qquad i = 1,\\dots,N\n\\]\nwhere \\(\\sigma^2\\) is the observation error, and the mean parameter \\(\\mu_i\\) is linked to the linear predictor (\\(\\eta_i\\)) through an identity function: \\[\n\\eta_i = \\mu_i = \\beta_0 + \\beta_1 x_i\n\\] where \\(x_i\\) is a covariate and \\(\\beta_0, \\beta_1\\) are parameters to be estimated. We assign \\(\\beta_0\\) and \\(\\beta_1\\) a vague Gaussian prior.\nTo finalize the Bayesian model we assign a \\(\\text{Gamma}(a,b)\\) prior to the precision parameter \\(\\tau = 1/\\sigma^2\\) and two independent Gaussian priors with mean \\(0\\) and precision \\(\\tau_{\\beta}\\) to the regression parameters \\(\\beta_0\\) and \\(\\beta_1\\) (we will use the default prior settings in INLA for now).\n\n\n\n\n\n\n Question\n\n\n\nWhat is the dimension of the hyperparameter vector and latent Gaussian field?\n\n\nAnswer\n\nThe hyperparameter vector has dimension 1, \\(\\pmb{\\theta} = (\\tau)\\) while the latent Gaussian field \\(\\pmb{u} = (\\beta_0, \\beta_1)\\) has dimension 2, \\(0\\) mean, and sparse precision matrix:\n\\[\n\\pmb{Q} = \\begin{bmatrix}\n\\tau_{\\beta_0} & 0\\\\\n0 & \\tau_{\\beta_1}\n\\end{bmatrix}\n\\] Note that, since \\(\\beta_0\\) and \\(\\beta_1\\) are fixed effects, the precision parameters \\(\\tau_{\\beta_0}\\) and \\(\\tau_{\\beta_1}\\) are fixed.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can write the linear predictor vector \\(\\pmb{\\eta} = (\\eta_1,\\dots,\\eta_N)\\) as\n\\[\n\\pmb{\\eta} = \\pmb{A}\\pmb{u} = \\pmb{A}_1\\pmb{u}_1 + \\pmb{A}_2\\pmb{u}_2 = \\begin{bmatrix}\n1 \\\\\n1\\\\\n\\vdots\\\\\n1\n\\end{bmatrix} \\beta_0 + \\begin{bmatrix}\nx_1 \\\\\nx_2\\\\\n\\vdots\\\\\nx_N\n\\end{bmatrix} \\beta_1\n\\]\nOur linear predictor consists then of two components: an intercept and a slope.\n\n\n\nSimulate example data\nFirst, we simulate data from the model\n\\[\ny_i\\sim\\mathcal{N}(\\eta_i,0.1^2), \\ i = 1,\\dots,100\n\\]\nwith\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_i\n\\]\nwhere \\(\\beta_0 = 2\\), \\(\\beta_1 = 0.5\\) and the values of the covariate \\(x\\) are generated from an Uniform(0,1) distribution. The simulated response and covariate data are then saved in a data.frame object.\n\n\nSimulate Data from a LM\nbeta = c(2,0.5)\nsd_error = 0.1\n\nn = 100\nx = rnorm(n)\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error)\n\ndf = data.frame(y = y, x = x)  \n\n\n\n\nFitting a linear regression model with inlabru\n\nDefining model components\nThe model has two parameters to be estimated \\(\\beta_1\\) and \\(\\beta_2\\). We need to define the two corresponding model components:\n\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n\nThe cmp object is here used to define model components. We can give them any useful names we like, in this case, beta_0 and beta_1.\n\n\n\n\n\n\nNote\n\n\n\nNote that we have excluded the default Intercept term in the model by typing -1 in the model components. However, inlabru has automatic intercept that can be called by typing Intercept() , which is one of inlabru special names and it is used to define a global intercept, e.g.\n\ncmp =  ~  Intercept(1) + beta_1(x, model = \"linear\")\n\n\n\nObservation model construction\nThe next step is to construct the observation model by defining the model likelihood. The most important inputs here are the formula, the family and the data.\nThe formula defines how the components should be combined in order to define the model predictor.\n\nformula = y ~ beta_0 + beta_1\n\n\n\n\n\n\n\nNote\n\n\n\nIn this case we can also use the shortcut formula = y ~ .. This will tell inlabru that the model is linear and that it is not necessary to linearize the model and assess convergence.\n\n\nThe likelihood is defined using the bru_obs() function as follows:\n\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n\nFit the model\nWe fit the model using the bru() functions which takes as input the components and the observation model:\n\nfit.lm = bru(cmp, lik)\n\nExtract results\nThe summary() function will give access to some basic information about model fit and estimates\n\nsummary(fit.lm)\n## inlabru version: 2.13.0\n## INLA version: 25.08.21-1\n## Components:\n## beta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\n## beta_1: main = linear(x), group = exchangeable(1L), replicate = iid(1L), NULL\n## Observation models:\n##   Family: 'gaussian'\n##     Tag: &lt;No tag&gt;\n##     Data class: 'data.frame'\n##     Response class: 'numeric'\n##     Predictor: y ~ .\n##     Additive/Linear: TRUE/TRUE\n##     Used components: effects[beta_0, beta_1], latent[]\n## Time used:\n##     Pre = 0.385, Running = 0.264, Post = 0.0978, Total = 0.747 \n## Fixed effects:\n##         mean    sd 0.025quant 0.5quant 0.975quant  mode kld\n## beta_0 1.988 0.009      1.969    1.988      2.006 1.988   0\n## beta_1 0.498 0.008      0.482    0.498      0.515 0.498   0\n## \n## Model hyperparameters:\n##                                           mean    sd 0.025quant 0.5quant\n## Precision for the Gaussian observations 116.31 16.45      86.36   115.54\n##                                         0.975quant   mode\n## Precision for the Gaussian observations     150.72 113.99\n## \n## Marginal log-Likelihood:  73.36 \n##  is computed \n## Posterior summaries for the linear predictor and the fitted values are computed\n## (Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\nWe can see that both the intercept and slope and the error precision are correctly estimated.\n\n\nGenerate model predictions\n\nNow we can take the fitted bru object and use the predict function to produce predictions for \\(\\mu\\) given a new set of values for the model covariates or the original values used for the model fit\n\nnew_data = data.frame(x = c(df$x, runif(10)),\n                      y = c(df$y, rep(NA,10)))\npred = predict(fit.lm, new_data, ~ beta_0 + beta_1,\n               n.samples = 1000)\n\nThe predict function generate samples from the fitted model. In this case we set the number of samples to 1000.\n\nPlotR Code\n\n\n\n\n\n\n\nData and 95% credible intervals\n\n\n\n\n\n\n\n\nCode\npred %&gt;% ggplot() + \n  geom_point(aes(x,y), alpha = 0.3) +\n  geom_line(aes(x,mean)) +\n  geom_line(aes(x, q0.025), linetype = \"dashed\")+\n  geom_line(aes(x, q0.975), linetype = \"dashed\")+\n  xlab(\"Covariate\") + ylab(\"Observations\")\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nGenerate predictions for a new observation with \\(x_0 = 0.45\\)\n\n\nTake hint\n\nYou can create a new data frame containing the new observation \\(x_0\\) and then use the predict function.\n\n\n\n\nClick here to see the solution\n\n\nCode\nnew_data = data.frame(x = 0.45)\npred = predict(fit.lm, new_data, ~ beta_0 + beta_1,\n               n.samples = 1000)",
    "crumbs": [
      "Home",
      "Practical 1"
    ]
  },
  {
    "objectID": "day1_practical.html#linear-mixed-model",
    "href": "day1_practical.html#linear-mixed-model",
    "title": "Practical 1",
    "section": "Linear Mixed Model",
    "text": "Linear Mixed Model\nIn this practical we will:\n\nUnderstand the basic structure of a Linear Mixed Model (LLM)\nSimulate data from a LMM\nLearn how to fit a LMM with inlabru and predict from the model.\n\nConsider the a simple linear regression model except with the addition that the data that comes in groups. Suppose that we want to include a random effect for each group \\(j\\) (equivalent to adding a group random intercept). The model is then: \\[\ny_{ij}  = \\beta_0 + \\beta_1 x_i + u_j + \\epsilon_{ij} ~~~  \\text{for}~i = 1,\\ldots,N~ \\text{and}~ j = 1,\\ldots,m.\n\\]\nHere the random group effect is given by the variable \\(u_j \\sim \\mathcal{N}(0, \\tau^{-1}_u)\\) with \\(\\tau_u = 1/\\sigma^2_u\\) describing the variability between groups (i.e., how much the group means differ from the overall mean). Then, \\(\\epsilon_j \\sim \\mathcal{N}(0, \\tau^{-1}_\\epsilon)\\) denotes the residuals of the model and \\(\\tau_\\epsilon = 1/\\sigma^2_\\epsilon\\) captures how much individual observations deviate from their group mean (i.e., variability within group).\nThe model design matrix for the random effect has one row for each observation (this is equivalent to a random intercept model). The row of the design matrix associated with the \\(ij\\)-th observation consists of zeros except for the element associated with \\(u_j\\), which has a one.\n\\[\n\\pmb{\\eta} = \\pmb{A}\\pmb{u} = \\pmb{A}_1\\pmb{u}_1 + \\pmb{A}_2\\pmb{u}_2 + \\pmb{A}_3\\pmb{u}_3\n\\]\n\n\n\n\n\n\nSupplementary material: LMM as a LGM\n\n\n\nIn matrix form, the linear mixed model for the j-th group can be written as:\n\\[ \\overbrace{\\mathbf{y}_j}^{ N \\times 1} = \\overbrace{X_j}^{ N \\times 2} \\underbrace{\\beta}_{1\\times 1} + \\overbrace{Z_j}^{n_j \\times 1} \\underbrace{u_j}_{1\\times1} + \\overbrace{\\epsilon_j}^{n_j \\times 1}, \\]\nIn a latent Gaussian model (LGM) formulation the mixed model predictor for the i-th observation can be written as :\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_i + \\sum_k^K f_k(u_j)\n\\]\nwhere \\(f_k(u_j) = u_j\\) since there’s only one random effect per group (i.e., a random intercept for group \\(j\\)). The fixed effects \\((\\beta_0,\\beta_1)\\) are assigned Gaussian priors (e.g., \\(\\beta \\sim \\mathcal{N}(0,\\tau_\\beta^{-1})\\)). The random effects \\(\\mathbf{u} = (u_1,\\ldots,u_m)^T\\) follow a Gaussian density \\(\\mathcal{N}(0,\\mathbf{Q}_u^{-1})\\) where \\(\\mathbf{Q}_u = \\tau_u\\mathbf{I}_m\\) is the precision matrix for the random intercepts. Then, the components for the LGM are the following:\n\nLatent field given by\n\\[\n\\begin{bmatrix} \\beta \\\\\\mathbf{u}\n\\end{bmatrix} \\sim \\mathcal{N}\\left(\\mathbf{0},\\begin{bmatrix}\\tau_\\beta^{-1}\\mathbf{I}_2&\\mathbf{0}\\\\\\mathbf{0} &\\tau_u^{-1}\\mathbf{I}_m\\end{bmatrix}\\right)\n\\]\nLikelihood:\n\\[\ny_i \\sim \\mathcal{N}(\\eta_i,\\tau_{\\epsilon}^{-1})\n\\]\nHyperparameters:\n\n\\(\\tau_u\\sim\\mathrm{Gamma}(a,b)\\)\n\\(\\tau_\\epsilon \\sim \\mathrm{Gamma}(c,d)\\)\n\n\n\n\n\nSimulate example data\n\nset.seed(12)\nbeta = c(1.5,1)\nsd_error = 1\ntau_group = 1\n\nn = 100\nn.groups = 5\nx = rnorm(n)\nv = rnorm(n.groups, sd = tau_group^{-1/2})\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error) +\n  rep(v, each = 20)\n\ndf = data.frame(y = y, x = x, j = rep(1:5, each = 20))  \n\nNote that inlabru expects an integer indexing variable to label the groups.\n\n\nCode\nggplot(df) +\n  geom_point(aes(x = x, colour = factor(j), y = y)) +\n  theme_classic() +\n  scale_colour_discrete(\"Group\")\n\n\n\n\n\nData for the linear mixed model example with 5 groups\n\n\n\n\n\n\nFitting a LMM in inlabru\n\nDefining model components and observational model\nIn order to specify this model we must use the group argument to tell inlabru which variable indexes the groups. The model = \"iid\" tells INLA that the groups are independent from one another.\n\n# Define model components\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\") +\n  u(j, model = \"iid\")\n\nThe group variable is indexed by column j in the dataset. We have chosen to name this component v() to connect with the mathematical notation that we used above.\n\n# Construct likelihood\nlik =  like(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n\nWarning: `like()` was deprecated in inlabru 2.12.0.\nℹ Please use `bru_obs()` instead.\n\n\nFitting the model\nThe model can be fitted exactly as in the previous examples by using the bru function with the components and likelihood objects.\n\nfit = bru(cmp, lik)\nsummary(fit)\n## inlabru version: 2.13.0\n## INLA version: 25.08.21-1\n## Components:\n## beta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\n## beta_1: main = linear(x), group = exchangeable(1L), replicate = iid(1L), NULL\n## u: main = iid(j), group = exchangeable(1L), replicate = iid(1L), NULL\n## Observation models:\n##   Family: 'gaussian'\n##     Tag: &lt;No tag&gt;\n##     Data class: 'data.frame'\n##     Response class: 'numeric'\n##     Predictor: y ~ .\n##     Additive/Linear: TRUE/TRUE\n##     Used components: effects[beta_0, beta_1, u], latent[]\n## Time used:\n##     Pre = 0.344, Running = 0.334, Post = 0.176, Total = 0.853 \n## Fixed effects:\n##         mean    sd 0.025quant 0.5quant 0.975quant  mode kld\n## beta_0 2.108 0.438      1.229    2.108      2.986 2.108   0\n## beta_1 1.172 0.120      0.936    1.172      1.407 1.172   0\n## \n## Random effects:\n##   Name     Model\n##     u IID model\n## \n## Model hyperparameters:\n##                                          mean    sd 0.025quant 0.5quant\n## Precision for the Gaussian observations 0.995 0.144      0.738    0.986\n## Precision for u                         1.613 1.060      0.369    1.356\n##                                         0.975quant  mode\n## Precision for the Gaussian observations       1.30 0.971\n## Precision for u                               4.35 0.918\n## \n## Marginal log-Likelihood:  -179.93 \n##  is computed \n## Posterior summaries for the linear predictor and the fitted values are computed\n## (Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\n\nModel predictions\nTo compute model predictions we can create a data.frame containing a range of values of covariate where we want the response to be predicted for each group. Then we simply call the predict function while spe\n\n\nLMM fitted values\n# New data\nxpred = seq(range(x)[1], range(x)[2], length.out = 100)\nj = 1:n.groups\npred_data = expand.grid(x = xpred, j = j)\npred = predict(fit, pred_data, formula = ~ beta_0 + beta_1 + u) \n\n\npred %&gt;%\n  ggplot(aes(x=x,y=mean,color=factor(j)))+\n  geom_line()+\n  geom_ribbon(aes(x,ymin = q0.025, ymax= q0.975,fill=factor(j)), alpha = 0.5) + \n  geom_point(data=df,aes(x=x,y=y,colour=factor(j)))+\n  facet_wrap(~j)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Question\n\n\n\nSuppose that we are also interested in including random slopes into our model. Assuming intercept and slopes are independent, can your write down the linear predictor and the components of this model as a LGM?\n\n\nGive me a hint\n\nIn general, the mixed model predictor can decomposed as:\n\\[ \\pmb{\\eta} = X\\beta + Z\\mathbf{u} \\]\nWhere \\(X\\) is a \\(n \\times p\\) design matrix and \\(\\beta\\) the corresponding p-dimensional vector of fixed effects. Then \\(Z\\) is a \\(n\\times q_J\\) design matrix for the \\(q_J\\) random effects and \\(J\\) groups; \\(\\mathbf{v}\\) is then a \\(q_J \\times 1\\) vector of \\(q\\) random effects for the \\(J\\) groups. In a latent Gaussian model (LGM) formulation this can be written as:\n\\[ \\eta_i = \\beta_0 + \\sum\\beta_j x_{ij} + \\sum_k f(k) (u_{ij}) \\]\n\n\n\nSee Solution\n\n\nThe linear predictor is given by\n\\[\n\\eta_i = \\beta_0 + \\beta_1x_i + u_{0j} + u_{1j}x_i\n\\]\nLatent field defined by:\n\n\\(\\beta \\sim \\mathcal{N}(0,\\tau_\\beta^{-1})\\)\n\\(\\mathbf{u}_j = \\begin{bmatrix}u_{0j} \\\\ u_{1j}\\end{bmatrix}, \\mathbf{u}_j \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{Q}_u^{-1})\\) where the precision matrix is a block-diagonal matrix with entries \\(\\mathbf{Q}_u= \\begin{bmatrix}\\tau_{u_0} & {0} \\\\{0} & \\tau_{u_1}\\end{bmatrix}\\)\n\nThe hyperparameters are then:\n\n\\(\\tau_{u_0},\\tau_{u_1} \\text{and}~\\tau_\\epsilon\\)\n\n\nTo fit this model in inlabru we can simply modify the model components as follows:\n\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\") +\n  u0(j, model = \"iid\") + u1(j,x, model = \"iid\")",
    "crumbs": [
      "Home",
      "Practical 1"
    ]
  },
  {
    "objectID": "day1_practical.html#sec-genlinmodel",
    "href": "day1_practical.html#sec-genlinmodel",
    "title": "Practical 1",
    "section": "Generalized Linear Model",
    "text": "Generalized Linear Model\nIn this practical we will:\n\nSimulate non-Gaussian data\nLearn how to fit a generalised linear model with inlabru\nGenerate predictions from the model\n\nA generalised linear model allows for the data likelihood to be non-Gaussian. In this example we have a discrete response variable which we model using a Poisson distribution. Thus, we assume that our data \\[\ny_i \\sim \\text{Poisson}(\\lambda_i)\n\\] with rate parameter \\(\\lambda_i\\) which, using a log link, has associated predictor \\[\n\\eta_i = \\log \\lambda_i = \\beta_0 + \\beta_1 x_i\n\\] with parameters \\(\\beta_0\\) and \\(\\beta_1\\), and covariate \\(x\\). This is identical in form to the predictor in Section 1. The only difference is now we must specify a different data likelihood.\n\nSimulate example data\nThis code generates 100 samples of covariate x and data y.\n\nset.seed(123)\nn = 100\nbeta = c(1,1)\nx = rnorm(n)\nlambda = exp(beta[1] + beta[2] * x)\ny = rpois(n, lambda  = lambda)\ndf = data.frame(y = y, x = x)  \n\n\n\nFitting a GLM in inlabru\n\nDefine model components and likelihood\nSince the predictor is the same as Section 1, we can use the same component definition:\n\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n\nHowever, when building the observation model likelihood we must now specify the Poisson likelihood using the family argument (the default link function for this family is the \\(\\log\\) link).\n\nlik =  bru_obs(formula = y ~.,\n            family = \"poisson\",\n            data = df)\n\nFit the model\nOnce the likelihood object is constructed, fitting the model is exactly the same process as in Section 1.\n\nfit_glm = bru(cmp, lik)\n\nAnd model summaries can be viewed using\n\nsummary(fit_glm)\n\ninlabru version: 2.13.0\nINLA version: 25.08.21-1\nComponents:\nbeta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\nbeta_1: main = linear(x), group = exchangeable(1L), replicate = iid(1L), NULL\nObservation models:\n  Family: 'poisson'\n    Tag: &lt;No tag&gt;\n    Data class: 'data.frame'\n    Response class: 'integer'\n    Predictor: y ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[beta_0, beta_1], latent[]\nTime used:\n    Pre = 0.292, Running = 0.263, Post = 0.0449, Total = 0.6 \nFixed effects:\n        mean    sd 0.025quant 0.5quant 0.975quant  mode kld\nbeta_0 0.915 0.071      0.775    0.915      1.054 0.915   0\nbeta_1 1.048 0.056      0.938    1.048      1.157 1.048   0\n\nMarginal log-Likelihood:  -204.02 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\n\n\nGenerate model predictions\n\nTo generate new predictions we must provide a data frame that contains the covariate values for \\(x\\) at which we want to predict.\nThis code block generates predictions for the data we used to fit the model (contained in df$x) as well as 10 new covariate values sampled from a uniform distribution runif(10).\n\n# Define new data, set to NA the values for prediction\n\nnew_data = data.frame(x = c(df$x, runif(10)),\n                      y = c(df$y, rep(NA,10)))\n\n# Define predictor formula\npred_fml &lt;- ~ exp(beta_0 + beta_1)\n\n# Generate predictions\npred_glm &lt;- predict(fit_glm, new_data, pred_fml)\n\nSince we used a log link (which is the default for family = \"poisson\"), we want to predict the exponential of the predictor. We specify this using a general R expression using the formula syntax.\n\n\n\n\n\n\nNote\n\n\n\nNote that the predict function will call the component names (i.e. the “labels”) that were decided when defining the model.\n\n\nSince the component definition is looking for a covariate named \\(x\\), all we need to provide is a data frame that contains one, and the software does the rest.\n\nPlotR Code\n\n\n\n\n\n\n\nData and 95% credible intervals\n\n\n\n\n\n\n\npred_glm %&gt;% ggplot() + \n  geom_point(aes(x,y), alpha = 0.3) +\n  geom_line(aes(x,mean)) +\n    geom_ribbon(aes(x = x, ymax = q0.975, ymin = q0.025),fill = \"tomato\", alpha = 0.3)+\n  xlab(\"Covariate\") + ylab(\"Observations (counts)\")\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nSuppose a binary response such that\n\\[\n    \\begin{aligned}\ny_i &\\sim \\mathrm{Bernoulli}(\\psi_i)\\\\\n\\eta_i &= \\mathrm{logit}(\\psi_i) = \\alpha_0 +\\alpha_1 \\times w_i\n\\end{aligned}\n\\] Using the following simulated data, use inlabru to fit the logistic regression above. Then, plot the predictions for the data used to fit the model along with 10 new covariate values\n\nset.seed(123)\nn = 100\nalpha = c(0.5,1.5)\nw = rnorm(n)\npsi = plogis(alpha[1] + alpha[2] * w)\ny = rbinom(n = n, size = 1, prob =  psi) # set size = 1 to draw binary observations\ndf_logis = data.frame(y = y, w = w)  \n\nHere we use the logit link function \\(\\mathrm{logit}(x) = \\log\\left(\\frac{x}{1-x}\\right)\\) (plogis() function in R) to link the linear predictor to the probabilities \\(\\psi\\).\n\n\nTake hint\n\nYou can set family = \"binomial\" for binary responses and the plogis() function for computing the predicted values.\n\n\n\n\n\n\nNote\n\n\n\nThe Bernoulli distribution is equivalent to a \\(\\mathrm{Binomial}(1, \\psi)\\) pmf. If you have proportional data (e.g. no. successes/no. trials) you can specify the number of events as your response and then the number of trials via the Ntrials = n argument of the bru_obs function (where n is the known vector of trials in your data set).\n\n\n\n\n\n\nClick here to see the solution\n\n\nCode\n# Model components\ncmp_logis =  ~ -1 + alpha_0(1) + alpha_1(w, model = \"linear\")\n# Model likelihood\nlik_logis =  bru_obs(formula = y ~.,\n            family = \"binomial\",\n            data = df_logis)\n# fit the model\nfit_logis &lt;- bru(cmp_logis,lik_logis)\n\n# Define data for prediction\nnew_data = data.frame(w = c(df_logis$w, runif(10)),\n                      y = c(df_logis$y, rep(NA,10)))\n# Define predictor formula\npred_fml &lt;- ~ plogis(alpha_0 + alpha_1)\n\n# Generate predictions\npred_logis &lt;- predict(fit_logis, new_data, pred_fml)\n\n# Plot predictions\npred_logis %&gt;% ggplot() + \n  geom_point(aes(w,y), alpha = 0.3) +\n  geom_line(aes(w,mean)) +\n    geom_ribbon(aes(x = w, ymax = q0.975, ymin = q0.025),fill = \"tomato\", alpha = 0.3)+\n  xlab(\"Covariate\") + ylab(\"Observations\")",
    "crumbs": [
      "Home",
      "Practical 1"
    ]
  },
  {
    "objectID": "day1_practical.html#sec-gam_ex",
    "href": "day1_practical.html#sec-gam_ex",
    "title": "Practical 1",
    "section": "Generalised Additive Model",
    "text": "Generalised Additive Model\nIn this practical we will:\n\nLearn how to fit a GAM with inlabru\nGenerate predictions from the model\n\nGeneralised Additive Models (GAMs) are very similar to linear models, but with an additional basis set that provides flexibility.\nAdditive models are a general form of statistical model which allows us to incorporate smooth functions alongside linear terms. A general expression for the linear predictor of a GAM is given by\n\\[\n\\eta_i = g(\\mu_i) = \\beta_0 + \\sum_{j=1}^L f_j(x_{ij})\n\\]\nwhere the mean \\(\\pmb{\\mu} = E(\\mathbf{y}|\\mathbf{x}_1,\\ldots,\\mathbf{x}_L)\\) and \\(g()\\) is a link function (notice that the distribution of the response and the link between the predictors and this distribution can be quite general). The term \\(f_j()\\) is a smooth function for the j-th explanatory variable that can be represented as\n\\[\nf(x_i) = \\sum_{k=0}^q\\beta_k b_k(x_i)\n\\]\nwhere \\(b_k\\) denote the basis functions and \\(\\beta_K\\) are their coefficients.\nIncreasing the number of basis functions leads to a more wiggly line. Too few basis functions might make the line too smooth, too many might lead to overfitting.To avoid this, we place further constraints on the spline coefficients which leads to constrained optimization problem where the objective function to be minimized is given by:\n\\[\n\\mathrm{min}\\sum_i(y_i-f(x_i))^2 + \\lambda(\\sum_kb^2_k)\n\\] The first term measures how close the function \\(f()\\) is to the data while the second term \\(\\lambda(\\sum_kb^2_k)\\), penalizes the roughness in the function. Here, \\(\\lambda &gt;0\\) is known as the smoothing parameter because it controls the degree of smoothing (i.e. the trade-off between the two terms). In a Bayesian setting,including the penalty term is equivalent to setting a specific prior on the coefficients of the covariates.\nIn this exercise we will set a random walk prior of order 1 on \\(f\\), i.e. \\(f(x_i)-f(x_i-1) \\sim \\mathcal{N}(0,\\sigma^2_f)\\) where \\(\\sigma_f^2\\) is the smoothing parameter such that small values give large smoothing. Notice that we will assume \\(x_i\\)’s are equally spaced for now (we will cover a stochastic differential equation approach that relaxes this assumption later on in the course).\n\nSimulate Data\nLets generate some data so evaluate how RW models perform when estimating a smooth curve. The data are simulated from the following model:\n\\[\ny_i = 1 + \\mathrm{cos}(x) + \\epsilon_i, ~ \\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2_\\epsilon)\n\\] where \\(\\sigma_\\epsilon^2 = 0.25\\)\n\nn = 100\nx = rnorm(n)\neta = (1 + cos(x))\ny = rnorm(n, mean =  eta, sd = 0.5)\n\ndf = data.frame(y = y, \n                x_smooth = inla.group(x)) # equidistant x's \n\n\n\nFitting a GAM in inlabru\nNow lets fit a flexible model by setting a random walk of order 1 prior on the coefficients. This can be done bye specifying model = \"rw1\" in the model component (similarly,a random walk of order 2 can be placed by setting model = \"rw2\" )\n\ncmp =  ~ Intercept(1) + \n  smooth(x_smooth, model = \"rw1\")\n\nNow we define the observational model:\n\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n\nWe then can fit the model:\n\nfit = bru(cmp, lik)\nfit$summary.fixed\n\n            mean         sd 0.025quant 0.5quant 0.975quant    mode          kld\nIntercept 1.2988 0.06506725   1.171641 1.298566   1.427294 1.29857 9.965741e-09\n\n\nThe posterior summary regarding the estimated function using RW1 can be accessed through fit$summary.random$smooth, the output includes the value of \\(x_i\\) (ID) as well as the posterior mean, standard deviation, quantiles and mode of each \\(f(x_i)\\). We can use this information to plot the posterior mean and associated 95% credible intervals.\n\nR plotR Code\n\n\n\n\n\n\n\nSmooth effect of the covariate\n\n\n\n\n\n\n\ndata.frame(fit$summary.random$smooth) %&gt;% \n  ggplot() + \n  geom_ribbon(aes(ID,ymin = X0.025quant, ymax= X0.975quant), alpha = 0.5) + \n  geom_line(aes(ID,mean)) + \n  xlab(\"covariate\") + ylab(\"\")\n\n\n\n\n\n\nModel Predictions\nWe can obtain the model predictions using the predict function.\n\npred = predict(fit, df, ~ (Intercept + smooth))\n\nThe we can plot them together with the true curve and data points:\n\n\nCode\npred %&gt;% ggplot() + \n  geom_point(aes(x_smooth,y), alpha = 0.3) +\n  geom_line(aes(x_smooth,1+cos(x_smooth)),col=2)+\n  geom_line(aes(x_smooth,mean)) +\n  geom_line(aes(x_smooth, q0.025), linetype = \"dashed\")+\n  geom_line(aes(x_smooth, q0.975), linetype = \"dashed\")+\n  xlab(\"Covariate\") + ylab(\"Observations\")\n\n\n\n\n\nData and 95% credible intervals\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nFit a flexible model using a random walk of order 2 (RW2) and compare the results with the ones above.\n\n\nTake hint\n\nYou can set model = \"rw2\" for assigning a random walk 2 prior.\n\n\n\n\nClick here to see the solution\n\n\nCode\ncmp_rw2 =  ~ Intercept(1) + \n  smooth(x_smooth, model = \"rw2\")\nlik_rw2 =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\nfit_rw2 = bru(cmp_rw2, lik_rw2)\n\n# Plot the fitted functions\nggplot() + \n  geom_line(data= fit$summary.random$smooth,aes(ID,mean,colour=\"RW1\"),lty=2) + \n  geom_line(data= fit_rw2$summary.random$smooth,aes(ID,mean,colour=\"RW2\")) + \n  xlab(\"covariate\") + ylab(\"\") + scale_color_discrete(name=\"Model\")\n\n\n\n\n\n\n\n\n\n\nWe see that the RW1 fit is too wiggly while the RW2 is smoother and seems to have better fit.",
    "crumbs": [
      "Home",
      "Practical 1"
    ]
  },
  {
    "objectID": "day2_practical_3.html",
    "href": "day2_practical_3.html",
    "title": "Practical 3",
    "section": "",
    "text": "Aim of this practical:",
    "crumbs": [
      "Home",
      "Practical 3"
    ]
  },
  {
    "objectID": "day2_practical_3.html#ar1-models-in-inlabru",
    "href": "day2_practical_3.html#ar1-models-in-inlabru",
    "title": "Practical 3",
    "section": "AR(1) models in inlabru",
    "text": "AR(1) models in inlabru\nIn this exercise we will:\n\nSimulate a time series with autocorrelated errors.\nFit an AR(1) process with inlabru\nVisualize model predictions.\nForecasting for future observations\n\nStart by loading useful libraries:\n\nlibrary(tidyverse)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nTime series analysis is particularly valuable for modelling data with temporal dependence or autocorrelation, where observations taken at nearby time points tend to be more similar than those further apart.\nA time series process is a stochastic process \\(\\{X_t~|~t \\in T\\}\\), which is a collection of random variables that are ordered in time where \\(T\\) is the index set that determines the set of discrete and equally spaced time points at which the process is defined and observations are made.\nAutoregressive processes allow us to account for the time dependence by regressing \\(X_t\\) on past values \\(X_{t-1},\\ldots,X_{t-p}\\) with associated coefficients \\(\\phi_k\\) for each lag \\(k = 1,\\ldots,p\\). Thus an autoregressive process of order \\(p\\), denoted AR(\\(p\\)) , is given by:\n\\[\nX_t = \\phi_1 X_{t-1} + \\ldots + \\phi_p X_{t-p} + \\varepsilon_t; ~~ \\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2_e)\n\\]\nConsider now an univariate time series \\(y_t\\) which evolves over time according to some autoregressive stochastic process. For example, a time series where the system follows an AR(1) process can be defined as:\n\\[\n\\begin{aligned}\ny_t &\\sim \\mathcal{N}(\\mu_t,\\tau_y^{-1})\\\\\n\\eta_t &= g^{-1}(\\mu_t) = \\alpha + u_t \\\\\nu_t &= \\phi u_{t-1} + \\delta_t ; ~~ \\delta_t \\sim \\mathcal{N}(0,\\tau_u^{-1}); ~~ t &gt; 1 \\\\\nu_1 &= \\mathcal{N}(0,\\kappa^{-1})\\\\\n\\kappa &= \\tau_u (1-\\phi^2)\n\\end{aligned}\n\\]\nThe response \\(y_t\\) is assumed to be normal distributed with mean \\(\\alpha + u_t\\) and precision error \\(\\tau_y\\) ( here \\(g(\\cdot)\\) is just the identity link function that maps the linear predictor to the mean of the process). Then, the process \\(u_t\\) follows an AR(1) process where \\(u_1\\) is drawn from a stationary normal distribution such that \\(\\kappa\\) denotes the marginal precision for state \\(u_t\\)\nThe covariance matrix is then given by:\n\\[\n\\Sigma = \\frac{\\tau^{-1}_u}{1-\\phi^2}\n\\begin{bmatrix}\n1 & \\phi & \\phi^2 & \\ldots & \\phi^{n-1}\\\\\n\\phi & 1 & \\phi & \\ldots & \\phi^{n-2} \\\\\n\\phi^2 & \\phi & 1 & \\ldots & \\phi^{n-3} \\\\\n\\phi^{n-1} & \\phi^{n-2} & \\phi^{n-3} & \\ldots & 1\n\\end{bmatrix}\n\\]\nNotice that conditionally on \\(u_t\\), the observed data \\(y_y\\) are independent from\\(y_{t-1},y_{t-2}.y_{t-3},\\ldots\\), also the conditional distribution of \\(u_t\\) is a markov chain such that \\(\\pi(u_t|u_{t-1},u_{t-2},u_{t-3}) = \\pi(u_t|u_{t-1})\\). Thus, each time point is only conditionally dependent on the two closest time points:\n\\[\nu_t|\\mathbf{u}_{-t} \\sim \\mathcal{N}\\left(\\frac{\\phi}{1-\\phi^2}(u_{t-1}+u_{t+1}),\\frac{\\tau_u^{-1}}{1-\\phi^2}\\right)\n\\]\n\nSimulate example data\nFirst, we simulate data from the model:\n\\[\n\\begin{aligned}\ny_t &= \\alpha + u_t + \\varepsilon_t;~ \\varepsilon_t \\sim \\mathcal{N}(0,\\tau_y^{-1})\\\\\nu_t &= \\phi y_{t-1} + \\delta_t; ~ \\delta_t \\sim \\mathcal{N}(0,\\tau_u^{-1})\n\\end{aligned}\n\\]\n\nset.seed(123)\n\nphi = 0.8\ntau_u = 10\nmarg.prec = tau_u * (1-phi^2) # ar1 in INLA is parametrized as marginal variance\nu_t =  as.vector(arima.sim(list(order = c(1,0,0), ar = phi), \n                          n = 100,\n                          sd=sqrt(1/tau_u)))\na = 1\ntau_e = 5\nepsilon_t = rnorm(100, sd = sqrt(1/tau_e))\ny = a + u_t + epsilon_t\n\n\nts_dat &lt;- data.frame(y =y , x= 1:100)\n\n\n\n\n\n\n\n\n\n\n\n\nFitting an AR(1) model with inlabru\nModel components\nFirst, we define the model components, notice that the latent field is defined by two components: the intercept \\(\\alpha\\) and the autoregressive random effects \\(u_t\\):\n\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(x,model = \"ar1\")\n\nThe we can define the formula for the linear predictor and specify the observational model\n\n# Model formula\nformula = y ~ alpha + ut\n# Observational model\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = ts_dat)\n\nLastly, we fit the model using the bru function and compare the model estimates with the true mdeol parameters we simulate our data from.\n\n# fit the model\nfit.ar1 = bru(cmp, lik)\n\n# compare against the true values\n\ndata.frame(\n  true = c(a,tau_e,marg.prec,phi),\n  rbind(fit.ar1$summary.fixed[,c(1,3,5)],\n        fit.ar1$summary.hyperpar[,c(1,3,5)])\n        ) %&gt;% round(2)\n\n                                        true mean X0.025quant X0.975quant\nalpha                                    1.0 1.00        0.69        1.28\nPrecision for the Gaussian observations  5.0 4.91        3.11        7.29\nPrecision for ut                         3.6 7.96        2.91       18.10\nRho for ut                               0.8 0.80        0.54        0.94\n\n\nModel predictions\nHere, we will predict the mean of our time series along with 95% credible intervals. Note that this interval are for the mean and not for new observations, we will cover forecasting new observations next.\n\npred_ar1 = predict(fit.ar1, ts_dat, ~ alpha + ut)\n\nggplot(pred_ar1,aes(y=mean,x=x))+\n  geom_line()+\n    geom_ribbon(aes(x = x, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(aes(y=y,x=x))\n\n\n\n\n\n\n\n\n\n\nForecasting\nA common goal in time series modelling is forecasting into the future. Forecasting can be treated as a missing data problem where future values of the response variable are missing. Let \\(y_m\\) be the missing response, then, by fitting a statistical model to the observed data \\(\\mathbf{y}_{obs}\\), we condition on its parameters to obtain the posterior predictive distribution:\n\\[\n\\pi(y_{m} \\mid \\mathbf{y}_{obs}) = \\int \\pi(y_{m}, \\theta \\mid \\mathbf{y}_{obs})  d\\theta = \\int \\pi(y_{m} \\mid \\mathbf{y}_{obs}, \\theta) \\pi(\\theta \\mid \\mathbf{y}_{obs})  d\\theta\n\\]\nThis distribution, which integrates over all parameter uncertainty, provides the complete probabilistic forecast for the missing values. INLA will automatically compute the predictive distributions for all missing values in the response. To do so, we can augment our data set by including the new time points at which the prediction will be made and setting the response value to NA for these new time points:\n\nts.forecast &lt;- rbind(ts_dat, \n  data.frame(y = rep(NA, 50), x = 101:150))\n\nNext, we fit the ar1 model to the new dataset so that the predictive distributions are computed:\n\ncmp =  ~ -1 + alpha(1) + ut(x,model = \"ar1\")\n\npred_lik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = ts.forecast)\n\nfit.forecast = bru(cmp, pred_lik)\n\nLastly, we can draw samples from the posterior predictive distribution using the predict function and visualize our forecast as follows:\n\npred_forecast = predict(fit.forecast, ts.forecast, ~ alpha + ut)\n\np1= ggplot(pred_forecast,aes(y=mean,x=x))+\n  geom_line()+\n    geom_ribbon(aes(x = x, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(data=ts_dat, aes(y=y,x=x))",
    "crumbs": [
      "Home",
      "Practical 3"
    ]
  },
  {
    "objectID": "day2_practical_3.html#modelling-great-lakes-water-level",
    "href": "day2_practical_3.html#modelling-great-lakes-water-level",
    "title": "Practical 3",
    "section": "Modelling Great Lakes water level",
    "text": "Modelling Great Lakes water level\nIn this exercise we will:\n\nFit an AR(1) process with inlabru to model lakes water levels\nChange the default priors for the observational error\nSet penalized complexity priors for the correlation and precision parameters of the latent effects.\nFit a RW(1) model\nFit a an AR(1) model with group-level correlation\n\nStart by loading useful libraries:\n\nlibrary(tidyverse) \nlibrary(INLA) \nlibrary(ggplot2)\nlibrary(patchwork) \nlibrary(inlabru)\nlibrary(DAAG)\n\nIn this exercise we will look at greatLakes dataset from the DAAG package. The data set contains the water level heights for the lakes Erie, Michigan/Huron, Ontario and St Clair from 1918 to 2009.\nLets begin by loading and formatting the data into a tidy format.\n\ndata(\"greatLakes\")\n\ngreatLakes.df = data.frame(as.matrix(greatLakes),\n                           year = time(greatLakes)) %&gt;%\n  pivot_longer(cols = c(\"Erie\",\"michHuron\",\"Ontario\",\"StClair\"),\n               names_to = \"Lakes\",\n               values_to = \"height\" ) \n\n\n\n\n\n\n\n\n\n\n\nFitting an AR(1) model in inlabru\nWe will focus on the Erin lake for now. Lets begin by fitting an AR(1) model of the form:\n\\[\n\\begin{aligned}\n\\text{height}_t &= \\alpha + u_t +\\varepsilon_t~; ~~ \\varepsilon_t\\sim \\mathcal{N}(0,\\tau_e^{-1}) \\\\\nu_t &= \\phi u_{t-1} + \\delta_t~~~ ; ~~ \\delta_t \\sim \\mathcal{N}(0,\\tau_u^{-1}); ~~ t &gt; 1 \\\\\nx_1 &= \\mathcal{N}(0,\\kappa^{-1})\\\\\n\\kappa &= \\tau_u (1-\\phi^2)\n\\end{aligned}\n\\]\nWhere \\(\\alpha\\) is the intercept, \\(\\phi\\) is the correlation term, \\(\\varepsilon\\) is the observational Gaussian error with mean zero and precision \\(\\tau_e\\) and \\(\\kappa\\) is the marginal precision for the state \\(u_t\\) for \\(t= 1,\\ldots,92\\).\nFirst we make a subset of the dataset and create a time index \\(T\\):\n\ngreatLakes.df$t.idx &lt;- greatLakes.df$year-1917\n\nErie.df = greatLakes.df %&gt;% filter(Lakes == \"Erie\")\n\n\n\n\n\n\n\n Task\n\n\n\nFit an AR(1) model to the Erie lake data using inlabru, then plot the model fitted values showing 95% credible intervals.\n\n\nTake hint\n\nRemember this is done by (1) defining the model components, (2) the formula and (3) the observational model. Then you can use the predict function to compute the predicted values for the mean along with 95% credible intervals.\n\n\n\n\nClick here to see the solution\n\n\nCode\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(t.idx,model = \"ar1\")\n# Model formula\nformula = height ~ alpha + ut\n\n\n# Observational model\nlik =  bru_obs(formula = height   ~.,\n            family = \"gaussian\",\n            data = Erie.df )\n\n# fit the model\nfit.Erie_ar1 = bru(cmp, lik)\n\n# Model predictions \n\npred_ar1.Erie = predict(fit.Erie_ar1, Erie.df, ~ alpha + ut)\n\n# plot model fitted values\nggplot(pred_ar1.Erie,aes(y=mean,x=year))+\n  geom_line()+\n    geom_ribbon(aes(x = year, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(aes(y=height,x=year))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Question\n\n\n\nAre there any issues with the fitted model, and if so, how do you think we should address them?\n\n\nAnswer\n\nIt is clear that the model overfits the data, leading to poor predictive performance. Thus, we need to introduce some prior information on the what we expect the variation of the process to be.\n\n\n\nPriors\nLet review INLA’ prior parametrization for autoregressive models:\nLet \\(\\pmb{\\theta} = \\{\\theta_y,\\theta_u,\\theta_\\phi\\}\\) be INLA’s internal representation of the hyperparameters such that:\n\\(\\theta_y = \\log(\\tau^2_y)\\)\n\\(\\theta_u = \\log(\\kappa) = \\log\\left(\\tau_u[1-\\phi^2]\\right)\\)\n\\(\\theta_\\phi = \\log \\left(\\frac{1+\\phi}{1-\\phi}\\right)\\)\nThe default priors for \\(\\{\\theta_y,\\theta_u\\}\\) are \\(\\text{log-gamma} (1, 5\\times 10^{-5} )\\) priors with default initial values set to 4 in each case. Then, Gaussian priors \\(\\alpha \\sim \\mathcal{N}(0,\\tau_y = 0.001)\\) and \\(\\theta_\\phi \\sim \\mathcal{N}(0, \\tau_y= 0.15)\\) are used for the intercept and correlation parameter respectively.\n\n\n\n\n\n\nNote\n\n\n\nSpecifically for AR(1) correlation parameter \\(\\phi\\), INLA uses the following logit transformation on \\(\\theta_\\phi\\):\n\\[ \\phi = \\frac{2\\exp(\\theta_\\phi)}{1+ \\exp(\\theta_\\phi)} -1. \\]\n\n\nSetting priors and PC-priors\nLets now set a Gamma prior with parameters 1 and 1, so that the precision of the Gaussian osbervational error is centered at 1 with a variance of 1. Additionally we will set Penalized Complexity (PC) priors according to the following probability statements:\n\n\\(P(\\sigma &gt; 1) = 0.01\\)\n\\(P(\\phi &gt; 0.5) = 0.3\\)\n\nNotice that the PC prior for the precision \\(\\tau_u\\) is defined on the standard deviation \\(\\sigma_u = \\tau_u^{-1/2}\\)\n\npc_prior &lt;- list(theta = list(prior = \"pc.prec\", param = c(1, 0.01)),\n                 rho = list(prior = \"pc.cor0\", param = c(0.5, 0.3))) \n\nprec.tau_e &lt;- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(1, 1))) # prior values\n\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(t.idx, model = \"ar1\",  hyper = pc_prior)\n# Model formula\nformula = height ~ alpha + ut\n\n\n# Observational model\nlik =  bru_obs(formula = height  ~.,\n            family = \"gaussian\",\n            data = Erie.df,\n            control.family = list(hyper = prec.tau_e))\n\n# fit the model\nfit.Erie_ar1 = bru(cmp, lik)\n\n\n\n\n\n\n\n Question\n\n\n\nWhat is the posterior mean for the correlation parameter \\(\\rho\\)? \n\n\n\n\n\n\n\n\n Task\n\n\n\nPlot the fitted values of the model, has the overfitting problem being alleviated?\n\n\n\nClick here to see the solution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting a RW(1) model\nNow we fit a random walk of order 1 to the Erie lake data:\n\\[\n\\begin{aligned}\ny_t &= \\alpha + u_t + \\varepsilon_t, ~ t = 1,\\ldots,92 \\\\\n\\varepsilon_t & \\sim \\mathcal{N}(0,\\tau_e) \\\\\nu_t - u_{t-1} &\\sim \\mathcal{N}(0,\\tau_u),~ t = 2,\\ldots,92 \\\\\n\\end{aligned}\n\\]\nFirs we define model priors:\n\npc_prior &lt;- list(theta = list(prior = \"pc.prec\", param = c(1, 0.01))) \n\nprec.tau_e &lt;- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(1, 1))) # prior values\n\nNow we define model components:\n\ncmp_rw =  ~ -1 + alpha(1) + \n  ut(t.idx ,\n     constr=FALSE,\n     model = \"rw1\",\n     hyper=pc_prior,\n     scale.model = TRUE)\n\nNotice that we have set scale.model = TRUE to scale the latent effects. This is particularly important when Intrinsic Gaussian Markov random fields (IGMRFs) are used as priors (e.g., random walk models or some spatial models) for the latent effects. By defining scale.model = TRUE, the rw1-model is scaled to have a generalized variance equal to one. By scaling scaling the models we ensure that a fixed hyperprior for the precision parameter has a similar interpretation for different types of IGMRFs, making precision estimates comparable between different models. Scaling also allows estimates to be less sensitive to re-scaling covariates in the linear predictor and makes the precision invariant to changes in the shape and size of the latent effect (see Sørbye (2014) for further details) .\nWe can now fit the model with the updated components and plot the predicted values\n\n# Model formula\nformula = height ~ alpha + ut\n# Observational model\nlik =  bru_obs(formula = height  ~.,\n            family = \"gaussian\",\n            data = Erie.df,\n            control.family = list(hyper = prec.tau_e))\n# fit the model\nfit.Erie_rw1 = bru(cmp_rw, lik)\n# Model predictions\npred_rw1.Erie = predict(fit.Erie_rw1, Erie.df, ~ alpha + ut)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Question\n\n\n\nTake a look at the model summaries using the summary function, do you see anything odd?\n\n\nAnswer\n\nThe intercept has zero mean and a very large variance. This is because we have not imposed a sum-to-zero constraint on the model random effects (constr=FALSE). Without this constraint, intrinsic models are non-identifiable. The intercept and the random effects are confounded, For example, you could add a constant value to every random effect and subtract it from the intercept without changing the model’s predictions. Take for instance for any constant \\(c\\), the following models are identical:\n\\(y_t = \\alpha + u_{t} + \\varepsilon_t\\) \\(y_t = (\\alpha - c) + (u_{t} + c) + \\varepsilon_t\\)\nThus, you need to set constr=FALSE so that \\(\\sum_t u_t=0\\) to ensure identifiability of \\(\\alpha\\)\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nFit an RW(1) model to the Erie data but now set constr=TRUE to impose a sum-to-zero constraint on the random effect. Then compare your results with the unconstrained model.\n\n\n\nClick here to see the solution\n\n\nCode\n# Model components\ncmp_rw =  ~ -1 + alpha(1) + \n  ut(t.idx ,\n     constr=TRUE,\n     model = \"rw1\",\n     hyper=pc_prior,\n     scale.model = TRUE)\n\nfit.Erie_rw1_constr = bru(cmp_rw, lik)\n\nfit.Erie_rw1_constr$summary.fixed\n\n\n          mean         sd 0.025quant 0.5quant 0.975quant     mode         kld\nalpha 174.1381 0.02377577   174.0914 174.1381   174.1849 174.1381 1.77002e-08\n\n\n\n\n\n\n\nGroup-level effects\nNow we will model the height water levels for all four lakes by grouping the random effects. This will allow a within-lakes correlation to be included. In the next example, we allow for correlated effects using an ar1 model for the years and iid random effects on the lakes. First we create a lakes id and set the priors for our model:\n\ngreatLakes.df$lake_id &lt;- as.numeric(as.factor(greatLakes.df$Lakes))\n\npc_prior &lt;- list(theta = list(prior = \"pc.prec\", param = c(1, 0.01)),\n                 rho = list(prior = \"pc.cor0\", param = c(0.5, 0.3))) \n\nprec.tau_e &lt;- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(1, 10))) # prior values\n\nNow we define the model components. The lakes IDs that define the group are passed with parameter group argument and the iid model and other parameters are passed through the control.group parameter.\n\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(year,model = \"ar1\",\n                            hyper = pc_prior,\n                            group =lake_id,\n                            control.group = \n                            list(model = \"iid\", \n                                 scale.model = TRUE))\n\nWe fit the model in a similar fashion as we did before:\n\n# Model formula\nformula = height ~ alpha + ut\n\n\n# Observational model\nlik =  bru_obs(formula = height  ~.,\n            family = \"gaussian\",\n            data = greatLakes.df,\n            control.family = list(hyper = prec.tau_e))\n\n# fit the model\nfit.all_lakes_ar1 = bru(cmp, lik)\n\n# Model predictions\npred_ar1.all = predict(fit.all_lakes_ar1, greatLakes.df, ~ alpha + ut)\n\nLastly we can visualize group-level model predictions as follows:\n\nggplot(pred_ar1.all,aes(y=mean,x=year))+\n  geom_line()+\n    geom_ribbon(aes(x = year, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(aes(y=height,x=year)) + facet_wrap(~Lakes,scales = \"free\")",
    "crumbs": [
      "Home",
      "Practical 3"
    ]
  },
  {
    "objectID": "day2_practical_3.html#non-gaussian-data",
    "href": "day2_practical_3.html#non-gaussian-data",
    "title": "Practical 3",
    "section": "Non-Gaussian data",
    "text": "Non-Gaussian data\nIn the next example we will use the Toyo data set to illustrate how temporal models can be fit to non-Gaussian data.\nThe Tokyo data set available in INLA contains the recorded days of rain above 1 mm in Tokyo for 2 years, 1983:84. The data set contains the following variables:\n\ny : number of days with rain\nn : total number of days\ntime : day of the year\n\n\nlibrary(INLA)\nlibrary(inlabru)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n\ndata(\"Tokyo\")\n\nA possible observational model for these data is\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim\\text{Bin}(n_t, p_t) \\\\\n\\eta_t &= \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\end{aligned}\n\\]\n\\[\nn_t = \\left\\{\n\\begin{array}{lr}\n1, & \\text{for}\\; 29\\; \\text{February}\\\\\n2, & \\text{other days}\n\\end{array}\\right.\n\\]\n\\[\ny_t =\n\\begin{cases}\n\\{0,1\\}, & \\text{for}\\; 29\\; \\text{February}\\\\\n\\{0,1,2\\}, & \\text{other days}\n\\end{cases}\n\\]\nThen, the latent field is given by\n\\[\n\\eta_t = \\beta_0 + f(\\text{time}_t)\n\\]\n\nWhere the probability of rain depends on on the day of the year \\(t\\)\n\\(\\beta_0\\) is an intercept\n\\(f(\\text{time}_t)\\) is a temporal model, e.g., a RW2 model (this is just a smoother).\n\nThe smoothness is controlled by a hyperparameter \\(\\tau_f\\) . Thus, we assign a prior to \\(\\tau_f\\) to finalize the model.\nWe can fit the model as follows:\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)\n\nNotice that we have set cyclic = TRUE as this is a cyclic effect. Finally, we can produce model predictions in a similar fashion as we did before:\n\npTokyo = predict(fit, Tokyo, ~ plogis(beta0 + time_effect))\n\nggplot(data=pTokyo , aes(x= time, y= y) ) +\n  geom_point() + \n  ylab(\"\") + xlab(\"\") +\n  # Custom the Y scales:\n  scale_y_continuous(\n    # Features of the first axis\n    name = \"\",\n    # Add a second axis and specify its features\n    sec.axis = sec_axis( transform=~./2, name=\"Probability\")\n  )  + geom_line(aes(y=mean*2,x=time)) +\n  geom_ribbon(aes( ymin = q0.025*2, \n                             ymax = q0.975*2), alpha = 0.5)",
    "crumbs": [
      "Home",
      "Practical 3"
    ]
  },
  {
    "objectID": "day3_practical_5.html",
    "href": "day3_practical_5.html",
    "title": "Practical 5",
    "section": "",
    "text": "Aim of this practical:\nIn this first practical we are going to fit spatial modes for Areal, Geostatistical and Point-Process Data.\nDownload Practical 5 R script",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#models-for-areal-data",
    "href": "day3_practical_5.html#models-for-areal-data",
    "title": "Practical 5",
    "section": "0.1 Models for areal data",
    "text": "0.1 Models for areal data\nIn this practical we are going to fit an areal model. We will:\n\nLearn how to fit an areal model in inlabru\nLearn how to do predictions\nLearn how to simulate from the fitted model\n\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)   \nlibrary(mapview)\n\n# load some libraries to generate nice map plots\nlibrary(scico)\n\n\n0.1.1 The data\nWe consider data on respiratory hospitalizations for Greater Glasgow and Clyde in 2007. The data are available from the CARBayesdata R Package:\n\nlibrary(CARBayesdata)\n\ndata(pollutionhealthdata)\ndata(GGHB.IZ)\n\nThe pollutionhealthdata contains the spatiotemporal data on respiratory hospitalizations, air pollution concentrations and socio-economic deprivation covariates for the 271 Intermediate Zones (IZ) that make up the Greater Glasgow and Clyde health board in Scotland. Data are provided by the Scottish Government and the available variables are:\n\nIZ: unique identifier for each IZ.\nyear: the year when the measurements were taken\nobserved: observed numbers of hospitalizations due to respiratory disease.\nexpected: expected numbers of hospitalizations due to respiratory disease computed using indirect standardisation from Scotland-wide respiratory hospitalization rates.\npm10: Average particulate matter (less than 10 microns) concentrations.\njsa: The percentage of working age people who are in receipt of Job Seekers Allowance\nprice: Average property price (divided by 100,000).\n\nThe GGHB.IZ data is a Simple Features (sf) object containing the spatial polygon information for the set of 271 Intermediate Zones (IZ), that make up of the Greater Glasgow and Clyde health board in Scotland ( Figure 1 ).\n\n\n\n\n\n\n\n\nFigure 1: Greater Glasgow and Clyde health board represented by 271 Intermediate Zones\n\n\n\n\nWe first merge the two dataset and select only one year of data, compute the SME and plot the observed\n\nresp_cases &lt;- merge(GGHB.IZ %&gt;%\n                      mutate(space = 1:dim(GGHB.IZ)[1]),\n                             pollutionhealthdata, by = \"IZ\") %&gt;%\n  filter(year == 2007) %&gt;%\n    mutate(SMR = observed/expected)\n\nggplot() + \n  geom_sf(data = resp_cases, aes(fill = SMR)) +\n  scale_fill_scico(direction = -1)\n\n\n\n\n\n\n\n\nThen we compute the adjacency matrix using the functions poly2nb() and nb2mat() in the spdep library. We then convert the adjacency matrix into the precision matrix \\(\\mathbf{Q}\\) of the CAR model. Remember this matrix has, on the diagonal the number of e\n\nlibrary(spdep)\n\nW.nb &lt;- poly2nb(GGHB.IZ,queen = TRUE)\nR &lt;- nb2mat(W.nb, style = \"B\", zero.policy = TRUE)\n\ndiag = apply(R,1,sum)\nQ = -R\ndiag(Q) = diag\n\n\n\n0.1.2 The model\nWe fit a first model to the data where we consider a Poisson model for the observed cases.\nStage 1 Model for the response \\[\ny_i|\\eta_i\\sim\\text{Poisson}(E_i\\lambda_i)\n\\] where \\(E_i\\) are the expected cases for area \\(i\\).\nStage 2 Latent field model \\[\n\\eta_i = \\text{log}(\\lambda_i) = \\beta_0 + \\omega_i + z_i\n\\] where\n\n\\(\\beta_0\\) is a common intercept\n\\(\\mathbf{\\omega} = (\\omega_1, \\dots, \\omega_k)\\) is a conditional Autorgressive model (CAR) with precision matrix \\(\\tau_1\\mathbf{Q}\\)\n\\(\\mathbf{z} = (z_1, \\dots, z_k)\\) is an unstrictured random effect with precision \\(\\tau_2\\)\n\nStage 3 Hyperparameters\nThe hyperparameters of the model are \\(\\tau_1\\) and \\(\\tau_2\\)\nNOTE In this case the linear predictor \\(\\eta\\) consists of three components!!\n\n\n\n\n\n\n Question\n\n\n\nFit the above model in using inlabru by completing the following code:\n\ncmp = ~ Intercept(1) + space(...) + iid(...)\n\nformula = ...\n\n\nlik = bru_obs(formula = formula, \n              family = ...,\n              E = ...,\n              data = ...)\n\nfit = bru(cmp, lik)\n\n\n\nAnswer\n\n\ncmp = ~ Intercept(1) + space(space, model = \"besag\", graph = Q) + iid(space, model = \"iid\")\n\nformula = observed ~ Intercept + space + iid\n\nlik = bru_obs(formula = formula, \n              family = \"poisson\",\n              E = expected,\n              data = resp_cases)\n\nfit = bru(cmp, lik)\n\n\n\n\nAfter fitting the model we want to extract results.\n\n\n\n\n\n\n Question\n\n\n\n\nWhat is the estimated value for \\(\\beta_0\\)?\nLook at the estimated values of the hyperparameters using fit$summary.hyperpar which of the two spatial components (structured or unstructured) explains more of the variability in the counts?\n\n\n\nWe now look at the predictions over space.\n\n\n\n\n\n\n Question\n\n\n\nComplete the code below to produce prediction of the linear predictor \\(\\eta_i\\) and of the risk \\(\\lambda_i\\) and of the expected cases \\(E_i\\exp(\\lambda_i)\\) over the whole space of interest. Then plot the mean and sd of the resulting surfaces.\n\npred = predict(fit, resp_cases, ~data.frame(log_risk = ...,\n                                             risk = exp(...),\n                                             cases = ...\n                                             ),\n               n.samples = 1000)\n\n\n\nAnswer\n\n\n# produce predictions\npred = predict(fit, resp_cases, ~data.frame(log_risk = Intercept + space,\n                                             risk = exp(Intercept + space),\n                                             cases = expected * exp(Intercept + space)\n                                             ),\n               n.samples = 1000)\n# plot the predictions\n\np1 = ggplot() + geom_sf(data = pred$log_risk, aes(fill = mean)) + scale_fill_scico(direction = -1) + ggtitle(\"mean log risk\")\np2 = ggplot() + geom_sf(data = pred$log_risk, aes(fill = sd)) + scale_fill_scico(direction = -1) + ggtitle(\"sd log risk\")\np1 + p2\n\n\n\n\n\n\n\np1 = ggplot() + geom_sf(data = pred$risk, aes(fill = mean)) + scale_fill_scico(direction = -1) + ggtitle(\"mean  risk\")\np2 = ggplot() + geom_sf(data = pred$risk, aes(fill = sd)) + scale_fill_scico(direction = -1) + ggtitle(\"sd  risk\")\np1 + p2\n\n\n\n\n\n\n\np1 = ggplot() + geom_sf(data = pred$cases, aes(fill = mean)) + scale_fill_scico(direction = -1)+ ggtitle(\"mean  expected counts\")\np2 = ggplot() + geom_sf(data = pred$cases, aes(fill = sd)) + scale_fill_scico(direction = -1)+ ggtitle(\"sd  expected counts\")\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\nFinally we want to compare our observations \\(y_i\\) with the predicted means of the Poisson distribution \\(E_i\\exp(\\lambda_i)\\)\n\npred$cases %&gt;% ggplot() + geom_point(aes(observed, mean)) + \n  geom_errorbar(aes(observed, ymin = q0.025, ymax = q0.975)) +\n  geom_abline(intercept = 0, slope = 1)\n\n\n\n\n\n\n\n\nNote: Here we are predicting the mean of counts, not the counts!!! Predicting counts is the theme of the next task!\n\n\n0.1.3 Getting prediction densities\nPosterior predictive distributions, that is \\(\\pi(y_i^{\\text{new}}|\\mathbf{y})\\) are of interest in many applied problems. The bru() function does not return predictive densities. In the previous step we have computed predictions for the expected counts \\(\\pi(E_i\\lambda_i|\\mathbf{y})\\). The predictive distribution is then: \\[\n\\pi(y_i^{\\text{new}}|\\mathbf{y}) = \\int \\pi(y_i|E_i\\lambda_i)\\pi(E_i\\lambda_i|\\mathbf{y})\\ dE_i\\lambda_i\n\\] where, in our case, \\(\\pi(y_i|E_i\\lambda_i)\\) is Poisson with mean \\(E_i\\lambda_i\\). We can achieve this using the following algorith:\n\nSimulate \\(n\\) replicates of \\(g^k = E_i\\lambda_i\\) for \\(k = 1,\\dots,n\\) using the function generate() which takes the same input as predict()\nFor each of the \\(k\\) replicates simulate a new value \\(y_i^{new}\\) using the function rpois()\nSummarise the \\(n\\) samples of \\(y_i^{new}\\) using, for example the mean and the 0.025 and 0.975 quantiles.\n\nHere is the code:\n\n# simulate 1000 realizations of E_i\\lambda_i\nexpected_counts = generate(fit, resp_cases, \n                           ~ expected * exp(Intercept + space),\n                           n.samples = 1000)\n\n\n# simulate poisson data\naa = rpois(271*1000, lambda = as.vector(expected_counts))\nsim_counts = matrix(aa, 271, 1000)\n\n# summarise the samples with posterior means and quantiles\npred_counts = data.frame(observed = resp_cases$observed,\n                         m = apply(sim_counts,1,mean),\n                         q1 = apply(sim_counts,1,quantile, 0.025),\n                         q2 = apply(sim_counts,1,quantile, 0.975),\n                         vv = apply(sim_counts,1,var)\n                         )\n\n\n\n\n\n\n\n Task\n\n\n\nPlot the observations against the predicted new counts and the predicted expected counts. Include the uncertainty and compare the two.\n\n\nTake hint\n\n\n\n\n\nClick here to see the solution\n\n\nCode\nggplot() + \n  geom_point(data = pred_counts, aes(observed, m, color = \"Pred_obs\")) + \n  geom_errorbar(data = pred_counts, aes(observed, ymin = q1, ymax = q2, color = \"Pred_obs\")) +\n  geom_point(data = pred$cases, aes(observed, mean, color = \"Pred_means\")) + \n  geom_errorbar(data = pred$cases, aes(observed, ymin = q0.025, ymax = q0.975, color = \"Pred_means\")) +\n  \n  geom_abline(intercept = 0, slope =1)",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#sec-linmodel",
    "href": "day3_practical_5.html#sec-linmodel",
    "title": "Practical 5",
    "section": "0.2 Geostatistics",
    "text": "0.2 Geostatistics\nIn this practical we are going to fit a geostatistical model. We will:\n\nLearn how to fit a geostatistical model in inlabru\nLearn how to build a mesh for the SPDE representation\nLearn how to add spatial covariates to the model\nLearn how to do predictions\nLearn how to simulate from the fitted model\n\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(inlabru) \nlibrary(sf)\nlibrary(terra)\n\n\n# load some libraries to generate nice map plots\nlibrary(scico)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(mapview)\nlibrary(tidyterra)\n\n\n0.2.1 The data\nIn this practical, we will explore data on the Pacific Cod (Gadus macrocephalus) from a trawl survey in Queen Charlotte Sound. The pcod dataset is available from the sdmTMB package and contains the presence/absence records of the Pacific Cod during each surveys along with the biomass density of Pacific cod in the area swept (kg/Km\\(^2\\)). The qcs_grid data contain the depth values stored as \\(2\\times 2\\) km grid for Queen Charlotte Sound.\nThe dataset contains presence/absence data from 2003 to 2017. In this practical we only consider year 2003.\nWe first load the dataset and select the year of interest\n\nlibrary(sdmTMB)\n\npcod_df = sdmTMB::pcod %&gt;% filter(year==2003)\nqcs_grid = sdmTMB::qcs_grid\n\nThen, we create ab sf object and assign the rough coordinate reference to it:\n\npcod_sf =   st_as_sf(pcod_df, coords = c(\"lon\",\"lat\"), crs = 4326)\npcod_sf = st_transform(pcod_sf,\n                       crs = \"+proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\" )\n\nWe convert the covariate into a raster and assign the same coordinate reference:\n\ndepth_r &lt;- rast(qcs_grid, type = \"xyz\")\ncrs(depth_r) &lt;- crs(pcod_sf)\n\nFinally we can plot our dataset. Note that to plot the raster we need to upload also the tidyterra library.\n\nggplot()+ \n  geom_spatraster(data=depth_r$depth)+\n  geom_sf(data=pcod_sf,aes(color=factor(present))) +\n    scale_color_manual(name=\"Occupancy status for the Pacific Cod\",\n                     values = c(\"black\",\"orange\"),\n                     labels= c(\"Absence\",\"Presence\"))+\n  scale_fill_scico(name = \"Depth\",\n                   palette = \"nuuk\",\n                   na.value = \"transparent\" ) + xlab(\"\") + ylab(\"\")\n\n\n\n\n\n\n\n\n\n\n0.2.2 The model\nWe first fit a simple model where we consider the observation as Bernoulli and where the linear predictor contains only one intercept and the GR field defined through the SPDE approach. The model is defined as:\nStage 1 Model for the response\n\\[\ny(s)|\\eta(s)\\sim\\text{Binom}(1, p(s))\n\\] Stage 2 Latent field model\n\\[\n\\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s)\n\\]\nwith\n\\[\n\\omega(s)\\sim \\text{  GF with range } \\rho\\  \\text{ and maginal variance }\\ \\sigma^2\n\\]\nStage 3 Hyperparameters\nThe hyperparameters of the model are \\(\\rho\\) and \\(\\sigma\\)\nNOTE In this case the linear predictor \\(\\eta\\) consists of two components!!\n\n\n0.2.3 The workflow\nWhen fitting a geostatistical model we need to fulfill the following tasks:\n\nBuild the mesh\nDefine the SPDE representation of the spatial GF. This includes defining the priors for the range and sd of the spatial GF\nDefine the components of the linear predictor. This includes the spatial GF and all eventual covariates\nDefine the observation model using the bru_obs() function\nRun the model using the bru() function\n\n\n\n0.2.4 1. Building the mesh\nThe first task, when dealing with geostatistical models in inlabru is to build the mesh that covers the area of interest. For this purpose we use the function fm_messh_2d.\nOne way to build the mesh is to start from the locations where we have observations, these are contained in the dataset pcod_sf.\n\nmesh = fm_mesh_2d(loc = pcod_sf,           # Build the mesh\n                  max.edge = c(10,20),     # The largest allowed triangle edge length.\n                  offset = c(5,50))       # The automatic extension distance\nggplot() + gg(mesh) + geom_sf(data= pcod_sf, aes(color = factor(present)), size = 0.1) + xlab(\"\") + ylab(\"\")\n\n\n\n\n\n\n\n\nAs you can see from the plot above, some of the locations are very close to each other, this causes some very small triangles. This can be avoided using the option cutoff = which collapses the locations that are closer than a cutoff (those points are collapsed in the mesh construction but, of course, not when it come to estimaation.)\n\nmesh = fm_mesh_2d(loc = pcod_sf,           # Build the mesh\n                  cutoff = 2,\n                  max.edge = c(10,20),     # The largest allowed triangle edge length.\n                  offset = c(5,50))       # The automatic extension distance\nggplot() + gg(mesh) + geom_sf(data= pcod_sf, aes(color = factor(present)), size = 0.1) + xlab(\"\") + ylab(\"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nLook at the documentation for the fm_mesh_2d function typing\n\n?fm_mesh_2d\n\nplay around with the different options and create different meshes.\nThe rule of thumb is that your mesh should be:\n\nfine enough to well represent the spatial variability of your process, but not too fine in order to avoid computation burden\nthe triangles should be regular, avoid long and thin triangles.\nThe mesh should contain a buffer around your area of interest (this is what is defined in the offset option) in order to avoid boundary artefact in the estimated variance.\n\n\n\n\n\n0.2.5 2. Define the SPDE representation of the spatial GF\nTo define the SPDE representation of the spatial GF we use the function inla.spde2.pcmatern. This takes as input the mesh we have defined and the PC-priors definition for \\(\\rho\\) and \\(\\sigma\\) (the range and the marginal standard deviation of the field).\nPC priors Gaussian Random field are defined in (Fuglstad et al. 2018). From a practical perspective for the range \\(\\rho\\) you need to define two paramters \\(\\rho_0\\) and \\(p_{\\rho}\\) such that you believe it is reasonable that\n\\[\nP(\\rho&lt;\\rho_0)=p_{\\rho}\n\\]\nwhile for the margianal variance \\(\\sigma\\) you need to define two parameters \\(\\sigma_0\\) and \\(p_{\\sigma}\\) such that you believe it is reasonable that\n\\[\nP(\\sigma&lt;\\sigma_0)=p_{\\sigma}\n\\]\nYou can use the following function to compute and plot the prior distributions for the range and sd of the Matern field.\n\ndens_prior_range = function(rho_0, p_alpha)\n{\n  # compute the density of the PC prior for the\n  # range rho of the Matern field\n  # rho_0 and p_alpha are defined such that\n  # P(rho&lt;rho_0) = p_alpha\n  rho = seq(0, rho_0*10, length.out =100)\n  alpha1_tilde = -log(p_alpha) * rho_0\n  dens_rho =  alpha1_tilde / rho^2 * exp(-alpha1_tilde / rho)\n  return(data.frame(x = rho, y = dens_rho))\n}\n\ndens_prior_sd = function(sigma_0, p_sigma)\n{\n  # compute the density of the PC prior for the\n  # sd sigma of the Matern field\n  # sigma_0 and p_sigma are defined such that\n  # P(sigma&gt;sigma_0) = p_sigma\n  sigma = seq(0, sigma_0*10, length.out =100)\n  alpha2_tilde = -log(p_sigma)/sigma_0\n  dens_sigma = alpha2_tilde* exp(-alpha2_tilde * sigma) \n  return(data.frame(x = sigma, y = dens_sigma))\n}\n\nHere are some alternatives for defining priors for our model\n\nspde_model1 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(.1, 0.5),\n                                  prior.range = c(30, 0.5))\nspde_model2 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(10, 0.5),\n                                  prior.range = c(1000, 0.5))\nspde_model3 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n\nAnd here we plot the different priors for the range:\n\nggplot() + geom_line(data = dens_prior_range(30,.5), aes(x,y, color = \"model1\")) +\ngeom_line(data = dens_prior_range(1000,.5), aes(x,y, color = \"model2\")) +\ngeom_line(data = dens_prior_range(100,.5), aes(x,y, color = \"model3\")) \n\n\n\n\n\n\n\n\nand for the sd:\n\nggplot() + geom_line(data = dens_prior_range(1,.5), aes(x,y, color = \"model1\")) +\ngeom_line(data = dens_prior_range(10,.5), aes(x,y, color = \"model2\")) +\ngeom_line(data = dens_prior_range(.1,.5), aes(x,y, color = \"model3\")) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nConsider the pcod_sf, the spatial extension and type of the data…is some of the previous choices more resonable than other?\nNOTE Remember that a prior should be reasonable..but the model should not totally depend on it.\n\n\n\n\n0.2.6 3. Define the components of the linear predictor\nWe have now defined a mesh and a SPDE representation of the spatial GF. We now need to define the model components:\n\ncmp = ~ Intercept(1) + space(geometry, model = spde_model3)\n\nNOTE since the dataframe we use (pcod_sf) is an sf object the input in the space() component is the geometry of the dataset.\n\n\n0.2.7 4. Define the observation model\nOur data are Bernulli distributed so we can define the observation model as:\n\nformula = present ~ Intercept  + space\n\nlik = bru_obs(formula = formula, \n              data = pcod_sf, \n              family = \"binomial\")\n\n\n\n0.2.8 5. Run the model\nFinally we are ready to run the model\n\nfit1 = bru(cmp,lik)\n\n\n\n0.2.9 Extract results\n\n0.2.9.1 Hyperparameters\n\n\n\n\n\n\n Task\n\n\n\nWhat are the posterior for the range \\(\\rho\\) and the standard deviation \\(\\sigma\\)? Plot the posterior together with the prior for both parameters.\n\n\nTake hint\n\nPosterior marginals for the hyperparameters can be extracted from the fitted model as:\n\n\n\nClick here to see the solution\n\n\nCode\nfit1$marginals.hyperpar$'name of the parameter'\n\n\n\n\n\n\n\nClick here to see the solution\n\n\nCode\n# Extract marginal for the range\n\nggplot() + \n  geom_line(data = fit1$marginals.hyperpar$`Range for space`, aes(x,y, color = \"Posterior\")) +\n  geom_line(data = dens_prior_range(100,.5), aes(x,y, color = \"Prior\"))\n\n\nggplot() + \n  geom_line(data = fit1$marginals.hyperpar$`Stdev for space`, aes(x,y, color = \"Posterior\")) +\n  geom_line(data = dens_prior_sd(1,.5), aes(x,y, color = \"Prior\"))\n\n\n\n\n\n\n\n\n0.2.10 Spatial prediction\nWe now want to extract the estimated posterior mean and sd of spatial GF. To do this we first need to define a grid of points where we want to predict. We do this using the function fm_pixel() which creates a regular grid of points covering the mesh\n\npxl = fm_pixels(mesh)\n\nthen compute the prediction for both the spatial GF and the linear predictor (spatial GF + intercept)\n\npreds = predict(fit1, pxl, ~data.frame(spatial = space,\n                                      total = Intercept + space))\n\nFinally, we can plot the maps\n\nggplot() + geom_sf(data = preds$spatial,aes(color = mean)) + scale_color_scico() + ggtitle(\"Posterior mean\")\n\n\n\n\n\n\n\nggplot() + geom_sf(data = preds$spatial,aes(color = sd)) + scale_color_scico() + ggtitle(\"Posterior sd\")\n\n\n\n\n\n\n\n\nNote The posterior sd is lowest at the observation points. Note how the posterior sd is inflated around the border, this is the “border effect” due to the SPDE representation.\nInstead of predicting over a grid covering the whole mesh, we can limit our predictions to the points where the covariate is defined. We can do this by defining a sf object using coordinates in the object depth_r.\n\npxl1 = data.frame(crds(depth_r), \n                  as.data.frame(depth_r$depth)) %&gt;% \n       filter(!is.na(depth)) %&gt;%\nst_as_sf(coords = c(\"x\",\"y\"))\n\n\n\n\n\n\n\n Task\n\n\n\nProduce prediction over pxl1 unsing the same techniques as before. Plot your results.\n\n\nTake hint\n\nAdd hint details here…\n\n\n\n\nClick here to see the solution\n\n\nCode\npred_pxl1 = predict(fit1, pxl1, ~data.frame(spatial = space,\n                                      total = Intercept + space))\n\nggplot() + geom_sf(data = pred_pxl1$total,aes(color = mean)) + scale_color_scico() + ggtitle(\"Posterior mean\")\n\n\n\n\n\n\n\n\n\nCode\nggplot() + geom_sf(data = pred_pxl1$total,aes(color = sd)) + scale_color_scico() + ggtitle(\"Posterior sd\")\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of computing the posterior mean and standard deviations of the estimated surface, one can also simulate possible realizations of such surface. This will give the user a better idea of the type of realized surfaces one can expect. We can do this using the function generate().\n\n# we simulate 4 samples from the \ngens = generate(fit1, pxl1, ~ (Intercept + space),\n                n.samples = 4)\n\npp = cbind(pxl1, gens)\n\npp %&gt;% select(-depth) %&gt;%\n  pivot_longer(-geometry) %&gt;%\n    ggplot() + \n      geom_sf(aes(color = value)) +\n      facet_wrap(.~name) +\n        scale_color_scico(direction = -1) +\n        ggtitle(\"Sample from the fitted model\")\n\n\n\n\n\n\n\n\n\n\n0.2.11 An alternative model\nWe now want to check if the depth covatiate has an influende on the probability of presence. We do this in two different models\n\nModel 1 The depth enters the model in a linear way. The linear predictor is then defined as:\n\n\\[\n  \\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s) + \\beta_1\\ \\text{depth}(s)\n\\]\n\nModel 1 The depth enters the model in a non linear way. The linear predictor is then defined as:\n\n\\[\n  \\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s) +  f(\\text{depth}(s))\n\\] where \\(f(.)\\) is a smooth function. We will use a RW2 model for this.\n\n\n\n\n\n\n Task\n\n\n\nFit model 1. Define components, observation model and use the bru() function to estimate the parameters.\nNote Use the scaled version of the covariate stored in depth_r$depth_scaled.\nWhat is the liner effect of depth on the logit probability?\n\n\nTake hint\n\nAdd hint details here…\n\n\n\n\nClick here to see the solution\n\n\nCode\ncmp = ~ Intercept(1) + space(geometry, model = spde_model3) +\n        covariate(depth_r$depth_scaled, model = \"linear\")\n\nformula = present ~ Intercept  + space + covariate\n\nlik = bru_obs(formula = formula, \n              data = pcod_sf, \n              family = \"binomial\")\n\n\nfit2 = bru(cmp, lik)\n\n\n\n\n\nWe now want to fit Model 2 where we allow the effect of depth to be non-linear. To use the RW2 model we need to group the values of depth into distinct classe. To do this we use the function inla.group() which, by default, creates 20 groups. The we can fit the model as usual\n\n# create the grouped variable\ndepth_r$depth_group = inla.group(values(depth_r$depth_scaled))\n\n# run the model\ncmp = ~ Intercept(1) + space(geometry, model = spde_model3) +\n        covariate(depth_r$depth_group, model = \"rw2\")\n\nformula = present ~ Intercept  + space + covariate\n\nlik = bru_obs(formula = formula, \n              data = pcod_sf, \n              family = \"binomial\")\n\n\nfit3 = bru(cmp, lik)\n\n# plot the estimated effect of depth\n\nfit3$summary.random$covariate %&gt;% ggplot() + geom_line(aes(ID,mean)) + \n                                  geom_ribbon(aes(ID, ymin = `0.025quant`, \n                                                      ymax = `0.975quant`), alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nCreate a map of predicted probability from Model 3. You can use a inverse logit function defined as\n\ninv_logit = function(x) (1+exp(-x))^(-1)\n\n\n\nTake hint\n\nThe predict() function can take as input also functions of elements of the components you want to consider\n\n\n\n\nClick here to see the solution\n\n\nCode\ninv_logit = function(x) (1+exp(-x))^(-1)\n\npred3  = predict(fit3, pxl1, ~inv_logit(Intercept + space + covariate) )\n\npred3 %&gt;% ggplot() + \n      geom_sf(aes(color = mean)) +\n        scale_color_scico(direction = -1) +\n        ggtitle(\"Sample from the fitted model\")",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#log-gaussian-cox-processes",
    "href": "day3_practical_5.html#log-gaussian-cox-processes",
    "title": "Practical 5",
    "section": "0.3 Log-Gaussian Cox Processes",
    "text": "0.3 Log-Gaussian Cox Processes\nIn this practical we will:\n\nFit a homogeneous Point Process\nFit a non-homogeneous Point Process\nFit a LGCP (log-Gaussian Cox Process)\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \nlibrary(spatstat)\nlibrary(sf)\nlibrary(scico)\nlibrary(spatstat)\nlibrary(lubridate)\nlibrary(terra)\nlibrary(tidyterra)\n\n\nIn this practical we consider the data clmfires in the spatstat library.\nThis dataset is a record of forest fires in the Castilla-La Mancha region of Spain between 1998 and 2007. This region is approximately 400 by 400 kilometres. The coordinates are recorded in kilometres. For more info about the data you can type:\n\n?clmfires\n\nWe first read the data and transform them into an sf object. We also create a polygon that represents the border of the Castilla-La Mancha region. We select the data for year 2004 and only those fires caused by lightning.\n\ndata(\"clmfires\")\npp = st_as_sf(as.data.frame(clmfires) %&gt;%\n                mutate(x = x, \n                       y = y),\n              coords = c(\"x\",\"y\"),\n              crs = NA) %&gt;%\n  filter(cause == \"lightning\",\n         year(date) == 2004)\n\npoly = as.data.frame(clmfires$window$bdry[[1]]) %&gt;%\n  mutate(ID = 1)\n\nregion = poly %&gt;% \n  st_as_sf(coords = c(\"x\", \"y\"), crs = NA) %&gt;% \n  dplyr::group_by(ID) %&gt;% \n  summarise(geometry = st_combine(geometry)) %&gt;%\n  st_cast(\"POLYGON\") \n  \nggplot() + geom_sf(data = region, alpha = 0) + geom_sf(data = pp)  \n\n\n\n\n\n\n\nFigure 2: Distribution of the observed forest fires caused by lightning in Castilla-La Mancha in 2004",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#HPP",
    "href": "day3_practical_5.html#HPP",
    "title": "Practical 5",
    "section": "0.4 Fit a homogeneous Poisson Process",
    "text": "0.4 Fit a homogeneous Poisson Process\nAs a first exercise we are going to fit a homogeneous Poisson process (HPP) to the data. This is a model that assume constant intensity over the whole space so our liner predictor is\n\\[\n\\eta(s) = \\log\\lambda(s) = \\beta_0 , \\ \\mathbf{s}\\in\\Omega\n\\] so the likelihood can be written as:\n\\[\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n& = \\exp\\left( -\\int_{\\Omega}\\exp(\\beta_0)ds\\right)\\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n\\end{aligned}\n\\]\nwhere \\(|\\Omega|\\) is the area of the domain of interest.\nWe need to approximate the integral using a numerical integration scheme as:\n\n\\[\n\\approx\\exp\\left(-\\sum_{k=1}^{N_k}w_k\\lambda(s_k)\\right)\\prod_{i=1}^n \\lambda(\\mathbf{s}_i)\n\\] Where \\(N_k\\) is the number of integration points \\(s_1,\\dots,s_{N_k}\\) and \\(w_1,\\dots,w_{N_k}\\) are the integration weights.\nIn this case, since the intensity is constant, the integration scheme is really simple: it is enough to consider one random point inside the domain with weight equal to the area of the domain.\n\n# define integration scheme\n\nips = st_sf(\ngeometry = st_sample(region, 1)) # some random location inside the domain\nips$weight = st_area(region) # integration weight is the area of the domain\n\ncmp = ~ 0 + beta_0(1)\n\nformula = geometry ~ beta_0\n\nlik = bru_obs(data = pp,\n              family = \"cp\",\n              formula = formula,\n              ips = ips)\nfit1 = bru(cmp, lik)\n\n\n\n\n\n\n\n Task\n\n\n\n\nWhat is the estimated intensity?\nCompare the estimated expected number of fires on the whole domain with the observed ones.\n\n\nTake hint\n\n\n\nRemember that in the inlabru framework we model the log intensity \\(\\eta = log(\\lambda)\\)\n\n\n\n\nClick here to see the solution\n\n\nCode\n# 1) The estimated posterior distribution of the  intensity is\n\npost_int = inla.tmarginal(function(x) exp(x), fit1$marginals.fixed$beta_0)\npost_int %&gt;% ggplot() + geom_line(aes(x,y))\n\n\n\n\n\n\n\n\n\nCode\n# 2) To compute the expected number of points in the area we need to multiply the \n# estimated intensity by the area of the domain. \n# In the same plot we also show the number of observed fires as a vertical line.\n\npost_int = inla.tmarginal(function(x) st_area(region)* exp(x), fit1$marginals.fixed$beta_0)\npost_int %&gt;% ggplot() + geom_line(aes(x,y)) +\n  geom_vline(xintercept = dim(pp)[1])",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "slides/slides_1.html#course-structure-day-1",
    "href": "slides/slides_1.html#course-structure-day-1",
    "title": "inlabru workshop",
    "section": "Course Structure: Day 1",
    "text": "Course Structure: Day 1\n\n\n\n\n\n\n\n\n\nTime\nTopic\nContent\nExcercises\n\n\n\n\nXXXX am\nCore concepts\n\nLGM and INLA\ninlabru workflow\nModel selection\n\n\n\n\nXXXX am\nTemporal Models\n\nDiscrete time models\nContinuous time models"
  },
  {
    "objectID": "slides/slides_1.html#course-structure-day-2",
    "href": "slides/slides_1.html#course-structure-day-2",
    "title": "inlabru workshop",
    "section": "Course Structure: Day 2",
    "text": "Course Structure: Day 2\n\n\n\n\n\n\n\n\n\nTime\nTopic\nContent\nExcercises\n\n\n\n\nXXXX am\nIntroduction to Spatial Modelling\n\nTypes of spatial data\nSpatial data wrangling and manipulation in R (e.g, terra & sf)\nAreal processes\n\n\n\n\nXXXX am\nModelling geostatistical data\n\nSPDE & the mesh\nGeostatistical Data\nSpatial predictions"
  },
  {
    "objectID": "slides/slides_1.html#course-structure-day-3",
    "href": "slides/slides_1.html#course-structure-day-3",
    "title": "inlabru workshop",
    "section": "Course Structure: Day 3",
    "text": "Course Structure: Day 3\n\n\n\n\n\n\n\n\n\nTime\nTopic\nContent\nExcercises\n\n\n\n\nXXXX am\nSpatial Point processes\n\nSpatial point process\nDistance sampling\n\n\n\n\nXXXX am\nSpatiotemporal Models\n\nSeparable time-space models\nnon-separable space-time models"
  },
  {
    "objectID": "slides/slides_1.html#course-structure-day-4",
    "href": "slides/slides_1.html#course-structure-day-4",
    "title": "inlabru workshop",
    "section": "Course Structure: Day 4",
    "text": "Course Structure: Day 4\n\n\n\n\n\n\n\n\n\nTime\nTopic\nContent\nExcercises\n\n\n\n\nXXXX am\nMultilikelihood and Non-linear models\n\niterated inla\nlogistic growth\nCorregionalization models"
  }
]