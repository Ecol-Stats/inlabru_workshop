[
  {
    "objectID": "slides/slides_13.html#outline",
    "href": "slides/slides_13.html#outline",
    "title": "Lecture",
    "section": "Outline",
    "text": "Outline\n\n\n\n\nWhat are INLA and inlabru?\nWhy the Bayesian framework?\nWhich model are inlabru-friendly?\nWhat are Latent Gaussian Models?\nHow are they implemented in inlabru?"
  },
  {
    "objectID": "slides/slides_13.html#what-is-inla-what-is-inlabru",
    "href": "slides/slides_13.html#what-is-inla-what-is-inlabru",
    "title": "Lecture",
    "section": "What is INLA? What is inlabru?",
    "text": "What is INLA? What is inlabru?\nThe short answer:\n\nINLA is a fast method to do Bayesian inference with latent Gaussian models and inlabru is an R-package that implements this method with a flexible and simple interface.\n\n\nThe (much) longer answer:\n\nRue, H., Martino, S. and Chopin, N. (2009), Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71: 319-392.\nVan Niekerk, J., Krainski, E., Rustand, D., & Rue, H. (2023). A new avenue for Bayesian inference with INLA. Computational Statistics & Data Analysis, 181, 107692.\nLindgren, F., Bachl, F., Illian, J., Suen, M. H., Rue, H., & Seaton, A. E. (2024). inlabru: software for fitting latent Gaussian models with non-linear predictors. arXiv preprint arXiv:2407.00791.\nLindgren, F., Bolin, D., & Rue, H. (2022). The SPDE approach for Gaussian and non-Gaussian fields: 10 years and still running. Spatial Statistics, 50, 100599."
  },
  {
    "objectID": "slides/slides_13.html#where",
    "href": "slides/slides_13.html#where",
    "title": "Lecture",
    "section": "Where?",
    "text": "Where?\n\n\n\n Website-tutorials\n\n\ninlabru https://inlabru-org.github.io/inlabru/\nR-INLA https://www.r-inla.org/home\n\n\n\n\n\n\n Discussion forums\n\n\ninlabru https://github.com/inlabru-org/inlabru/discussions\nR-INLA https://groups.google.com/g/r-inla-discussion-group\n\n\n\n\n\n\n Books\n\n\n\nBlangiardo, M., & Cameletti, M. (2015). Spatial and spatio-temporal Bayesian models with R-INLA. John Wiley & Sons.\nGómez-Rubio, V. (2020). Bayesian inference with INLA. Chapman and Hall/CRC.\nKrainski, E., Gómez-Rubio, V., Bakka, H., Lenzi, A., Castro-Camilo, D., Simpson, D., … & Rue, H. (2018). Advanced spatial modeling with stochastic partial differential equations using R and INLA. Chapman and Hall/CRC.\nWang, X., Yue, Y. R., & Faraway, J. J. (2018). Bayesian regression modeling with INLA. Chapman and Hall/CRC."
  },
  {
    "objectID": "slides/slides_13.html#so-why-should-you-use-inlabru",
    "href": "slides/slides_13.html#so-why-should-you-use-inlabru",
    "title": "Lecture",
    "section": "So… Why should you use inlabru?",
    "text": "So… Why should you use inlabru?\n\n\nWhat type of problems can we solve?\nWhat type of models can we use?\nWhen can we use it?"
  },
  {
    "objectID": "slides/slides_13.html#overview-of-distance-sampling",
    "href": "slides/slides_13.html#overview-of-distance-sampling",
    "title": "Lecture",
    "section": "Overview of Distance Sampling",
    "text": "Overview of Distance Sampling\n\nDistance sampling is a family of related methods for estimating the abundance and spatial distribution of wild populations.\nDistance sampling is based on the idea that animals further away from observers are harder to detect than animals that are nearer.\n\n\n\n\nThis idea is implemented in the model as a detection function that depends on distance.\n\nSpecies at greater distances are harder to detect and the detection function therefore declines as distance increases."
  },
  {
    "objectID": "slides/slides_13.html#density-surface-models",
    "href": "slides/slides_13.html#density-surface-models",
    "title": "Lecture",
    "section": "Density surface models",
    "text": "Density surface models\nCoupling distance sampling data with spatial modelling allows maps of spatially varying density to be produced.\n\nTraditionally, this is achieved in a two-stages approach by (i) using a detectability point estimates to create an offset vector to (ii) use within GLM or GAM for count response data."
  },
  {
    "objectID": "slides/slides_13.html#density-surface-models-1",
    "href": "slides/slides_13.html#density-surface-models-1",
    "title": "Lecture",
    "section": "Density surface models",
    "text": "Density surface models\nCoupling distance sampling data with spatial modelling allows maps of spatially varying density to be produced.\n\nTraditionally, this is achieved in a two-stages approach by (i) using a detectability point estimates to create an offset vector to (ii) use within GLM or GAM for count response data.\nThis requires binning the data into counts based on some discretisation of space."
  },
  {
    "objectID": "slides/slides_13.html#density-surface-models-2",
    "href": "slides/slides_13.html#density-surface-models-2",
    "title": "Lecture",
    "section": "Density surface models",
    "text": "Density surface models\nCoupling distance sampling data with spatial modelling allows maps of spatially varying density to be produced.\n\nTraditionally, this is achieved in a two-stages approach by (i) using a detectability point estimates to create an offset vector to (ii) use within GLM or GAM for count response data.\nA major downside to this approach is the propagation of uncertainty from the detection model to the second-stage spatial model.\n\n\n\nThe goal: one-stage distance sampling model, simultaneously estimating the detectability and the spatial distribution of animals using a point process framework."
  },
  {
    "objectID": "slides/slides_13.html#point-process-data",
    "href": "slides/slides_13.html#point-process-data",
    "title": "Lecture",
    "section": "Point process data",
    "text": "Point process data\n\n\n\nMany of the ecological and environmental processes of interest can be represented by a spatial point process or can be view as an aggregation of one.\n\n\nMany contemporary data sources collect georeferenced information about the location where an event has occur (e.g., species occurrence, wildfire, flood events).\nThis point-based information provides valuable insights into ecosystem dynamics."
  },
  {
    "objectID": "slides/slides_13.html#defining-a-point-process",
    "href": "slides/slides_13.html#defining-a-point-process",
    "title": "Lecture",
    "section": "Defining a Point Process",
    "text": "Defining a Point Process\n\nConsider a fixed geographical region \\(A\\).\nThe set of locations at which events occur are denoted by \\(\\mathbf{s} = (\\mathbf{s}_1, \\ldots, \\mathbf{s}_n)\\).\nWe let \\(N(A)\\) be the random variable which represents the total number of events in region \\(A\\).\nOur primary interest is in measuring where events occur, so the locations are our data."
  },
  {
    "objectID": "slides/slides_13.html#homogeneous-poisson-process",
    "href": "slides/slides_13.html#homogeneous-poisson-process",
    "title": "Lecture",
    "section": "Homogeneous Poisson Process",
    "text": "Homogeneous Poisson Process\n\n\n\n\nThe simplest version of a point process model is the homogeneous Poisson process (HPP).\nThe likelihood of a point pattern \\(\\mathbf{y} = \\left[ \\mathbf{s}_1, \\ldots, \\mathbf{s}_n \\right]^\\intercal\\) distributed as a HPP with intensity \\(\\lambda\\) and observation window \\(\\Omega\\) is\n\\[\np(\\mathbf{y} | \\lambda) \\propto \\lambda^n e^{ \\left( - |\\Omega| \\lambda \\right)} ,\n\\]\n\n\\(|\\Omega|\\) is the size of the observation window.\n\\(\\lambda\\) is the expected number of points per unit area.\n\\(|\\Omega|\\lambda\\) the total expected number of points in the observation window.\n\n\n\n\n\n\nA key property of a Poisson process is that the number of points within a region \\(A\\) is Poisson distributed with constant rate \\(|A|\\lambda\\)."
  },
  {
    "objectID": "slides/slides_13.html#inhomogeneous-poisson-process",
    "href": "slides/slides_13.html#inhomogeneous-poisson-process",
    "title": "Lecture",
    "section": "Inhomogeneous Poisson process",
    "text": "Inhomogeneous Poisson process\nThe inhomogeneous Poisson process has a spatially varying intensity \\(\\lambda(\\mathbf{s})\\).\nThe likelihood in this case is\n\\[\np(\\mathbf{y} | \\lambda) \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i).\n\\]\n\nIf the case of an HPP the integral in the likelihood can easily be computed as \\(\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} =|\\Omega|\\lambda\\)\nFor IPP, integral in the likelihood has to be approximated numerically as a weighted sum."
  },
  {
    "objectID": "slides/slides_13.html#inhomogeneous-poisson-process-1",
    "href": "slides/slides_13.html#inhomogeneous-poisson-process-1",
    "title": "Lecture",
    "section": "Inhomogeneous Poisson process",
    "text": "Inhomogeneous Poisson process\nThe integral in is approximated as \\(\\sum_{j=1}^J w_j \\lambda(\\mathbf{s}_j)\\)\n\n\\(w_j\\) are the integration weights\n\\(\\mathbf{s}_j\\) are the quadrature locations.\n\nThis serves two purposes:\n\nApproximates the integral\nre-write the inhomogeneous Poisson process likelihood as a regular Poisson likelihood."
  },
  {
    "objectID": "slides/slides_13.html#inhomogeneous-poisson-process-2",
    "href": "slides/slides_13.html#inhomogeneous-poisson-process-2",
    "title": "Lecture",
    "section": "Inhomogeneous Poisson process",
    "text": "Inhomogeneous Poisson process\nThe idea behind the trick to rewrite the approximate likelihood is to introduce a dummy vector \\(\\mathbf{z}\\) and an integration weights vector \\(\\mathbf{w}\\) of length \\(J + n\\)\n\n\n\\[\\mathbf{z} = \\left[\\underbrace{0_1, \\ldots,0_J}_\\text{quadrature locations}, \\underbrace{1_1, \\ldots ,1_n}_{\\text{data points}} \\right]^\\intercal\\]\n\n\\[\\mathbf{w} = \\left[ \\underbrace{w_1, \\ldots, w_J}_\\text{quadrature locations}, \\underbrace{0_1, \\ldots, 0_n}_\\text{data points} \\right]^\\intercal\\]\n\nThen the approximate likelihood can be written as\n\\[\n\\begin{aligned}\np(\\mathbf{z} | \\lambda) &\\propto \\prod_{i=1}^{J + n} \\eta_i^{z_i} \\exp\\left(-w_i \\eta_i \\right) \\\\\n\\eta_i &= \\log\\lambda(\\mathbf{s}_i) = \\mathbf{x}(s)'\\beta\n\\end{aligned}\n\\]\n\nThis is similar to a product of Poisson distributions with means \\(\\eta_i\\), exposures \\(w_i\\) and observations \\(z_i\\).\nThis is the basis for the implementation of Cox process models in inlabru, which can be specified using family = \"cp\"."
  },
  {
    "objectID": "slides/slides_13.html#limitations-with-ipp",
    "href": "slides/slides_13.html#limitations-with-ipp",
    "title": "Lecture",
    "section": "Limitations with IPP",
    "text": "Limitations with IPP\n\n\n\n\n\nIPP models assume that data points are conditionally independent given the covariates, meaning that any spatial variation is fully explained by environmental and sampling factors.\nUnmeasured endogenous and exogenous factors can create spatial\nIgnoring them can lead to bias in our conclusions."
  },
  {
    "objectID": "slides/slides_13.html#the-log-gaussian-cox-process",
    "href": "slides/slides_13.html#the-log-gaussian-cox-process",
    "title": "Lecture",
    "section": "The Log-Gaussian Cox Process",
    "text": "The Log-Gaussian Cox Process\n\n\n\n\nLog-Gaussian Cox processes (LGCP) extends the IPP by allowing the intensity function to vary spatially according to a structured spatial random effect.\n\n\\[\n\\log~\\lambda(s)= \\mathbf{x}(s)'\\beta + \\xi(s)\n\\]\n\nThe events are then assumed to be independent given \\(\\xi(s)\\) - a GMRF with Matérn covariance.\ninlabru has implemented some integration schemes that are especially well suited to integrating the intensity in models with an SPDE effect.\n\n\n\nSee for further reference: Simpson, Daniel, Janine B. Illian, Finn Lindgren, Sigrunn H. Sørbye, and Håvard Rue. 2016. “Going off grid: computationally efficient inference for log-Gaussian Cox processes.” Biometrika 103 (1): 49–70."
  },
  {
    "objectID": "slides/slides_13.html#gaussian-random-fields",
    "href": "slides/slides_13.html#gaussian-random-fields",
    "title": "Lecture",
    "section": "Gaussian Random Fields",
    "text": "Gaussian Random Fields\nIf we have a process that is occurring everywhere in space, it is natural to try to model it using some sort of function.\n\nIf \\(z\\) is a vector of observations of \\(z(\\mathbf{s})\\) at different locations, we want this to be normally distributed:\n\n\\[\n\\mathbf{z} = (z(\\mathbf{s}_1),\\ldots,z(\\mathbf{s}_m)) \\sim \\mathcal{N}(0,\\Sigma)\n\\]\nwhere \\(\\Sigma_{ij} = \\mathrm{Cov}(z(\\mathbf{s}_i),z(\\mathbf{s}_j))\\) is a dense \\(m \\times m\\) matrix."
  },
  {
    "objectID": "slides/slides_13.html#gaussian-random-fields-1",
    "href": "slides/slides_13.html#gaussian-random-fields-1",
    "title": "Lecture",
    "section": "Gaussian Random Fields",
    "text": "Gaussian Random Fields\n\nA Gaussian random field (GRF) is a collection of random variables where observations occur in a continuous domain, and where every finite collection of random variables has a multivariate normal distribution\n\n\n\n\nStationary random fields\n\n\nA GRF is stationary if:\n\nhas mean zero.\nthe covariance between two points depends only on the distance and direction between those points.\n\nIt is isotropic if the covariance only depends on the distance between the points."
  },
  {
    "objectID": "slides/slides_13.html#the-spde-approach",
    "href": "slides/slides_13.html#the-spde-approach",
    "title": "Lecture",
    "section": "The SPDE approach",
    "text": "The SPDE approach\nThe goal: approximate the GRF using a triangulated mesh via the so-called SPDE approach.\nThe SPDE approach represents the continuous spatial process as a discretely indexed Gaussian Markov Random Field (GMRF)\n\nWe construct an appropriate lower-resolution approximation of the surface by sampling it in a set of well designed points and constructing a piecewise linear interpolant."
  },
  {
    "objectID": "slides/slides_13.html#the-spde-approach-1",
    "href": "slides/slides_13.html#the-spde-approach-1",
    "title": "Lecture",
    "section": "The SPDE approach",
    "text": "The SPDE approach\n\nA GF with Matérn covariance \\(c_{\\nu}(d;\\sigma,\\rho)\\) is a solution to a particular PDE.\n\n\\[\nc_{\\nu}(d;\\sigma,\\rho) = \\sigma^2\\frac{2^{1-\\nu}}{\\Gamma(\\nu)}\\left(\\sqrt{8\\nu}\\frac{d}{\\rho}\\right)^{\\nu}K_{\\nu}\\left(\\sqrt{8\\nu}\\frac{d}{\\rho}\\right)\n\\]\n\nThis solution is then approximated using a finite combination of piecewise linear basis functions defined on a triangulation .\nThe solution is completely defined by a Gaussian vector of weights (defined on the triangulation vertices) with zero mean and a sparse precision matrix.\nHow do we choose sensible priors for \\(\\sigma,\\rho\\)?"
  },
  {
    "objectID": "slides/slides_13.html#penalized-complexity-pc-priors",
    "href": "slides/slides_13.html#penalized-complexity-pc-priors",
    "title": "Lecture",
    "section": "Penalized Complexity (PC) priors",
    "text": "Penalized Complexity (PC) priors\nPenalized Complexity (PC) priors proposed by Simpson et al. (2017) allow us to control the amount of spatial smoothing and avoid overfitting.\n\nPC priors shrink the model towards a simpler baseline unless the data provide strong evidence for a more complex structure.\nTo define the prior for the marginal precision \\(\\sigma^{-2}\\) and the range parameter \\(\\rho\\), we use the probability statements:\n\nDefine the prior for the range \\(\\text{Prob}(\\rho&lt;\\rho_0) = p_{\\rho}\\)\nDefine the prior for the range \\(\\text{Prob}(\\sigma&gt;\\sigma_0) = p_{\\sigma}\\)"
  },
  {
    "objectID": "slides/slides_13.html#learning-about-the-spde-approach",
    "href": "slides/slides_13.html#learning-about-the-spde-approach",
    "title": "Lecture",
    "section": "Learning about the SPDE approach",
    "text": "Learning about the SPDE approach\n\n\n\n\nF. Lindgren, H. Rue, and J. Lindström. An explicit link between Gaussian fields and Gaussian Markov random fields: The SPDE approach (with discussion). In: Journal of the Royal Statistical Society, Series B 73.4 (2011), pp. 423–498.\nH. Bakka, H. Rue, G. A. Fuglstad, A. Riebler, D. Bolin, J. Illian, E. Krainski, D. Simpson, and F. Lindgren. Spatial modelling with R-INLA: A review. In: WIREs Computational Statistics 10:e1443.6 (2018). (Invited extended review). DOI: 10.1002/wics.1443.\nE. T. Krainski, V. Gómez-Rubio, H. Bakka, A. Lenzi, D. Castro-Camilio, D. Simpson, F. Lindgren, and H. Rue. Advanced Spatial Modeling with Stochastic Partial Differential Equations using R and INLA. Github version . CRC press, Dec. 20"
  },
  {
    "objectID": "slides/slides_13.html#spde-models",
    "href": "slides/slides_13.html#spde-models",
    "title": "Lecture",
    "section": "SPDE models",
    "text": "SPDE models\nWe call spatial Markov models defined on a mesh SPDE models.\nSPDE models have 3 parts\n\nA mesh\nA range parameter \\(\\kappa\\)\nA precision parameter \\(\\tau\\)\n\n\n\nWe use the SPDE effect to model the intensity of a point process that represents the locations of animal sightings.\nOften such sightings are made by observers who cannot detect all the animals\nTo accurately estimate abundance, we require an estimate of the number of animals that remained undetected."
  },
  {
    "objectID": "slides/slides_13.html#thinned-point-process-1",
    "href": "slides/slides_13.html#thinned-point-process-1",
    "title": "Lecture",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\n\n\n\nThe LGCP is a flexible approach that can include spatial covariates to model the mean intensity and a mean-zero spatially structured random effect to account for unexplained heterogeneity not captured by the covariates.\n\nTo account for the imperfect detection of points we specify a thinning probability function \\[g(s) = \\mathbb{P}(\\text{a point at s is detected}|\\text{a point is at s})\\]\nA key property of LGCP is that a realisation of a point process with intensity \\(\\lambda(s)\\) that is thinned by probability function \\(g(s)\\), follows also a LGCP with intensity:\n\n\\[\n\\underbrace{\\tilde{\\lambda}(s)}_{\\text{observed process}} = \\underbrace{\\lambda(s)}_{\\text{true process}} \\times \\underbrace{g(s)}_{\\text{thinning probability}}\n\\]"
  },
  {
    "objectID": "slides/slides_13.html#thinned-point-process-2",
    "href": "slides/slides_13.html#thinned-point-process-2",
    "title": "Lecture",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\nLets visualize this on 1D: Intensity function with points"
  },
  {
    "objectID": "slides/slides_13.html#thinned-point-process-3",
    "href": "slides/slides_13.html#thinned-point-process-3",
    "title": "Lecture",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\nIntensity (density) function with points and transect locations"
  },
  {
    "objectID": "slides/slides_13.html#thinned-point-process-4",
    "href": "slides/slides_13.html#thinned-point-process-4",
    "title": "Lecture",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\n\n\nDetection function \\(\\color{red}{g(s)}\\)\nHere \\(\\color{red}{g(s) =1}\\) on the transects (at x = 10,30 and 50)."
  },
  {
    "objectID": "slides/slides_13.html#thinned-point-process-5",
    "href": "slides/slides_13.html#thinned-point-process-5",
    "title": "Lecture",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\n\nDetection function \\(\\color{red}{g(s)}\\) and detected points"
  },
  {
    "objectID": "slides/slides_13.html#thinned-point-process-6",
    "href": "slides/slides_13.html#thinned-point-process-6",
    "title": "Lecture",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process"
  },
  {
    "objectID": "slides/slides_13.html#thinned-point-process-7",
    "href": "slides/slides_13.html#thinned-point-process-7",
    "title": "Lecture",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\n The detection function describes the probability \\(\\color{red}{p(s)}\\) that an point is detected"
  },
  {
    "objectID": "slides/slides_13.html#thinned-point-process-8",
    "href": "slides/slides_13.html#thinned-point-process-8",
    "title": "Lecture",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process"
  },
  {
    "objectID": "slides/slides_13.html#thinned-point-process-9",
    "href": "slides/slides_13.html#thinned-point-process-9",
    "title": "Lecture",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\n\nObservations are from a thinned Poisson process with intensity \\(\\lambda(s) \\color{red}{p(s)}\\)"
  },
  {
    "objectID": "slides/slides_13.html#detection-function",
    "href": "slides/slides_13.html#detection-function",
    "title": "Lecture",
    "section": "Detection Function",
    "text": "Detection Function\n\n\n\n\nStandard distance sampling approaches specify \\(g(s)\\) as a function that declines with increasing distance\n\nhorizontal distance to the observer for point transects\nperpendicular distance to the transect line for line transects"
  },
  {
    "objectID": "slides/slides_13.html#detection-function-1",
    "href": "slides/slides_13.html#detection-function-1",
    "title": "Lecture",
    "section": "Detection Function",
    "text": "Detection Function\n\n\n\n\nStandard distance sampling approaches specify \\(g(s)\\) as a function that declines with increasing distance\n\nhorizontal distance to the observer for point transects\nperpendicular distance to the transect line for line transects"
  },
  {
    "objectID": "slides/slides_13.html#detection-function-2",
    "href": "slides/slides_13.html#detection-function-2",
    "title": "Lecture",
    "section": "Detection Function",
    "text": "Detection Function\n\n\n\n\nStandard distance sampling approaches specify \\(g(s)\\) as a function that declines with increasing distance\n\nhorizontal distance to the observer for point transects\nperpendicular distance to the transect line for line transects\n\nThe thinning probability function is specified as a parametric family of functions.\n\n\n\nHalf-normal: \\(g(\\mathbf{s}|\\sigma) = \\exp(-0.5 (d(\\mathbf{s})/\\sigma)^2)\\)\nHazard-rate :\\(g(\\mathbf{s}|\\sigma) = 1 - \\exp(-(d(\\mathbf{s})/\\sigma)^{-1})\\)"
  },
  {
    "objectID": "slides/slides_13.html#detection-function-3",
    "href": "slides/slides_13.html#detection-function-3",
    "title": "Lecture",
    "section": "Detection Function",
    "text": "Detection Function\n\n\n\n\nStandard distance sampling approaches specify \\(g(s)\\) as a function that declines with increasing distance\n\nhorizontal distance to the observer for point transects\nperpendicular distance to the transect line for line transects\n\nThe thinning probability function is specified as a parametric family of functions.\nThe thinned-LGCP likelihood is given by:\n\n\\[\n\\pi(\\mathbf{s_1},\\ldots,\\mathbf{s_m}) = \\exp\\left( |\\Omega| - \\int_{\\mathbf{s}\\in\\Omega}\\lambda(s)g(s)\\text{d}s \\right) \\prod_{i=1}^m \\lambda(\\mathbf{s}_i)g(\\mathbf{s}_i)\n\\]\n\nTo make \\(g(s)\\) and \\(\\lambda(s)\\) identifiable, we assume intensity is constant with respect to distance from the observer.\n\nIn practice this means we assume animals are uniformly distributed with respect to distance from the line"
  },
  {
    "objectID": "slides/slides_13.html#putting-all-the-pieces-together",
    "href": "slides/slides_13.html#putting-all-the-pieces-together",
    "title": "Lecture",
    "section": "Putting all the pieces together",
    "text": "Putting all the pieces together\n\n\n\n\n\n\n\n\n\nThe true point pattern \\(Y = \\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\) are a realization of a Point process with intensity \\(\\lambda(s)\\)"
  },
  {
    "objectID": "slides/slides_13.html#putting-all-the-pieces-together-1",
    "href": "slides/slides_13.html#putting-all-the-pieces-together-1",
    "title": "Lecture",
    "section": "Putting all the pieces together",
    "text": "Putting all the pieces together\n\n\n\n\n\n\n\n\n\nThe true point pattern \\(Y = \\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\) are a realization of a Point process with intensity \\(\\lambda(s)\\)\nWe design a sampling survey to collect the data along transects"
  },
  {
    "objectID": "slides/slides_13.html#putting-all-the-pieces-together-2",
    "href": "slides/slides_13.html#putting-all-the-pieces-together-2",
    "title": "Lecture",
    "section": "Putting all the pieces together",
    "text": "Putting all the pieces together\n\n\n\n\n\n\n\n\n\nThe true point pattern \\(Y = \\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\) are a realization of a Point process with intensity \\(\\lambda(s)\\)\nWe design a sampling survey to collect the data along transects\ndetected points are generated from the thinned PP with intensity \\(\\color{red}{\\tilde{\\lambda}(s)}= \\lambda(s)\\color{red}{g(d(s))}\\)\n\nThe log intensity \\(\\log \\color{red}{\\tilde{\\lambda}(s)} = \\overbrace{\\log \\lambda (s)}^{\\mathbf{x}'\\beta + \\xi(s)} + \\overbrace{\\log \\color{red}{g(d(s))}}^{-0.5~d(\\mathbf{s})^2\\sigma^{-2}}\\)"
  },
  {
    "objectID": "slides/slides_13.html#putting-all-the-pieces-together-3",
    "href": "slides/slides_13.html#putting-all-the-pieces-together-3",
    "title": "Lecture",
    "section": "Putting all the pieces together",
    "text": "Putting all the pieces together\n\n\n\n\n\n\n\n\n\nThe true point pattern \\(Y = \\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\) are a realization of a Point process with intensity \\(\\lambda(s)\\)\nWe design a sampling survey to collect the data along transects\ndetected points are generated from the thinned PP with intensity \\(\\color{red}{\\tilde{\\lambda}(s)}= \\lambda(s)\\color{red}{g(d(s))}\\)\n\nThe log intensity \\(\\log \\color{red}{\\tilde{\\lambda}(s)} = \\overbrace{\\log \\lambda (s)}^{\\mathbf{x}'\\beta + \\xi(s)} + \\overbrace{\\log \\color{red}{g(d(s))}}^{-0.5~d(\\mathbf{s})^2\\sigma^{-2}}\\)\nThe encounter rate, i.e. the number of observed animals within a distance \\(W\\) follows \\(m \\sim \\text{Poisson} \\left(\\int_0^W \\tilde{\\lambda}(d)\\text{d}d\\right)\\)\n\n\n\n\nThe pdf of detected distances is \\(\\pi(d_1,\\ldots,d_m|m) \\propto \\prod_{i=1}^m\\dfrac{\\tilde{\\lambda}(d_i)}{\\int_0^W \\tilde{\\lambda}(d)\\text{d}d}\\)"
  },
  {
    "objectID": "slides/slides_13.html#putting-all-the-pieces-together-4",
    "href": "slides/slides_13.html#putting-all-the-pieces-together-4",
    "title": "Lecture",
    "section": "Putting all the pieces together",
    "text": "Putting all the pieces together\n\n\n\n\n\n\n\n\n\nThe true point pattern \\(Y = \\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\) are a realization of a Point process with intensity \\(\\lambda(s)\\)\nWe design a sampling survey to collect the data along transects\ndetected points are generated from the thinned PP with intensity \\(\\color{red}{\\tilde{\\lambda}(s)}= \\lambda(s)\\color{red}{g(d(s))}\\)\n\nThe log intensity \\(\\log \\color{red}{\\tilde{\\lambda}(s)} = \\overbrace{\\log \\lambda (s)}^{\\mathbf{x}'\\beta + \\xi(s)} + \\overbrace{\\log \\color{red}{g(d(s))}}^{-0.5~d(\\mathbf{s})^2\\sigma^{-2}}\\)\nThe encounter rate, i.e. the number of observed animals within a distance \\(W\\) follows \\(m \\sim \\text{Poisson} \\left(\\int_0^W \\tilde{\\lambda}(d)\\text{d}d\\right)\\)\n\n\n\n\nThe pdf of detected distances is \\(\\pi(d_1,\\ldots,d_m|m) \\propto \\prod_{i=1}^m\\dfrac{ g(d_i)}{\\int_o^W g(d) \\text{d}d}\\) if \\(\\color{red}{\\tilde{\\lambda}(d_i)} = \\lambda \\color{red}{g(d_i)}\\)"
  },
  {
    "objectID": "slides/slides_13.html#an-approximation-strips-as-lines",
    "href": "slides/slides_13.html#an-approximation-strips-as-lines",
    "title": "Lecture",
    "section": "An approximation: Strips as lines",
    "text": "An approximation: Strips as lines\n\nIf the strips width ( \\(2W\\) ) is narrow compared to study region (\\(\\Omega\\)) we can treat them as lines.\n\nWe need to adjust the intensity at a point \\(\\mathbf{s}\\) on the line to take account of the actual width of the strip\nAdjust the thinning probability to account for having collapsed all points onto the line."
  },
  {
    "objectID": "slides/slides_13.html#an-approximation-strips-as-lines-1",
    "href": "slides/slides_13.html#an-approximation-strips-as-lines-1",
    "title": "Lecture",
    "section": "An approximation: Strips as lines",
    "text": "An approximation: Strips as lines\nThe intensity at a point \\(\\mathbf{s}\\) on the line becomes \\(2W\\lambda(s)\\) instead of \\(\\lambda(s)\\).\n\nLet \\(\\pi(d)\\) be the probability that the point is at a distance \\(d\\) from the line.\nLet \\(p(d)\\) be the probability that is detected given it is at \\(d\\).\n\nThen, the thinning probability becomes \\(\\pi(d)\\times p(d)\\), assuming the points are uniformly distributed within the strip then \\(\\pi(d) = 1/W\\) (the density of distances is assumed to be constant on the interval \\([0,W]\\)).\nThis updates our thinning intensity to\n\\[\n\\log \\tilde{\\lambda}(s) = \\underbrace{\\mathbf{x}'\\beta + \\xi(s)}_{\\log \\lambda(s)} + \\log p(d) + \\log \\times(2/2W)\n\\]\n\nTypically \\(p(d)\\) is a non-linear function, that is where inlabru can help via a Fixed point iteration scheme (further details available in this vignette)\n\n\n\\[\n\\mathbb{E}[N_{\\text{det}}] = 2W \\lambda(s) \\times \\frac{1}{W} \\int_0^W p(r)  dr = 2\\lambda(s) \\int_0^W p(r)  dr\n\\]"
  },
  {
    "objectID": "slides/slides_13.html#example-dolphins-in-the-gulf-of-mexico",
    "href": "slides/slides_13.html#example-dolphins-in-the-gulf-of-mexico",
    "title": "Lecture",
    "section": "Example: Dolphins in the Gulf of Mexico",
    "text": "Example: Dolphins in the Gulf of Mexico\nIn the next example, we will explore data from a combination of several NOAA shipboard surveys conducted on pan-tropical spotted dolphins in the Gulf of Mexico.\n\nA total of 47 observations of groups of dolphins were detected. The group size was recorded, as well as the Beaufort sea state at the time of the observation.\nTransect width is 16 km, i.e. maximal detection distance 8 km (transect half-width 8 km)."
  },
  {
    "objectID": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh",
    "href": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh",
    "title": "Lecture",
    "section": "Step 1: Define the SPDE representation: The mesh",
    "text": "Step 1: Define the SPDE representation: The mesh\nFirst, we need to create the mesh used to approximate the random field. We can either:\n\nCreate a nonconvex extension of the points using the fm_mesh_2d and fm_nonconvex_hull functions from the fmesher package:\n\n\nlibrary(fmesher)\n\nboundary0 = fm_nonconvex_hull(mexdolphin$points,convex = -0.1)\n\nmesh_0 = fm_mesh_2d(boundary = boundary0,\n                          max.edge = c(30, 150), # The largest allowed triangle edge length.\n                          cutoff = 15,\n                          crs = fm_crs(mexdolphin$points))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmax.edge for maximum triangle edge lengths\ncutoff to avoid overly small triangles in clustered areas"
  },
  {
    "objectID": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh-1",
    "href": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh-1",
    "title": "Lecture",
    "section": "Step 1: Define the SPDE representation: The mesh",
    "text": "Step 1: Define the SPDE representation: The mesh\nFirst, we need to create the mesh used to approximate the random field. We can either:\n\nUse a pre-define sf boundary and specify this directly into the mesh construction via the fm_mesh_2d function\n\n\nlibrary(fmesher)\n\n\nmesh_1 = fm_mesh_2d(boundary = mexdolphin$ppoly,\n                    max.edge = c(30, 150),\n                    cutoff = 15,\n                    crs = fm_crs(mexdolphin$points))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmax.edge for maximum triangle edge lengths\ncutoff to avoid overly small triangles in clustered areas"
  },
  {
    "objectID": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh-2",
    "href": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh-2",
    "title": "Lecture",
    "section": "Step 1: Define the SPDE representation: The mesh",
    "text": "Step 1: Define the SPDE representation: The mesh\n\n\nAll random field models need to be discretised for practical calculations.\nThe SPDE models were developed to provide a consistent model definition across a range of discretisations.\nWe use finite element methods with local, piecewise linear basis functions defined on a triangulation of a region of space containing the domain of interest.\nDeviation from stationarity is generated near the boundary of the region.\nThe choice of region and choice of triangulation affects the numerical accuracy."
  },
  {
    "objectID": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh-3",
    "href": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh-3",
    "title": "Lecture",
    "section": "Step 1: Define the SPDE representation: The mesh",
    "text": "Step 1: Define the SPDE representation: The mesh\n\nToo fine meshes \\(\\rightarrow\\) heavy computation\nToo coarse mesh \\(\\rightarrow\\) not accurate enough"
  },
  {
    "objectID": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh-4",
    "href": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh-4",
    "title": "Lecture",
    "section": "Step 1: Define the SPDE representation: The mesh",
    "text": "Step 1: Define the SPDE representation: The mesh\nSome guidelines\n\nCreate triangulation meshes with fm_mesh_2d():\nedge length should be around a third to a tenth of the spatial range\nMove undesired boundary effects away from the domain of interest by extending to a smooth external boundary:\nUse a coarser resolution in the extension to reduce computational cost (max.edge=c(inner, outer)), i.e., add extra, larger triangles around the border"
  },
  {
    "objectID": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh-5",
    "href": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh-5",
    "title": "Lecture",
    "section": "Step 1: Define the SPDE representation: The mesh",
    "text": "Step 1: Define the SPDE representation: The mesh\n\nUse a fine resolution (subject to available computational resources) for the domain of interest (inner correlation range) and avoid small edges ,i.e., filter out small input point clusters (0 \\(&lt;\\) cutoff \\(&lt;\\) inner)\nCoastlines and similar can be added to the domain specification in fm_mesh_2d() through the boundary argument.\nsimplify the border"
  },
  {
    "objectID": "slides/slides_13.html#step-1-define-the-spde-representation-the-spde",
    "href": "slides/slides_13.html#step-1-define-the-spde-representation-the-spde",
    "title": "Lecture",
    "section": "Step 1: Define the SPDE representation: The SPDE",
    "text": "Step 1: Define the SPDE representation: The SPDE\nWe use the inla.spde2.pcmatern to define the SPDE model using PC priors through the following probability statements\n\n\n\n\\(P(\\rho &lt; 50) = 0.1\\)\n\\(P(\\sigma &gt; 2) = 0.1\\)\n\n\n\nspde_model =  inla.spde2.pcmatern(\n  mexdolphin$mesh,\n  prior.sigma = c(2, 0.1),\n  prior.range = c(50, 0.1)\n)"
  },
  {
    "objectID": "slides/slides_13.html#step-2-define-the-detection-function",
    "href": "slides/slides_13.html#step-2-define-the-detection-function",
    "title": "Lecture",
    "section": "Step 2: Define the Detection function",
    "text": "Step 2: Define the Detection function\nWe start by plotting the distances and histogram of frequencies in distance intervals.\n\nThen, we need to define a half-normal detection probability function. This must take distance as its first argument and the linear predictor of the sigma parameter as its second:\n\n# define detection function\nhn &lt;- function(distance, sigma) {\n  exp(-0.5 * (distance / sigma)^2)\n}"
  },
  {
    "objectID": "slides/slides_13.html#example-dolphins-in-the-gulf-of-mexico-1",
    "href": "slides/slides_13.html#example-dolphins-in-the-gulf-of-mexico-1",
    "title": "Lecture",
    "section": "Example: Dolphins in the Gulf of Mexico",
    "text": "Example: Dolphins in the Gulf of Mexico\n\n\nThe LGCP Model\n\\[\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) p(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) p(\\mathbf{s}_i)) \\\\\n\\eta(s) &  = \\color{#FF6B6B}{\\boxed{\\beta_0}} + \\color{#FF6B6B}{\\boxed{ \\omega(s)}} + \\color{#FF6B6B}{\\boxed{ \\log p(s)}} \\\\\n\\end{aligned}\n\\]\nThe code\n\n# define model component\ncmp = ~ Intercept(1) + \n  space(main = geometry, model = spde_model) +\n  sigma(1,\n    prec.linear = 1,\n    marginal = bru_mapper_marginal(qexp, pexp, dexp, rate = 1 / 8)\n  ) \n\n# define model predictor\neta  = geometry + distance ~ space +\n  log(hn(distance, sigma)) +\n  Intercept + log(2)\n\n# build the observation model\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = mexdolphin$points,\n              ips = ips)\n\n# fit the model\nfit = bru(cmp, lik)\n\n\nThe integration scheme\n\n# build integration scheme\ndistance_domain &lt;-  fm_mesh_1d(seq(0, 8,\n                              length.out = 30))\nips = fm_int(list(geometry = mesh,\n                  distance = distance_domain),\n             samplers = mexdolphin$samplers)\n\n\n\n\n\n\n\n\n\n\n\n\nThe samplers in this dataset are lines, not polygons, so we need to tell inlabru about the strip half-width, W, which in the case of these data is 8.\nTo control the prior distribution for the \\(\\sigma\\) parameter, we use a transformation mapper that converts a latent variable into an exponentially distributed variable with expectation 8\nThe marginal argument in the sigma component specifies the transformation function taking N(0,1) to Exponential(1/8)."
  },
  {
    "objectID": "slides/slides_13.html#example-dolphins-in-the-gulf-of-mexico-2",
    "href": "slides/slides_13.html#example-dolphins-in-the-gulf-of-mexico-2",
    "title": "Lecture",
    "section": "Example: Dolphins in the Gulf of Mexico",
    "text": "Example: Dolphins in the Gulf of Mexico\n\n\nThe LGCP Model\n\\[\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) p(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) p(\\mathbf{s}_i)) \\\\\n\\color{#FF6B6B}{\\boxed{\\eta(s)}} &  = \\color{#FF6B6B}{\\boxed{\\beta_0 +  \\omega(s) +  \\log p(s)}}\\\\\n\\end{aligned}\n\\]\nThe code\n\n# define model component\ncmp = ~ Intercept(1) + \n  space(main = geometry, model = spde_model) +\n  sigma(1,\n    prec.linear = 1,\n    marginal = bru_mapper_marginal(qexp, pexp, dexp, rate = 1 / 8)\n  ) \n\n# define model predictor\neta  = geometry + distance ~ space +\n  log(hn(distance, sigma)) +\n  Intercept + log(2)\n\n# build the observation model\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = mexdolphin$points,\n              ips = ips)\n\n# fit the model\nfit = bru(cmp, lik)\n\n\nThe integration scheme\n\n# build integration scheme\ndistance_domain &lt;-  fm_mesh_1d(seq(0, 8,\n                              length.out = 30))\nips = fm_int(list(geometry = mesh,\n                  distance = distance_domain),\n             samplers = mexdolphin$samplers)\n\n\n\n\n\n\n\n\n\n\n\n\nwe need an offset due to the unknown direction of the detections"
  },
  {
    "objectID": "slides/slides_13.html#example-dolphins-in-the-gulf-of-mexico-3",
    "href": "slides/slides_13.html#example-dolphins-in-the-gulf-of-mexico-3",
    "title": "Lecture",
    "section": "Example: Dolphins in the Gulf of Mexico",
    "text": "Example: Dolphins in the Gulf of Mexico\n\n\nThe LGCP Model\n\\[\n\\begin{aligned}\n\\color{#FF6B6B}{\\boxed{p(\\mathbf{y} | \\lambda)}} & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) p(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) p(\\mathbf{s}_i)) \\\\\n\\eta(s) &  = \\beta_0 +  \\omega(s) +  \\log p(s) \\\\\n\\end{aligned}\n\\]\nThe code\n\n# define model component\ncmp = ~ Intercept(1) + \n  space(main = geometry, model = spde_model) +\n  sigma(1,\n    prec.linear = 1,\n    marginal = bru_mapper_marginal(qexp, pexp, dexp, rate = 1 / 8)\n  ) \n\n# define model predictor\neta  = geometry + distance ~ space +\n  log(hn(distance, sigma)) +\n  Intercept + log(2)\n\n# build the observation model\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = mexdolphin$points,\n              ips = ips)\n\n# fit the model\nfit = bru(cmp, lik)\n\n\nThe integration scheme\n\n# build integration scheme\ndistance_domain &lt;-  fm_mesh_1d(seq(0, 8,\n                              length.out = 30))\nips = fm_int(list(geometry = mesh,\n                  distance = distance_domain),\n             samplers = mexdolphin$samplers)"
  },
  {
    "objectID": "slides/slides_13.html#example-dolphins-in-the-gulf-of-mexico-4",
    "href": "slides/slides_13.html#example-dolphins-in-the-gulf-of-mexico-4",
    "title": "Lecture",
    "section": "Example: Dolphins in the Gulf of Mexico",
    "text": "Example: Dolphins in the Gulf of Mexico\n\n\nThe LGCP Model\n\\[\n\\begin{aligned}\n\\color{#FF6B6B}{\\boxed{p(\\mathbf{y} | \\lambda)}} & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) p(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) p(\\mathbf{s}_i)) \\\\\n\\eta(s) &  = \\beta_0 +  \\omega(s) +  \\log p(s) \\\\\n\\end{aligned}\n\\]\nThe code\n\n# define model component\ncmp = ~ Intercept(1) + \n  space(main = geometry, model = spde_model) +\n  sigma(1,\n    prec.linear = 1,\n    marginal = bru_mapper_marginal(qexp, pexp, dexp, rate = 1 / 8)\n  ) \n\n# define model predictor\neta  = geometry + distance ~ space +\n  log(hn(distance, sigma)) +\n  Intercept + log(2)\n\n# build the observation model\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = mexdolphin$points,\n              ips = ips)\n\n# fit the model\nfit = bru(cmp, lik)\n\n\nThe integration scheme\n\n# build integration scheme\ndistance_domain &lt;-  fm_mesh_1d(seq(0, 8,\n                              length.out = 30))\nips = fm_int(list(geometry = mesh,\n                  distance = distance_domain),\n             samplers = mexdolphin$samplers)"
  },
  {
    "objectID": "slides/slides_13.html#results-posterior-summaries",
    "href": "slides/slides_13.html#results-posterior-summaries",
    "title": "Lecture",
    "section": "Results: posterior summaries",
    "text": "Results: posterior summaries\n\n\nWe can use the fit$summary.fixed and summary.hyperpar to obtain psoterior summarie sof the mdoel parameters.\n\n\n\n\n\n\n\n\n\nmean\n0.025quant\n0.975quant\n\n\n\n\nIntercept\n−8.41\n−9.47\n−7.62\n\n\nsigma\n−0.05\n−0.46\n0.36\n\n\nRange for space\n131.74\n41.79\n320.28\n\n\nStdev for space\n1.17\n0.72\n1.78\n\n\n\n\n\n\n\n\nThe spde.posterior allow us to plot the posterior density of the Matern field parameters\n\nspde.posterior(fit, \"space\", what = \"range\") %&gt;% plot()"
  },
  {
    "objectID": "slides/slides_13.html#results-posterior-summaries-1",
    "href": "slides/slides_13.html#results-posterior-summaries-1",
    "title": "Lecture",
    "section": "Results: posterior summaries",
    "text": "Results: posterior summaries\n\n\nWe can use the fit$summary.fixed and summary.hyperpar to obtain posterior summaries of the model parameters.\n\n\n\n\n\n\n\n\n\nmean\n0.025quant\n0.975quant\n\n\n\n\nIntercept\n−8.41\n−9.47\n−7.62\n\n\nsigma\n−0.05\n−0.46\n0.36\n\n\nRange for space\n131.74\n41.79\n320.28\n\n\nStdev for space\n1.17\n0.72\n1.78\n\n\n\n\n\n\n\n\nThe spde.posterior allow us to plot the posterior density of the Matern field parameters\n\nspde.posterior(fit, \"space\", what = \"log.variance\") %&gt;% plot()"
  },
  {
    "objectID": "slides/slides_13.html#results-predicted-densities",
    "href": "slides/slides_13.html#results-predicted-densities",
    "title": "Lecture",
    "section": "Results: predicted densities",
    "text": "Results: predicted densities\n\n\nTo map the spatial intensity we first need to define a grid of points where we want to predict.\n\nWe do this using the function fm_pixel() which creates a regular grid of points covering the mesh\nThen, we use the predict function which takes as input\n\nthe fitted model (fit)\nthe prediction points (pxl)\nthe model components we want to predict (e.g., \\(e^{\\beta_0 + \\xi(s)}\\))\n\nTo plot this you can use ggplot and add a gg() layer with your output of interest (E.g., pr.int$spatial)\n\n\n\nlibrary(patchwork)\npxl &lt;- fm_pixels(mesh, dims = c(200, 100), mask = mexdolphin$ppoly)\npr.int &lt;- predict(fit, pxl, ~ data.frame(spatial = space,\n                                      loglambda = Intercept + space,\n                                      lambda = exp(Intercept + space)))\n\n\nggplot() +\n  gg(pr.int$spatial, geom = \"tile\")"
  },
  {
    "objectID": "slides/slides_13.html#results-predicted-densities-1",
    "href": "slides/slides_13.html#results-predicted-densities-1",
    "title": "Lecture",
    "section": "Results: predicted densities",
    "text": "Results: predicted densities\nWe can also use the predict function to predict the detection function:\n\ndistdf &lt;- data.frame(distance = seq(0, 8, length.out = 100))\ndfun &lt;- predict(fit, distdf, ~ hn(distance, sigma))\nplot(dfun)"
  },
  {
    "objectID": "slides/slides_13.html#results-predicted-expected-counts",
    "href": "slides/slides_13.html#results-predicted-expected-counts",
    "title": "Lecture",
    "section": "Results: predicted expected counts",
    "text": "Results: predicted expected counts\nWe can look at the posterior for the mean expected number of dolphins:\n\npredpts &lt;- fm_int(mexdolphin$mesh, mexdolphin$ppoly)\nLambda &lt;- predict(fit, predpts, ~ sum(weight * exp(space + Intercept)))\nLambda\n\n      mean       sd   q0.025     q0.5   q0.975   median sd.mc_std_err\n1 335.9973 100.7252 193.2297 317.6007 563.7969 317.6007      8.063935\n  mean.mc_std_err\n1        11.68531"
  },
  {
    "objectID": "slides/slides_13.html#results-predicted-expected-counts-1",
    "href": "slides/slides_13.html#results-predicted-expected-counts-1",
    "title": "Lecture",
    "section": "Results: predicted expected counts",
    "text": "Results: predicted expected counts\nWe can also get Monte Carlo samples for the expected number of dolphins as follows:\n\n\n\nNs &lt;- seq(50, 450, by = 1)\n\nNest &lt;- predict(fit, predpts,\n  ~ data.frame(\n    N = Ns,\n    density = dpois(\n      Ns,\n      lambda = sum(weight * exp(space + Intercept))\n    )\n  ),\n  n.samples = 2000\n)\n\nNest &lt;- dplyr::bind_rows(\n  cbind(Nest, Method = \"Posterior\"),\n  data.frame(\n    N = Nest$N,\n    mean = dpois(Nest$N, lambda = Lambda$mean),\n    mean.mc_std_err = 0,\n    Method = \"Plugin\"\n  )\n)"
  },
  {
    "objectID": "slides/slides_13.html#fitting-a-model-with-a-hazard-detection-function",
    "href": "slides/slides_13.html#fitting-a-model-with-a-hazard-detection-function",
    "title": "Lecture",
    "section": "Fitting a Model with a hazard detection function",
    "text": "Fitting a Model with a hazard detection function\n\n\n\nhr &lt;- function(distance, sigma) {\n  1 - exp(-(distance / sigma)^-1)\n}\neta_2 &lt;- geometry + distance ~ space +\n  log(hr(distance, sigma)) +\n  Intercept + log(2)\n\nlik_2 = bru_obs(\"cp\",\n              formula = eta_2,\n              data = mexdolphin$points,\n              ips = ips)\n\n# fit the model\nfit_hazard = bru(cmp, lik_2)\n\n\n\npr.int1 &lt;- predict(fit_hazard, pxl, ~ data.frame(spatial = space,\n                                      loglambda = Intercept + space,\n                                      lambda = exp(Intercept + space)))\ndistdf &lt;- data.frame(distance = seq(0, 8, length.out = 100))\ndfun1 &lt;- predict(fit_hazard, distdf, ~ hr(distance, sigma))\n\nggplot() +\n  gg(pr.int1$loglambda, geom = \"tile\") +\n  scale_fill_scico(palette=\"imola\",name=expression(log(lambda)))+\n  plot(dfun1)+plot_layout(ncol=1)"
  },
  {
    "objectID": "slides/slides_13.html#model-comparison-with-gof",
    "href": "slides/slides_13.html#model-comparison-with-gof",
    "title": "Lecture",
    "section": "Model comparison with GoF",
    "text": "Model comparison with GoF\nLook at the goodness-of-fit of the two models in the distance dimension\n\n\n\nbc &lt;- bincount(\n  result = fit,\n  observations = mexdolphin$points$distance,\n  breaks = seq(0, max(mexdolphin$points$distance), length.out = 9),\n  predictor = distance ~ hn(distance, sigma)\n)\nattributes(bc)$ggp\n\n\n\n\n\n\n\n\n\n\nbc1 &lt;- bincount(\n  result = fit_hazard,\n  observations = mexdolphin$points$distance,\n  breaks = seq(0, max(mexdolphin$points$distance), length.out = 9),\n  predictor = distance ~ hn(distance, sigma)\n)\nattributes(bc1)$ggp\n\n\n\n\n\n\n\n\n\n\nCompared the observed binned counts against expected number of detections within a given distance"
  },
  {
    "objectID": "slides/slides_10.html#motivation",
    "href": "slides/slides_10.html#motivation",
    "title": "Lecture ??",
    "section": "Motivation",
    "text": "Motivation\n\n“All models are wrong, some models are useful” — George Box\n\n\n\nHow do we check that our model fits our data? - Model Validation\n\n\n\n\nHow do we choose the best model? - Model Comparison\n\n\n\nBut..it is not always easy to distinguish between the two!"
  },
  {
    "objectID": "slides/slides_10.html#bayesian-model-comparison-1",
    "href": "slides/slides_10.html#bayesian-model-comparison-1",
    "title": "Lecture ??",
    "section": "Bayesian Model Comparison",
    "text": "Bayesian Model Comparison\n\nThere is no golden standard\nIt really depends what you want to do!\nBasically two types\n\nOnes that look at the posterior probability of the data under the model\nOnes that look at how model the data fits the data\n\nIn general it is not an easy task\ninlabru provides some options available"
  },
  {
    "objectID": "slides/slides_10.html#model-comparisonvalidation-in-inlabru",
    "href": "slides/slides_10.html#model-comparisonvalidation-in-inlabru",
    "title": "Lecture ??",
    "section": "Model Comparison/Validation in inlabru",
    "text": "Model Comparison/Validation in inlabru\n\nCriteria of fit\n\nMarginal likelihood ⇒Bayes factors\nDeviance information criterion (DIC)\nWidely applicable information criterion (WAIC)\n\nThere are also some predictive checks for the model:\n\nConditional predictive ordinate (CPO)\nProbability integral transform (PIT)\n\nYou can sample from the posterior using generate() and compute\n\nlog-scores\nCRPS\n…"
  },
  {
    "objectID": "slides/slides_10.html#marginal-likelihood",
    "href": "slides/slides_10.html#marginal-likelihood",
    "title": "Lecture ??",
    "section": "Marginal likelihood",
    "text": "Marginal likelihood\n\n# tell inlabru you want to compute mlik\nbru_options_set(control.compute = list(mlik = TRUE))\nfit = bru(cmp, lik)\n# see the results\nfit$mlik\n\n                                           [,1]\nlog marginal-likelihood (integration) -171.8930\nlog marginal-likelihood (Gaussian)    -172.2766\n\n\n\n\nCalculates \\(\\log(\\pi(\\mathbf{y}))\\)\nCan calculate Bayes factors through differences in value\nNB: Problematic for intrinsic models"
  },
  {
    "objectID": "slides/slides_10.html#deviance-information-criteria-dic",
    "href": "slides/slides_10.html#deviance-information-criteria-dic",
    "title": "Lecture ??",
    "section": "Deviance Information Criteria (DIC)",
    "text": "Deviance Information Criteria (DIC)\n\n# tell inlabru you want to compute DIC\nbru_options_set(control.compute = list(dic = TRUE))\nfit = bru(cmp, lik)\n# see the results\nfit$dic$dic\n\n[1] 305.5565\n\n\n\n\nMeasure of complexity and fit.\nDefined as: \\(\\text{DIC} = \\bar{D} + p_D\\)\n\n\\(\\bar{D}\\) is the posterior mean of the deviance\n\\(p_D\\) is the effective number of parameters.\n\nSmaller values indicate better trade-off between complexity and fit."
  },
  {
    "objectID": "slides/slides_10.html#widely-applicable-information-criterion-waic",
    "href": "slides/slides_10.html#widely-applicable-information-criterion-waic",
    "title": "Lecture ??",
    "section": "Widely applicable information criterion (WAIC)",
    "text": "Widely applicable information criterion (WAIC)\nalso known as Watanabe–Akaike information criterion.\n\n# tell inlabru you want to compute WAIC\nbru_options_set(control.compute = list(waic = TRUE))\nfit = bru(cmp, lik)\n# see the results\nfit$waic$waic\n\n[1] 305.6304\n\n\n\n\nsimilar to the DIC…but maybe better1\nLinked to the leave-one-out crossvalidation\nSmaller values indicate better trade-off between complexity and fit\n\nSee “Understanding predictive information criteria for Bayesian models” (2013) by Andrew Gelman, Jessica Hwang, and Aki Vehtari"
  },
  {
    "objectID": "slides/slides_10.html#posterior-predictive-distribution",
    "href": "slides/slides_10.html#posterior-predictive-distribution",
    "title": "Lecture ??",
    "section": "Posterior predictive distribution",
    "text": "Posterior predictive distribution\nRemember: Our GLM is defined as: \\[\n\\begin{eqnarray}\n\\pi(\\mathbf{y}|\\mathbf{u},\\theta) & =\\prod_i \\pi(y_i|\\mathbf{u},\\theta)& \\ \\text{  likelihood}\\\\\n\\pi(\\mathbf{u}|\\theta)& &\\ \\text{  LGM}\\\\\n\\pi(\\theta )& &\\ \\text{  hyperprior}\\\\\n\\end{eqnarray}\n\\] using inlabru we estimate the posterior distribution \\(\\pi(\\mathbf{u},\\theta|\\mathbf{y})\\).\nThe posterior predictive distribution for a new data \\(\\hat{y}\\) is then: \\[\n\\pi(\\hat{y}|\\mathbf{y}) = \\int\\pi(y_i|\\mathbf{u},\\theta)\\pi(\\mathbf{u},\\theta|\\mathbf{y})\\ d\\mathbf{u}\\ d\\theta\n\\]\nThis ditribution can be used to check the model fit!"
  },
  {
    "objectID": "slides/slides_10.html#posterior-predictive-distribution-1",
    "href": "slides/slides_10.html#posterior-predictive-distribution-1",
    "title": "Lecture ??",
    "section": "Posterior predictive distribution",
    "text": "Posterior predictive distribution\n\\[\n\\pi(\\hat{y}|\\mathbf{y}) = \\int\\pi(y_i|\\mathbf{u},\\theta)\\pi(\\mathbf{u},\\theta|\\mathbf{y})\\ d\\mathbf{u}\\ d\\theta\n\\]\nNOTE: In general this is NOT computed by inlabru but needs to be approximated\n\nUse generate to sample from the posterior \\((\\mathbf{u}^*_i,\\theta^*_i)\\sim\\pi(\\mathbf{u},\\theta|\\mathbf{y})\\)\nSimulate a new datapoint \\(y^*_i\\sim(y_i|\\mathbf{u}^*_i,\\theta^*_i)\\)\nUse \\(y^*_1,\\dots, y^*_N\\) to approximate the posterior predictive distribution.\n\n\nBUT inlabru computes automatically two quantities that are useful for model check!\n\nConditional predictive ordinate (CPO)\nProbability integral transform (PIT)"
  },
  {
    "objectID": "slides/slides_10.html#conditional-predictive-ordinate-cpo",
    "href": "slides/slides_10.html#conditional-predictive-ordinate-cpo",
    "title": "Lecture ??",
    "section": "Conditional predictive ordinate (CPO)",
    "text": "Conditional predictive ordinate (CPO)\nDefinition: \\[\ncpo_i = \\pi(y^{obs}_i|\\mathbf{y}_{-i})\n\\]\n\nIntroduced in Pettit (1990)1\nMeasures fit through the predictive density\nCan be used to compute the log-score as \\[\n\\text{Score} = -\\sum \\log(cpo_i)\n\\] lower score correspond to better models\n\nPettit, L. I. 1990. “The Conditional Predictive Ordinate for the Normal Distribution.” Journal of the Royal Statistical Society. Series B (Methodological)"
  },
  {
    "objectID": "slides/slides_10.html#probability-integral-transform-pit",
    "href": "slides/slides_10.html#probability-integral-transform-pit",
    "title": "Lecture ??",
    "section": "Probability integral transform (PIT)",
    "text": "Probability integral transform (PIT)\nHow to compute:\n\n# tell inlabru you want to compute DIC\nbru_options_set(control.compute = list(cpo = TRUE))\nfit = bru(cmp, lik)\n# see the results\nhead(fit$cpo$cpo)\n\n[1] 0.2227513 0.3299944 0.3606176 0.3096869 0.1151661 0.3024391\n\n\nNote it is possible to check for possible fails in computed CPOs\n\nhead(fit$cpo$failure)\n\n[1] 0 0 0 0 0 0"
  },
  {
    "objectID": "slides/slides_10.html#probability-integral-transform-pit-1",
    "href": "slides/slides_10.html#probability-integral-transform-pit-1",
    "title": "Lecture ??",
    "section": "Probability integral transform (PIT)",
    "text": "Probability integral transform (PIT)\nDefinition: \\[\npit_i = \\text{Prob}(\\hat{y}_i&lt;y_i|\\mathbf{y}_{-i})\n\\]\n\nLinked to leave-one-out cross-validation\n\\(pit_i\\) shows how well the ith data point is predicted by the rest of the data\nVery small values indicate “suprising” observation under the model\nFor well-calibrated, the PIT values should be approximately uniformly distributed."
  },
  {
    "objectID": "slides/slides_10.html#probability-integral-transform-pit-2",
    "href": "slides/slides_10.html#probability-integral-transform-pit-2",
    "title": "Lecture ??",
    "section": "Probability integral transform (PIT)",
    "text": "Probability integral transform (PIT)\nHow to compute:\n\n# tell inlabru you want to compute DIC\nbru_options_set(control.compute = list(cpo = TRUE))\nfit = bru(cmp, lik)\n# see the results\nhead(fit$cpo$pit)\n\n[1] 0.83437082 0.67072028 0.49111106 0.28561306 0.06350822 0.72495860"
  },
  {
    "objectID": "slides/slides_10.html#good-and-bad-pit-plots",
    "href": "slides/slides_10.html#good-and-bad-pit-plots",
    "title": "Lecture ??",
    "section": "Good and Bad PIT plots",
    "text": "Good and Bad PIT plots"
  },
  {
    "objectID": "slides/slides_10.html#other-scores",
    "href": "slides/slides_10.html#other-scores",
    "title": "Lecture ??",
    "section": "Other scores",
    "text": "Other scores\nIn the literature there are many proposed scores for evaluate predictions. For example:\n\nDawid-Sebastian score\nLog-score\nContinuous rank probility score (CRPS)\nBrier score\n…\n\nThey all have their strength and wakness and which one is better depends on the goals of the model.\ninlabru does not provide such scores automatcally, but they can be computed using simulations from the posterior distribution."
  },
  {
    "objectID": "slides/slides_10.html#example-crps-for-poisson-data",
    "href": "slides/slides_10.html#example-crps-for-poisson-data",
    "title": "Lecture ??",
    "section": "Example: CRPS for Poisson data",
    "text": "Example: CRPS for Poisson data\nOur model: \\[\n\\begin{eqnarray}\ny_i|\\lambda_i & \\sim \\text{Poisson}(\\lambda_i),&\\ i = 1,\\dots,N_{\\text{data}}\\\\\n\\log(\\lambda_i)  = \\eta_i &= \\beta_0 + \\beta_1 x_i\n\\end{eqnarray}\n\\]\nSimulate data and fit the model:\n\ndf_pois &lt;- data.frame(\n  x = rnorm(50),\n  y = rpois(length(x), exp(2 + 1 * x))\n)\ncmp = ~ Intercept(1) + cov(x, model = \"linear\")\nlik = bru_obs(formula = y~.,\n              family = \"poisson\",\n              data = df_pois)\nfit_pois &lt;- bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_10.html#example-crps-for-poisson-data-1",
    "href": "slides/slides_10.html#example-crps-for-poisson-data-1",
    "title": "Lecture ??",
    "section": "Example: CRPS for Poisson data",
    "text": "Example: CRPS for Poisson data\nThe CRPS score is defined as: \\[\n\\text{S}_{\\text{CRPS}}(F_i, y_i) = \\sum_{k=0}^\\infty\\left[\\text{Prob}(Y_i\\leq k|\\mathbf{y})-I(y_i\\leq k)\\right]^2\n\\]\nComputational algorithm:\n\nSimulate \\(\\lambda^{(j)}\\sim p(\\lambda|\\text{data}), j = 1,\\dots, N_{\\text{samples}}\\) using generate() (size \\(N\\times N_\\text{samples}\\)).\nFor each \\(i=1,\\dots,N_{\\text{data}}\\), estimate \\(r_{ik}=\\text{Prob}(Y\\leq k|\\text{data})-I(y_i\\leq k)\\) as \\[\n  \\hat{r}_{ik} = \\frac{1}{N_\\text{samples}} \\sum_{j=1}^{N_\\text{samples}}\n  \\{\n  \\text{Prob}(Y\\leq k|\\lambda^{(j)}_i)-I(y_i\\leq k)\n  \\} .\n\\]\nCompute \\[\n  S_\\text{CRPS}(F_i,y_i) = \\sum_{k=0}^{K} \\hat{r}_{ik}^2\n\\]"
  },
  {
    "objectID": "slides/slides_10.html#example-crps-for-poisson-data-2",
    "href": "slides/slides_10.html#example-crps-for-poisson-data-2",
    "title": "Lecture ??",
    "section": "Example: CRPS for Poisson data",
    "text": "Example: CRPS for Poisson data\nImplementation:\n\n# some large value, so that 1-F(K) is small\nmax_K &lt;- ceiling(max(df_pois$y) + 4 * sqrt(max(df_pois$y)))\nk &lt;- seq(0, max_K)\nkk &lt;- rep(k, times = length(df_pois$y))\ni &lt;- seq_along(df_pois$y)\npred_pois &lt;- generate(fit_pois, df_pois,\n  formula = ~ {\n    lambda &lt;- exp(Intercept + x)\n    ppois(kk, lambda = rep(lambda, each = length(k)))\n  },\n  n.samples = 2000\n)\nresults &lt;- data.frame(\n  i = rep(i, each = length(k)),\n  k = kk,\n  Fpred = rowMeans(pred_pois),\n  residuals =\n    rowMeans(pred_pois) - (rep(df_pois$y, each = length(k)) &lt;= kk)\n)\n\ncrps_scores &lt;-\n  (results %&gt;%\n    group_by(i) %&gt;%\n    summarise(crps = sum(residuals^2), .groups = \"drop\") %&gt;%\n    pull(crps))\nsummary(crps_scores)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.4159  2.9192 10.1177 14.6513 19.8971 82.9191"
  },
  {
    "objectID": "slides/slides_10.html#residuals-frequentist-vs-bayesian",
    "href": "slides/slides_10.html#residuals-frequentist-vs-bayesian",
    "title": "Lecture ??",
    "section": "Residuals: Frequentist vs Bayesian",
    "text": "Residuals: Frequentist vs Bayesian\n\n\n🎯 Frequentist View\n\nModel parameters are fixed but unknown.\nFitted values (predictions): \\(\\hat{y}_i\\) are point estimates\nResiduals:\n\\[\nr_i = y_i - \\hat{y}_i\n\\]\nSingle number per data point.\nUsed for:\n\nChecking model fit / outliers\n\n\n\n🔮 Bayesian View"
  },
  {
    "objectID": "slides/slides_10.html#residuals-frequentist-vs-bayesian-1",
    "href": "slides/slides_10.html#residuals-frequentist-vs-bayesian-1",
    "title": "Lecture ??",
    "section": "Residuals: Frequentist vs Bayesian",
    "text": "Residuals: Frequentist vs Bayesian\n\n\n🎯 Frequentist View\n\nModel parameters are fixed but unknown.\nFitted values (predictions): \\(\\hat{y}_i\\) are point estimates\nResiduals:\n\\[\nr_i = y_i - \\hat{y}_i\n\\]\nSingle number per data point.\nUsed for:\n\nChecking model fit / outliers\n\n\n\n🔮 Bayesian View\n\nParameters are random variables with posterior \\(\\pi(\\theta \\mid y)\\).\nPredictions \\(\\tilde{y}_i\\) also have a posterior distribution.\nNo single “true” fitted value → residuals are not uniquely defined.\n\n\\(r_i^{(\\text{mean})} = (y_i - E[\\tilde{y}_i \\mid y])\\) (mean residual)\n\n\\(r_i^{(\\text{sample})} = (y_i - \\tilde{y}^s)\\) for posterior sample \\(s\\)\nDistribution of \\(y_i - \\tilde{y}_i^{(s)}\\) (posterior residuals)\n\n\n\n\n\nA better option is to use posterior predictive checks1\n\nSee Gelman et. al (2020) “Bayesian Workflow”"
  },
  {
    "objectID": "slides/slides_10.html#one-example-of-posterior-predictive-checks",
    "href": "slides/slides_10.html#one-example-of-posterior-predictive-checks",
    "title": "Lecture ??",
    "section": "One example of posterior predictive checks",
    "text": "One example of posterior predictive checks\n\n\n\\[\ny_i|\\eta_i\\sim\\mathcal{N}(\\eta_i, \\sigma^2)\n\\]\n\nModel 1 \\[\n\\eta_i = \\beta_0 + \\beta_1 x_i\n\\]\nModel 2 \\[\n\\eta_i = \\beta_0 +  f(x_i)\n\\]"
  },
  {
    "objectID": "slides/slides_10.html#one-example-of-posterior-predictive-checks-1",
    "href": "slides/slides_10.html#one-example-of-posterior-predictive-checks-1",
    "title": "Lecture ??",
    "section": "One example of posterior predictive checks",
    "text": "One example of posterior predictive checks\n\nSample \\(y^{1k}_i\\sim\\pi(y_i|\\mathbf{y})\\) \\(k = 1,\\dots,M\\) using generate()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSamples\n\n\n\n\n1\n2\n3\n4\n...\nM\n\n\n\n\n$y_1$\n5.45\n5.96\n5.54\n3.3\n...\n4.9\n\n\n$y_3$\n2.77\n2.17\n4.02\n2.64\n...\n3.47\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n$y_N$\n5.57\n4.86\n6.12\n5.04\n...\n5.31\n\n\n\n\n\n\n\n\nCompare some summaries of the simulated data with the one of the observed one"
  },
  {
    "objectID": "slides/slides_10.html#one-example-of-posterior-predictive-checks-2",
    "href": "slides/slides_10.html#one-example-of-posterior-predictive-checks-2",
    "title": "Lecture ??",
    "section": "One example of posterior predictive checks",
    "text": "One example of posterior predictive checks\nHere we compare the estimated posterior densities \\(\\hat{\\pi}^k(y|\\mathbf{y})\\) with the estimated data density\n\n\nThis is just a simple example, but more complex checks can be computed with the same idea!"
  },
  {
    "objectID": "slides/slides_10.html#leave-group-our-cross-validation",
    "href": "slides/slides_10.html#leave-group-our-cross-validation",
    "title": "Lecture ??",
    "section": "Leave Group Our Cross-Validation",
    "text": "Leave Group Our Cross-Validation\nThis is a new option for cross-validation in inlabru . . .\n\nLeave-one-out cross-validation (LOOCV) is a very common technique to evaluate predictions from models\nWhen data are correlated (as in spatial statistics) LOOCV might be too optimistic and overestimate model performances.\n\n\n\nOne possible solution is to then remove “chunck(s)” of data (for example one station and all its nearest neighbours)\nThis is the solution implemented in the inla.group.cv() function1\n\n\nAdin & al (2024) Automatic cross-validation in structured models: Is it time to leave out leave-one-out?, Spat. Statistics and Liu & al. (2022) Leave-group-out cross-validation for latent Gaussian models"
  },
  {
    "objectID": "slides/slides_10.html#model-validation-for-lgcp-1",
    "href": "slides/slides_10.html#model-validation-for-lgcp-1",
    "title": "Lecture ??",
    "section": "Model validation for LGCP",
    "text": "Model validation for LGCP\nPoint processes (and so LGCP) are different from all other spatial models:\n\nThe data are presence and absence of points\nThe likelihood depends on the data location and the integrated intensity over the whole domain\n\n\nThis makes model evaluation especially challenging. Especially cross-validation based measures are hard to define… why?"
  },
  {
    "objectID": "slides/slides_10.html#cross-validation-for-point-processes",
    "href": "slides/slides_10.html#cross-validation-for-point-processes",
    "title": "Lecture ??",
    "section": "Cross validation for point processes",
    "text": "Cross validation for point processes\n\n\n\nIn a Point process also empty areas are “data”\nWe cannot just remove points as this will change the underlying intensity\nWe need to remove a whole subdomain in order to cross-validate"
  },
  {
    "objectID": "slides/slides_10.html#a-warning-note",
    "href": "slides/slides_10.html#a-warning-note",
    "title": "Lecture ??",
    "section": "A warning note!",
    "text": "A warning note!\nWarning⚠️\n\nDo not use WAIC and DIC as computed today by inlabru to compare LGCP models\n\n\nWAIC is linked to leave-one-out crossvalidation therefore it is ill-defined for point processes\nDIC is ok “in theory” but not the way it is computed today\n\n\nWork is going on about this and measures of fit for LGCP will soon be available in inlabru"
  },
  {
    "objectID": "slides/slides_10.html#some-ideas-on-what-one-can-do",
    "href": "slides/slides_10.html#some-ideas-on-what-one-can-do",
    "title": "Lecture ??",
    "section": "Some ideas on what one can do",
    "text": "Some ideas on what one can do\n\nIt is possible to define residuals for PP, for example as: \\[\n\\hat{R}_B = n(B)- \\int_B\\hat{\\lambda}(s)\\ ds\n\\] where\n\\(n(B)\\) is the number of observed points in \\(B\\)\n\\(\\hat{\\lambda}(s)\\) is the estimated intensity\n\nThese residuals can be used to evaluate the model.\n\nThis is still work in progress and at the moment non easily available… but it will be 😀\nHere you can see some examples of computation and use."
  },
  {
    "objectID": "slides/slides_10.html#take-home-messages",
    "href": "slides/slides_10.html#take-home-messages",
    "title": "Lecture ??",
    "section": "Take home messages",
    "text": "Take home messages\n\nModel check and model comparison are complex topics\nThere are no universal solutions, it all depends on which model characteristics you are interested in.\ninlabru provides some easy to compute alternatives\nLGCP require own tools to validate the model\n\nwork is ongoing here… stay tuned 😀"
  },
  {
    "objectID": "slides/slides_3.html#outline",
    "href": "slides/slides_3.html#outline",
    "title": "Lecture 2",
    "section": "Outline",
    "text": "Outline\n\nLatent Gaussian Models"
  },
  {
    "objectID": "slides/slides_3.html#repetition",
    "href": "slides/slides_3.html#repetition",
    "title": "Lecture 2",
    "section": "Repetition",
    "text": "Repetition\nEverything in R-INLA is based on so-called latent Gaussian models\n\n— A few hyperparameters \\(\\theta\\sim\\pi(\\theta)\\) control variances, range and so on\n— Given these hyperparameters we have an underlying Gaussian distribution \\(\\mathbf{u}|\\theta\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{Q}^{-1}(\\theta))\\) that we cannot directly observe\n— Instead we make indirect observations \\(\\mathbf{y}|\\mathbf{u},\\theta\\sim\\pi(\\mathbf{y}|\\mathbf{u},\\theta)\\) of the underlying latent Gaussian field"
  },
  {
    "objectID": "slides/slides_3.html#repetition-1",
    "href": "slides/slides_3.html#repetition-1",
    "title": "Lecture 2",
    "section": "Repetition",
    "text": "Repetition\nModels of this kind: \\[\n\\begin{aligned}\n\\mathbf{y}|\\mathbf{x},\\theta &\\sim \\prod_i \\pi(y_i|\\eta_i,\\theta)\\\\\n\\mathbf{\\eta} & = A_1\\mathbf{u}_1 + A_2\\mathbf{u}_2+\\dots + A_k\\mathbf{u}_k\\\\\n\\mathbf{u},\\theta &\\sim \\mathcal{N}(0,\\mathbf{Q}(θ)^{−1})\\\\\n\\theta & \\sim \\pi(\\theta)\n\\end{aligned}\n\\]\noccurs in many, seemingly unrelated, statistical models."
  },
  {
    "objectID": "slides/slides_3.html#examples",
    "href": "slides/slides_3.html#examples",
    "title": "Lecture 2",
    "section": "Examples",
    "text": "Examples\n\nGeneralised linear (mixed) models\nStochastic volatility\nGeneralised additive (mixed) models\nMeasurement error models\nSpline smoothing\nSemiparametric regression\nSpace-varying (semiparametric) regression models\nDisease mapping\nLog-Gaussian Cox-processes\nModel-based geostatistics (*)\nSpatio-temporal models\nSurvival analysis\n+++"
  },
  {
    "objectID": "slides/slides_3.html#main-characteristics",
    "href": "slides/slides_3.html#main-characteristics",
    "title": "Lecture 2",
    "section": "Main Characteristics",
    "text": "Main Characteristics\n\nLatent Gaussian model\nThe data are conditionally independent given the latent field\nThe predictor is linear1\nThe dimension of \\(\\mathbf{u}\\) can be big (\\(10^3-10^6\\))\nThe dimension of \\(\\theta\\) should be not too big.\n\n\n\n\n\n \n\n\n\n\n\nwe will see that this can be partly relaxed 😃"
  },
  {
    "objectID": "slides/slides_1.html#course-structure-day-1",
    "href": "slides/slides_1.html#course-structure-day-1",
    "title": "inlabru workshop",
    "section": "Course Structure: Day 1",
    "text": "Course Structure: Day 1\n\n\n\n\n\n\n\n\n\nTime\nTopic\nContent\nExcercises\n\n\n\n\nXXXX am\nCore concepts\n\nLGM and INLA\ninlabru workflow\nModel selection\n\n\n\n\nXXXX am\nTemporal Models\n\nDiscrete time models\nContinuous time models"
  },
  {
    "objectID": "slides/slides_1.html#course-structure-day-2",
    "href": "slides/slides_1.html#course-structure-day-2",
    "title": "inlabru workshop",
    "section": "Course Structure: Day 2",
    "text": "Course Structure: Day 2\n\n\n\n\n\n\n\n\n\nTime\nTopic\nContent\nExcercises\n\n\n\n\nXXXX am\nIntroduction to Spatial Modelling\n\nTypes of spatial data\nSpatial data wrangling and manipulation in R (e.g, terra & sf)\nAreal processes\n\n\n\n\nXXXX am\nModelling geostatistical data\n\nSPDE & the mesh\nGeostatistical Data\nSpatial predictions"
  },
  {
    "objectID": "slides/slides_1.html#course-structure-day-3",
    "href": "slides/slides_1.html#course-structure-day-3",
    "title": "inlabru workshop",
    "section": "Course Structure: Day 3",
    "text": "Course Structure: Day 3\n\n\n\n\n\n\n\n\n\nTime\nTopic\nContent\nExcercises\n\n\n\n\nXXXX am\nSpatial Point processes\n\nSpatial point process\nDistance sampling\n\n\n\n\nXXXX am\nSpatiotemporal Models\n\nSeparable time-space models\nnon-separable space-time models"
  },
  {
    "objectID": "slides/slides_1.html#course-structure-day-4",
    "href": "slides/slides_1.html#course-structure-day-4",
    "title": "inlabru workshop",
    "section": "Course Structure: Day 4",
    "text": "Course Structure: Day 4\n\n\n\n\n\n\n\n\n\nTime\nTopic\nContent\nExcercises\n\n\n\n\nXXXX am\nMultilikelihood and Non-linear models\n\niterated inla\nlogistic growth\nCorregionalization models"
  },
  {
    "objectID": "day4_practical_6.html",
    "href": "day4_practical_6.html",
    "title": "Practical 6",
    "section": "",
    "text": "Aim of this practical:\nIn this practical we are going to look at some model comparison and validation techniques.\nDownload Practical 6 R script",
    "crumbs": [
      "Home",
      "Practical 6"
    ]
  },
  {
    "objectID": "day4_practical_6.html#model-checking-for-linear-models",
    "href": "day4_practical_6.html#model-checking-for-linear-models",
    "title": "Practical 6",
    "section": "Model Checking for Linear Models",
    "text": "Model Checking for Linear Models\nIn this exercise we will:\n\nLearn about some model assessments techniques available in INLA\nConduct posterior predictive model checking\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nRecall a simple linear regression model with Gaussian observations\n\\[\ny_i\\sim\\mathcal{N}(\\mu_i, \\sigma^2), \\qquad i = 1,\\dots,N\n\\]\nwhere \\(\\sigma^2\\) is the observation error, and the mean parameter \\(\\mu_i\\) is linked to the linear predictor through an identity function:\n\\[\n\\eta_i = \\mu_i = \\beta_0 + \\beta_1 x_i\n\\] where \\(x_i\\) is a covariate and \\(\\beta_0, \\beta_1\\) are parameters to be estimated.\n\nSimulate example data\nWe simulate data from a simple linear regression model\n\n\nCode\nbeta = c(2,0.5)\nsd_error = 0.1\n\nn = 100\nx = rnorm(n)\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error)\n\ndf = data.frame(y = y, x = x)  \n\n\n\n\nFitting the linear regression model with inlabru\nNow we fit a simple linear regression model in inalbru by defining (1) the model components, (2) the linear predictor and (3) the likelihood.\n\n# Model components\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n# Linear predictor\nformula = y ~ Intercept + beta_1\n# Observational model likelihood\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n# Fit the Model\nfit.lm = bru(cmp, lik)\n\n\n\nResiduals analysis\nA common way for model diagnostics in regression analysis is by checking residual plots. In a Bayesian setting residuals can be defined in multiple ways depending on how you account for posterior uncertainty. Here, we will adopt a Bayesian approach by generating samples from the posterior distribution of the model parameters and then draw samples from the residuals defined as:\n\\[\nr_i = y_i - x_i^T\\beta\n\\]\nWe can use the predict function to achieve this:\n\nres_samples &lt;- predict(\n  fit.lm,         # the fitted model\n  df,             # the original data set\n  ~ data.frame(   \n    res = y-(beta_0 + beta_1)  # compute the residuals\n  ),\n  n.samples = 1000   # draw 1000 samples\n)\n\nThe resulting data frame contains the posterior draw of the residuals mean for which we can produce some diagnostics plots , e.g.\n\n\nResiduals checks for Linear Model\nggplot(res_samples,aes(y=mean,x=1:100))+geom_point() +\nggplot(res_samples,aes(y=mean,x=x))+geom_point()\n\n\n\n\n\nBayesian residual plots: the left panel is the residual index plot; the right panel is the plot of the residual versus the covariate x\n\n\n\n\nWe can also compare these against the theoretical quantiles of the Normal distribution as follows:\n\n\nQQPlot for Linear Model\narrange(res_samples, mean) %&gt;%\n  mutate(theortical_quantiles = qnorm(1:100 / (1+100))) %&gt;%\n  ggplot(aes(x=theortical_quantiles,y= mean)) + \n  geom_ribbon(aes(ymin = q0.025, ymax = q0.975), fill = \"grey70\")+\n  geom_abline(intercept = mean(res_samples$mean),\n              slope = sd(res_samples$mean)) +\n  geom_point() +\n  labs(x = \"Theoretical Quantiles (Normal)\",\n       y= \"Sample Quantiles (Residuals)\") \n\n\n\n\n\n\n\n\n\n\n\nPosterior Predictive Checks\nNow, instead of generating samples from the mean, we will account for the observational process uncertainty by:\n\nSampling \\(y^{1k}_i\\sim\\pi(y_i|\\mathbf{y})\\) \\(k = 1,\\dots,M;~i = 1,\\ldots,100\\) using generate() (here we will draw \\(M=500\\) samples)\n\n\nsamples =  generate(fit.lm, df,\n  formula = ~ {\n    mu &lt;- (beta_0 + beta_1)\n    sd &lt;- sqrt(1 / Precision_for_the_Gaussian_observations)\n    rnorm(100, mean = mu, sd = sd)\n  },\n  n.samples = 500\n) \n\n\nComparing some summaries of the simulated data with the one of the observed one\n\nHere we compare (i) the estimated posterior densities \\(\\hat{\\pi}^k(y|\\mathbf{y})\\) with the estimated data density and (ii) the samples means and 95% credible intervals against the observations.\n\n# Tidy format for plotting\nsamples_long = data.frame(samples) %&gt;% \n  mutate(id = 1:100) %&gt;% # i-th observation\n  pivot_longer(-id)\n\n# compute the mean and quantiles for the samples\ndraws_summaries = data.frame(mean_samples = apply(samples,1,mean),\nq25 = apply(samples,1,function(x)quantile(x,0.025)),  \nq975 = apply(samples,1,function(x)quantile(x,0.975)),\nobservations = df$y)  \n\np1 = ggplot() + geom_density(data = samples_long, \n                        aes(value, group = name),  color = \"#E69F00\") +\n  geom_density(data = df, aes(y))  +\n  xlab(\"\") + ylab(\"\") \n\np2 = ggplot(draws_summaries,aes(y=mean_samples,x=observations))+\n  geom_errorbar(aes(ymin = q25,\n                   ymax = q975), \n               alpha = 0.5, color = \"grey50\")+\ngeom_point()+geom_abline(slope = 1,intercept = 0,lty=2)+labs()\n\np1 +p2",
    "crumbs": [
      "Home",
      "Practical 6"
    ]
  },
  {
    "objectID": "day4_practical_6.html#sec-linmodel",
    "href": "day4_practical_6.html#sec-linmodel",
    "title": "Practical 6",
    "section": "GLM model checking",
    "text": "GLM model checking\nIn this exercise we will:\n\nLearn about some model assessments techniques available in INLA\nConduct posterior predictive model checking using CPO and PIT\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nIn this exercise, we will use data on horseshoe crabs (Limulus polyphemus) where the number of satellites males surrounding a breeding female are counted along with the female’s color and carapace width.\n Download data set \nA possible model to study the factors that affect the number of satellites for female crabs is\n\\[\n\\begin{aligned}\ny_i&\\sim\\mathrm{Poisson}(\\mu_i), \\qquad i = 1,\\dots,N \\\\\n\\eta_i &= \\mu_i = \\beta_0 + \\beta_1 x_i + \\ldots\n\\end{aligned}\n\\]\nWe can explore the conditional means and variances given the female’s color:\n\ncrabs &lt;- read.csv(\"datasets/crabs.csv\")\n\n# conditional means and variances\ncrabs %&gt;%\n  summarise( Mean = mean(satell ),\n             Variance = var(satell),\n                     .by = color)\n\n   color     Mean  Variance\n1 medium 3.294737 10.273908\n2   dark 2.227273  6.737844\n3  light 4.083333  9.719697\n4 darker 2.045455 13.093074\n\n\nThe mean of the number of satellites vary by color which gives a good indication that color might be useful for predicting satellites numbers. However, notice that the mean is lower than its variance suggesting that overdispersion might be present and that a negative binomial model would be more appropriate for the data (we will cover this later).\nFitting the model\nFirst, lets begin fitting the Poisson model above using the carapace’s color and width as predictors. Since, color is a categorical variable in our model we need to create a dummy variable for it. We can use the model.matrix function to help us constructing the design matrix and then append this to our data:\n\ncrabs_df = model.matrix( ~  color , crabs) %&gt;%\n  as.data.frame() %&gt;%\n  select(-1) %&gt;%        # drop intercept\n  bind_cols(crabs) %&gt;%  # append to original data\n  select(-color)        # remove original color categorical variable\n\nThe new data set crabs_df contains a dummy variable for the different color categories (dark being the reference category). Then we can fit the model in inlabru as follows:\n\ncmp =  ~ -1 + beta0(1) +  colordarker +\n       colorlight + colormedium +\n       w(weight, model = \"linear\")\n\nlik =  bru_obs(formula = satell ~.,\n            family = \"poisson\",\n            data = crabs_df)\n\nfit_pois = bru(cmp, lik)\n\nsummary(fit_pois)\n\ninlabru version: 2.13.0\nINLA version: 25.08.21-1\nComponents:\nbeta0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\ncolordarker: main = linear(colordarker), group = exchangeable(1L), replicate = iid(1L), NULL\ncolorlight: main = linear(colorlight), group = exchangeable(1L), replicate = iid(1L), NULL\ncolormedium: main = linear(colormedium), group = exchangeable(1L), replicate = iid(1L), NULL\nw: main = linear(weight), group = exchangeable(1L), replicate = iid(1L), NULL\nObservation models:\n  Family: 'poisson'\n    Tag: &lt;No tag&gt;\n    Data class: 'data.frame'\n    Response class: 'integer'\n    Predictor: satell ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[beta0, colordarker, colorlight, colormedium, w], latent[]\nTime used:\n    Pre = 0.379, Running = 0.296, Post = 0.0837, Total = 0.759 \nFixed effects:\n              mean    sd 0.025quant 0.5quant 0.975quant   mode kld\nbeta0       -0.501 0.196     -0.885   -0.501     -0.117 -0.501   0\ncolordarker -0.008 0.180     -0.362   -0.008      0.345 -0.008   0\ncolorlight   0.445 0.176      0.101    0.445      0.790  0.445   0\ncolormedium  0.248 0.118      0.017    0.248      0.479  0.248   0\nw            0.001 0.000      0.000    0.001      0.001  0.001   0\n\nMarginal log-Likelihood:  -489.43 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\n\nModel assessment and model choice\nNow that we have fitted the model we would like to carry some model assessments. In a Bayesian setting, this is often based on posterior predictive checks. To do so, we will use the CPO and PIT - two commonly used Bayesian model assessment criteria based on the posterior predictive distribution.\n\n\n\n\n\n\nPosterior predictive model checking\n\n\n\nThe posterior predictive distribution for a predicted value \\(\\hat{y}\\) is\n\\[\n\\pi(\\hat{y}|\\mathbf{y}) = \\int_\\theta \\pi(\\hat{y}|\\theta)\\pi(\\theta|\\mathbf{y})d\\theta.\n\\]\nThe probability integral transform (PIT) introduced by Dawid (1984) is defined for each observation as:\n\\[\n\\mathrm{PIT}_i = \\pi(\\hat{y}_i \\leq y_i |\\mathbf{y}{-i})\n\\]\nThe PIT evaluates how well a model’s predicted values match the observed data distribution. It is computed as the cumulative distribution function (CDF) of the observed data evaluated at each predicted value. If the model is well-calibrated, the PIT values should be approximately uniformly distributed. Deviations from this uniform distribution may indicate issues with model calibration or overfitting.\nAnother metric we could used to asses the model fit is the conditional predictive ordinate (CPO) introduced by Pettit (1990), and deﬁned as:\n\\[\n\\text{CPO}_i = \\pi(y_i| \\mathbf{y}{-i})\n\\]\nThe CPO measures the density of the observed value of \\(y_i\\) when model is fit using all data but \\(y_i\\). CPO provides a measure of how well the model predicts each individual observation while taking into account the rest of the data and the model. Large values indicate a better fit of the model to the data, while small values indicate a bad fitting of the model\n\n\nTo compute PIT and CPO we can either:\n\nask inlabru to compute them by set options = list(control.compute = list(cpo = TRUE)) in the bru() function arguments.\nset this as default in inlabru global option using the bru_options_set function.\n\nHere we will do the later and re-run the model\n\nbru_options_set(control.compute = list(cpo = TRUE))\n\nfit_pois = bru(cmp, lik)\n\nNow we can produce histograms and QQ plots to assess for uniformity in the PIT values which can be accessed through inlabru_model$cpo$pit :\n\nPlotR Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfit_pois$cpo$pit %&gt;%\n  hist(main = \"Histogram of PIT values\")\n\nqqplot(qunif(ppoints(length(fit_pois$cpo$pit))),\n       fit_pois$cpo$pit,\n       main = \"Q-Q plot for Unif(0,1)\",\n       xlab = \"Theoretical Quantiles\",\n       ylab = \"Sample Quantiles\")\n\nqqline(fit_pois$cpo$pit,\n       distribution = function(p) qunif(p),\n       prob = c(0.1, 0.9))\n\n\n\n\nBoth Q-Q plots and histogram of the PIT values suggest a not so great model fit. For the CPO values, usually the following summary of the CPO is often used:\n\\[\n-\\sum_{i=1}^n \\log (\\text{CPO}\\_i)\n\\]\nThis quantities is useful when comparing different models - a smaller values indicate a better model fit. CPO values can be accessed by typing inlabru_model$cpo$cpo.\n\n\n\n\n\n\n Task\n\n\n\nThe model assessment above suggests that a Poisson model might not be the most appropriate model, likely due to the overdispersion we detected previously. Fit a Negative binomial to relax the Poisson model assumption that the conditional mean and variance are equal. Then, compute the CPO summary statistic and PIT QQ plot to decide which model gives the better fit.\n\n\nTake hint\n\nTo specify a negative binomial model you only need to change the family distribution to family =  \"nbinomial\".\n\n\n\n\nClick here to see the solution\n\npar(mfrow=c(1,2))\n\n# Fit the negative binomial model\n\nlik_nbinom =  bru_obs(formula = satell ~.,\n            family = \"nbinomial\",\n            data = crabs_df)\n\nfit_nbinom = bru(cmp, lik_nbinom)\n\n# PIT checks\n\nfit_nbinom$cpo$pit %&gt;%\n  hist(main = \"Histogram of PIT values\")\n\nqqplot(qunif(ppoints(length(fit_nbinom$cpo$pit))),\n       fit_nbinom$cpo$pit,\n       main = \"Q-Q plot for Unif(0,1)\",\n       xlab = \"Theoretical Quantiles\",\n       ylab = \"Sample Quantiles\")\n\nqqline(fit_nbinom$cpo$pit,\n       distribution = function(p) qunif(p),\n       prob = c(0.1, 0.9))\n\n\n\n\n\n\n\n# CPO comparison\n\ndata.frame( CPO = c(-sum(log(fit_pois$cpo$cpo)),\n                    -sum(log(fit_nbinom$cpo$cpo))),\n          Model = c(\"Poisson\",\"Negative Binomial\"))\n\n       CPO             Model\n1 465.4061           Poisson\n2 379.3340 Negative Binomial\n\n# Overall, we can see that the negative binomial model provides a better fit to the data.",
    "crumbs": [
      "Home",
      "Practical 6"
    ]
  },
  {
    "objectID": "day4_practical_6.html#leave-group-out-cross-validation",
    "href": "day4_practical_6.html#leave-group-out-cross-validation",
    "title": "Practical 6",
    "section": "Leave-Group-Out Cross validation",
    "text": "Leave-Group-Out Cross validation\nIn this practical we are going to fit a geostatistical model. We will:\n\nLearn how to to compute model comparison in inlabru using LGOCV\n\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(inlabru) \nlibrary(sf)\nlibrary(terra)\n\n\n# load some libraries to generate nice map plots\nlibrary(scico)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(mapview)\nlibrary(tidyterra)\n\n\nThe data\nIn this practical, we will revisit the data on the Pacific Cod (Gadus macrocephalus) from a trawl survey in Queen Charlotte Sound. The pcod dataset is available from the sdmTMB package and contains the presence/absence records of the Pacific Cod during each surveys along with the biomass density of Pacific cod in the area swept (kg/Km\\(^2\\)). The qcs_grid data contain the depth values stored as \\(2\\times 2\\) km grid for Queen Charlotte Sound.\nThe dataset contains presence/absence data from 2003 to 2017. Lets consider year 2003 for now.\nWe first load the dataset and select the year of interest\n\nlibrary(sdmTMB)\n\npcod_df = sdmTMB::pcod %&gt;% filter(year==2003)\nqcs_grid = sdmTMB::qcs_grid\n\nThen, we create ab sf object and assign the rough coordinate reference to it:\n\npcod_sf =   st_as_sf(pcod_df, coords = c(\"lon\",\"lat\"), crs = 4326)\npcod_sf = st_transform(pcod_sf,\n          crs = \"+proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\" )\n\nWe convert the covariate into a raster and assign the same coordinate reference:\n\ndepth_r &lt;- rast(qcs_grid, type = \"xyz\")\ncrs(depth_r) &lt;- crs(pcod_sf)\n\n\n\n\n\n\n\n\n\n\n\n\nThe models\nHere, we will model the binary presence-absence data as a Binomial model of the form:\n\\[\n\\begin{aligned}\ny(s)|\\eta(s)&\\sim\\text{Binom}(1, p(s))\\\\\n\\eta(s) &= \\text{logit}(p(s)) \\\\\n\\omega(s) &\\sim \\text{  GF with range } \\rho\\  \\text{ and maginal variance }\\ \\sigma^2\n\\end{aligned}\n\\]\nWe will fit three models. One where we consider the observation as Bernoulli and where the linear predictor contains only one intercept and the GR field \\(\\omega(s)\\) defined through the SPDE approach. The other two models will also include depth as a linear and non-linear smoothed covariate in the linear predictor.\nConstruct the mesh and the SPDE model\n\nmesh = fm_mesh_2d(loc = pcod_sf,         \n                  max.edge = c(10,20),     \n                  offset = c(5,50))   \n\nspde_model =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n\n\n\n\n\n\n\n\n\n\nModel 1 - spatial only\n\nModel 1 - spatial only This is a model with that includes a structured spatial effect only. Here the linear predictor is defined as:\n\n\\[\n  \\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s)\n\\]\n\n# Model 1 components\ncmp_1 = ~ Intercept(1) + space(geometry, model = spde_model)\n\n# Model 1 linear predictor\nformula_1 = present ~ Intercept  + space\n\n# Model 1 observational model\nlik_1 = bru_obs(formula = formula_1, \n              data = pcod_sf, \n              family = \"binomial\")\n# fit Model 1\nfit_spatial = bru(cmp_1,lik_1)\n\n\nModel 2 - Depth covariate The depth enters the model in a linear way. The linear predictor is then defined as:\n\n\\[\n  \\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s) + \\beta_1\\ \\text{depth}(s)\n\\]\n\n# Model 2 components\ncmp_2 = ~ Intercept(1) + \n  space(geometry, model = spde_model)+\n  covariate(depth_r$depth_scaled, model = \"linear\") \n\n# Model 2 linear predictor\nformula_2 = present ~ Intercept  + covariate + space\n\n# Model 2 observational model\nlik_2 = bru_obs(formula = formula_2, \n              data = pcod_sf, \n              family = \"binomial\")\n\n# Fit Model 2\nfit_depth_linear = bru(cmp_2,lik_2)\n\n\nModel 2 - Smooth depth covariate The depth enters the model in a non linear way. The linear predictor is then defined as:\n\n\\[\n  \\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s) +  f(\\text{depth}(s))\n\\]\n\n# create the grouped variable\ndepth_r$depth_group = inla.group(values(depth_r$depth_scaled))\n\n# Model 3 components\ncmp_3 = ~ Intercept(1) + \n  space(geometry, model = spde_model)+\n  covariate(depth_r$depth_group, model = \"rw2\")\n\n# Model 3 linear predictor\nformula_3 = present ~ Intercept  + covariate + space\n\n# Model 2 observational model\nlik_3 = bru_obs(formula = formula_2, \n              data = pcod_sf, \n              family = \"binomial\")\n\n# Fit Model 2\nfit_depth_smooth = bru(cmp_3,lik_3)\n\n\n\nModel Comparison through LGOCV\nNext we illustrate how to implement modelling comparison using leave-out group cross validation (LGOCV). The underlying idea is that of a Bayesian prediction setting where we approximate the posterior predictive density \\(\\pi(\\mathbf{\\tilde{Y}}|\\mathbf{y})\\) defined as the integral over the posterior distribution of the parameters, i.e.\n\\[\n\\pi(\\mathbf{\\tilde{Y}}|\\mathbf{y}) = \\int_\\theta \\pi(\\mathbf{\\tilde{Y}}|\\theta,\\mathbf{y}) \\pi(\\theta|\\mathbf{y})d\\theta\n\\]\nthe LGOCV selects a fixed test point \\(i\\) and remove a certain group of data \\(\\mathbb{I}_i\\) according to a specific prediction task. Thus, we are interested in the posterior predictive density\n\\[\n\\pi(Y_i|\\mathbf{y}{-\\mathcal{I}i}) = \\int\\theta \\pi(Y_i|\\theta,\\mathbf{y}{-\\mathbb{I}_i}) \\pi(\\theta|\\mathbf{y})d\\theta\n\\]\nWith this, a point estimate \\(\\tilde{Y_i}\\) can be computed based on \\(\\pi(Y_i|\\mathbf{y}_{-\\mathbb{I}_i})\\) and the predictive performance be assessed using an appropriate scoring function \\(U(\\tilde{Y}_i,Y_i)\\), for example, the log-score function\n\\[\n\\frac{1}{n}\\sum_{i=1}^n \\mathrm{log}~ \\pi(\\mathbf{\\tilde{y}}|\\mathbf{y}).\n\\]\nIn this example, the LGOCV strategy will be used to compare the previous fitted spatially explicit models. Here, the leave-out group \\(\\mathbb{I}_i\\) is manually defined for the \\(i\\)th row of the data based on a buffer of size \\(b=25\\) centered at each data point:\n\n# create buffer of size 25 centred at each site\nbuffer &lt;- st_buffer(pcod_sf, dist = 25)\n\n# Lists of the indexes of the leave-out-group for each observation i\nIi &lt;- st_intersects(pcod_sf,buffer)\n\nFigure 1 illustrate the manual construction of the leave-out-group for the 2nd observation in our data.\n\n\n\n\n\n\n\n\nFigure 1: Example of the CV strategy for the 2nd testing point.\n\n\n\n\n\nWe then can use the inla.group.cv function to compute the log-scores for Model 3 as follows:\n\nlgocv_depth_smooth = inla.group.cv(result = fit_depth_smooth,\n                                   groups = Ii)\nlog_depth_smooth = mean(log(lgocv_depth_smooth$cv),\n                        na.rm=T)\n\nWhen we do model comparison, we want to have the same groups for different models. We can easily do this by passing an inla.group.cv class object to inla.group.cvfunction. If we want to use the groups constructed by model 3 to compute LGOCV, we have the following code:\n\nlgocv_spatial = inla.group.cv(result = fit_spatial, \n                              group.cv = lgocv_depth_smooth)\n\nlgocv_depth_linear = inla.group.cv(result = fit_depth_linear,\n                                   group.cv = lgocv_depth_smooth)\n\nlog_score_spatial&lt;- mean(log(lgocv_spatial$cv),na.rm=T)\nlog_depth_linear &lt;-mean(log(lgocv_depth_linear$cv),na.rm=T)\nlog_depth_smooth &lt;-mean(log(lgocv_depth_smooth$cv),na.rm=T)\n\nIn this example, the model that includes the smoothed covariate has the highest log-score and thus it is preferred over the other two “simpler” models.\n\ndata.frame(logspat= log_score_spatial,\n           logdepthl = log_depth_linear,\n           logdepths = log_depth_smooth )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog-Score LGOCV\n\n\nContinuous spatial\nGaussian field\nLinear depth\ncovariate effect\nSmooth depth\ncovariate effect\n\n\n\n\n−0.71\n−0.64\n−0.51\n\n\n\n\n\n\n\n\n\nSpatio-temporal LGOCV comparison\n\nModel fitting\nNow lets compare two different space-time models using LGOCV and some information criteria metrics. The general model structure is given by:\n\\[\n\\begin{aligned}\ny(s,t)|\\eta(s,t)&\\sim\\text{Binom}(1, p(s,t))\\\\\n\\eta(s,t) &= \\text{logit}(p(s,t)) \\\\\n\\end{aligned}\n\\] We begin by loading the data between 2003 and 2011 and convert it to an sf object (we will also create a time index for modelling):\n\npcod_spat = sdmTMB::pcod %&gt;%\n  filter(year %in% 2003:2011) %&gt;%\n  st_as_sf( coords = c(\"lon\",\"lat\"), crs = 4326) %&gt;%\n  st_transform(., crs = \"+proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\" ) %&gt;%\n   mutate(time_idx = match(year, c(2003, 2004, 2005, 2007, 2009, 2011, 2013, 2015, 2017)),\n         id = 1:nrow(.)) # Observation id for CV\n\nNow we tell inlabru that we want to compute WAIC, DIC and model likelihood as follows:\n\nbru_options_set(control.compute = list(waic = TRUE,dic= TRUE,mlik = TRUE))\n\n\nModel 1 - time iid effect We consider a separable space-time model with a linear predictors given by:\n\n\\[\n\\eta(s,t) = \\beta_0 + f_1(\\text{depth}(s)) + f_2(t) + \\omega(s)\n\\]\n\n\\(f_1(\\text{depth}(s))\\) is a smooth covariate effect of depth\n\\(f_2(t)\\) is an IID effect of time\n\\(\\omega(s)\\) is Matérn random field.\n\n\n# Model components\ncmp_spat = ~ Intercept(1) + \n  covariate(depth_r$depth_group, model = \"rw2\")+\n  trend(time_idx, model = \"iid\")+\n  space(geometry, model = spde_model)\n\n# Linear predictor\nformula_spat = present ~ Intercept  + covariate  + trend + space\n\n# Observational model\nlik_spat = bru_obs(formula = formula_spat, \n              data = pcod_spat, \n              family = \"binomial\")\n\n# Fit Model \nfit_spat = bru(cmp_spat,lik_spat)\n\n\n\n\n\n\n\nNote\n\n\n\nNote that there are some survey locations in certain years that fall outside the depth raster region. inlabru will input these missing covariate values using the nearest available value. This can be computationally expensive, but you can avoid it by supplying a raster layer that encompasses all of your data points (e.g., by pre-imputing these missing values with your preferred method of choice).\n\n\n\nModel 2 - spatiotemporal field We consider a separable space time model with a linear predictor given by:\n\n\\[\n\\eta(s,t) = \\beta_0 + f_1(\\text{depth}(s)) + \\omega(s,t)\n\\]\n\n\\(f_1(\\text{depth}(s))\\) is a smooth covariate effect of depth\n\\(\\omega(s,t)\\) is a space-time Matérn spatial field with AR1 time component\n\nNow we fit the model as follows (this might take a couple of minutes to run):\n\n# PC prior for AR(1) correlation parameter\nh.spec &lt;- list(rho = list(prior = 'pc.cor0', param = c(0.5, 0.1)))\n\n# Model components\ncmp_spat_ar1 = ~ Intercept(1) + \n  covariate(depth_r$depth_group, model = \"rw2\")+\n  space_time(geometry,\n        group = time_idx ,\n        model = spde_model,\n        control.group = list(model = 'ar1',hyper = h.spec))\n\n# Linear predictor\nformula_spat_ar1 = present ~ Intercept  + covariate  + space_time\n\n# Observational model\nlik_spat_ar1 = bru_obs(formula = formula_spat_ar1, \n              data = pcod_spat, \n              family = \"binomial\")\n\n# Fit Model \nfit_spat_ar1 = bru(cmp_spat_ar1,lik_spat_ar1)\n\n\n\nModel comparison\nTo manually define the leave-out groups we can apply the buffer approach as before while considering a time window of \\(t \\pm q\\). Here we set \\(q = 2\\) as we believe time dependency is weaker after a 2 year period (Figure 2 illustrate this strategy for the 750th observation). To compute this we can create a buffer surrounding each location and then loop through every observation to identify the leave-out groups based on those location that are inside the buffer within a 2 year span:\n\n# create buffer of size 25  centred at each site\n\nbuffer_25 &lt;- st_buffer(pcod_spat, dist = 25) \n\n\n# empty lists to include the indexes of the leave-out-group for each observation i\nI_i &lt;- list()\n\n# loop though each observation and store the leave-out-group based on the buffer\nfor( i in 1:nrow(pcod_spat)){\n  \n  # Temporal filtering of data within a 2 years of span of  observation i\n  df_sf_subset &lt;- pcod_spat %&gt;% \n    filter( between(time_idx,left = pcod_spat$time_idx[i]-2, \n                    right = pcod_spat$time_idx[i]+2)) \n  # Spatial filtering of the observations that are within the buffer of the ith observation\n  Buffer_i &lt;-df_sf_subset %&gt;% st_intersects(buffer_25[i,],sparse = FALSE) %&gt;% # identify \n    unlist()\n  \n  # obtain the indexes of the leave out group\n  I_i[[i]] &lt;-  df_sf_subset[Buffer_i,] %&gt;%  pull(id)\n  \n}\n\n\n\n\n\n\n\n\n\nFigure 2: Example of the spatiotemporal CV strategy for the 100nd testing point.\n\n\n\n\n\nNow that we have the leave-out group indexes for each observation, we can compute the log-score using the inla.group.cv function as before:\n\nlgocv_spat_ar1 &lt;- inla.group.cv(result = fit_spat_ar1, groups  = I_i)\nlogscore_spat_ar1 = mean(log(lgocv_spat_ar1$cv),na.rm=T)\n\n\nlgocv_spat &lt;- inla.group.cv(result = fit_spat, group.cv  = lgocv_spat_ar1)\nlogscore_spat = mean(log(lgocv_spat$cv),na.rm=T)\n\nThe following table summarises different model comparison metrics, which favors the model with a spatio-temporal field over the simples iid model.\n\ntable = data.frame( DIC = c(fit_spat_ar1$dic$dic, fit_spat$dic$dic),\n                    WAIC = c(fit_spat_ar1$waic$waic, fit_spat$waic$waic),\n                    mlik = c(fit_spat_ar1$mlik[1,1],fit_spat$mlik[1,1]),\n                    LGOCV = c(logscore_spat_ar1,logscore_spat))\n\n rownames(table) = c(\"Model 2 - spatiotemporal field\",\n                     \"Model 1 - time iid effect\")\n\n\n\n\n\n\n\n\n\n\n\n\nDIC\nWAIC\nmlik\nLGOCV\n\n\n\n\nModel 1 - time iid effect\n1356.179\n1351.832\n-763.2828\n-0.5001042\n\n\nModel 2 - spatiotemporal field\n1338.464\n1330.124\n-758.5321\n-0.4985709\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nModify Model 1 so that instead of an \\(iid\\) yearly trend, the \\(f_2(t)\\) trend follow an AR(1) process. Compare this again the the other two models using the LGOCV scores.\n\n\nTake hint\n\nYou only need to change the model component: trend(time_idx, model = ???). You may also use the PC prior we set for the spatiotemporal field in Model 2.\n\n\n\n\nClick here to see the solution\n\n# Model components\ncmp_spat_2 = ~ Intercept(1) + \n  covariate(depth_r$depth_group, model = \"rw2\")+\n  trend(time_idx, model = \"ar1\",\n        control.group = list(model = 'ar1',hyper = h.spec))+\n  space(geometry, model = spde_model)\n\n# Linear predictor\nformula_spat_2 = present ~ Intercept  + covariate  + trend + space\n\n# Observational model\nlik_spat_2 = bru_obs(formula = formula_spat, \n              data = pcod_spat, \n              family = \"binomial\")\n\n# Fit Model \nfit_spat_2 = bru(cmp_spat_2,lik_spat_2)\n\n# Compute log-score\n\nlgocv_spat_2 &lt;- inla.group.cv(result = fit_spat_2, group.cv  = lgocv_spat_ar1)\nmean(log(lgocv_spat_2$cv),na.rm=T)",
    "crumbs": [
      "Home",
      "Practical 6"
    ]
  },
  {
    "objectID": "day2_practical_4.html",
    "href": "day2_practical_4.html",
    "title": "Practical 4",
    "section": "",
    "text": "Aim of this practical:\nwe are going to learn:\nDownload Practical 3 R script\nIn this practical we will:",
    "crumbs": [
      "Home",
      "Practical 4"
    ]
  },
  {
    "objectID": "day2_practical_4.html#sec-areal_data",
    "href": "day2_practical_4.html#sec-areal_data",
    "title": "Practical 4",
    "section": "Areal (lattice) data",
    "text": "Areal (lattice) data\nAreal data our measurements are summarised across a set of discrete, non-overlapping spatial units such as postcode areas, health board or pixels on a satellite image. In consequence, the spatial domain is a countable collection of (regular or irregular) areal units at which variables are observed. Many public health studies use data aggregated over groups rather than data on individuals - often this is for privacy reasons, but it may also be for convenience.\nIn the next example we are going to explore data on respiratory hospitalisations for Greater Glasgow and Clyde between 2007 and 2011. The data are available from the CARBayesdata R Package:\n\nlibrary(CARBayesdata)\n\ndata(pollutionhealthdata)\ndata(GGHB.IZ)\n\nThe pollutionhealthdata contains the spatiotemporal data on respiratory hospitalisations, air pollution concentrations and socio-economic deprivation covariates for the 271 Intermediate Zones (IZ) that make up the Greater Glasgow and Clyde health board in Scotland. Data are provided by the Scottish Government and the available variables are:\n\nIZ: unique identifier for each IZ.\nyear: the year were the measruments were taken\nobserved: observed numbers of hospitalisations due to respiratory disease.\nexpected: expected numbers of hospitalisations due to respiratory disease computed using indirect standardisation from Scotland-wide respiratory hospitalisation rates.\npm10: Average particulate matter (less than 10 microns) concentrations.\njsa: The percentage of working age people who are in receipt of Job Seekers Allowance\nprice: Average property price (divided by 100,000).\n\nThe GGHB.IZ data is a Simple Features (sf) object containing the spatial polygon information for the set of 271 Intermediate Zones (IZ), that make up of the Greater Glasgow and Clyde health board in Scotland ( Figure 1 ).\n\n\n\n\n\n\n\n\nFigure 1: Greater Glasgow and Clyde health board represented by 271 Intermediate Zones\n\n\n\n\nLet’s start by loading useful libraries:\n\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(scico)\n\nThe sf package allows us to work with vector data which is used to represent points, lines, and polygons. It can also be used to read vector data stored as a shapefiles.\nFirst, lets combine both data sets based on the Intermediate Zones (IZ) variable using the merge function from base R:\n\nresp_cases &lt;- merge(GGHB.IZ, pollutionhealthdata, by = \"IZ\")\n\nIn epidemiology, disease risk is usually estimated using Standardized Mortality Ratios (SMR). The SMR for a given spatial areal unit \\(i\\) is defined as the ratio between the observed ( \\(Y_i\\) ) and expected ( \\(E_i\\) ) number of cases:\n\\[\nSMR_i = \\dfrac{Y_i}{E_i}\n\\]\nA value \\(SMR &gt; 1\\) indicates that there are more observed cases than expected which corresponds to a high risk area. On the other hand, if \\(SMR&lt;1\\) then there are fewer observed cases than expected, suggesting a low risk area.\nWe can manipulate sf objects the same way we manipulate standard data frame objects via the dplyr package. Lets use the pipeline command %&gt;% and the mutate function to calculate the yearly SMR values for each IZ:\n\nlibrary(dplyr)\nresp_cases &lt;- resp_cases %&gt;% \n  mutate(SMR = observed/expected, .by = year )\n\nNow we use ggplot to visualize our data by adding a geom_sf layer and coloring it according to our variable of interest (i.e., SMR). We can further use facet_wrap to create a layer per year and chose an appropriate color palette using the scale_fill_scico from the scico package:\n\nggplot()+\n  geom_sf(data=resp_cases,aes(fill=SMR))+\n  facet_wrap(~year)+scale_fill_scico(palette = \"roma\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nProduce a map that shows the spatial distribution of each of the following variables for the year 2011:\n\nAverage particulate matter pm10\nAverage property price price\nPercentage of working age people who are in receipt of Job Seekers Allowance jsa\n\n\n\nhint\n\nYou can use the filter function from dplyr to subset the data according to the year of interest.\n\n\n\n\nClick here to see the solution\n\n# Library for plotting multiple maps together\n\nlibrary(patchwork)\n\n# subset data set for 2011\n\nresp_cases_2011 &lt;- resp_cases %&gt;% filter(year ==2011)\n\n# pm10 plot\n\npm10_plot &lt;- ggplot()+\n  geom_sf(data=resp_cases_2011,aes(fill=pm10))+\n  scale_fill_scico(palette = \"navia\")\n\n# property price\n\nprice_plot &lt;- ggplot()+\n  geom_sf(data=resp_cases_2011,aes(fill=price))+\n  facet_wrap(~year)+scale_fill_scico(palette = \"bilbao\")\n\n#  percentage jsa\n\njsa_plot &lt;- ggplot()+\n  geom_sf(data=resp_cases_2011,aes(fill=jsa))+\n  facet_wrap(~year)+scale_fill_scico(palette = \"lapaz\") \n\n# plot maps together\n\npm10_plot + price_plot + jsa_plot + plot_layout(ncol=3)\n\n\n\n\n\n\n\n\n\n\n\nAs with the other types of spatial modelling, our goal is to observe and explain spatial variation in our data. Generally, we are aiming to produce a smoothed map which summarises the spatial patterns we observe in our data.\nA key aspect of any spatial analysis is that observations closer together in space are likely to have more in common than those further apart. This can lead us towards approaches similar to those used in time series, where we consider the spatial closeness of our regions in terms of a neighbourhood structure.\nThe function poly2nb() of the spdep package can be used to construct a list of neighbors based on areas with contiguous boundaries (e.g., using Queen contiguity).\n\nlibrary(spdep)\n\nW.nb &lt;- poly2nb(GGHB.IZ,queen = TRUE)\nW.nb\n\nNeighbour list object:\nNumber of regions: 271 \nNumber of nonzero links: 1424 \nPercentage nonzero weights: 1.938971 \nAverage number of links: 5.254613 \n2 disjoint connected subgraphs\n\n\nThe warning tell us that the neighbourhood is comprised of two interconnected regions. By looking at the neighbourhood graph below, we can see that these are the North and South Glasgow regions which are separated by the River Clyde.\n\nplot(st_geometry(GGHB.IZ), border = \"lightgray\")\nplot.nb(W.nb, st_geometry(GGHB.IZ), add = TRUE)\n\n\n\n\n\n\n\n\nYou could use the snap argument within poly2nb to set a distance at which the different regions centroids are consider neighbours. To do so we first need to be aware about the spatial units of the spatial coordinate reference system (CRS). We can check this as follows:\n\nst_crs(GGHB.IZ)$units\n\n[1] \"m\"\n\n\nThen, we could set a distance of 250m to join the IZ centroids that are are less than 250m apart.\n\nW.nb250 &lt;- poly2nb(GGHB.IZ,snap=250)\nW.nb250\n\nNeighbour list object:\nNumber of regions: 271 \nNumber of nonzero links: 1758 \nPercentage nonzero weights: 2.393758 \nAverage number of links: 6.487085 \n\n\n\nplot(st_geometry(GGHB.IZ), border = \"lightgray\")\nplot.nb(W.nb250, st_geometry(GGHB.IZ), add = TRUE)\n\n\n\n\n\n\n\n\nOnce we have identified a set of neighbours using our chosen method, we can use this to account for correlation.\nMoran’s \\(I\\) is a measure of global spatial autocorrelation, and can be considered an extension of the Pearson correlation coefficient. For a set of data \\(Z_1, \\ldots, Z_m\\) measured on regions \\(B_1, \\ldots B_m\\), with neighbourhood matrix \\(W\\), we can compute Moran’s I as:\n\\[\nI = \\frac{m}{\\sum_{i=1}^m \\sum_{j=1}^m w_{ij}}\\frac{\\sum_{i=1}^m \\sum_{j=1}^m w_{ij} (Z_i - \\bar{Z})(Z_j - \\bar{Z})}{\\sum_{i=1}^m (Z_i - \\bar{Z})^2}\n\\]\nThis is basically a function of differences in values between neighbouring areas. By far the most common approach is to use a binary neighbourhood matrix, \\(W\\), denoted by\n\\[ w_{ij} = \\begin{cases} 1 & \\text{if areas } (B_i, B_j) \\text{ are neighbours.}\\\\ 0 & \\text{otherwise.} \\end{cases} \\]\nBinary matrices are used for their simplicity. Fitting spatial models often requires us to invert \\(W\\), and this is less computationally intensive for sparse matrices.\nMoran’s \\(I\\) ranges between -1 and 1, and can be interpreted in a similar way to a standard correlation coefficient.\n\n\\(I=1\\) implies that we have perfect spatial correlation.\n\\(I=0\\) implies that we have complete spatial randomness.\n\\(I=-1\\) implies that we have perfect dispersion (negative correlation).\n\nOur observed \\(I\\) is a point estimate, and we may also wish to assess whether it is significantly different from zero. We can test for a statistically significant spatial correlation using a permutation test, with hypotheses:\n\\[\n\\begin{aligned}\nH_0&: \\text{ negative or no spatial association } (I \\leq 0)\\\\\nH_1&: \\text{ positive spatial association } (I &gt; 0)\n\\end{aligned}\n\\]\nWe can use moran.test() to test this hypothesis by setting alternative = \"greater\". To do so, we need to supply list containing the neighbors via the nb2listw() function from the spdep package. Lets assess now the spatial autocorrelation of the SMR in 2011:\n\n# subset the data\nresp_cases_2011 &lt;- resp_cases %&gt;% filter(year ==2011)\n\n# neighbors list \nnbw &lt;- nb2listw(W.nb, style = \"W\")\n\n# Global Moran's I\ngmoran &lt;- moran.test(resp_cases_2011$SMR, nbw,\n                     alternative = \"greater\")\ngmoran\n\n\n    Moran I test under randomisation\n\ndata:  resp_cases_2011$SMR  \nweights: nbw    \n\nMoran I statistic standard deviate = 11.42, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.439899780      -0.003703704       0.001508809 \n\n\n\n\n\n\n\n\n Question\n\n\n\nWhat do we conclude from the Moran’s I test?\n\n\nAnswer\n\nSince have set the alternative hypothesis to be $ I &gt; 0 $ and have a p-value \\(&lt;0.05\\), we then reject the null hypothesis and conclude there is evidence for positive spatial autocorrelation.\n\n\n\nA local version of Moran’s I can also be used to measure the similarity between each IZ via the localmoran function:\n\nlmoran &lt;- localmoran(resp_cases_2011$SMR, nbw, alternative = \"two.sided\")\n\nIn this case we set alternative = \"two.sided\" to test whether there is any evidence of spatial autocorrelation in our data:\n\\[\n\\begin{aligned}\nH_0&: \\text{ no spatial association } (I=0)\\\\\nH_1&: \\text{ some spatial association } (I \\neq 0)\n\\end{aligned}\n\\]\nWe can obtain the \\(Z\\)-scores from the test (Z.Ii) where any values smaller than –1.96 indicate negative spatial autocorrelation, and z-score values greater than 1.96 indicate positive spatial autocorrelation:\n\nresp_cases_2011_m &lt;- resp_cases_2011 %&gt;% mutate(Zscores = lmoran[,\"Z.Ii\"])\n\nresp_cases_2011_m &lt;- resp_cases_2011_m %&gt;% \n  mutate (SAC = case_when(\n  Zscores &gt; qnorm(0.975) ~ \" M &gt; 1\" ,\n  Zscores &lt; -1*qnorm(0.975) ~ \" M &lt; 1\",\n  .default = \"M = 0\"),\n  SAC=as.factor(SAC)\n)\n\nWe can visualize these results using either ggplot as we did before, or via the mapview library which contains interactive features:\n\nlibrary(mapview)\nmapview(resp_cases_2011_m, \n        zcol = \"SAC\",\n        layer.name = \"SAC\",\n        col.regions=c(\"turquoise\",\"orange\",\"grey40\"))\n\n\n\n\n\nIn this practical we will:\n\nExplore tools for geostatistical spatial data wrangling and visualization.\nCompute a variogram to assess for spatial autocorrelation in our data.\n\nFirst, lets load some useful libraries for data wrangling and visualization\n\n# For plotting\nlibrary(mapview)\nlibrary(ggplot2)\nlibrary(scico) # for colouring palettes\n\n# Data manipulation\nlibrary(dplyr)",
    "crumbs": [
      "Home",
      "Practical 4"
    ]
  },
  {
    "objectID": "day2_practical_4.html#georeferenced-data",
    "href": "day2_practical_4.html#georeferenced-data",
    "title": "Practical 4",
    "section": "Georeferenced data",
    "text": "Georeferenced data\nTobler’s first law of geography states that:\n“Everything is related to everything else, but near things are more related than distant things”\nSpatial patterns are fundamental in environmental and ecological data. In many ecological and environmental settings, measurements from fixed sampling units, aiming to quantify spatial variation and interpolate values at unobserved sites.\nGeorefernced data are the most common form of spatial data found in environmental setting. In these data we regularly take measurements of a spatial ecological or environmental process at a set of fixed locations. This could be data from transects (e.g, where the height of trees is recorded), samples taken across a region (e.g., water depth in a lake) or from monitoring stations as part of a network (e.g., air pollution). In each of these cases, our goal is to estimate the value of our variable across the entire space.\nLet \\(D\\) be our two-dimensional region of interest. In principle, there is aninfinite number of locations within \\(D\\), each of which can be represented by mathematical coordinates (e.g., latitude and longitude). We then can identify any individual location as \\(s_i = (x_i, y_i)\\), where \\(x_i\\) and \\(y_i\\) are their coordinates.\nWe can treat our variable of interest as a random variable, \\(Z\\) which can be observed at any location as \\(Z(\\mathbf{s}_i)\\).\nOur geostatistical process can therefore be written as: \\[\\{Z(\\mathbf{s}); \\mathbf{s} \\in D\\}\\]\nIn practice, our data are observed in a finite number of locations, \\(m\\), and can be denoted as:\n\\[z = \\{z(\\mathbf{s}_1), \\ldots z(\\mathbf{s}_m) \\}\\]\nIn the next example, we will explore data on the Pacific Cod (Gadus macrocephalus) from a trawl survey in Queen Charlotte Sound. The pcod dataset is available from the sdmTMB package and contains the presence/absence records of the Pacific Cod during each surveys along with the biomass density of Pacific cod in the area swept (kg/Km\\(^2\\)). The qcs_grid data contain the depth values stored as \\(2\\times 2\\) km grid for Queen Charlotte Sound.\n\nlibrary(sdmTMB)\n\npcod_df = sdmTMB::pcod \nqcs_grid = sdmTMB::qcs_grid\n\n\nGeoreferrenced data\nLet’s create an initial sf spatial object using the standard geographic coordinate system (EPSG:4326). This correctly defines the point locations based on latitude and longitude.\n\nlibrary(sf)\npcod_sf =   st_as_sf(pcod_df, coords = c(\"lon\",\"lat\"), crs = 4326)\n\nNow we can transform to the standard UTM Zone 9N projection (EPSG:32609) which uses meters:\n\npcod_sf_proj &lt;- st_transform(pcod_sf, crs = 32609)\nst_crs(pcod_sf_proj)$units\n\n[1] \"m\"\n\n\nWe can change the spatial units to km to better reflect the scale of our ecological study and to make resulting distance/area values more intuitive to interpret:\n\npcod_sf_proj = st_transform(pcod_sf_proj,\n                            gsub(\"units=m\",\"units=km\",\n                                 st_crs(pcod_sf_proj)$proj4string)) \nst_crs(pcod_sf_proj)$units\n\n[1] \"km\"\n\n\nInstead of first setting an EPSG code and then transforming, we can define the target Coordinate Reference System (CRS) directly using a proj4string. This allows us to customize non-standard parameters in a single step, in this case, explicitly setting the projection units to kilometers (+units=km).\n\npcod_sf = st_transform(pcod_sf,\n                       crs = \"+proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\" )\nst_crs(pcod_sf)$units\n\n[1] \"km\"\n\n\nSpatial sf objects can be manipulated the same way we manipulate standard data frame objects via the dplyr package. For example, you can select a specific year using the filter function from dplyr. Let’s map the present/absence of the Pacific Cod in 2017 using the mapview function:\n\npcod_sf %&gt;% \n  filter(year== 2017) %&gt;%\n  mutate(present = as.factor(present)) %&gt;%\nmapview(zcol = \"present\",\n        layer.name = \"Occupancy status of Pacific Cod in 2017\")\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nUse ggplot and the sf library to map the biomass density of the pacific cod across years.\n\n\nhint\n\nYou can plot ansf object by adding a geom_sf layer to a ggplot object. You can also use the facet_wrap argument to plot an arrange of plots according to a grouping variable.\n\n\n\n\nClick here to see the solution\n\nggplot()+ \n  geom_sf(data=pcod_sf,aes(color=density))+ \n  facet_wrap(~year)+\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nRaster Data\nEnvironmental data are typically stored in raster format, which represents spatially continuous phenomena by dividing a region into a grid of equally-sized cells, each storing a value for the variable of interest. In R, the terra package is a modern and powerful tool for efficiently working with raster data. The function rast(), can be used both to read raster files from standard formats (e.g., .tif or .tiff) and to create a new raster object from a data frame. For instance, the following code creates a raster from the qcs_grid grid data for Queen Charlotte Sound.\n\nlibrary(terra)\ndepth_r &lt;- rast(qcs_grid, type = \"xyz\")\ndepth_r\n\nclass       : SpatRaster \nsize        : 102, 121, 3  (nrow, ncol, nlyr)\nresolution  : 2, 2  (x, y)\nextent      : 341, 583, 5635, 5839  (xmin, xmax, ymin, ymax)\ncoord. ref. :  \nsource(s)   : memory\nnames       :    depth, depth_scaled, depth_scaled2 \nmin values  :  12.0120,    -6.000040,  4.892624e-08 \nmax values  : 805.7514,     3.453937,  3.600048e+01 \n\n\nThe raster object contains three layers corresponding to the (i) depth values, (ii) the scaled depth values and (iii) the squared depth values.\nNotice that there are no CRS associated with the raster. Thus, we can assign appropriate CRS using the crs function. Additionally, we also want the raster CRS to match the CRS in the survey data (recall that we have previously reprojected our data to utm coordinates). We can assign an appropiate CRS that matches the CRS of the sf object as follows:\n\ncrs(depth_r) &lt;- crs(pcod_sf)\n\nWe can use the tidyterra package to plot raster data using ggplot by adding a geom_spatraster function and then select an appropriate fill and color palettes:\n\nlibrary(tidyterra)\n\n\nAttaching package: 'tidyterra'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nggplot()+ \n  geom_spatraster(data=depth_r$depth)+\n  geom_sf(data=pcod_sf,aes(color=factor(present))) +\n  facet_wrap(~year)+\n    scale_color_manual(name=\"Occupancy status for the Pacific Cod\",\n                     values = c(\"black\",\"orange\"),\n                     labels= c(\"Absence\",\"Presence\"))+\n  scale_fill_scico(name = \"Depth\",\n                   palette = \"nuuk\",\n                   na.value = \"transparent\" )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nMap the scaled depth and the presence/absence records of the Pacific cod for 2003 to 2005 only.\n\n\nhint\n\nThe different layers of a raster can be accessed using the $ symbol.\n\n\n\n\nClick here to see the solution\n\nggplot()+ \n  geom_spatraster(data=depth_r$depth_scaled)+\n  geom_sf(data=pcod_sf %&gt;% filter(year %in% 2003:2005),\n          aes(color=factor(present)))+ \n  facet_wrap(~year)+\n  scale_color_manual(name=\"Occupancy status for the Pacific Cod\",\n                     values = c(\"black\",\"orange\"),\n                     labels= c(\"Absence\",\"Presence\"))+\n    scale_fill_scico(name = \"Scaled Depth\",\n                     palette = \"davos\",\n                     na.value = \"transparent\" )\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutocorrelation and Variograms\nSpatial statistics quantifies the fundamental principle that nearby things are closely related. This spatial dependence means that observation are not independent, as assumed by most classical statistical models, but are instead correlated relative to their proximity. While this correlation can be a valuable source of information, it must be explicitly accounted for to avoid wrong inference and incorrect conclusions.\nThe first step is to assess whether there is any evidence of spatial dependency in our data. Spatial dependence in georeferenced data can be explored by a function known as a variogram \\(2\\gamma(\\cdot)\\) (or semivariogram \\(\\gamma(\\cdot)\\)). The variogram is similar in many ways to the autocorrelation function used in time series modelling. In simple terms, it is a function which measures the difference in the spatial process between a pair of locations a fixed distance apart.\nThe variogram measures the variance of the difference in the process \\(Z(\\cdot)\\) at two spatial locations \\(\\mathbf{s}\\) and \\(\\mathbf{s+h}\\) and is defined as :\n\\[\\mathrm{Var}[Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h})] = E[(Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h}))^2] = 2\\gamma_z(\\mathbf{h}).\\]\nHere, \\(2\\gamma_z(\\mathbf{h})\\) is the variogram, but in practice we use the semi-variogram, \\(\\gamma_z(\\mathbf{h})\\). We use the semi-variogram because our points come in pairs, and the semi-variance is equivalent to the variance per point at a given lag.\n\nWhen the variance of the difference \\(Z(\\mathbf{s}) - Z(\\mathbf{t})\\) is relatively small, then \\(Z(\\mathbf{s})\\) and \\(Z(\\mathbf{t})\\) are similar (spatially correlated).\nWhen the variance of the difference \\(Z(\\mathbf{s}) - Z(\\mathbf{t})\\) is relatively large, then \\(Z(\\mathbf{s})\\) and \\(Z(\\mathbf{t})\\) are less similar (closer to independence).\n\nThe variogram is a function of the underlying geostatistical process \\(Z\\). In practice, we only have access to \\(m\\) realisations of this process, and therefore we have to estimate the variogram. This is known as the empirical variogram.\nWe obtain this by computing the semi-variance for all possible pairs of observations: \\(\\gamma(\\mathbf{s}, \\mathbf{t}) = 0.5(Z(\\mathbf{s}) - Z(\\mathbf{t}))^2\\).\n\n\n\n\n\n\n Example\n\n\n\nTo illustrate how an empirical variogram is computed, consider the biomass density of the Pacific Cod in 2017 for the two highlighted locations below.\n\n\n\n\n\n\n\n\n\n\nWe can first compute the distance between the two locations using the standard Euclidean distance formula as\n\n\\[h = \\sqrt{(441.6 -481)^2 + (5743.4-5748.2)^2} \\approx  39 ~\\text{Km}\\]\n\nNext, we compute the semi-variance between the points using their observed values as \\[\\gamma(\\mathbf{s}, \\mathbf{t}) = 0.5(Z(\\mathbf{s}) - Z(\\mathbf{t}))^2 = 0.5(149.5 - 40.64)^2 = 5925.25\\]\nWe repeat this process for every possible pair of points, and plot \\(h\\) against \\(\\gamma(\\mathbf{s}, \\mathbf{t})\\) for each.\n\n\n\nTo make the variogram easier to use and interpret, we divide the distances into a set of discrete bins, and compute the average semi-variance in each. We compute this binned empirical variogram as:\n\\[\\gamma(\\mathbf{h}) = \\frac{1}{2N(h_k)}\\sum_{(\\mathbf{s},\\mathbf{t}) \\in N(h_k)}[z(\\mathbf{s}) - z(\\mathbf{t})]^2\\]\nWe can calculate the binned- empirical variogram for the data using variogram function from the gstat library. This plot shows the semi-variances for each pair of points. Lets compute a variogram for the biomass density of the Pacific Cod in 2017:\n\nlibrary(gstat)\n\npcod_sf_subset &lt;- pcod_sf %&gt;% filter(year ==2017)\n\nvgm1 &lt;- variogram(density~1, pcod_sf_subset)\nplot(vgm1)\n\n\n\n\n\n\n\n\nAssessing spatial dependence\nWe can construct null envelope based on permutations of the data values across the locations, i.e. envelopes built under the assumption of no spatial correlation. By overlapping these envelopes with the empirical variograms we can determine whether there is some spatial dependence in our data ,e.g. if our observed variograms falls outside of the envelopes constructed under spatial randomness.\nWe can construct permutation envelopes on the gstat empirical variogram using the envelope function from the variosig R package. Then we can visualize the results using the envplot function:\n\nlibrary(variosig)\n\nvarioEnv &lt;- envelope(vgm1,\n                     data = pcod_sf_subset,\n                     locations = st_coordinates(pcod_sf_subset),\n                     formula = density ~ 1,\n                     nsim = 499)\n\nenvplot(varioEnv)\n\n\n\n\n\n\n\n\n[1] \"There are 1 out of 15 variogram estimates outside the 95% envelope.\"\n\n\nIn this practical we will:\n\nExplore tools for spatial point pattern data wrangling and visualization.\n\nFirst, lets load some useful libraries:\n\n# For plotting\nlibrary(ggplot2)\nlibrary(scico) # for colouring palettes\n\n# Data manipulation\nlibrary(dplyr)",
    "crumbs": [
      "Home",
      "Practical 4"
    ]
  },
  {
    "objectID": "day2_practical_4.html#spatial-point-processes-data",
    "href": "day2_practical_4.html#spatial-point-processes-data",
    "title": "Practical 4",
    "section": "Spatial Point processes data",
    "text": "Spatial Point processes data\nIn point processes we measure the locations where events occur and the coordinates of such occurrences are our data.\nPoint process models are probabilistic models that describe the likelihood of patterns of points that represent the random location of some event. A spatial point process is a set of locations that have been generated by some form of stochastic (random) mechanism. In other words, the point process is a random variable operating in continuous space, and we observe realisations of this variable as point patterns across space (and/or time).\nConsider a fixed geographical region \\(A\\). The set of locations at which events occur are denoted \\(\\mathbf{s} = s_1,\\ldots,s_n\\). We let \\(N(A)\\) be the random variable which represents the number of events in region \\(A\\).\nOur primary interest is in measuring where events occur, so the locations are our data. We typically assume that a spatial point pattern is generated by an unique point process over the whole study area. This means that the delimitation of the study area will affect the observed point patters.\nThe observed distribution of points can be described based on the intensity of points within a delimited region. We can define the (first order) intensity of a point process as the expected number of events per unit area. This can also be thought of as a measure of the density of our points. In some cases, the intensity will be constant over space (homogeneous), while in other cases it can vary by location (inhomogeneous or heterogenous).\nIn the next example, we will explore tools for visualizing spatial point patterns. Specifically, we will map the spatial distribution of the Ringlet butterfly in Scotland’s Cairngorms National Park (CNP).\n\nBNM citizen science program\nCitizen science initiatives have become an important source of information in ecological research, offering large volumes of species distribution data collected by volunters for multiple taxonomic groups across wide spatial and temporal scales\nButterflies for the New Millennium (BNM) is a large-scale monitoring scheme launched in the earlies 70’s to keep track of butterflies’ populations in the UK. With over 12 million butterflies sighting and more than 10,000 volunteers, this recording scheme has proven to be a successful program that has been used to assess long-term changes in the distributions of UK butterfly species.\nHere we will focus on the distribution of the Ringlet butterfly species, which holds particular significance in environmental studies as one of the Habitat specialists species (UK Government, 2024). The data set consists of Ringlet butterfly presence-only records collected by volunteers in Scotland’s Cairngorms National Park (CNP).\nReading shapefiles into R\nFirst, we load the geographical region of interest which can be downloaded here (i.e., CNP boundaries). We can use thre st_read function from the sf library to load the .shp file by specifying the directory where you downloaded the files:\n\nlibrary(sf)\nshp_SGC &lt;-  st_read(\"datasets/SG_CairngormsNationalPark/SG_CairngormsNationalPark_2010.shp\",quiet =T)\n\nThen, we can use appropriate CRS for the UK (i.e., EPSG code: 27700) :\n\nshp_SGC &lt;- shp_SGC %&gt;% st_transform(crs = 27700)\nst_crs(shp_SGC)$units\n\n[1] \"m\"\n\n\nNotice that the spatial resolution is in meters. Let’s change the spatial units to km to make resulting distance/area values more intuitive to interpret:\n\nshp_SGC &lt;- st_transform(shp_SGC,gsub(\"units=m\",\"units=km\",st_crs(shp_SGC)$proj4string)) \nst_crs(shp_SGC)$units\n\n[1] \"km\"\n\n\nWe can then plot the CNP boundary as follows:\n\nggplot()+\n  geom_sf(data=shp_SGC)\n\n\n\n\n\n\n\n\nCreating sf spatial objects in R\nNow we will read the Ringlet butterfly records which can be downloaded below:\n Download data set \n\nringlett &lt;- read.csv(\"datasets/bnm_ringlett.csv\")\nhead(ringlett)\n\n         y         x\n1 57.58752 -2.712498\n2 54.97742 -3.274879\n3 54.89929 -3.771451\n4 55.40323 -5.737059\n5 54.91438 -3.959336\n6 55.87255 -4.167174\n\n\nThe data set contains the longitude latitude where an observation was made. We can convert this into a spatial sf object using the st_as_sf function by declaring the columns in our data that contain the spatial coordinates:\n\nringlett_sf &lt;- ringlett %&gt;% st_as_sf(coords = c(\"x\",\"y\"),crs = \"+proj=longlat +datum=WGS84\") \n\n\n\n\n\n\n\n Task\n\n\n\nWe have set standard WGS84 coordinates for the Ringlet butterfly occurrence records. Set the CRS to match the CRS used in shapefile. Then, produce a map of the CNP region with the projected observations overlayed.\n\n\nTake hint\n\nYou can use the st_transform() function to change the coordinates of an sf object (type ?st_transform for more details)/\n\n\n\n\nClick here to see the solution\n\n\nCode\nringlett_sf &lt;- ringlett_sf %&gt;%\n  st_transform(st_crs(shp_SGC))\n\nggplot()+\n  geom_sf(data=shp_SGC)+\n  geom_sf(data=ringlett_sf)\n\n\n\n\n\n\n\n\n\n\n\n\nWe can subset two sf objects with the same CRS in the same way as we subset a data frame in R. For example, if we want to subset the Ringlet butterfly occurrence records to those contained only within the CNP, we can type the following:\n\nringlett_CNP &lt;- ringlett_sf[shp_SGC,] # crop to mainland\n\nIf we plot the ringlett_CNP object along with the CNP boundary, we should then obtain a map of the occurrence records within the park:\n\nggplot()+\n  geom_sf(data=shp_SGC)+\n  geom_sf(data=ringlett_CNP)\n\n\n\n\n\n\n\n\nReading Raster Data\nWe can use the terra R package to read raster files. The Scotland_elev.tiff raster contains the output of a digital elevation model for Scotland:\n Download raster data \nOnce you download the raster you can read it using the rast function after specifying the path where the file has been stored. Then, we assign the same CRS as our data.\n\nlibrary(terra)\nelevation_r &lt;- rast(\"datasets/Scotland_elev.tiff\")\ncrs(elevation_r) = crs(shp_SGC)\nplot(elevation_r)\n\n\n\n\n\n\n\n\nWe can apply different R functions to our rasters. For example, we can scale the elevation values as follows:\n\nelevation_r &lt;- elevation_r %&gt;% scale()\n\nLastly, we can crop the raster to the boundaries of our region of interest. Let’s crop the elevation raster to the CNP area using the crop function:\n\nelev_CNP &lt;- terra::crop(elevation_r,shp_SGC,mask=T)\nplot(elev_CNP)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nUsing tidyterra and ggplot, produce a map of the elevation profile in the CNP and overlay the spatial point pattern of the Ringlet butterfly occurrence records. Use an appropriate colouring scheme for the elevation values. Do you see any pattern?\n\n\nTake hint\n\nYou can use the geom_spatraster() to add a raster layer to a ggplot object. Furthermore the scico library contains a nice range of coloring palettes you can choose, type scico_palette_show() to see the color palettes that are available.\n\n\n\n\nClick here to see the solution\n\n\nCode\nlibrary(scico)\n\nggplot()+ \n  tidyterra::geom_spatraster(data=elev_CNP)+\n  geom_sf(data=ringlett_CNP)+\n  scale_fill_scico(name = \"Elevation scaled\",\n                   palette = \"devon\",\n                   na.value = \"transparent\" )",
    "crumbs": [
      "Home",
      "Practical 4"
    ]
  },
  {
    "objectID": "day1_practical_2.html",
    "href": "day1_practical_2.html",
    "title": "Practical 2",
    "section": "",
    "text": "Aim of this practical:\nwe are going to learn:\nDownload Practical 2 R script",
    "crumbs": [
      "Home",
      "Practical 2"
    ]
  },
  {
    "objectID": "day1_practical_2.html#setting-priors-and-model-checking-for-linear-models",
    "href": "day1_practical_2.html#setting-priors-and-model-checking-for-linear-models",
    "title": "Practical 2",
    "section": "Setting priors and model checking for Linear Models",
    "text": "Setting priors and model checking for Linear Models\nIn this exercise we will:\n\nLearn how to set priors for linear effects \\(\\beta_0\\) and \\(\\beta_1\\)\nLearn how to set the priors for the hyperparameter \\(\\tau = 1/\\sigma^2\\).\nVisualize marginal posterior distributions\n\nStart by loading useful libraries:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nRecall a simple linear regression model with Gaussian observations \\[\ny_i\\sim\\mathcal{N}(\\mu_i, \\sigma^2), \\qquad i = 1,\\dots,N\n\\]\nwhere \\(\\sigma^2\\) is the observation error, and the mean parameter \\(\\mu_i\\) is linked to the linear predictor through an identity function: \\[\n\\eta_i = \\mu_i = \\beta_0 + \\beta_1 x_i\n\\] where \\(x_i\\) is a covariate and \\(\\beta_0, \\beta_1\\) are parameters to be estimated. In INLA, we assume that the model is a latent Gaussian model, i.e., we have to assign \\(\\beta_0\\) and \\(\\beta_1\\) a Gaussian prior. For the precision hyperparameter \\(\\tau = 1/\\sigma^2\\) a typical prior choice is a \\(\\text{Gamma}(a,b)\\) prior.\nIn R-INLA, the default choice of priors for each \\(\\beta\\) is\n\\[\n\\beta \\sim \\mathcal{N}(0,10^3).\n\\]\nand the prior for the variance parameter in terms of the log precision is\n\\[ \\log(\\tau) \\sim \\mathrm{logGamma}(1,5 \\times 10^{-5}) \\]\n\n\n\n\n\n\nNote\n\n\n\nIf your model uses the default intercept construction (i.e., Intercept(1) in the linear predictor) INLA will assign a default \\(\\mathcal{N} (0,0)\\) prior to it.\n\n\nLets see how can we change the default priors using some simulated data\n\nSimulate example data\nWe simulate data from a simple linear regression model\n\n\nCode\nbeta = c(2,0.5)\nsd_error = 0.1\n\nn = 100\nx = rnorm(n)\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error)\n\ndf = data.frame(y = y, x = x)  \n\n\n\n\nFitting the linear regression model with inlabru\nNow we fit a simple linear regression model in inalbru by defining (1) the model components, (2) the linear predictor and (3) the likelihood.\n\n# Model components\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n# Linear predictor\nformula = y ~ Intercept + beta_1\n# Observational model likelihood\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n# Fit the Model\nfit.lm = bru(cmp, lik)\n\n\n\nChange the prior distributions\nUntil now, we have used the default priors for both the precision \\(\\tau\\) and the fixed effects \\(\\beta_0\\) and \\(\\beta_1\\). Let’s see how to customize these.\nTo check which priors are used in a fitted model one can use the function inla.prior.used()\n\ninla.priors.used(fit.lm)\n\nsection=[family]\n    tag=[INLA.Data1] component=[gaussian]\n        theta1:\n            parameter=[log precision]\n            prior=[loggamma]\n            param=[1e+00, 5e-05]\nsection=[linear]\n    tag=[beta_0] component=[beta_0]\n        beta:\n            parameter=[beta_0]\n            prior=[normal]\n            param=[0.000, 0.001]\n    tag=[beta_1] component=[beta_1]\n        beta:\n            parameter=[beta_1]\n            prior=[normal]\n            param=[0.000, 0.001]\n\n\nFrom the output we see that the precision for the observation \\(\\tau\\sim\\text{Gamma}(1e+00,5e-05)\\) while \\(\\beta_0\\) and \\(\\beta_1\\) have precision 0.001, that is variance \\(1/0.001\\).\nChange the precision for the linear effects\nThe precision for linear effects is set in the component definition. For example, if we want to increase the precision to 0.01 for \\(\\beta_0\\) we define the relative components as:\n\ncmp1 =  ~-1 +  beta_0(1, prec.linear = 0.01) + beta_1(x, model = \"linear\")\n\n\n\n\n\n\n\n Task\n\n\n\nRun the model again using 0.1 as default precision for both the intercept and the slope parameter.\n\n\n\nClick here to see the solution\n\ncmp2 =  ~ -1 + \n          beta_0(1, prec.linear = 0.1) + \n          beta_1(x, model = \"linear\", prec.linear = 0.1)\n\nlm.fit2 = bru(cmp2, lik) \n\n\nNote that we can use the same observation model as before since both the formula and the dataset are unchanged.\n\n\nChange the prior for the precision of the observation error \\(\\tau\\)\nPriors on the hyperparameters of the observation model must be passed by defining argument hyper within control.family in the call to the bru_obs() function.\n\n# First we define the logGamma (0.01,0.01) prior \n\nprec.tau &lt;- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(0.01, 0.01))) # prior values\n\nlik2 =  bru_obs(formula = y ~.,\n                family = \"gaussian\",\n                data = df,\n                control.family = list(hyper = prec.tau))\n\nfit.lm2 = bru(cmp2, lik2) \n\nThe names of the priors available in R-INLA can be seen with names(inla.models()$prior)\n\n\nVisualizing the posterior marginals\nPosterior marginal distributions of the ﬁxed effects parameters and the hyperparameters can be visualized using the plot() function by calling the name of the component. For example, if want to visualize the posterior density of the intercept \\(\\beta_0\\) we can type:\n\n\nCode\nplot(fit.lm, \"beta_0\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nPlot the posterior marginals for \\(\\beta_1\\) and for the precision of the observation error \\(\\pi(\\tau|y)\\)\n\n\nTake hint\n\nSee the summary() output to check the names for the different model components.\n\n\n\n\nClick here to see the solution\n\n\nCode\nplot(fit.lm, \"beta_1\") +\nplot(fit.lm, \"Precision for the Gaussian observations\")",
    "crumbs": [
      "Home",
      "Practical 2"
    ]
  },
  {
    "objectID": "day1_practical_2.html#sec-llm_fish",
    "href": "day1_practical_2.html#sec-llm_fish",
    "title": "Practical 2",
    "section": "Linear Mixed Model for fish weight-length relationship",
    "text": "Linear Mixed Model for fish weight-length relationship\nIn this exercise we will:\n\nPlot random effects of a LMM\nCompute posterior densities and summaries for the variance components\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nIn this exercise, we will use a subset of the Pygmy Whitefish (Prosopium coulterii) dataset from the FSAdata R package, containing biological data collected in 2001 from Dina Lake, British Columbia.\n Download data set \nThe data set contains the following information:\n\nnet_noUnique net identification number\nwt Fish weight (g)\ntl Total fish length (cm)\nsex Sex code (F=Female, M = Male)\n\nWe can visualize the distribution of the response (weight) across the nets split by sex as follows:\n\nPygmyWFBC &lt;- read.csv(\"datasets/PygmyWFBC.csv\")\n\nggplot(PygmyWFBC, aes(x = factor(net_no), y = wt,fill = sex)) + \n  geom_boxplot() + \n  labs(y=\"Weight (g)\",x = \"Net no.\")\n\n\n\n\n\n\n\n\nSuppose we are interested in modelling the weight-length relationship for captured fish. The exploratory plot suggest some important variability in this relationship, potentially attributable to differences among sampling nets deployed across various sites in the Dina Lake.\nTo account for this between-net variability, we model net as a random effect using the following linear mixed model:\n\\[\n\\begin{aligned}\ny_{ij} &\\sim\\mathcal{N}(\\mu_{ij}, \\sigma_e^2), \\qquad i = 1,\\dots,a \\qquad j = 1,\\ldots,n \\\\\n\\eta_{ij} &= \\mu_{ij} = \\beta_0 + \\beta_1 \\times \\text{length}_j + \\beta_2 \\times \\mathbb{I}(\\mathrm{Sex}_{ij}=\\mathrm{M}) +  u_i \\\\\nu_i &\\sim \\mathcal{N}(0,\\sigma^2_u)\n\\end{aligned}\n\\]\nwhere:\n\n\\(y_{ij}\\) is the weight of the \\(j\\)-th fish from net \\(i\\)\n\\(\\text{length}_{ij}\\) is the corresponding fish length\n\\(\\mathbb{I}(\\text{Sex}_{ij} = \\text{M})\\) is an indicator/dummy such that for the ith net \\[\n\\mathbb{I}(\\mathrm{Sex}_{ij}) \\begin{cases}1 & \\text{if the } j \\text{th fish is Male} \\\\0 & \\text{otherwise} \\end{cases}\n\\]\n\\(u_i\\) represents the random intercept for net \\(i\\)\n\\(\\sigma_u^2\\) and \\(\\sigma_\\epsilon^2\\) are the between-net and residual variances, respectively\n\nTo run this model ininlabru we first need to create our sex dummy variable :\n\nPygmyWFBC$sex_M &lt;- ifelse(PygmyWFBC$sex==\"F\",0,1)\n\ninlabru will treat 0 as the reference category (i.e., the intercept \\(\\beta_0\\) will represent the baseline weight for females ). Now we can define the model component, the likelihood and fit the model.\n\ncmp =  ~ -1 + sex_M +  beta_0(1)  + beta_1(tl, model = \"linear\") +   net_eff(net_no, model = \"iid\")\n\nlik =  bru_obs(formula = wt ~ .,\n            family = \"gaussian\",\n            data = PygmyWFBC)\n\nfit = bru(cmp, lik)\n\nsummary(fit)\n\ninlabru version: 2.13.0\nINLA version: 25.08.21-1\nComponents:\nsex_M: main = linear(sex_M), group = exchangeable(1L), replicate = iid(1L), NULL\nbeta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\nbeta_1: main = linear(tl), group = exchangeable(1L), replicate = iid(1L), NULL\nnet_eff: main = iid(net_no), group = exchangeable(1L), replicate = iid(1L), NULL\nObservation models:\n  Family: 'gaussian'\n    Tag: &lt;No tag&gt;\n    Data class: 'data.frame'\n    Response class: 'numeric'\n    Predictor: wt ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[sex_M, beta_0, beta_1, net_eff], latent[]\nTime used:\n    Pre = 0.497, Running = 0.389, Post = 0.226, Total = 1.11 \nFixed effects:\n          mean    sd 0.025quant 0.5quant 0.975quant    mode kld\nsex_M   -1.106 0.218     -1.534   -1.106     -0.678  -1.106   0\nbeta_0 -15.816 0.870    -17.515  -15.819    -14.099 -15.819   0\nbeta_1   2.555 0.072      2.414    2.555      2.696   2.555   0\n\nRandom effects:\n  Name    Model\n    net_eff IID model\n\nModel hyperparameters:\n                                         mean    sd 0.025quant 0.5quant\nPrecision for the Gaussian observations 0.475 0.044      0.392    0.473\nPrecision for net_eff                   2.150 1.326      0.561    1.837\n                                        0.975quant mode\nPrecision for the Gaussian observations      0.567 0.47\nPrecision for net_eff                        5.568 1.31\n\nMarginal log-Likelihood:  -467.53 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\nFor interpretability, we could have centered the predictors, but our primary focus here is on estimating the variance components of the mixed model.\nWe can plot the posterior density of the nets random intercept as follows:\n\nplot(fit,\"net_eff\")\n\n\n\n\n\n\n\n\nFor theoretical and computational purposes, INLA works with the precision which is the inverse of the variance. To obtain the posterior summaries on the SDs scale we can sample from the posterior distribution for the precision while back-transforming the samples and then computing the summary statistics. Transforming the samples is necessary because some quantities such as the mean and mode are not invariant to monotone transformation; alternatively we can use some of the in-built R-INLA functions to achieve this (see supplementary note).\nWe use the inla.hyperpar.sample function to draw samples from the approximated joint posterior for the hyperparameters, then invert them to get variances and lastly compute the mean, std. dev., quantiles, etc.\n\nsampvars &lt;- 1/inla.hyperpar.sample(1000,fit,improve.marginals = T)\n\ncolnames(sampvars) &lt;- c(\"Error variance\",\"Between-net Variance\")\n\napply(sampvars,2,\n      function(x) c(\"mean\"=mean(x),\n                    \"std.dev\" = sd(x),\n                    quantile(x,c(0.025,0.5,0.975))))\n\n        Error variance Between-net Variance\nmean         2.1250405            0.6520950\nstd.dev      0.1982666            0.4178071\n2.5%         1.7673618            0.1822351\n50%          2.1142705            0.5435795\n97.5%        2.5419884            1.7539233\n\n\n\n\n\n\n\n\n Task\n\n\n\nAnother useful quantity we can compute is the intraclass correlation coefﬁcient (ICC) which help us determine how much the response varies within groups compared to between groups. The intraclass correlation coefﬁcient is defined as:\n\\[\n\\text{ICC} = \\frac{\\sigma^2_u}{\\sigma^2_u + \\sigma^2_e}\n\\]\nCompute the median, and quantiles for the ICC using the posterior samples we draw for \\(\\sigma^2_e\\) and \\(\\sigma^2_u\\).\n\n\nTake hint\n\nThe rowSums function can be used to compute \\(\\sigma^2_{u,s} + \\sigma^2_{e,s}\\) for the \\(s\\)th posterior draw.\n\n\n\n\nClick here to see the solution\n\n\nCode\nsampicc &lt;- sampvars[,2]/(rowSums(sampvars))\nquantile(sampicc, c(0.025,0.5,0.975))\n\n\n      2.5%        50%      97.5% \n0.07916719 0.20410128 0.44925310 \n\n\n\n\n\n\n\n\n\n\n\nSupplementary Material\n\n\n\nThe marginal densities for the hyper parameters can be also found by callinginlabru_model$marginals.hyperpar. We can then apply a transformation using the inla.tmarginal function to transform the precision posterior distributions.\n\nvar_e &lt;- fit$marginals.hyperpar$`Precision for the Gaussian observations` %&gt;%\n  inla.tmarginal(function(x) 1/x,.) \n\nvar_u &lt;- fit$marginals.hyperpar$`Precision for net_eff` %&gt;%\n  inla.tmarginal(function(x) 1/x,.) \n\nThe marginal densities for the hyper parameters can be found with inlabru_model$marginals.hyperpar, then we can apply a transformation using the inla.tmarginal function to transform the precision posterior distributions. Then, we can compute posterior summaries using inla.zmarginal function as follows:\n\npost_var_summaries &lt;- cbind( inla.zmarginal(var_e,silent = T),\n                             inla.zmarginal(var_u,silent = T))\ncolnames(post_var_summaries) &lt;- c(\"sigma_e\",\"sigma_u\")\npost_var_summaries\n\n           sigma_e   sigma_u  \nmean       2.124621  0.6514273\nsd         0.1981789 0.4181368\nquant0.025 1.765362  0.1800776\nquant0.25  1.985385  0.3681995\nquant0.5   2.113528  0.5418924\nquant0.75  2.25202   0.8088907\nquant0.975 2.543139  1.760272",
    "crumbs": [
      "Home",
      "Practical 2"
    ]
  },
  {
    "objectID": "day1_practical_2.html#hierarchical-generalised-additive-mixed-models-with-inlabru",
    "href": "day1_practical_2.html#hierarchical-generalised-additive-mixed-models-with-inlabru",
    "title": "Practical 2",
    "section": "Hierarchical generalised additive mixed models with inlabru",
    "text": "Hierarchical generalised additive mixed models with inlabru\nIn this excercise we will:\n\nFit an hierarchical generalised additive mixed models\nFit a model with a global smooth term\nFit a model with global and group-level smooth terms\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nThe oceans represent Earth’s largest habitat, with life distributed unevenly across depths primarily due to variations in light, temperature, and pressure. Biomass generally decreases with depth, though complex factors like water density layers create non-linear patterns. A significant portion of deep-sea organisms exhibit bioluminescence, which scientists measure using specialized equipment like free-fall camera systems to profile vertical distribution.\nIn this exercise, we analyze the ISIT dataset, which contains bioluminescence measurements from the northeast Atlantic Ocean. This dataset was previously examined in Zuur et al. (2009) and Gillibrand et al. (2007) and consists of observations collected across a depth gradient (0–4,800 m) during spring and summer cruises in 2001–2002 using an ISIT free-fall profiler.\n Download data set \nThe focus of this excersice will be on characterizing seasonal variation in the relationship between bioluminescent source density (sources m\\(^{2}\\)) and depth. We begin by exploring distribution patterns of pelagic bioluminescence through source-depth profiles, with each profile representing measurements from an individual sampling station. These profiles will be grouped by month to examine temporal patterns in the water column’s bioluminescent structure.\n\nPlotR-Code\n\n\n\n\n\n\n\nSource–depth proﬁles per month. Each line represents a station.\n\n\n\n\n\n\n\nicit &lt;- read.csv(\"datasets/ISIT.csv\")\n\nicit$Month &lt;- as.factor(icit$Month)\nlevels(icit$Month) &lt;- month.abb[unique(icit$Month)]\n\nggplot(icit,aes(x=SampleDepth,y= Sources,\n                group=as.factor(Station),\n                colour=as.factor(Station)))+\n  geom_line()+\n  facet_wrap(~Month)+\n  theme(legend.position = \"none\")\n\n\n\n\nAs expected, there seems to be a non-linear depth effect with some important variability across months.\n\nFitting a global smoother\nWe could begin analysing these data with a global smoother and a random intercept for each month. Thus, a possible model is of the form:\n\\[\nS_{is} = \\beta_0 + f(\\text{Depth})_s + \\text{Month}_i +  \\epsilon_{is} ~~\\text{such that}~ \\epsilon \\sim \\mathcal{N}(0,\\sigma^2_e);~ \\text{Month} \\sim \\mathrm{N}(0,\\sigma^2_m).\n\\]\nwhere the source during month \\(i\\) at depth \\(s\\), \\(S_{is}\\), are modelled as smoothing function of depth and a month effect. The model has one smoothing curve for all months and can be fitted in inlabru as follows:\n\nicit$Month_id &lt;- as.numeric(icit$Month) # numeric index for the i-th month\n\ncmp_g =  ~ -1+ beta_0(1) + \n  smooth_g(SampleDepth, model = \"rw1\") + \n  month_reff(Month_id, model = \"iid\") \n\nlik =  bru_obs(formula = Sources ~.,\n               family = \"gaussian\",\n               data = icit)\n\nfit_g = bru(cmp_g, lik)\n\nsummary(fit_g)\n\ninlabru version: 2.13.0\nINLA version: 25.08.21-1\nComponents:\nbeta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\nsmooth_g: main = rw1(SampleDepth), group = exchangeable(1L), replicate = iid(1L), NULL\nmonth_reff: main = iid(Month_id), group = exchangeable(1L), replicate = iid(1L), NULL\nObservation models:\n  Family: 'gaussian'\n    Tag: &lt;No tag&gt;\n    Data class: 'data.frame'\n    Response class: 'numeric'\n    Predictor: Sources ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[beta_0, smooth_g, month_reff], latent[]\nTime used:\n    Pre = 0.429, Running = 0.576, Post = 0.228, Total = 1.23 \nFixed effects:\n         mean    sd 0.025quant 0.5quant 0.975quant   mode kld\nbeta_0 10.014 1.671      6.571   10.023     13.406 10.021   0\n\nRandom effects:\n  Name    Model\n    smooth_g RW1 model\n   month_reff IID model\n\nModel hyperparameters:\n                                          mean    sd 0.025quant 0.5quant\nPrecision for the Gaussian observations  0.024 0.001      0.021    0.024\nPrecision for smooth_g                  21.159 5.397     12.445   20.523\nPrecision for month_reff                 0.143 0.106      0.030    0.114\n                                        0.975quant   mode\nPrecision for the Gaussian observations      0.026  0.024\nPrecision for smooth_g                      33.545 19.327\nPrecision for month_reff                     0.423  0.073\n\nMarginal log-Likelihood:  -2217.23 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\nWe can plot the smoother marginal effect as follows:\n\n\nGlobal smoother marginal effect\ndata.frame(fit_g$summary.random$smooth_g) %&gt;% \n  ggplot() + \n  geom_ribbon(aes(ID,ymin = X0.025quant, ymax= X0.975quant), alpha = 0.5) + \n  geom_line(aes(ID,mean)) + \n  xlab(\"covariate\") + ylab(\"smooth effect\")\n\n\n\n\n\n\n\n\n\nYou might want to have a smoother function by placing a RW2 prior. Unfortunately, this assumes that all the knots are regularly spaced and some depth values are too close to be used for building the RW2 priors. For the case, it is possible to use function inla.group() to bin data into groups according to the values of the covariate:\n\nicit$depth_grouped &lt;- inla.group(icit$SampleDepth,n=50)\n\n\n\n\n\n\n\n Task\n\n\n\nRe-run the global smoother model using a RW2 prior for the depth smoother and compare your results with the RW1 model.\n\n\nTake hint\n\nUse the depth_grouped covariate to define the smoother.\n\n\n\n\nClick here to see the solution\n\ncmp_rw2 =  ~ -1+ beta_0(1) + \n  smooth_g(depth_grouped, model = \"rw2\") + \n  month_reff(Month_id, model = \"iid\") \n\nlik =  bru_obs(formula = Sources ~.,\n               family = \"gaussian\",\n               data = icit)\n\nfit_rw2 = bru(cmp_rw2, lik)\n\ndata.frame(fit_rw2$summary.random$smooth_g) %&gt;% \n  ggplot() + \n  geom_ribbon(aes(ID,ymin = X0.025quant, ymax= X0.975quant), alpha = 0.5) + \n  geom_line(aes(ID,mean)) + \n  xlab(\"covariate\") + ylab(\"Global smooth effect\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting group-level smoothers\nHere we fit a model where each month is allowed to have its own smoother for depth, i.e., \\(f_i(\\text{Depth})_s\\). The model structure is given by:\n\\[\nS_{is} = \\beta_0 + f_i(\\text{Depth})_s + \\text{Month}_i +  \\epsilon_{is}.\n\\]\nNotice the only different between the global smoother model (Model G) and the group level model (Model GS) is the indexing of the smooth function for depth. We can fit a group-level smoother using the group argument within the model component as follows:\n\ncmp_gs =  ~ -1+ beta_0(1) +\n  smooth_g(SampleDepth, model = \"rw1\") + \n  month_reff(Month_id, model = \"iid\")+\n  smooth_loc(SampleDepth, model = \"rw1\", group = Month_id)\n\nThen, we simply run the model (since the observational model has not changed -only the model components have):\n\nfit_gs = bru(cmp_gs, lik) \n\nLastly, we can generate model predictions using the predict function.\n\npred_gs = predict(fit_gs, icit, ~ (beta_0 + smooth_g+month_reff+smooth_loc))\n\nThen, we plot the predicted mean values with their corresponding 95% CrIs.\n\n\nGlobal + group smoother predictions\nggplot(pred_gs,aes(y=mean,x=SampleDepth))+\n  geom_ribbon(aes(SampleDepth,ymin = q0.025, ymax= q0.975), alpha = 0.5,fill=\"tomato\") +\n  geom_line()+\n  geom_point(aes(x=SampleDepth,y=Sources ),alpha=0.25,col=\"grey40\")+\n  facet_wrap(~Month)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nRe-fit the model GS without the global smoother. By omitting the global smoother, we do not longer force group-level smooths to follow a shared pattern, which is useful when groups may differ substantially from a common trend.\n\n\nTake hint\n\nYou only need to modify the model components in cmp_gs\nAdd hint details here…\n\n\n\n\nClick here to see the solution\n\n\nCode\ncmp_s =  ~ -1+ beta_0(1) +\n  month_reff(Month_id, model = \"iid\")+\n  smooth_loc(SampleDepth, model = \"rw1\", group = Month_id)\n\nfit_s = bru(cmp_s, lik) \n\npred_s = predict(fit_s, icit, ~ (beta_0 +month_reff+smooth_loc))\n\nggplot(pred_s,aes(y=mean,x=SampleDepth))+\n  geom_ribbon(aes(SampleDepth,ymin = q0.025, ymax= q0.975), alpha = 0.5,fill=\"tomato\") +\n  geom_line()+\n  geom_point(aes(x=SampleDepth,y=Sources ),alpha=0.25,col=\"grey40\")+\n  facet_wrap(~Month)",
    "crumbs": [
      "Home",
      "Practical 2"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "inlabru Workshop",
    "section": "",
    "text": "Welcome to the course!\n\nWelcome to the inlabru workshop!\nThe goal for the workshop is to …\nThe workshop is intended for … No knowledge of R-INLA is required.\nWorkshop materials in the github repository inlabru-workshop\n\n\n\nLearning Objectives for the workshop\nAt the end of the workshop, participants will be able to:\n\nILO1\nILO2\nILO3\n\nIntended audience and level: The tutorial is intended for … No knowledge of R-INLA is required.\n\n\nSchedule Program\n\nDay 1Day 2Day 3Day 4Day 5\n\n\n\n\n\nTime\nTopic\n\n\n\n\n10:00 - 10:30\nICES informative session\n\n\n10:30 - 11:30\nSession 1: Introduction to inlabru\n\n\n11:30 - 13:00\nPractical Session 1\n\n\n13:00 - 14:30\nLunch break 🍽️\n\n\n14:30 - 15:30\nSession 2: Latent Gaussian Models and INLA\n\n\n15:30 - 15:45\nCoffee Break ☕\n\n\n15:45 - 16:45\nPractical Session 2\n\n\n16:45 - 17:00\nwrap-up and outlook\n\n\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 10:00\nSession 3: Temporal modelling and smoothing part 1\n\n\n10:00 - 10:30\nSnack 🥙\n\n\n10:30 - 11:30\nSession 4: Temporal modelling and smoothing part 2\n\n\n11:30 - 13:00\nPractical Session 3\n\n\n13:00 - 14:30\nLunch break 🍽️\n\n\n14:30 - 15:30\nSession 5: Introduction to Spatial Statistics\n\n\n15:35 - 15:45\nCoffee Break ☕\n\n\n15:45 - 16:45\nPractical Session 4\n\n\n16:45 - 17:00\nwrap-up and outlook\n\n\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 10:00\nSession 6: Areal Processes\n\n\n10:00 - 10:30\nSnack 🥙\n\n\n10:30 - 11:30\nSession 7: Geostatistics\n\n\n11:30 - 13:00\nPractical Session 5\n\n\n13:00 - 14:30\nLunch break 🍽️\n\n\n14:30 - 15:30\nSession 8: Spatial Point processes\n\n\n15:35 - 15:45\nCoffee Break ☕\n\n\n15:45 - 16:45\nPractical Session 5 continued\n\n\n16:45 - 17:00\nwrap-up and outlook\n\n\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 10:00\nSession 9: Spatiotemporal models\n\n\n10:00 - 10:30\nSnack 🥙\n\n\n10:30 - 11:30\nSession 10: Model comparison and evaluation\n\n\n11:30 - 13:00\nPractical Session 6\n\n\n13:00 - 14:30\nLunch break 🍽️\n\n\n14:30 - 15:30\nSession 11: Multilikelihoods/joint likelihood\n\n\n15:35 - 15:45\nCoffee Break ☕\n\n\n15:45 - 16:45\nPractical Session 7\n\n\n16:45 - 17:00\nwrap-up and outlook\n\n\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 10:00\nSession 12: Zero inflated models\n\n\n10:00 - 10:30\nSnack 🥙\n\n\n10:30 - 11:30\nSession 13: Complex observational processes: Distance Sampling\n\n\n11:30 - 13:00\nPractical Session 8\n\n\n13:00 - 13:15\nCoffee Break ☕\n\n\n13:15 - 14:00\nClosing session\n\n\n\n\n\n\n\n\nIn preparation for the workshop\nParticipants are required to follow the next steps before the day of the workshop:\n\nInstall R-INLA\nInstall inlabru (available from CRAN)\n\n\n# Enable universe(s) by inlabru-org\noptions(repos = c(\n  inlabruorg = \"https://inlabru-org.r-universe.dev\",\n  INLA = \"https://inla.r-inla-download.org/R/testing\",\n  CRAN = \"https://cloud.r-project.org\"\n))\n\n# Install some packages\ninstall.packages(\"inlabru\")\n\n\nMake sure you have the latest R-INLA, inlabru and R versions installed.\nInstall the following libraries:\n\n\n\ninstall.packages(c(\n  \"CARBayesdata\",\n  \"DAAG\",\n  \"dplyr\",\n  \"FSAdata\",\n  \"ggplot2\",\n  \"gstat\",\n  \"gt\",\n  \"lubridate\",\n  \"magrittr\",\n  \"mapview\",\n  \"patchwork\",\n  \"scico\",\n  \"sdmTMB\",\n  \"sf\",\n  \"spatstat\",\n  \"spdep\",\n  \"terra\",\n  \"tidyr\",\n  \"tidyterra\",\n  \"tidyverse\",\n  \"variosig\"\n))",
    "crumbs": [
      "Home",
      "`inlabru` Workshop"
    ]
  },
  {
    "objectID": "day1_practical.html",
    "href": "day1_practical.html",
    "title": "Practical 1",
    "section": "",
    "text": "Aim of this practical:\nIn this first practical we are going to look at some simple models\nwe are going to learn:\nDownload Practical 1 R script",
    "crumbs": [
      "Home",
      "Practical 1"
    ]
  },
  {
    "objectID": "day1_practical.html#sec-linmodel",
    "href": "day1_practical.html#sec-linmodel",
    "title": "Practical 1",
    "section": "Linear Model",
    "text": "Linear Model\nIn this practical we will:\n\nSimulate Gaussian data\nLearn how to fit a linear model with inlabru\nGenerate predictions from the model\n\nStart by loading useful libraries:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n# load some libraries to generate nice plots\nlibrary(scico)\n\nAs our first example we consider a simple linear regression model with Gaussian observations \\[\ny_i\\sim\\mathcal{N}(\\mu_i, \\sigma^2), \\qquad i = 1,\\dots,N\n\\]\nwhere \\(\\sigma^2\\) is the observation error, and the mean parameter \\(\\mu_i\\) is linked to the linear predictor (\\(\\eta_i\\)) through an identity function: \\[\n\\eta_i = \\mu_i = \\beta_0 + \\beta_1 x_i\n\\] where \\(x_i\\) is a covariate and \\(\\beta_0, \\beta_1\\) are parameters to be estimated. We assign \\(\\beta_0\\) and \\(\\beta_1\\) a vague Gaussian prior.\nTo finalize the Bayesian model we assign a \\(\\text{Gamma}(a,b)\\) prior to the precision parameter \\(\\tau = 1/\\sigma^2\\) and two independent Gaussian priors with mean \\(0\\) and precision \\(\\tau_{\\beta}\\) to the regression parameters \\(\\beta_0\\) and \\(\\beta_1\\) (we will use the default prior settings in INLA for now).\n\n\n\n\n\n\n Question\n\n\n\nWhat is the dimension of the hyperparameter vector and latent Gaussian field?\n\n\nAnswer\n\nThe hyperparameter vector has dimension 1, \\(\\pmb{\\theta} = (\\tau)\\) while the latent Gaussian field \\(\\pmb{u} = (\\beta_0, \\beta_1)\\) has dimension 2, \\(0\\) mean, and sparse precision matrix:\n\\[\n\\pmb{Q} = \\begin{bmatrix}\n\\tau_{\\beta_0} & 0\\\\\n0 & \\tau_{\\beta_1}\n\\end{bmatrix}\n\\] Note that, since \\(\\beta_0\\) and \\(\\beta_1\\) are fixed effects, the precision parameters \\(\\tau_{\\beta_0}\\) and \\(\\tau_{\\beta_1}\\) are fixed.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can write the linear predictor vector \\(\\pmb{\\eta} = (\\eta_1,\\dots,\\eta_N)\\) as\n\\[\n\\pmb{\\eta} = \\pmb{A}\\pmb{u} = \\pmb{A}_1\\pmb{u}_1 + \\pmb{A}_2\\pmb{u}_2 = \\begin{bmatrix}\n1 \\\\\n1\\\\\n\\vdots\\\\\n1\n\\end{bmatrix} \\beta_0 + \\begin{bmatrix}\nx_1 \\\\\nx_2\\\\\n\\vdots\\\\\nx_N\n\\end{bmatrix} \\beta_1\n\\]\nOur linear predictor consists then of two components: an intercept and a slope.\n\n\n\nSimulate example data\nFirst, we simulate data from the model\n\\[\ny_i\\sim\\mathcal{N}(\\eta_i,0.1^2), \\ i = 1,\\dots,100\n\\]\nwith\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_i\n\\]\nwhere \\(\\beta_0 = 2\\), \\(\\beta_1 = 0.5\\) and the values of the covariate \\(x\\) are generated from an Uniform(0,1) distribution. The simulated response and covariate data are then saved in a data.frame object.\n\n\nSimulate Data from a LM\nbeta = c(2,0.5)\nsd_error = 0.1\n\nn = 100\nx = rnorm(n)\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error)\n\ndf = data.frame(y = y, x = x)  \n\n\n\n\nFitting a linear regression model with inlabru\n\nDefining model components\nThe model has two parameters to be estimated \\(\\beta_1\\) and \\(\\beta_2\\). We need to define the two corresponding model components:\n\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n\nThe cmp object is here used to define model components. We can give them any useful names we like, in this case, beta_0 and beta_1.\n\n\n\n\n\n\nNote\n\n\n\nNote that we have excluded the default Intercept term in the model by typing -1 in the model components. However, inlabru has automatic intercept that can be called by typing Intercept() , which is one of inlabru special names and it is used to define a global intercept, e.g.\n\ncmp =  ~  Intercept(1) + beta_1(x, model = \"linear\")\n\n\n\nObservation model construction\nThe next step is to construct the observation model by defining the model likelihood. The most important inputs here are the formula, the family and the data.\nThe formula defines how the components should be combined in order to define the model predictor.\n\nformula = y ~ beta_0 + beta_1\n\n\n\n\n\n\n\nNote\n\n\n\nIn this case we can also use the shortcut formula = y ~ .. This will tell inlabru that the model is linear and that it is not necessary to linearize the model and assess convergence.\n\n\nThe likelihood is defined using the bru_obs() function as follows:\n\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n\nFit the model\nWe fit the model using the bru() functions which takes as input the components and the observation model:\n\nfit.lm = bru(cmp, lik)\n\nExtract results\nThe summary() function will give access to some basic information about model fit and estimates\n\nsummary(fit.lm)\n## inlabru version: 2.13.0\n## INLA version: 25.08.21-1\n## Components:\n## beta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\n## beta_1: main = linear(x), group = exchangeable(1L), replicate = iid(1L), NULL\n## Observation models:\n##   Family: 'gaussian'\n##     Tag: &lt;No tag&gt;\n##     Data class: 'data.frame'\n##     Response class: 'numeric'\n##     Predictor: y ~ .\n##     Additive/Linear: TRUE/TRUE\n##     Used components: effects[beta_0, beta_1], latent[]\n## Time used:\n##     Pre = 0.386, Running = 0.276, Post = 0.105, Total = 0.768 \n## Fixed effects:\n##         mean    sd 0.025quant 0.5quant 0.975quant  mode kld\n## beta_0 2.002 0.009      1.984    2.002      2.021 2.002   0\n## beta_1 0.515 0.010      0.495    0.515      0.535 0.515   0\n## \n## Model hyperparameters:\n##                                           mean    sd 0.025quant 0.5quant\n## Precision for the Gaussian observations 117.99 16.68      87.61   117.20\n##                                         0.975quant   mode\n## Precision for the Gaussian observations     152.89 115.63\n## \n## Marginal log-Likelihood:  74.28 \n##  is computed \n## Posterior summaries for the linear predictor and the fitted values are computed\n## (Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\nWe can see that both the intercept and slope and the error precision are correctly estimated.\n\n\nGenerate model predictions\n\nNow we can take the fitted bru object and use the predict function to produce predictions for \\(\\mu\\) given a new set of values for the model covariates or the original values used for the model fit\n\nnew_data = data.frame(x = c(df$x, runif(10)),\n                      y = c(df$y, rep(NA,10)))\npred = predict(fit.lm, new_data, ~ beta_0 + beta_1,\n               n.samples = 1000)\n\nThe predict function generate samples from the fitted model. In this case we set the number of samples to 1000.\n\nPlotR Code\n\n\n\n\n\n\n\nData and 95% credible intervals\n\n\n\n\n\n\n\n\nCode\npred %&gt;% ggplot() + \n  geom_point(aes(x,y), alpha = 0.3) +\n  geom_line(aes(x,mean)) +\n  geom_line(aes(x, q0.025), linetype = \"dashed\")+\n  geom_line(aes(x, q0.975), linetype = \"dashed\")+\n  xlab(\"Covariate\") + ylab(\"Observations\")\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nGenerate predictions for a new observation with \\(x_0 = 0.45\\)\n\n\nTake hint\n\nYou can create a new data frame containing the new observation \\(x_0\\) and then use the predict function.\n\n\n\n\nClick here to see the solution\n\n\nCode\nnew_data = data.frame(x = 0.45)\npred = predict(fit.lm, new_data, ~ beta_0 + beta_1,\n               n.samples = 1000)",
    "crumbs": [
      "Home",
      "Practical 1"
    ]
  },
  {
    "objectID": "day1_practical.html#linear-mixed-model",
    "href": "day1_practical.html#linear-mixed-model",
    "title": "Practical 1",
    "section": "Linear Mixed Model",
    "text": "Linear Mixed Model\nIn this practical we will:\n\nUnderstand the basic structure of a Linear Mixed Model (LLM)\nSimulate data from a LMM\nLearn how to fit a LMM with inlabru and predict from the model.\n\nConsider the a simple linear regression model except with the addition that the data that comes in groups. Suppose that we want to include a random effect for each group \\(j\\) (equivalent to adding a group random intercept). The model is then: \\[\ny_{ij}  = \\beta_0 + \\beta_1 x_i + u_j + \\epsilon_{ij} ~~~  \\text{for}~i = 1,\\ldots,N~ \\text{and}~ j = 1,\\ldots,m.\n\\]\nHere the random group effect is given by the variable \\(u_j \\sim \\mathcal{N}(0, \\tau^{-1}_u)\\) with \\(\\tau_u = 1/\\sigma^2_u\\) describing the variability between groups (i.e., how much the group means differ from the overall mean). Then, \\(\\epsilon_j \\sim \\mathcal{N}(0, \\tau^{-1}_\\epsilon)\\) denotes the residuals of the model and \\(\\tau_\\epsilon = 1/\\sigma^2_\\epsilon\\) captures how much individual observations deviate from their group mean (i.e., variability within group).\nThe model design matrix for the random effect has one row for each observation (this is equivalent to a random intercept model). The row of the design matrix associated with the \\(ij\\)-th observation consists of zeros except for the element associated with \\(u_j\\), which has a one.\n\\[\n\\pmb{\\eta} = \\pmb{A}\\pmb{u} = \\pmb{A}_1\\pmb{u}_1 + \\pmb{A}_2\\pmb{u}_2 + \\pmb{A}_3\\pmb{u}_3\n\\]\n\n\n\n\n\n\nSupplementary material: LMM as a LGM\n\n\n\nIn matrix form, the linear mixed model for the j-th group can be written as:\n\\[ \\overbrace{\\mathbf{y}_j}^{ N \\times 1} = \\overbrace{X_j}^{ N \\times 2} \\underbrace{\\beta}_{1\\times 1} + \\overbrace{Z_j}^{n_j \\times 1} \\underbrace{u_j}_{1\\times1} + \\overbrace{\\epsilon_j}^{n_j \\times 1}, \\]\nIn a latent Gaussian model (LGM) formulation the mixed model predictor for the i-th observation can be written as :\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_i + \\sum_k^K f_k(u_j)\n\\]\nwhere \\(f_k(u_j) = u_j\\) since there’s only one random effect per group (i.e., a random intercept for group \\(j\\)). The fixed effects \\((\\beta_0,\\beta_1)\\) are assigned Gaussian priors (e.g., \\(\\beta \\sim \\mathcal{N}(0,\\tau_\\beta^{-1})\\)). The random effects \\(\\mathbf{u} = (u_1,\\ldots,u_m)^T\\) follow a Gaussian density \\(\\mathcal{N}(0,\\mathbf{Q}_u^{-1})\\) where \\(\\mathbf{Q}_u = \\tau_u\\mathbf{I}_m\\) is the precision matrix for the random intercepts. Then, the components for the LGM are the following:\n\nLatent field given by\n\\[\n\\begin{bmatrix} \\beta \\\\\\mathbf{u}\n\\end{bmatrix} \\sim \\mathcal{N}\\left(\\mathbf{0},\\begin{bmatrix}\\tau_\\beta^{-1}\\mathbf{I}_2&\\mathbf{0}\\\\\\mathbf{0} &\\tau_u^{-1}\\mathbf{I}_m\\end{bmatrix}\\right)\n\\]\nLikelihood:\n\\[\ny_i \\sim \\mathcal{N}(\\eta_i,\\tau_{\\epsilon}^{-1})\n\\]\nHyperparameters:\n\n\\(\\tau_u\\sim\\mathrm{Gamma}(a,b)\\)\n\\(\\tau_\\epsilon \\sim \\mathrm{Gamma}(c,d)\\)\n\n\n\n\n\nSimulate example data\n\nset.seed(12)\nbeta = c(1.5,1)\nsd_error = 1\ntau_group = 1\n\nn = 100\nn.groups = 5\nx = rnorm(n)\nv = rnorm(n.groups, sd = tau_group^{-1/2})\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error) +\n  rep(v, each = 20)\n\ndf = data.frame(y = y, x = x, j = rep(1:5, each = 20))  \n\nNote that inlabru expects an integer indexing variable to label the groups.\n\n\nCode\nggplot(df) +\n  geom_point(aes(x = x, colour = factor(j), y = y)) +\n  theme_classic() +\n  scale_colour_discrete(\"Group\")\n\n\n\n\n\nData for the linear mixed model example with 5 groups\n\n\n\n\n\n\nFitting a LMM in inlabru\n\nDefining model components and observational model\nIn order to specify this model we must use the group argument to tell inlabru which variable indexes the groups. The model = \"iid\" tells INLA that the groups are independent from one another.\n\n# Define model components\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\") +\n  u(j, model = \"iid\")\n\nThe group variable is indexed by column j in the dataset. We have chosen to name this component v() to connect with the mathematical notation that we used above.\n\n# Construct likelihood\nlik =  like(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n\nWarning: `like()` was deprecated in inlabru 2.12.0.\nℹ Please use `bru_obs()` instead.\n\n\nFitting the model\nThe model can be fitted exactly as in the previous examples by using the bru function with the components and likelihood objects.\n\nfit = bru(cmp, lik)\nsummary(fit)\n## inlabru version: 2.13.0\n## INLA version: 25.08.21-1\n## Components:\n## beta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\n## beta_1: main = linear(x), group = exchangeable(1L), replicate = iid(1L), NULL\n## u: main = iid(j), group = exchangeable(1L), replicate = iid(1L), NULL\n## Observation models:\n##   Family: 'gaussian'\n##     Tag: &lt;No tag&gt;\n##     Data class: 'data.frame'\n##     Response class: 'numeric'\n##     Predictor: y ~ .\n##     Additive/Linear: TRUE/TRUE\n##     Used components: effects[beta_0, beta_1, u], latent[]\n## Time used:\n##     Pre = 0.336, Running = 0.28, Post = 0.177, Total = 0.794 \n## Fixed effects:\n##         mean    sd 0.025quant 0.5quant 0.975quant  mode kld\n## beta_0 2.108 0.438      1.229    2.108      2.986 2.108   0\n## beta_1 1.172 0.120      0.936    1.172      1.407 1.172   0\n## \n## Random effects:\n##   Name     Model\n##     u IID model\n## \n## Model hyperparameters:\n##                                          mean    sd 0.025quant 0.5quant\n## Precision for the Gaussian observations 0.995 0.144      0.738    0.986\n## Precision for u                         1.613 1.060      0.369    1.356\n##                                         0.975quant  mode\n## Precision for the Gaussian observations       1.30 0.971\n## Precision for u                               4.35 0.918\n## \n## Marginal log-Likelihood:  -179.93 \n##  is computed \n## Posterior summaries for the linear predictor and the fitted values are computed\n## (Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\n\nModel predictions\nTo compute model predictions we can create a data.frame containing a range of values of covariate where we want the response to be predicted for each group. Then we simply call the predict function while spe\n\n\nLMM fitted values\n# New data\nxpred = seq(range(x)[1], range(x)[2], length.out = 100)\nj = 1:n.groups\npred_data = expand.grid(x = xpred, j = j)\npred = predict(fit, pred_data, formula = ~ beta_0 + beta_1 + u) \n\n\npred %&gt;%\n  ggplot(aes(x=x,y=mean,color=factor(j)))+\n  geom_line()+\n  geom_ribbon(aes(x,ymin = q0.025, ymax= q0.975,fill=factor(j)), alpha = 0.5) + \n  geom_point(data=df,aes(x=x,y=y,colour=factor(j)))+\n  facet_wrap(~j)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Question\n\n\n\nSuppose that we are also interested in including random slopes into our model. Assuming intercept and slopes are independent, can your write down the linear predictor and the components of this model as a LGM?\n\n\nGive me a hint\n\nIn general, the mixed model predictor can decomposed as:\n\\[ \\pmb{\\eta} = X\\beta + Z\\mathbf{u} \\]\nWhere \\(X\\) is a \\(n \\times p\\) design matrix and \\(\\beta\\) the corresponding p-dimensional vector of fixed effects. Then \\(Z\\) is a \\(n\\times q_J\\) design matrix for the \\(q_J\\) random effects and \\(J\\) groups; \\(\\mathbf{v}\\) is then a \\(q_J \\times 1\\) vector of \\(q\\) random effects for the \\(J\\) groups. In a latent Gaussian model (LGM) formulation this can be written as:\n\\[ \\eta_i = \\beta_0 + \\sum\\beta_j x_{ij} + \\sum_k f(k) (u_{ij}) \\]\n\n\n\nSee Solution\n\n\nThe linear predictor is given by\n\\[\n\\eta_i = \\beta_0 + \\beta_1x_i + u_{0j} + u_{1j}x_i\n\\]\nLatent field defined by:\n\n\\(\\beta \\sim \\mathcal{N}(0,\\tau_\\beta^{-1})\\)\n\\(\\mathbf{u}_j = \\begin{bmatrix}u_{0j} \\\\ u_{1j}\\end{bmatrix}, \\mathbf{u}_j \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{Q}_u^{-1})\\) where the precision matrix is a block-diagonal matrix with entries \\(\\mathbf{Q}_u= \\begin{bmatrix}\\tau_{u_0} & {0} \\\\{0} & \\tau_{u_1}\\end{bmatrix}\\)\n\nThe hyperparameters are then:\n\n\\(\\tau_{u_0},\\tau_{u_1} \\text{and}~\\tau_\\epsilon\\)\n\n\nTo fit this model in inlabru we can simply modify the model components as follows:\n\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\") +\n  u0(j, model = \"iid\") + u1(j,x, model = \"iid\")",
    "crumbs": [
      "Home",
      "Practical 1"
    ]
  },
  {
    "objectID": "day1_practical.html#sec-genlinmodel",
    "href": "day1_practical.html#sec-genlinmodel",
    "title": "Practical 1",
    "section": "Generalized Linear Model",
    "text": "Generalized Linear Model\nIn this practical we will:\n\nSimulate non-Gaussian data\nLearn how to fit a generalised linear model with inlabru\nGenerate predictions from the model\n\nA generalised linear model allows for the data likelihood to be non-Gaussian. In this example we have a discrete response variable which we model using a Poisson distribution. Thus, we assume that our data \\[\ny_i \\sim \\text{Poisson}(\\lambda_i)\n\\] with rate parameter \\(\\lambda_i\\) which, using a log link, has associated predictor \\[\n\\eta_i = \\log \\lambda_i = \\beta_0 + \\beta_1 x_i\n\\] with parameters \\(\\beta_0\\) and \\(\\beta_1\\), and covariate \\(x\\). This is identical in form to the predictor in Section 1. The only difference is now we must specify a different data likelihood.\n\nSimulate example data\nThis code generates 100 samples of covariate x and data y.\n\nset.seed(123)\nn = 100\nbeta = c(1,1)\nx = rnorm(n)\nlambda = exp(beta[1] + beta[2] * x)\ny = rpois(n, lambda  = lambda)\ndf = data.frame(y = y, x = x)  \n\n\n\nFitting a GLM in inlabru\n\nDefine model components and likelihood\nSince the predictor is the same as Section 1, we can use the same component definition:\n\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n\nHowever, when building the observation model likelihood we must now specify the Poisson likelihood using the family argument (the default link function for this family is the \\(\\log\\) link).\n\nlik =  bru_obs(formula = y ~.,\n            family = \"poisson\",\n            data = df)\n\nFit the model\nOnce the likelihood object is constructed, fitting the model is exactly the same process as in Section 1.\n\nfit_glm = bru(cmp, lik)\n\nAnd model summaries can be viewed using\n\nsummary(fit_glm)\n\ninlabru version: 2.13.0\nINLA version: 25.08.21-1\nComponents:\nbeta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\nbeta_1: main = linear(x), group = exchangeable(1L), replicate = iid(1L), NULL\nObservation models:\n  Family: 'poisson'\n    Tag: &lt;No tag&gt;\n    Data class: 'data.frame'\n    Response class: 'integer'\n    Predictor: y ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[beta_0, beta_1], latent[]\nTime used:\n    Pre = 0.301, Running = 0.245, Post = 0.0397, Total = 0.586 \nFixed effects:\n        mean    sd 0.025quant 0.5quant 0.975quant  mode kld\nbeta_0 0.915 0.071      0.775    0.915      1.054 0.915   0\nbeta_1 1.048 0.056      0.938    1.048      1.157 1.048   0\n\nMarginal log-Likelihood:  -204.02 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\n\n\nGenerate model predictions\n\nTo generate new predictions we must provide a data frame that contains the covariate values for \\(x\\) at which we want to predict.\nThis code block generates predictions for the data we used to fit the model (contained in df$x) as well as 10 new covariate values sampled from a uniform distribution runif(10).\n\n# Define new data, set to NA the values for prediction\n\nnew_data = data.frame(x = c(df$x, runif(10)),\n                      y = c(df$y, rep(NA,10)))\n\n# Define predictor formula\npred_fml &lt;- ~ exp(beta_0 + beta_1)\n\n# Generate predictions\npred_glm &lt;- predict(fit_glm, new_data, pred_fml)\n\nSince we used a log link (which is the default for family = \"poisson\"), we want to predict the exponential of the predictor. We specify this using a general R expression using the formula syntax.\n\n\n\n\n\n\nNote\n\n\n\nNote that the predict function will call the component names (i.e. the “labels”) that were decided when defining the model.\n\n\nSince the component definition is looking for a covariate named \\(x\\), all we need to provide is a data frame that contains one, and the software does the rest.\n\nPlotR Code\n\n\n\n\n\n\n\nData and 95% credible intervals\n\n\n\n\n\n\n\npred_glm %&gt;% ggplot() + \n  geom_point(aes(x,y), alpha = 0.3) +\n  geom_line(aes(x,mean)) +\n    geom_ribbon(aes(x = x, ymax = q0.975, ymin = q0.025),fill = \"tomato\", alpha = 0.3)+\n  xlab(\"Covariate\") + ylab(\"Observations (counts)\")\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nSuppose a binary response such that\n\\[\n    \\begin{aligned}\ny_i &\\sim \\mathrm{Bernoulli}(\\psi_i)\\\\\n\\eta_i &= \\mathrm{logit}(\\psi_i) = \\alpha_0 +\\alpha_1 \\times w_i\n\\end{aligned}\n\\] Using the following simulated data, use inlabru to fit the logistic regression above. Then, plot the predictions for the data used to fit the model along with 10 new covariate values\n\nset.seed(123)\nn = 100\nalpha = c(0.5,1.5)\nw = rnorm(n)\npsi = plogis(alpha[1] + alpha[2] * w)\ny = rbinom(n = n, size = 1, prob =  psi) # set size = 1 to draw binary observations\ndf_logis = data.frame(y = y, w = w)  \n\nHere we use the logit link function \\(\\mathrm{logit}(x) = \\log\\left(\\frac{x}{1-x}\\right)\\) (plogis() function in R) to link the linear predictor to the probabilities \\(\\psi\\).\n\n\nTake hint\n\nYou can set family = \"binomial\" for binary responses and the plogis() function for computing the predicted values.\n\n\n\n\n\n\nNote\n\n\n\nThe Bernoulli distribution is equivalent to a \\(\\mathrm{Binomial}(1, \\psi)\\) pmf. If you have proportional data (e.g. no. successes/no. trials) you can specify the number of events as your response and then the number of trials via the Ntrials = n argument of the bru_obs function (where n is the known vector of trials in your data set).\n\n\n\n\n\n\nClick here to see the solution\n\n\nCode\n# Model components\ncmp_logis =  ~ -1 + alpha_0(1) + alpha_1(w, model = \"linear\")\n# Model likelihood\nlik_logis =  bru_obs(formula = y ~.,\n            family = \"binomial\",\n            data = df_logis)\n# fit the model\nfit_logis &lt;- bru(cmp_logis,lik_logis)\n\n# Define data for prediction\nnew_data = data.frame(w = c(df_logis$w, runif(10)),\n                      y = c(df_logis$y, rep(NA,10)))\n# Define predictor formula\npred_fml &lt;- ~ plogis(alpha_0 + alpha_1)\n\n# Generate predictions\npred_logis &lt;- predict(fit_logis, new_data, pred_fml)\n\n# Plot predictions\npred_logis %&gt;% ggplot() + \n  geom_point(aes(w,y), alpha = 0.3) +\n  geom_line(aes(w,mean)) +\n    geom_ribbon(aes(x = w, ymax = q0.975, ymin = q0.025),fill = \"tomato\", alpha = 0.3)+\n  xlab(\"Covariate\") + ylab(\"Observations\")",
    "crumbs": [
      "Home",
      "Practical 1"
    ]
  },
  {
    "objectID": "day1_practical.html#sec-gam_ex",
    "href": "day1_practical.html#sec-gam_ex",
    "title": "Practical 1",
    "section": "Generalised Additive Model",
    "text": "Generalised Additive Model\nIn this practical we will:\n\nLearn how to fit a GAM with inlabru\nGenerate predictions from the model\n\nGeneralised Additive Models (GAMs) are very similar to linear models, but with an additional basis set that provides flexibility.\nAdditive models are a general form of statistical model which allows us to incorporate smooth functions alongside linear terms. A general expression for the linear predictor of a GAM is given by\n\\[\n\\eta_i = g(\\mu_i) = \\beta_0 + \\sum_{j=1}^L f_j(x_{ij})\n\\]\nwhere the mean \\(\\pmb{\\mu} = E(\\mathbf{y}|\\mathbf{x}_1,\\ldots,\\mathbf{x}_L)\\) and \\(g()\\) is a link function (notice that the distribution of the response and the link between the predictors and this distribution can be quite general). The term \\(f_j()\\) is a smooth function for the j-th explanatory variable that can be represented as\n\\[\nf(x_i) = \\sum_{k=0}^q\\beta_k b_k(x_i)\n\\]\nwhere \\(b_k\\) denote the basis functions and \\(\\beta_K\\) are their coefficients.\nIncreasing the number of basis functions leads to a more wiggly line. Too few basis functions might make the line too smooth, too many might lead to overfitting.To avoid this, we place further constraints on the spline coefficients which leads to constrained optimization problem where the objective function to be minimized is given by:\n\\[\n\\mathrm{min}\\sum_i(y_i-f(x_i))^2 + \\lambda(\\sum_kb^2_k)\n\\] The first term measures how close the function \\(f()\\) is to the data while the second term \\(\\lambda(\\sum_kb^2_k)\\), penalizes the roughness in the function. Here, \\(\\lambda &gt;0\\) is known as the smoothing parameter because it controls the degree of smoothing (i.e. the trade-off between the two terms). In a Bayesian setting,including the penalty term is equivalent to setting a specific prior on the coefficients of the covariates.\nIn this exercise we will set a random walk prior of order 1 on \\(f\\), i.e. \\(f(x_i)-f(x_i-1) \\sim \\mathcal{N}(0,\\sigma^2_f)\\) where \\(\\sigma_f^2\\) is the smoothing parameter such that small values give large smoothing. Notice that we will assume \\(x_i\\)’s are equally spaced for now (we will cover a stochastic differential equation approach that relaxes this assumption later on in the course).\n\nSimulate Data\nLets generate some data so evaluate how RW models perform when estimating a smooth curve. The data are simulated from the following model:\n\\[\ny_i = 1 + \\mathrm{cos}(x) + \\epsilon_i, ~ \\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2_\\epsilon)\n\\] where \\(\\sigma_\\epsilon^2 = 0.25\\)\n\nn = 100\nx = rnorm(n)\neta = (1 + cos(x))\ny = rnorm(n, mean =  eta, sd = 0.5)\n\ndf = data.frame(y = y, \n                x_smooth = inla.group(x)) # equidistant x's \n\n\n\nFitting a GAM in inlabru\nNow lets fit a flexible model by setting a random walk of order 1 prior on the coefficients. This can be done bye specifying model = \"rw1\" in the model component (similarly,a random walk of order 2 can be placed by setting model = \"rw2\" )\n\ncmp =  ~ Intercept(1) + \n  smooth(x_smooth, model = \"rw1\")\n\nNow we define the observational model:\n\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n\nWe then can fit the model:\n\nfit = bru(cmp, lik)\nfit$summary.fixed\n\n            mean         sd 0.025quant 0.5quant 0.975quant    mode          kld\nIntercept 1.2988 0.06506725   1.171641 1.298566   1.427294 1.29857 9.965741e-09\n\n\nThe posterior summary regarding the estimated function using RW1 can be accessed through fit$summary.random$smooth, the output includes the value of \\(x_i\\) (ID) as well as the posterior mean, standard deviation, quantiles and mode of each \\(f(x_i)\\). We can use this information to plot the posterior mean and associated 95% credible intervals.\n\nR plotR Code\n\n\n\n\n\n\n\nSmooth effect of the covariate\n\n\n\n\n\n\n\ndata.frame(fit$summary.random$smooth) %&gt;% \n  ggplot() + \n  geom_ribbon(aes(ID,ymin = X0.025quant, ymax= X0.975quant), alpha = 0.5) + \n  geom_line(aes(ID,mean)) + \n  xlab(\"covariate\") + ylab(\"\")\n\n\n\n\n\n\nModel Predictions\nWe can obtain the model predictions using the predict function.\n\npred = predict(fit, df, ~ (Intercept + smooth))\n\nThe we can plot them together with the true curve and data points:\n\n\nCode\npred %&gt;% ggplot() + \n  geom_point(aes(x_smooth,y), alpha = 0.3) +\n  geom_line(aes(x_smooth,1+cos(x_smooth)),col=2)+\n  geom_line(aes(x_smooth,mean)) +\n  geom_line(aes(x_smooth, q0.025), linetype = \"dashed\")+\n  geom_line(aes(x_smooth, q0.975), linetype = \"dashed\")+\n  xlab(\"Covariate\") + ylab(\"Observations\")\n\n\n\n\n\nData and 95% credible intervals\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nFit a flexible model using a random walk of order 2 (RW2) and compare the results with the ones above.\n\n\nTake hint\n\nYou can set model = \"rw2\" for assigning a random walk 2 prior.\n\n\n\n\nClick here to see the solution\n\n\nCode\ncmp_rw2 =  ~ Intercept(1) + \n  smooth(x_smooth, model = \"rw2\")\nlik_rw2 =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\nfit_rw2 = bru(cmp_rw2, lik_rw2)\n\n# Plot the fitted functions\nggplot() + \n  geom_line(data= fit$summary.random$smooth,aes(ID,mean,colour=\"RW1\"),lty=2) + \n  geom_line(data= fit_rw2$summary.random$smooth,aes(ID,mean,colour=\"RW2\")) + \n  xlab(\"covariate\") + ylab(\"\") + scale_color_discrete(name=\"Model\")\n\n\n\n\n\n\n\n\n\n\nWe see that the RW1 fit is too wiggly while the RW2 is smoother and seems to have better fit.",
    "crumbs": [
      "Home",
      "Practical 1"
    ]
  },
  {
    "objectID": "day2_practical_3.html",
    "href": "day2_practical_3.html",
    "title": "Practical 3",
    "section": "",
    "text": "Aim of this practical:\nwe are going to learn:\nDownload Practical 3 R script",
    "crumbs": [
      "Home",
      "Practical 3"
    ]
  },
  {
    "objectID": "day2_practical_3.html#ar1-models-in-inlabru",
    "href": "day2_practical_3.html#ar1-models-in-inlabru",
    "title": "Practical 3",
    "section": "AR(1) models in inlabru",
    "text": "AR(1) models in inlabru\nIn this exercise we will:\n\nSimulate a time series with autocorrelated errors.\nFit an AR(1) process with inlabru\nVisualize model predictions.\nForecasting for future observations\n\nStart by loading useful libraries:\n\nlibrary(tidyverse)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nTime series analysis is particularly valuable for modelling data with temporal dependence or autocorrelation, where observations taken at nearby time points tend to be more similar than those further apart.\nA time series process is a stochastic process \\(\\{X_t~|~t \\in T\\}\\), which is a collection of random variables that are ordered in time where \\(T\\) is the index set that determines the set of discrete and equally spaced time points at which the process is defined and observations are made.\nAutoregressive processes allow us to account for the time dependence by regressing \\(X_t\\) on past values \\(X_{t-1},\\ldots,X_{t-p}\\) with associated coefficients \\(\\phi_k\\) for each lag \\(k = 1,\\ldots,p\\). Thus an autoregressive process of order \\(p\\), denoted AR(\\(p\\)) , is given by:\n\\[\nX_t = \\phi_1 X_{t-1} + \\ldots + \\phi_p X_{t-p} + \\varepsilon_t; ~~ \\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2_e)\n\\]\nConsider now an univariate time series \\(y_t\\) which evolves over time according to some autoregressive stochastic process. For example, a time series where the system follows an AR(1) process can be defined as:\n\\[\n\\begin{aligned}\ny_t &\\sim \\mathcal{N}(\\mu_t,\\tau_y^{-1})\\\\\n\\eta_t &= g^{-1}(\\mu_t) = \\alpha + u_t \\\\\nu_t &= \\phi u_{t-1} + \\delta_t ; ~~ \\delta_t \\sim \\mathcal{N}(0,\\tau_u^{-1}); ~~ t &gt; 1 \\\\\nu_1 &= \\mathcal{N}(0,\\kappa^{-1})\\\\\n\\kappa &= \\tau_u (1-\\phi^2)\n\\end{aligned}\n\\]\nThe response \\(y_t\\) is assumed to be normal distributed with mean \\(\\alpha + u_t\\) and precision error \\(\\tau_y\\) ( here \\(g(\\cdot)\\) is just the identity link function that maps the linear predictor to the mean of the process). Then, the process \\(u_t\\) follows an AR(1) process where \\(u_1\\) is drawn from a stationary normal distribution such that \\(\\kappa\\) denotes the marginal precision for state \\(u_t\\)\nThe covariance matrix is then given by:\n\\[\n\\Sigma = \\frac{\\tau^{-1}_u}{1-\\phi^2}\n\\begin{bmatrix}\n1 & \\phi & \\phi^2 & \\ldots & \\phi^{n-1}\\\\\n\\phi & 1 & \\phi & \\ldots & \\phi^{n-2} \\\\\n\\phi^2 & \\phi & 1 & \\ldots & \\phi^{n-3} \\\\\n\\phi^{n-1} & \\phi^{n-2} & \\phi^{n-3} & \\ldots & 1\n\\end{bmatrix}\n\\]\nNotice that conditionally on \\(u_t\\), the observed data \\(y_y\\) are independent from\\(y_{t-1},y_{t-2}.y_{t-3},\\ldots\\), also the conditional distribution of \\(u_t\\) is a markov chain such that \\(\\pi(u_t|u_{t-1},u_{t-2},u_{t-3}) = \\pi(u_t|u_{t-1})\\). Thus, each time point is only conditionally dependent on the two closest time points:\n\\[\nu_t|\\mathbf{u}_{-t} \\sim \\mathcal{N}\\left(\\frac{\\phi}{1-\\phi^2}(u_{t-1}+u_{t+1}),\\frac{\\tau_u^{-1}}{1-\\phi^2}\\right)\n\\]\n\nSimulate example data\nFirst, we simulate data from the model:\n\\[\n\\begin{aligned}\ny_t &= \\alpha + u_t + \\varepsilon_t;~ \\varepsilon_t \\sim \\mathcal{N}(0,\\tau_y^{-1})\\\\\nu_t &= \\phi y_{t-1} + \\delta_t; ~ \\delta_t \\sim \\mathcal{N}(0,\\tau_u^{-1})\n\\end{aligned}\n\\]\n\nset.seed(123)\n\nphi = 0.8\ntau_u = 10\nmarg.prec = tau_u * (1-phi^2) # ar1 in INLA is parametrized as marginal variance\nu_t =  as.vector(arima.sim(list(order = c(1,0,0), ar = phi), \n                          n = 100,\n                          sd=sqrt(1/tau_u)))\na = 1\ntau_e = 5\nepsilon_t = rnorm(100, sd = sqrt(1/tau_e))\ny = a + u_t + epsilon_t\n\n\nts_dat &lt;- data.frame(y =y , x= 1:100)\n\n\n\n\n\n\n\n\n\n\n\n\nFitting an AR(1) model with inlabru\nModel components\nFirst, we define the model components, notice that the latent field is defined by two components: the intercept \\(\\alpha\\) and the autoregressive random effects \\(u_t\\):\n\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(x,model = \"ar1\")\n\nThe we can define the formula for the linear predictor and specify the observational model\n\n# Model formula\nformula = y ~ alpha + ut\n# Observational model\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = ts_dat)\n\nLastly, we fit the model using the bru function and compare the model estimates with the true mdeol parameters we simulate our data from.\n\n# fit the model\nfit.ar1 = bru(cmp, lik)\n\n# compare against the true values\n\ndata.frame(\n  true = c(a,tau_e,marg.prec,phi),\n  rbind(fit.ar1$summary.fixed[,c(1,3,5)],\n        fit.ar1$summary.hyperpar[,c(1,3,5)])\n        ) %&gt;% round(2)\n\n                                        true mean X0.025quant X0.975quant\nalpha                                    1.0 1.00        0.69        1.28\nPrecision for the Gaussian observations  5.0 4.91        3.11        7.29\nPrecision for ut                         3.6 7.96        2.91       18.09\nRho for ut                               0.8 0.80        0.54        0.94\n\n\nModel predictions\nHere, we will predict the mean of our time series along with 95% credible intervals. Note that this interval are for the mean and not for new observations, we will cover forecasting new observations next.\n\npred_ar1 = predict(fit.ar1, ts_dat, ~ alpha + ut)\n\nggplot(pred_ar1,aes(y=mean,x=x))+\n  geom_line()+\n    geom_ribbon(aes(x = x, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(aes(y=y,x=x))\n\n\n\n\n\n\n\n\n\n\nForecasting\nA common goal in time series modelling is forecasting into the future. Forecasting can be treated as a missing data problem where future values of the response variable are missing. Let \\(y_m\\) be the missing response, then, by fitting a statistical model to the observed data \\(\\mathbf{y}_{obs}\\), we condition on its parameters to obtain the posterior predictive distribution:\n\\[\n\\pi(y_{m} \\mid \\mathbf{y}_{obs}) = \\int \\pi(y_{m}, \\theta \\mid \\mathbf{y}_{obs})  d\\theta = \\int \\pi(y_{m} \\mid \\mathbf{y}_{obs}, \\theta) \\pi(\\theta \\mid \\mathbf{y}_{obs})  d\\theta\n\\]\nThis distribution, which integrates over all parameter uncertainty, provides the complete probabilistic forecast for the missing values. INLA will automatically compute the predictive distributions for all missing values in the response. To do so, we can augment our data set by including the new time points at which the prediction will be made and setting the response value to NA for these new time points:\n\nts.forecast &lt;- rbind(ts_dat, \n  data.frame(y = rep(NA, 50), x = 101:150))\n\nNext, we fit the ar1 model to the new dataset so that the predictive distributions are computed:\n\ncmp =  ~ -1 + alpha(1) + ut(x,model = \"ar1\")\n\npred_lik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = ts.forecast)\n\nfit.forecast = bru(cmp, pred_lik)\n\nLastly, we can draw samples from the posterior predictive distribution using the predict function and visualize our forecast as follows:\n\npred_forecast = predict(fit.forecast, ts.forecast, ~ alpha + ut)\n\np1= ggplot(pred_forecast,aes(y=mean,x=x))+\n  geom_line()+\n    geom_ribbon(aes(x = x, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(data=ts_dat, aes(y=y,x=x))",
    "crumbs": [
      "Home",
      "Practical 3"
    ]
  },
  {
    "objectID": "day2_practical_3.html#modelling-great-lakes-water-level",
    "href": "day2_practical_3.html#modelling-great-lakes-water-level",
    "title": "Practical 3",
    "section": "Modelling Great Lakes water level",
    "text": "Modelling Great Lakes water level\nIn this exercise we will:\n\nFit an AR(1) process with inlabru to model lakes water levels\nChange the default priors for the observational error\nSet penalized complexity priors for the correlation and precision parameters of the latent effects.\nFit a RW(1) model\nFit a an AR(1) model with group-level correlation\n\nStart by loading useful libraries:\n\nlibrary(tidyverse) \nlibrary(INLA) \nlibrary(ggplot2)\nlibrary(patchwork) \nlibrary(inlabru)\nlibrary(DAAG)\n\nIn this exercise we will look at greatLakes dataset from the DAAG package. The data set contains the water level heights for the lakes Erie, Michigan/Huron, Ontario and St Clair from 1918 to 2009.\nLets begin by loading and formatting the data into a tidy format.\n\ndata(\"greatLakes\")\n\ngreatLakes.df = data.frame(as.matrix(greatLakes),\n                           year = time(greatLakes)) %&gt;%\n  pivot_longer(cols = c(\"Erie\",\"michHuron\",\"Ontario\",\"StClair\"),\n               names_to = \"Lakes\",\n               values_to = \"height\" ) \n\n\n\n\n\n\n\n\n\n\n\nFitting an AR(1) model in inlabru\nWe will focus on the Erin lake for now. Lets begin by fitting an AR(1) model of the form:\n\\[\n\\begin{aligned}\n\\text{height}_t &= \\alpha + u_t +\\varepsilon_t~; ~~ \\varepsilon_t\\sim \\mathcal{N}(0,\\tau_e^{-1}) \\\\\nu_t &= \\phi u_{t-1} + \\delta_t~~~ ; ~~ \\delta_t \\sim \\mathcal{N}(0,\\tau_u^{-1}); ~~ t &gt; 1 \\\\\nx_1 &= \\mathcal{N}(0,\\kappa^{-1})\\\\\n\\kappa &= \\tau_u (1-\\phi^2)\n\\end{aligned}\n\\]\nWhere \\(\\alpha\\) is the intercept, \\(\\phi\\) is the correlation term, \\(\\varepsilon\\) is the observational Gaussian error with mean zero and precision \\(\\tau_e\\) and \\(\\kappa\\) is the marginal precision for the state \\(u_t\\) for \\(t= 1,\\ldots,92\\).\nFirst we make a subset of the dataset and create a time index \\(T\\):\n\ngreatLakes.df$t.idx &lt;- greatLakes.df$year-1917\n\nErie.df = greatLakes.df %&gt;% filter(Lakes == \"Erie\")\n\n\n\n\n\n\n\n Task\n\n\n\nFit an AR(1) model to the Erie lake data using inlabru, then plot the model fitted values showing 95% credible intervals.\n\n\nTake hint\n\nRemember this is done by (1) defining the model components, (2) the formula and (3) the observational model. Then you can use the predict function to compute the predicted values for the mean along with 95% credible intervals.\n\n\n\n\nClick here to see the solution\n\n\nCode\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(t.idx,model = \"ar1\")\n# Model formula\nformula = height ~ alpha + ut\n\n\n# Observational model\nlik =  bru_obs(formula = height   ~.,\n            family = \"gaussian\",\n            data = Erie.df )\n\n# fit the model\nfit.Erie_ar1 = bru(cmp, lik)\n\n# Model predictions \n\npred_ar1.Erie = predict(fit.Erie_ar1, Erie.df, ~ alpha + ut)\n\n# plot model fitted values\nggplot(pred_ar1.Erie,aes(y=mean,x=year))+\n  geom_line()+\n    geom_ribbon(aes(x = year, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(aes(y=height,x=year))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Question\n\n\n\nAre there any issues with the fitted model, and if so, how do you think we should address them?\n\n\nAnswer\n\nIt is clear that the model overfits the data, leading to poor predictive performance. Thus, we need to introduce some prior information on the what we expect the variation of the process to be.\n\n\n\nPriors\nLet review INLA’ prior parametrization for autoregressive models:\nLet \\(\\pmb{\\theta} = \\{\\theta_y,\\theta_u,\\theta_\\phi\\}\\) be INLA’s internal representation of the hyperparameters such that:\n\\(\\theta_y = \\log(\\tau^2_y)\\)\n\\(\\theta_u = \\log(\\kappa) = \\log\\left(\\tau_u[1-\\phi^2]\\right)\\)\n\\(\\theta_\\phi = \\log \\left(\\frac{1+\\phi}{1-\\phi}\\right)\\)\nThe default priors for \\(\\{\\theta_y,\\theta_u\\}\\) are \\(\\text{log-gamma} (1, 5\\times 10^{-5} )\\) priors with default initial values set to 4 in each case. Then, Gaussian priors \\(\\alpha \\sim \\mathcal{N}(0,\\tau_y = 0.001)\\) and \\(\\theta_\\phi \\sim \\mathcal{N}(0, \\tau_y= 0.15)\\) are used for the intercept and correlation parameter respectively.\n\n\n\n\n\n\nNote\n\n\n\nSpecifically for AR(1) correlation parameter \\(\\phi\\), INLA uses the following logit transformation on \\(\\theta_\\phi\\):\n\\[ \\phi = \\frac{2\\exp(\\theta_\\phi)}{1+ \\exp(\\theta_\\phi)} -1. \\]\n\n\nSetting priors and PC-priors\nLets now set a Gamma prior with parameters 1 and 1, so that the precision of the Gaussian osbervational error is centered at 1 with a variance of 1. Additionally we will set Penalized Complexity (PC) priors according to the following probability statements:\n\n\\(P(\\sigma &gt; 1) = 0.01\\)\n\\(P(\\phi &gt; 0.5) = 0.3\\)\n\nNotice that the PC prior for the precision \\(\\tau_u\\) is defined on the standard deviation \\(\\sigma_u = \\tau_u^{-1/2}\\)\n\npc_prior &lt;- list(theta = list(prior = \"pc.prec\", param = c(1, 0.01)),\n                 rho = list(prior = \"pc.cor0\", param = c(0.5, 0.3))) \n\nprec.tau_e &lt;- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(1, 1))) # prior values\n\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(t.idx, model = \"ar1\",  hyper = pc_prior)\n# Model formula\nformula = height ~ alpha + ut\n\n\n# Observational model\nlik =  bru_obs(formula = height  ~.,\n            family = \"gaussian\",\n            data = Erie.df,\n            control.family = list(hyper = prec.tau_e))\n\n# fit the model\nfit.Erie_ar1 = bru(cmp, lik)\n\n\n\n\n\n\n\n Question\n\n\n\nWhat is the posterior mean for the correlation parameter \\(\\rho\\)? \n\n\n\n\n\n\n\n\n Task\n\n\n\nPlot the fitted values of the model, has the overfitting problem being alleviated?\n\n\n\nClick here to see the solution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting a RW(1) model\nNow we fit a random walk of order 1 to the Erie lake data:\n\\[\n\\begin{aligned}\ny_t &= \\alpha + u_t + \\varepsilon_t, ~ t = 1,\\ldots,92 \\\\\n\\varepsilon_t & \\sim \\mathcal{N}(0,\\tau_e) \\\\\nu_t - u_{t-1} &\\sim \\mathcal{N}(0,\\tau_u),~ t = 2,\\ldots,92 \\\\\n\\end{aligned}\n\\]\nFirs we define model priors:\n\npc_prior &lt;- list(theta = list(prior = \"pc.prec\", param = c(1, 0.01))) \n\nprec.tau_e &lt;- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(1, 1))) # prior values\n\nNow we define model components:\n\ncmp_rw =  ~ -1 + alpha(1) + \n  ut(t.idx ,\n     constr=FALSE,\n     model = \"rw1\",\n     hyper=pc_prior,\n     scale.model = TRUE)\n\nNotice that we have set scale.model = TRUE to scale the latent effects. This is particularly important when Intrinsic Gaussian Markov random fields (IGMRFs) are used as priors (e.g., random walk models or some spatial models) for the latent effects. By defining scale.model = TRUE, the rw1-model is scaled to have a generalized variance equal to one. By scaling scaling the models we ensure that a fixed hyperprior for the precision parameter has a similar interpretation for different types of IGMRFs, making precision estimates comparable between different models. Scaling also allows estimates to be less sensitive to re-scaling covariates in the linear predictor and makes the precision invariant to changes in the shape and size of the latent effect (see Sørbye (2014) for further details) .\nWe can now fit the model with the updated components and plot the predicted values\n\n# Model formula\nformula = height ~ alpha + ut\n# Observational model\nlik =  bru_obs(formula = height  ~.,\n            family = \"gaussian\",\n            data = Erie.df,\n            control.family = list(hyper = prec.tau_e))\n# fit the model\nfit.Erie_rw1 = bru(cmp_rw, lik)\n# Model predictions\npred_rw1.Erie = predict(fit.Erie_rw1, Erie.df, ~ alpha + ut)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Question\n\n\n\nTake a look at the model summaries using the summary function, do you see anything odd?\n\n\nAnswer\n\nThe intercept has zero mean and a very large variance. This is because we have not imposed a sum-to-zero constraint on the model random effects (constr=FALSE). Without this constraint, intrinsic models are non-identifiable. The intercept and the random effects are confounded, For example, you could add a constant value to every random effect and subtract it from the intercept without changing the model’s predictions. Take for instance for any constant \\(c\\), the following models are identical:\n\\(y_t = \\alpha + u_{t} + \\varepsilon_t\\) \\(y_t = (\\alpha - c) + (u_{t} + c) + \\varepsilon_t\\)\nThus, you need to set constr=FALSE so that \\(\\sum_t u_t=0\\) to ensure identifiability of \\(\\alpha\\)\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nFit an RW(1) model to the Erie data but now set constr=TRUE to impose a sum-to-zero constraint on the random effect. Then compare your results with the unconstrained model.\n\n\n\nClick here to see the solution\n\n\nCode\n# Model components\ncmp_rw =  ~ -1 + alpha(1) + \n  ut(t.idx ,\n     constr=TRUE,\n     model = \"rw1\",\n     hyper=pc_prior,\n     scale.model = TRUE)\n\nfit.Erie_rw1_constr = bru(cmp_rw, lik)\n\nfit.Erie_rw1_constr$summary.fixed\n\n\n          mean         sd 0.025quant 0.5quant 0.975quant     mode          kld\nalpha 174.1381 0.02383785   174.0913 174.1381    174.185 174.1381 1.440666e-08\n\n\n\n\n\n\n\nGroup-level effects\nNow we will model the height water levels for all four lakes by grouping the random effects. This will allow a within-lakes correlation to be included. In the next example, we allow for correlated effects using an ar1 model for the years and iid random effects on the lakes. First we create a lakes id and set the priors for our model:\n\ngreatLakes.df$lake_id &lt;- as.numeric(as.factor(greatLakes.df$Lakes))\n\npc_prior &lt;- list(theta = list(prior = \"pc.prec\", param = c(1, 0.01)),\n                 rho = list(prior = \"pc.cor0\", param = c(0.5, 0.3))) \n\nprec.tau_e &lt;- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(1, 10))) # prior values\n\nNow we define the model components. The lakes IDs that define the group are passed with parameter group argument and the iid model and other parameters are passed through the control.group parameter.\n\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(year,model = \"ar1\",\n                            hyper = pc_prior,\n                            group =lake_id,\n                            control.group = \n                            list(model = \"iid\", \n                                 scale.model = TRUE))\n\nWe fit the model in a similar fashion as we did before:\n\n# Model formula\nformula = height ~ alpha + ut\n\n\n# Observational model\nlik =  bru_obs(formula = height  ~.,\n            family = \"gaussian\",\n            data = greatLakes.df,\n            control.family = list(hyper = prec.tau_e))\n\n# fit the model\nfit.all_lakes_ar1 = bru(cmp, lik)\n\n# Model predictions\npred_ar1.all = predict(fit.all_lakes_ar1, greatLakes.df, ~ alpha + ut)\n\nLastly we can visualize group-level model predictions as follows:\n\nggplot(pred_ar1.all,aes(y=mean,x=year))+\n  geom_line()+\n    geom_ribbon(aes(x = year, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(aes(y=height,x=year)) + facet_wrap(~Lakes,scales = \"free\")",
    "crumbs": [
      "Home",
      "Practical 3"
    ]
  },
  {
    "objectID": "day2_practical_3.html#non-gaussian-data",
    "href": "day2_practical_3.html#non-gaussian-data",
    "title": "Practical 3",
    "section": "Non-Gaussian data",
    "text": "Non-Gaussian data\nIn the next example we will use the Toyo data set to illustrate how temporal models can be fit to non-Gaussian data.\nThe Tokyo data set available in INLA contains the recorded days of rain above 1 mm in Tokyo for 2 years, 1983:84. The data set contains the following variables:\n\ny : number of days with rain\nn : total number of days\ntime : day of the year\n\n\nlibrary(INLA)\nlibrary(inlabru)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n\ndata(\"Tokyo\")\n\nA possible observational model for these data is\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim\\text{Bin}(n_t, p_t) \\\\\n\\eta_t &= \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\end{aligned}\n\\]\n\\[\nn_t = \\left\\{\n\\begin{array}{lr}\n1, & \\text{for}\\; 29\\; \\text{February}\\\\\n2, & \\text{other days}\n\\end{array}\\right.\n\\]\n\\[\ny_t =\n\\begin{cases}\n\\{0,1\\}, & \\text{for}\\; 29\\; \\text{February}\\\\\n\\{0,1,2\\}, & \\text{other days}\n\\end{cases}\n\\]\nThen, the latent field is given by\n\\[\n\\eta_t = \\beta_0 + f(\\text{time}_t)\n\\]\n\nWhere the probability of rain depends on on the day of the year \\(t\\)\n\\(\\beta_0\\) is an intercept\n\\(f(\\text{time}_t)\\) is a temporal model, e.g., a RW2 model (this is just a smoother).\n\nThe smoothness is controlled by a hyperparameter \\(\\tau_f\\) . Thus, we assign a prior to \\(\\tau_f\\) to finalize the model.\nWe can fit the model as follows:\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)\n\nNotice that we have set cyclic = TRUE as this is a cyclic effect. Finally, we can produce model predictions in a similar fashion as we did before:\n\npTokyo = predict(fit, Tokyo, ~ plogis(beta0 + time_effect))\n\nggplot(data=pTokyo , aes(x= time, y= y) ) +\n  geom_point() + \n  ylab(\"\") + xlab(\"\") +\n  # Custom the Y scales:\n  scale_y_continuous(\n    # Features of the first axis\n    name = \"\",\n    # Add a second axis and specify its features\n    sec.axis = sec_axis( transform=~./2, name=\"Probability\")\n  )  + geom_line(aes(y=mean*2,x=time)) +\n  geom_ribbon(aes( ymin = q0.025*2, \n                             ymax = q0.975*2), alpha = 0.5)",
    "crumbs": [
      "Home",
      "Practical 3"
    ]
  },
  {
    "objectID": "day3_practical_5.html",
    "href": "day3_practical_5.html",
    "title": "Practical 5",
    "section": "",
    "text": "Aim of this practical:\nIn this first practical we are going to fit spatial modes for Areal, Geostatistical and Point-Process Data.\nDownload Practical 5 R script",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#the-data",
    "href": "day3_practical_5.html#the-data",
    "title": "Practical 5",
    "section": "The data",
    "text": "The data\nWe consider data on respiratory hospitalizations for Greater Glasgow and Clyde in 2007. The data are available from the CARBayesdata R Package:\n\nlibrary(CARBayesdata)\n\ndata(pollutionhealthdata)\ndata(GGHB.IZ)\n\nThe pollutionhealthdata contains the spatiotemporal data on respiratory hospitalizations, air pollution concentrations and socio-economic deprivation covariates for the 271 Intermediate Zones (IZ) that make up the Greater Glasgow and Clyde health board in Scotland. Data are provided by the Scottish Government and the available variables are:\n\nIZ: unique identifier for each IZ.\nyear: the year when the measurements were taken\nobserved: observed numbers of hospitalizations due to respiratory disease.\nexpected: expected numbers of hospitalizations due to respiratory disease computed using indirect standardisation from Scotland-wide respiratory hospitalization rates.\npm10: Average particulate matter (less than 10 microns) concentrations.\njsa: The percentage of working age people who are in receipt of Job Seekers Allowance\nprice: Average property price (divided by 100,000).\n\nThe GGHB.IZ data is a Simple Features (sf) object containing the spatial polygon information for the set of 271 Intermediate Zones (IZ), that make up of the Greater Glasgow and Clyde health board in Scotland ( Figure 1 ).\n\n\n\n\n\n\n\n\nFigure 1: Greater Glasgow and Clyde health board represented by 271 Intermediate Zones\n\n\n\n\nWe first merge the two dataset and select only one year of data, compute the SME and plot the observed\n\nresp_cases &lt;- merge(GGHB.IZ %&gt;%\n                      mutate(space = 1:dim(GGHB.IZ)[1]),\n                             pollutionhealthdata, by = \"IZ\") %&gt;%\n  filter(year == 2007) %&gt;%\n    mutate(SMR = observed/expected)\n\nggplot() + geom_sf(data = resp_cases, aes(fill = SMR)) + scale_fill_scico(direction = -1)\n\n\n\n\n\n\n\n\nThen we compute the adjacency matrix using the functions poly2nb() and nb2mat() in the spdep library. We then convert the adjacency matrix into the precision matrix \\(\\mathbf{Q}\\) of the CAR model. Remember this matrix has, on the diagonal the number of e\n\nlibrary(spdep)\n\nW.nb &lt;- poly2nb(GGHB.IZ,queen = TRUE)\nR &lt;- nb2mat(W.nb, style = \"B\", zero.policy = TRUE)\n\ndiag = apply(R,1,sum)\nQ = -R\ndiag(Q) = diag",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#the-model",
    "href": "day3_practical_5.html#the-model",
    "title": "Practical 5",
    "section": "The model",
    "text": "The model\nWe fit a first model to the data where we consider a Poisson model for the observed cases.\nStage 1 Model for the response \\[\ny_i|\\eta_i\\sim\\text{Poisson}(E_i\\lambda_i)\n\\] where \\(E_i\\) are the expected cases for area \\(i\\).\nStage 2 Latent field model \\[\n\\eta_i = \\text{log}(\\lambda_i) = \\beta_0 + \\omega_i + z_i\n\\] where\n\n\\(\\beta_0\\) is a common intercept\n\\(\\mathbf{\\omega} = (\\omega_1, \\dots, \\omega_k)\\) is a conditional Autorgressive model (CAR) with precision matrix \\(\\tau_1\\mathbf{Q}\\)\n\\(\\mathbf{z} = (z_1, \\dots, z_k)\\) is an unstrictured random effect with precision \\(\\tau_2\\)\n\nStage 3 Hyperparameters\nThe hyperparameters of the model are \\(\\tau_1\\) and \\(\\tau_2\\)\nNOTE In this case the linear predictor \\(\\eta\\) consists of three components!!\n\n\n\n\n\n\n Task\n\n\n\nFit the above model in using inlabru by completing the following code:\n\ncmp = ~ Intercept(1) + space(...) + iid(...)\n\nformula = ...\n\n\nlik = bru_obs(formula = formula, \n              family = ...,\n              E = ...,\n              data = ...)\n\nfit = bru(cmp, lik)\n\n\n\nAnswer\n\n\ncmp = ~ Intercept(1) + space(space, model = \"besag\", graph = Q) + iid(space, model = \"iid\")\n\nformula = observed ~ Intercept + space + iid\n\nlik = bru_obs(formula = formula, \n              family = \"poisson\",\n              E = expected,\n              data = resp_cases)\n\nfit = bru(cmp, lik)\n\n\n\n\nAfter fitting the model we want to extract results.\n\n\n\n\n\n\n Question\n\n\n\n\nWhat is the estimated value for \\(\\beta_0\\)? \nLook at the estimated values of the hyperparameters using fit$summary.hyperpar , which of the two spatial components (structured or unstructured) explains more of the variability in the counts? structuredunstructured\n\n\n\nWe now look at the predictions over space.\n\n\n\n\n\n\n Task\n\n\n\nComplete the code below to produce prediction of the linear predictor \\(\\eta_i\\) and of the risk \\(\\lambda_i\\) and of the expected cases \\(E_i\\exp(\\lambda_i)\\) over the whole space of interest. Then plot the mean and sd of the resulting surfaces.\n\npred = predict(fit, resp_cases, ~data.frame(log_risk = ...,\n                                             risk = exp(...),\n                                             cases = ...\n                                             ),\n               n.samples = 1000)\n\n\n\nShow Answer\n\n\n# produce predictions\npred = predict(fit, resp_cases, ~data.frame(log_risk = Intercept + space,\n                                             risk = exp(Intercept + space),\n                                             cases = expected * exp(Intercept + space)\n                                             ),\n               n.samples = 1000)\n# plot the predictions\n\np1 = ggplot() + geom_sf(data = pred$log_risk, aes(fill = mean)) + scale_fill_scico(direction = -1) + ggtitle(\"mean log risk\")\np2 = ggplot() + geom_sf(data = pred$log_risk, aes(fill = sd)) + scale_fill_scico(direction = -1) + ggtitle(\"sd log risk\")\np1 + p2\n\n\n\n\n\n\n\np1 = ggplot() + geom_sf(data = pred$risk, aes(fill = mean)) + scale_fill_scico(direction = -1) + ggtitle(\"mean  risk\")\np2 = ggplot() + geom_sf(data = pred$risk, aes(fill = sd)) + scale_fill_scico(direction = -1) + ggtitle(\"sd  risk\")\np1 + p2\n\n\n\n\n\n\n\np1 = ggplot() + geom_sf(data = pred$cases, aes(fill = mean)) + scale_fill_scico(direction = -1)+ ggtitle(\"mean  expected counts\")\np2 = ggplot() + geom_sf(data = pred$cases, aes(fill = sd)) + scale_fill_scico(direction = -1)+ ggtitle(\"sd  expected counts\")\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\nFinally we want to compare our observations \\(y_i\\) with the predicted means of the Poisson distribution \\(E_i\\exp(\\lambda_i)\\)\n\npred$cases %&gt;% ggplot() + geom_point(aes(observed, mean)) + \n  geom_errorbar(aes(observed, ymin = q0.025, ymax = q0.975)) +\n  geom_abline(intercept = 0, slope = 1)\n\n\n\n\n\n\n\n\nNote: Here we are predicting the mean of counts, not the counts!!! Predicting counts is the theme of the next task!",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#getting-prediction-densities",
    "href": "day3_practical_5.html#getting-prediction-densities",
    "title": "Practical 5",
    "section": "Getting prediction densities",
    "text": "Getting prediction densities\nPosterior predictive distributions, that is \\(\\pi(y_i^{\\text{new}}|\\mathbf{y})\\) are of interest in many applied problems. The bru() function does not return predictive densities. In the previous step we have computed predictions for the expected counts \\(\\pi(E_i\\lambda_i|\\mathbf{y})\\). The predictive distribution is then: \\[\n\\pi(y_i^{\\text{new}}|\\mathbf{y}) = \\int \\pi(y_i|E_i\\lambda_i)\\pi(E_i\\lambda_i|\\mathbf{y})\\ dE_i\\lambda_i\n\\] where, in our case, \\(\\pi(y_i|E_i\\lambda_i)\\) is Poisson with mean \\(E_i\\lambda_i\\). We can achieve this using the following algorith:\n\nSimulate \\(n\\) replicates of \\(g^k = E_i\\lambda_i\\) for \\(k = 1,\\dots,n\\) using the function generate() which takes the same input as predict()\nFor each of the \\(k\\) replicates simulate a new value \\(y_i^{new}\\) using the function rpois()\nSummarise the \\(n\\) samples of \\(y_i^{new}\\) using, for example the mean and the 0.025 and 0.975 quantiles.\n\nHere is the code:\n\n# simulate 1000 realizations of E_i\\lambda_i\nexpected_counts = generate(fit, resp_cases, \n                           ~ expected * exp(Intercept + space),\n                           n.samples = 1000)\n\n\n# simulate poisson data\naa = rpois(271*1000, lambda = as.vector(expected_counts))\nsim_counts = matrix(aa, 271, 1000)\n\n# summarise the samples with posterior means and quantiles\npred_counts = data.frame(observed = resp_cases$observed,\n                         m = apply(sim_counts,1,mean),\n                         q1 = apply(sim_counts,1,quantile, 0.025),\n                         q2 = apply(sim_counts,1,quantile, 0.975),\n                         vv = apply(sim_counts,1,var)\n                         )\n\n\n\n\n\n\n\n Task\n\n\n\nPlot the observations against the predicted new counts and the predicted expected counts. Include the uncertainty and compare the two.\n\n\nTake hint\n\n\n\n\n\nClick here to see the solution\n\n\nCode\nggplot() + \n  geom_point(data = pred_counts, aes(observed, m, color = \"Pred_obs\")) + \n  geom_errorbar(data = pred_counts, aes(observed, ymin = q1, ymax = q2, color = \"Pred_obs\")) +\n  geom_point(data = pred$cases, aes(observed, mean, color = \"Pred_means\")) + \n  geom_errorbar(data = pred$cases, aes(observed, ymin = q0.025, ymax = q0.975, color = \"Pred_means\")) +\n  \n  geom_abline(intercept = 0, slope =1)",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#the-data-1",
    "href": "day3_practical_5.html#the-data-1",
    "title": "Practical 5",
    "section": "The data",
    "text": "The data\nIn this practical, we will explore data on the Pacific Cod (Gadus macrocephalus) from a trawl survey in Queen Charlotte Sound. The pcod dataset is available from the sdmTMB package and contains the presence/absence records of the Pacific Cod during each surveys along with the biomass density of Pacific cod in the area swept (kg/Km\\(^2\\)). The qcs_grid data contain the depth values stored as \\(2\\times 2\\) km grid for Queen Charlotte Sound.\nThe dataset contains presence/absence data from 2003 to 2017. In this practical we only consider year 2003.\nWe first load the dataset and select the year of interest\n\nlibrary(sdmTMB)\n\npcod_df = sdmTMB::pcod %&gt;% filter(year==2003)\nqcs_grid = sdmTMB::qcs_grid\n\nThen, we create ab sf object and assign the rough coordinate reference to it:\n\npcod_sf =   st_as_sf(pcod_df, coords = c(\"lon\",\"lat\"), crs = 4326)\npcod_sf = st_transform(pcod_sf,\ncrs = \"+proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\" )\n\nWe convert the covariate into a raster and assign the same coordinate reference:\n\ndepth_r &lt;- rast(qcs_grid, type = \"xyz\")\ncrs(depth_r) &lt;- crs(pcod_sf)\n\nFinally we can plot our dataset. Note that to plot the raster we need to upload also the tidyterra library.\n\nggplot()+ \n  geom_spatraster(data=depth_r$depth)+\n  geom_sf(data=pcod_sf,aes(color=factor(present))) +\n    scale_color_manual(name=\"Occupancy status for the Pacific Cod\",\n                     values = c(\"black\",\"orange\"),\n                     labels= c(\"Absence\",\"Presence\"))+\n  scale_fill_scico(name = \"Depth\",\n                   palette = \"nuuk\",\n                   na.value = \"transparent\" ) + xlab(\"\") + ylab(\"\")",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#the-model-1",
    "href": "day3_practical_5.html#the-model-1",
    "title": "Practical 5",
    "section": "The model",
    "text": "The model\nWe first fit a simple model where we consider the observation as Bernoulli and where the linear predictor contains only one intercept and the GR field defined through the SPDE approach. The model is defined as:\nStage 1 Model for the response\n\\[\ny(s)|\\eta(s)\\sim\\text{Binom}(1, p(s))\n\\] Stage 2 Latent field model\n\\[\n\\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s)\n\\]\nwith\n\\[\n\\omega(s)\\sim \\text{  GF with range } \\rho\\  \\text{ and maginal variance }\\ \\sigma^2\n\\]\nStage 3 Hyperparameters\nThe hyperparameters of the model are \\(\\rho\\) and \\(\\sigma\\)\nNOTE In this case the linear predictor \\(\\eta\\) consists of two components!!\n\nThe workflow\nWhen fitting a geostatistical model we need to fulfill the following tasks:\n\nBuild the mesh\nDefine the SPDE representation of the spatial GF. This includes defining the priors for the range and sd of the spatial GF\nDefine the components of the linear predictor. This includes the spatial GF and all eventual covariates\nDefine the observation model using the bru_obs() function\nRun the model using the bru() function\n\n\n\n1. Building the mesh\nThe first task, when dealing with geostatistical models in inlabru is to build the mesh that covers the area of interest. For this purpose we use the function fm_messh_2d.\nOne way to build the mesh is to start from the locations where we have observations, these are contained in the dataset pcod_sf.\n\nmesh = fm_mesh_2d(loc = pcod_sf,           # Build the mesh\n                  max.edge = c(10,20),     # The largest allowed triangle edge length.\n                  offset = c(5,50))       # The automatic extension distance\nggplot() + gg(mesh) + geom_sf(data= pcod_sf, aes(color = factor(present)), size = 0.1) + xlab(\"\") + ylab(\"\")\n\n\n\n\n\n\n\n\nAs you can see from the plot above, some of the locations are very close to each other, this causes some very small triangles. This can be avoided using the option cutoff = which collapses the locations that are closer than a cutoff (those points are collapsed in the mesh construction but, of course, not when it come to estimaation.)\n\nmesh = fm_mesh_2d(loc = pcod_sf,           # Build the mesh\n                  cutoff = 2,\n                  max.edge = c(10,20),     # The largest allowed triangle edge length.\n                  offset = c(5,50))       # The automatic extension distance\nggplot() + gg(mesh) + geom_sf(data= pcod_sf, aes(color = factor(present)), size = 0.1) + xlab(\"\") + ylab(\"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nLook at the documentation for the fm_mesh_2d function typing\n\n?fm_mesh_2d\n\nplay around with the different options and create different meshes.\nThe rule of thumb is that your mesh should be:\n\nfine enough to well represent the spatial variability of your process, but not too fine in order to avoid computation burden\nthe triangles should be regular, avoid long and thin triangles.\nThe mesh should contain a buffer around your area of interest (this is what is defined in the offset option) in order to avoid boundary artefact in the estimated variance.\n\n\n\n\n\n2. Define the SPDE representation of the spatial GF\nTo define the SPDE representation of the spatial GF we use the function inla.spde2.pcmatern. This takes as input the mesh we have defined and the PC-priors definition for \\(\\rho\\) and \\(\\sigma\\) (the range and the marginal standard deviation of the field).\nPC priors Gaussian Random field are defined in (Fuglstad et al. 2018). From a practical perspective for the range \\(\\rho\\) you need to define two paramters \\(\\rho_0\\) and \\(p_{\\rho}\\) such that you believe it is reasonable that\n\\[\nP(\\rho&lt;\\rho_0)=p_{\\rho}\n\\]\nwhile for the margianal variance \\(\\sigma\\) you need to define two parameters \\(\\sigma_0\\) and \\(p_{\\sigma}\\) such that you believe it is reasonable that\n\\[\nP(\\sigma&lt;\\sigma_0)=p_{\\sigma}\n\\]\nYou can use the following function to compute and plot the prior distributions for the range and sd of the Matern field.\n\ndens_prior_range = function(rho_0, p_alpha)\n{\n  # compute the density of the PC prior for the\n  # range rho of the Matern field\n  # rho_0 and p_alpha are defined such that\n  # P(rho&lt;rho_0) = p_alpha\n  rho = seq(0, rho_0*10, length.out =100)\n  alpha1_tilde = -log(p_alpha) * rho_0\n  dens_rho =  alpha1_tilde / rho^2 * exp(-alpha1_tilde / rho)\n  return(data.frame(x = rho, y = dens_rho))\n}\n\ndens_prior_sd = function(sigma_0, p_sigma)\n{\n  # compute the density of the PC prior for the\n  # sd sigma of the Matern field\n  # sigma_0 and p_sigma are defined such that\n  # P(sigma&gt;sigma_0) = p_sigma\n  sigma = seq(0, sigma_0*10, length.out =100)\n  alpha2_tilde = -log(p_sigma)/sigma_0\n  dens_sigma = alpha2_tilde* exp(-alpha2_tilde * sigma) \n  return(data.frame(x = sigma, y = dens_sigma))\n}\n\nHere are some alternatives for defining priors for our model\n\nspde_model1 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(.1, 0.5),\n                                  prior.range = c(30, 0.5))\nspde_model2 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(10, 0.5),\n                                  prior.range = c(1000, 0.5))\nspde_model3 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n\nAnd here we plot the different priors for the range:\n\nggplot() + \n  geom_line(data = dens_prior_range(30,.5), aes(x,y, color = \"model1\")) +\n  geom_line(data = dens_prior_range(1000,.5), aes(x,y, color = \"model2\")) +\n  geom_line(data = dens_prior_range(100,.5), aes(x,y, color = \"model3\")) \n\n\n\n\n\n\n\n\nand for the sd:\n\nggplot() + \n  geom_line(data = dens_prior_sd(1,.5), aes(x,y, color = \"model1\")) +\n  geom_line(data = dens_prior_sd(10,.5), aes(x,y, color = \"model2\")) +\n  geom_line(data = dens_prior_sd(.1,.5), aes(x,y, color = \"model3\")) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Question\n\n\n\nConsider the pcod_sf, the spatial extension and type of the data…is some of the previous choices more reasonable than other? spde_model1spde_model2spde_model3\nNOTE Remember that a prior should be reasonable..but the model should not totally depend on it.\n\n\n\n\n3. Define the components of the linear predictor\nWe have now defined a mesh and a SPDE representation of the spatial GF. We now need to define the model components:\n\ncmp = ~ Intercept(1) + space(geometry, model = spde_model3)\n\nNOTE since the dataframe we use (pcod_sf) is an sf object the input in the space() component is the geometry of the dataset.\n\n\n4. Define the observation model\nOur data are Bernoulli distributed so we can define the observation model as:\n\nformula = present ~ Intercept  + space\n\nlik = bru_obs(formula = formula, \n              data = pcod_sf, \n              family = \"binomial\")\n\n\n\n5. Run the model\nFinally we are ready to run the model\n\nfit1 = bru(cmp,lik)",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#extract-results",
    "href": "day3_practical_5.html#extract-results",
    "title": "Practical 5",
    "section": "Extract results",
    "text": "Extract results\n\nHyperparameters\n\n\n\n\n\n\n Task\n\n\n\nPlot the posterior densities for the range \\(\\rho\\) and the standard deviation \\(\\sigma\\) alog with the prior for both parameters.\n\n\nTake hint\n\nPosterior marginals for the hyperparameters can be extracted from the fitted model as:\n\n\n\nClick here to see the solution\n\n\nCode\nfit1$marginals.hyperpar$'name of the parameter'\n\n\n\n\n\n\n\nClick here to see the solution\n\n\nCode\n# Extract marginal for the range\n\nggplot() + \n  geom_line(data = fit1$marginals.hyperpar$`Range for space`,\n            aes(x,y, color = \"Posterior\")) +\n  geom_line(data = dens_prior_range(100,.5),\n            aes(x,y, color = \"Prior\"))\n\n\nggplot() + \n  geom_line(data = fit1$marginals.hyperpar$`Stdev for space`,\n            aes(x,y, color = \"Posterior\")) +\n  geom_line(data = dens_prior_sd(1,.5), aes(x,y, color = \"Prior\"))",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#spatial-prediction",
    "href": "day3_practical_5.html#spatial-prediction",
    "title": "Practical 5",
    "section": "Spatial prediction",
    "text": "Spatial prediction\nWe now want to extract the estimated posterior mean and sd of spatial GF. To do this we first need to define a grid of points where we want to predict. We do this using the function fm_pixel() which creates a regular grid of points covering the mesh\n\npxl = fm_pixels(mesh)\n\nthen compute the prediction for both the spatial GF and the linear predictor (spatial GF + intercept)\n\npreds = predict(fit1, pxl, ~data.frame(spatial = space,\n                                      total = Intercept + space))\n\nFinally, we can plot the maps\n\nggplot() + geom_sf(data = preds$spatial,aes(color = mean)) + scale_color_scico() + ggtitle(\"Posterior mean\")\n\n\n\n\n\n\n\nggplot() + geom_sf(data = preds$spatial,aes(color = sd)) + scale_color_scico() + ggtitle(\"Posterior sd\")\n\n\n\n\n\n\n\n\nNote The posterior sd is lowest at the observation points. Note how the posterior sd is inflated around the border, this is the “border effect” due to the SPDE representation.\nInstead of predicting over a grid covering the whole mesh, we can limit our predictions to the points where the covariate is defined. We can do this by defining a sf object using coordinates in the object depth_r.\n\npxl1 = data.frame(crds(depth_r), \n                  as.data.frame(depth_r$depth)) %&gt;% \n       filter(!is.na(depth)) %&gt;%\nst_as_sf(coords = c(\"x\",\"y\"))\n\n\n\n\n\n\n\n Task\n\n\n\nProduce prediction over pxl1 unsing the same techniques as before. Plot your results.\n\n\nTake hint\n\nAdd hint details here…\n\n\n\n\nClick here to see the solution\n\n\nCode\npred_pxl1 = predict(fit1, pxl1, ~data.frame(spatial = space,\n                                      total = Intercept + space))\n\nggplot() + geom_sf(data = pred_pxl1$total,aes(color = mean)) + scale_color_scico() + ggtitle(\"Posterior mean\")\n\n\n\n\n\n\n\n\n\nCode\nggplot() + geom_sf(data = pred_pxl1$total,aes(color = sd)) + scale_color_scico() + ggtitle(\"Posterior sd\")\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of computing the posterior mean and standard deviations of the estimated surface, one can also simulate possible realizations of such surface. This will give the user a better idea of the type of realized surfaces one can expect. We can do this using the function generate().\n\n# we simulate 4 samples from the \ngens = generate(fit1, pxl1, ~ (Intercept + space),\n                n.samples = 4)\n\npp = cbind(pxl1, gens)\n\npp %&gt;% select(-depth) %&gt;%\n  pivot_longer(-geometry) %&gt;%\n    ggplot() + \n      geom_sf(aes(color = value)) +\n      facet_wrap(.~name) +\n        scale_color_scico(direction = -1) +\n        ggtitle(\"Sample from the fitted model\")",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#an-alternative-model",
    "href": "day3_practical_5.html#an-alternative-model",
    "title": "Practical 5",
    "section": "An alternative model",
    "text": "An alternative model\nWe now want to check if the depth covatiate has an influende on the probability of presence. We do this in two different models\n\nModel 1 The depth enters the model in a linear way. The linear predictor is then defined as:\n\n\\[\n  \\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s) + \\beta_1\\ \\text{depth}(s)\n\\]\n\nModel 1 The depth enters the model in a non linear way. The linear predictor is then defined as:\n\n\\[\n  \\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s) +  f(\\text{depth}(s))\n\\] where \\(f(.)\\) is a smooth function. We will use a RW2 model for this.\n\n\n\n\n\n\n Task\n\n\n\nFit model 1. Define components, observation model and use the bru() function to estimate the parameters.\nNote Use the scaled version of the covariate stored in depth_r$depth_scaled.\nWhat is the liner effect of depth on the logit probability?\n\n\nTake hint\n\nAdd hint details here…\n\n\n\n\nClick here to see the solution\n\n\nCode\ncmp = ~ Intercept(1) + space(geometry, model = spde_model3) +\n        covariate(depth_r$depth_scaled, model = \"linear\")\n\nformula = present ~ Intercept  + space + covariate\n\nlik = bru_obs(formula = formula, \n              data = pcod_sf, \n              family = \"binomial\")\n\n\nfit2 = bru(cmp, lik)\n\n\n\n\n\nWe now want to fit Model 2 where we allow the effect of depth to be non-linear. To use the RW2 model we need to group the values of depth into distinct classe. To do this we use the function inla.group() which, by default, creates 20 groups. The we can fit the model as usual\n\n# create the grouped variable\ndepth_r$depth_group = inla.group(values(depth_r$depth_scaled))\n\n# run the model\ncmp = ~ Intercept(1) + space(geometry, model = spde_model3) +\n        covariate(depth_r$depth_group, model = \"rw2\")\n\nformula = present ~ Intercept  + space + covariate\n\nlik = bru_obs(formula = formula, \n              data = pcod_sf, \n              family = \"binomial\")\n\n\nfit3 = bru(cmp, lik)\n\n# plot the estimated effect of depth\n\nfit3$summary.random$covariate %&gt;% \n  ggplot() + geom_line(aes(ID,mean)) + \n                                  geom_ribbon(aes(ID, ymin = `0.025quant`, \n                                                      ymax = `0.975quant`), alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nCreate a map of predicted probability from Model 3. You can use a inverse logit function defined as\n\ninv_logit = function(x) (1+exp(-x))^(-1)\n\n\n\nTake hint\n\nThe predict() function can take as input also functions of elements of the components you want to consider\n\n\n\n\nClick here to see the solution\n\n\nCode\ninv_logit = function(x) (1+exp(-x))^(-1)\n\npred3  = predict(fit3, pxl1, ~inv_logit(Intercept + space + covariate) )\n\npred3 %&gt;% ggplot() + \n      geom_sf(aes(color = mean)) +\n        scale_color_scico(direction = -1) +\n        ggtitle(\"Sample from the fitted model\")",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#the-data-2",
    "href": "day3_practical_5.html#the-data-2",
    "title": "Practical 5",
    "section": "The data",
    "text": "The data\nIn this practical we consider the data clmfires in the spatstat library.\nThis dataset is a record of forest fires in the Castilla-La Mancha region of Spain between 1998 and 2007. This region is approximately 400 by 400 kilometres. The coordinates are recorded in kilometres. For more info about the data you can type:\n\n?clmfires\n\nWe first read the data and transform them into an sf object. We also create a polygon that represents the border of the Castilla-La Mancha region. We select the data for year 2004 and only those fires caused by lightning.\n\ndata(\"clmfires\")\npp = st_as_sf(as.data.frame(clmfires) %&gt;%\n                mutate(x = x, \n                       y = y),\n              coords = c(\"x\",\"y\"),\n              crs = NA) %&gt;%\n  filter(cause == \"lightning\",\n         year(date) == 2004)\n\npoly = as.data.frame(clmfires$window$bdry[[1]]) %&gt;%\n  mutate(ID = 1)\n\nregion = poly %&gt;% \n  st_as_sf(coords = c(\"x\", \"y\"), crs = NA) %&gt;% \n  dplyr::group_by(ID) %&gt;% \n  summarise(geometry = st_combine(geometry)) %&gt;%\n  st_cast(\"POLYGON\") \n  \nggplot() + geom_sf(data = region, alpha = 0) + geom_sf(data = pp)  \n\n\n\n\n\n\n\nFigure 2: Distribution of the observed forest fires caused by lightning in Castilla-La Mancha in 2004",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#HPP",
    "href": "day3_practical_5.html#HPP",
    "title": "Practical 5",
    "section": "Fit a homogeneous Poisson Process",
    "text": "Fit a homogeneous Poisson Process\nAs a first exercise we are going to fit a homogeneous Poisson process (HPP) to the data. This is a model that assume constant intensity over the whole space so our linear predictor is then:\n\\[\n\\eta(s) = \\log\\lambda(s) = \\beta_0 , \\ \\mathbf{s}\\in\\Omega\n\\]\nso the likelihood can be written as:\n\\[\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n& = \\exp\\left( -\\int_{\\Omega}\\exp(\\beta_0)ds\\right)\\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n\\end{aligned}\n\\]\nwhere \\(|\\Omega|\\) is the area of the domain of interest.\nWe need to approximate the integral using a numerical integration scheme as:\n\\[\n\\approx\\exp\\left(-\\sum_{k=1}^{N_k}w_k\\lambda(s_k)\\right)\\prod_{i=1}^n \\lambda(\\mathbf{s}_i)\n\\]\nWhere \\(N_k\\) is the number of integration points \\(s_1,\\dots,s_{N_k}\\) and \\(w_1,\\dots,w_{N_k}\\) are the integration weights.\nIn this case, since the intensity is constant, the integration scheme is really simple: it is enough to consider one random point inside the domain with weight equal to the area of the domain.\n\n# define integration scheme\n\nips = st_sf(\ngeometry = st_sample(region, 1)) # some random location inside the domain\nips$weight = st_area(region) # integration weight is the area of the domain\n\ncmp = ~ 0 + beta_0(1)\n\nformula = geometry ~ beta_0\n\nlik = bru_obs(data = pp,\n              family = \"cp\",\n              formula = formula,\n              ips = ips)\nfit1 = bru(cmp, lik)\n\n\n\n\n\n\n\n Task\n\n\n\n\nPlot the estimated posterior distribution of the intensity\nCompare the estimated expected number of fires on the whole domain with the observed ones.\n\n\n\nTake hint\n\nRemember that in the inlabru framework we model the log intensity \\(\\eta = log(\\lambda)\\)\n\n\n\n\nClick here to see the solution\n\n\nCode\n# 1) The estimated posterior distribution of the  intensity is\n\npost_int = inla.tmarginal(function(x) exp(x), fit1$marginals.fixed$beta_0)\npost_int %&gt;% ggplot() + geom_line(aes(x,y))\n\n\n\n\n\n\n\n\n\nCode\n# 2) To compute the expected number of points in the area we need to multiply the\n# estimated intensity by the area of the domain.\n# In the same plot we also show the number of observed fires as a vertical line.\n\npost_int = inla.tmarginal(function(x) st_area(region)* exp(x), fit1$marginals.fixed$beta_0)\npost_int %&gt;% ggplot() + geom_line(aes(x,y)) +\n  geom_vline(xintercept = dim(pp)[1])",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#NHPP",
    "href": "day3_practical_5.html#NHPP",
    "title": "Practical 5",
    "section": "Fit an Inhomogeneous Poisson Process",
    "text": "Fit an Inhomogeneous Poisson Process\nThe model above has the clear disadvantages that assumes a constant intensity and from Figure 2 we clearly see that this is not the case.\nThe library spatstat contains also some covariates that can help explain the fires distribution. Figure @fit-altitude shows the location of fires together with the (scaled) altitude.\n\n#|label: fig-altitude\n#|fig-cap: \"Distribution of the observed forest fires and scaled altitude\"\n#| \nelev_raster = rast(clmfires.extra[[2]]$elevation)\nelev_raster = scale(elev_raster)\nggplot() + \n  geom_spatraster(data = elev_raster) + \n  geom_sf(data = pp) +\n  scale_fill_scico()\n\n\n\n\n\n\n\n\nWe are now going to use the altitude as a covariate to explain the variability of the intensity \\(\\lambda(s)\\) over the domain of interest.\nOur model is \\[\n\\log\\lambda(s) = \\beta_0 + \\beta_1x(s)\n\\] where \\(x(s)\\) is the altitude at location \\(s\\).\nThe likelihood becomes:\n\\[\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n& = \\exp \\left( -\\int_\\Omega \\exp(\\beta_0 + \\beta_1x(s)) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n\\end{aligned}\n\\]\nNow we need to choose an integration scheme to solve the integral.\nIn this case we will take a simple grid based approach where each quadrature location has an equal weight. Our grid consists of \\(N_k = 1000\\) points and the weights are all equal to \\(|\\Omega|/N_k\\).\n\n#|label: fig-int2\n#|fig-cap: \"Integration scheme.\"\n\nn.int = 1000\nips = st_sf(geometry = st_sample(region,\n            size = n.int,\n            type = \"regular\"))\n\nips$weight = st_area(region) / n.int\nggplot() + geom_sf(data = ips, aes(color = weight)) + geom_sf(data= region, alpha = 0)\n\n\n\n\n\n\n\n\nOBS: The implicit assumption here is that the intensity is constant inside each grid box, and so is the covariate!!\nWe can now fit the model:\n\ncmp = ~ Intercept(1) + elev(elev_raster, model = \"linear\")\nformula = geometry ~ Intercept + elev\nlik = bru_obs(data = pp,\n              family = \"cp\",\n              formula = formula,\n              ips = ips)\nfit2 = bru(cmp, lik)\n\n\n\n\n\n\n\n Task\n\n\n\nWhat is the effect of the altitude on the (log) intensity of the process?\n\n\nTake hint\n\nYou can look at the summary for the fixed effects\n\n\n\n\nClick here to see the solution\n\n\nCode\nfit2$summary.fixed\n\n\n                mean         sd 0.025quant   0.5quant 0.975quant       mode kld\nIntercept -6.6116808 0.10220853 -6.8120059 -6.6116808 -6.4113558 -6.6116808   0\nelev       0.6770984 0.06987283  0.5401502  0.6770984  0.8140466  0.6770984   0\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n⚠️ WARNING!!⚠️ When fitting a Point process, the integration scheme has to be fine enough to capture the spatial variability of the covariate!!\n\n\n\n\n\n\n\n\n Task\n\n\n\nRerun the model with the altitude as covariate, but this time change the integration scheme as follows:\n\nn.int2 = 50\n\nips2 = st_sf(geometry = st_sample(region,\n            size = n.int2,\n            type = \"regular\"))\nips2$weight = st_area(region) / n.int2\n\nWhat happens to the effect of the covariate?\n\n\nTake hint\n\nRe-run the model changing the integration scheme in the ips input of the bru_obs() function.\n\n\n\n\nClick here to see the solution\n\n\nCode\nlik_bis = bru_obs(data = pp,\n              family = \"cp\",\n              formula = formula,\n              ips = ips2)\n\nfit2bis = bru(cmp, lik_bis)\n\n# you can the check the differences between the two models\nrbind(fit2$summary.fixed,\nfit2bis$summary.fixed)\n\n\n                 mean         sd 0.025quant   0.5quant 0.975quant       mode\nIntercept  -6.6116808 0.10220853 -6.8120059 -6.6116808 -6.4113558 -6.6116808\nelev        0.6770984 0.06987283  0.5401502  0.6770984  0.8140466  0.6770984\nIntercept1 -6.6719824 0.11650402 -6.9003261 -6.6719824 -6.4436387 -6.6719824\nelev1       0.8725306 0.09496757  0.6863976  0.8725306  1.0586636  0.8725306\n           kld\nIntercept    0\nelev         0\nIntercept1   0\nelev1        0\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nNow we want to predict the log-intensity over the whole domain. Use the grid from the elevation raster to predict the intensity over the domain.\n\nest_grid = st_as_sf(data.frame(crds(elev_raster)), coords = c(\"x\",\"y\"))\nest_grid  = st_intersection(est_grid, region)\n\n\n\n\nClick here to see the solution\n\n\nCode\npreds2 = predict(fit2, est_grid, ~ data.frame(log_scale = Intercept + elev,\n\n                                              lin_scale = exp(Intercept + elev)))\n# then visualize it like\npreds2$log_scale %&gt;% \n  ggplot() +\n  geom_sf(aes(color = mean)) +\n  scale_color_scico()\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we want to use the fitted model to estimate the total number of fires over the whole region. To do this we first have to fine the expected number of fires as:\n\\[\nE(N_{\\Omega}) = \\int_{\\Omega}\\exp(\\lambda(s))\\ ds\n\\]\nThen simulate possible realizations of \\(N_{\\Omega}\\) to include also the likelihood variability in our estimate:\n\nN_fires = generate(fit2, ips,\n                      formula = ~ {\n                        lambda = sum(weight * exp(elev + Intercept))\n                        rpois(1, lambda)},\n                    n.samples = 2000)\n\nggplot(data = data.frame(N = as.vector(N_fires))) +\n  geom_histogram(aes(x = N),\n                 colour = \"blue\",\n                 alpha = 0.5,\n                 bins = 20) +\n  geom_vline(xintercept = nrow(pp),\n             colour = \"red\") +\n  theme_minimal() +\n  xlab(expression(Lambda))",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#LGCP",
    "href": "day3_practical_5.html#LGCP",
    "title": "Practical 5",
    "section": "Fit a Log-Gaussian Cox Process",
    "text": "Fit a Log-Gaussian Cox Process\nFinally we want to fit a LGCP with log intensity:\n\\[\n\\log(s) = \\beta_0 + \\beta_1x + u(s)\n\\]\nwhere \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) the effect of (standarized) altitude \\(x(s)\\) as before and \\(u(s)\\) is a Gaussian Random field defined through the SPDE approach.\n\nDefine the mesh\nThe first step, as any time we use the SPDE approach is to defie the mesh and the priors for the marginal variance and range:\n\nmesh = fm_mesh_2d(boundary = region,\n                  max.edge = c(5, 10),\n                  cutoff = 4, crs = NA)\n\nggplot() + gg(mesh) + geom_sf(data = pp)\n\n\n\n\n\n\n\nspde_model =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n\nWe can then define the integration weight. Here we use the same points to define the SPDE approximation and to approximate the integral in the likelihood. We will see later that this does not have to be like this, BUT integration weight and SPDE weights have to be consistent with each other!\n\nips = fm_int(mesh, samplers = region)\n\nggplot() + geom_sf(data = ips, aes(color = weight)) +\n  gg(mesh) +\n   scale_color_scico()\n\n\n\n\n\n\n\n\n\n\nRun the model\n\n\n\n\n\n\n Task\n\n\n\nSet up the components and the formula for the model above by completing the code below and run the model.\n\ncmp = ~ ...\n\nformula = geometry ~ ...\n\nlik = bru_obs(\"cp\",\n              formula = formula,\n              data = pp,\n              ips = ...)\n\nfit3 = bru(cmp, lik)\n\n\n\nTake hint\n\nThe model has three components: intercept, linear effect of altitude and the spatial GRF\n\n\n\n\nClick here to see the solution\n\n\nCode\ncmp = ~ Intercept(1) + space(geometry, model = spde_model) + elev(elev_raster, model = \"linear\")\n\nformula = geometry ~ Intercept + space + elev\n\nlik = bru_obs(\"cp\",\n              formula = formula,\n              data = pp,\n              ips = ips)\n\nfit3 = bru(cmp, lik)\n\n\n\n\n\nNote when running the model above you will get a warning:\n\n\nWarning in bru_log_warn(msg): Model input 'elev_raster' for 'elev' returned some NA values.\nAttempting to fill in spatially by nearest available value.\nTo avoid this basic covariate imputation, supply complete data.\n\n\nIt means that the bru() function cannot find the covariate values for some of the mesh nodes. This is a common situation. As the warning says, the bru() function automatically imputes the value of the covarite using the nearest nodes. This increases the running time of the bru() function, so one solution is to impute the values of the covariate over the whole mesh ‘before’ running the bru() function.\nHere, we notice that there is a single point for which elevation values are missing (see Figure 3 the red point that lies outside the raster extension ).\n\n\n\n\n\n\n\n\nFigure 3: Integration scheme for numerical approximation of the stochastic integral in La Mancha Region\n\n\n\n\n\nTo solve this, we can increase the raster extension so it covers all both data-points and quadrature locations as well. Then, we can use the bru_fill_missing() function to input the missing values with the nearest-available-value. We can achieve this using the following code:\n\n# Extend raster ext by 30 % of the original raster so it covers the whole mesh\nre &lt;- extend(elev_raster, ext(elev_raster)*1.3)\n# Convert to an sf spatial object\nre_df &lt;- re %&gt;% stars::st_as_stars() %&gt;%  st_as_sf(na.rm=F)\n# fill in missing values using the original raster \nre_df$lyr.1 &lt;- bru_fill_missing(elev_raster,re_df,re_df$lyr.1)\n# rasterize\nelev_rast_p &lt;- stars::st_rasterize(re_df) %&gt;% rast()\nggplot() + geom_spatraster(data = elev_rast_p) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe bru_fill_missing() function was added mainly to handle very local infilling on domain boundaries. For properly missing data, one should consider doing a proper model of the spatial field instead.",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#results",
    "href": "day3_practical_5.html#results",
    "title": "Practical 5",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\n Task\n\n\n\nPlot the estimated mean and standard deviation of the spatial GF and the log-intensity over the domain of interest\n\n\nTake hint\n\nUse the fm_pixels() and predict() functions.\n\n\n\n\nClick here to see the solution\n\n\nCode\npxl = fm_pixels(mesh, mask= region, dims = c(200,200))\npreds = predict(fit3, pxl, ~data.frame(spde = space,\n                                       log_int = Intercept + space + elev))\n\n#and plot as \nlibrary(scico)\nlibrary(patchwork)\n\nggplot(data= preds$spde) + \n  geom_sf(aes(color = mean)) + \n  scale_color_scico() +\n ggtitle(\"spde mean\") +\nggplot(data=preds$spde ) +\n  geom_sf(aes(color = sd)) +\n  scale_color_scico() +\n ggtitle(\"spde sd\") +\n\nggplot(data=preds$log_int) + \n  geom_sf(aes(color = mean)) + \n  scale_color_scico() +\n ggtitle(\"log-int mean\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=preds$log_int) + \n  geom_sf(aes(color = sd)) +\n  scale_color_scico() +\n ggtitle(\"log-int sd\") +\n  plot_layout(ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of just looking at the posterior mean and standard deviation, it can be usefull to look at simulated fields from the posterior distribution. This is because the mean field is, by definition, smoother than any realization of the field. So looking at simulation can give us a better idea of how the field might look like. We can do this using the generate() function:\n\nsim_fields = generate(fit3, pxl, ~data.frame(spde = space,\n                                       log_int = Intercept + space + elev),\n                     n.samples = 4)\n\ncbind(pxl,sapply(sim_fields, function(x) x$spde)) %&gt;%\n  pivot_longer(-geometry) %&gt;%\n  ggplot() + geom_sf(aes(color = value)) + \n  facet_wrap(.~name) + scale_color_scico() +\n  ggtitle(\"simulated spatial fields\")\n\n\n\n\n\n\n\ncbind(pxl,sapply(sim_fields, function(x) x$log_int)) %&gt;%\n  pivot_longer(-geometry) %&gt;%\n  ggplot() + geom_sf(aes(color = value)) + \n  facet_wrap(.~name) + scale_color_scico() + \n  ggtitle(\"simulated log intensity\")",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day5_practical_8.html",
    "href": "day5_practical_8.html",
    "title": "Practical 8",
    "section": "",
    "text": "Aim of this practical:\nIn this practical we are going to look at some model comparison and validation techniques.\nDownload Practical 8 R script",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#the-data",
    "href": "day5_practical_8.html#the-data",
    "title": "Practical 8",
    "section": "The data",
    "text": "The data\nIn the next exercise, we will explore data from a combination of several NOAA shipboard surveys conducted on pan-tropical spotted dolphins in the Gulf of Mexico. The data set is available in inlabru (originally obtianed from the dsm R package) and contains the following information:\n\nA total of 47 observations of groups of dolphins were detected. The group size was recorded, as well as the Beaufort sea state at the time of the observation.\nTransect width is 16 km, i.e. maximal detection distance 8 km (transect half-width 8 km).\n\nWe can load and visualize the data as follows:\n\nmexdolphin &lt;- mexdolphin_sf\nmexdolphin$depth &lt;- mexdolphin$depth %&gt;% mutate(depth=scale(depth)%&gt;%c())\nmapviewOptions(basemaps = c( \"OpenStreetMap.DE\"))\n\nmapview(mexdolphin$points,zcol=\"size\")+\n  mapview(mexdolphin$samplers)+\n mapview(mexdolphin$ppoly )",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#the-workflow",
    "href": "day5_practical_8.html#the-workflow",
    "title": "Practical 8",
    "section": "The workflow",
    "text": "The workflow\nTo model the density of spotted dolphins we take a thinned point process model of the form:\n\\[\np(\\mathbf{y} | \\lambda)  \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) p(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) p(\\mathbf{s}_i))\n\\tag{1}\\]\nWhen fitting a distance sampling model we need to fulfill the following tasks:\n\nBuild the mesh\nDefine the SPDE representation of the spatial GF. This includes defining the priors for the range and sd of the spatial GF\nDefine the components of the linear predictor. This includes the spatial GF and all eventual covariates\nDefine the observation model using the bru_obs() function\nRun the model using the bru() function\n\n\nBuilding the mesh\nThe first task is to build the mesh that covers the area of interest. For this purpose we use the function fm_mesh_2d. To do so, we need to define the area of interest. We can either use a predefined boundary or create a non convex hull surrounding the location of the specie sightseeings\n\nnon-covex hulldomain boundary\n\n\n\nboundary0 = fm_nonconvex_hull(mexdolphin$points,convex = -0.1)\n\nmesh_0 = fm_mesh_2d(boundary = boundary0,\n                          max.edge = c(30, 150), # The largest allowed triangle edge length.\n                          cutoff = 15,\n                          crs = fm_crs(mexdolphin$points))\nggplot() + gg(mesh_0)\n\n\n\n\n\n\n\n\n\n\nThe mexdolphin object contains a predefined region of interest which can be accessed through mexdolphin$ppoly\n\nmesh_1 = fm_mesh_2d(boundary = mexdolphin$ppoly,\n                    max.edge = c(30, 150),\n                    cutoff = 15,\n                    crs = fm_crs(mexdolphin$points))\nggplot() + gg(mesh_1)\n\n\n\n\n\n\n\n\n\n\n\nKey parameters in mesh construction include: max.edge for maximum triangle edge lengths, offset for inner and outer extensions (to prevent edge effects), and cutoff to avoid overly small triangles in clustered areas.\n\n\n\n\n\n\nNote\n\n\n\nGeneral guidelines for creating the mesh\n\nCreate triangulation meshes with fm_mesh_2d()\nMove undesired boundary effects away from the domain of interest by extending to a smooth external boundary\nUse a coarser resolution in the extension to reduce computational cost (max.edge=c(inner, outer))\nUse a fine resolution (subject to available computational resources) for the domain of interest (inner correlation range) and filter out small input point clusters (0 &lt; cutoff &lt; inner)\nCoastlines and similar can be added to the domain specification in fm_mesh_2d() through the boundary argument.\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nLook at the documentation for the fm_mesh_2d function typing\n\n?fm_mesh_2d\n\nplay around with the different options and create different meshes. You can compare these against a pre-computed mesh available by typing plot(mexdolphin$mesh)\nThe rule of thumb is that your mesh should be:\n\nfine enough to well represent the spatial variability of your process, but not too fine in order to avoid computation burden\nthe triangles should be regular, avoid long and thin triangles.\nThe mesh should contain a buffer around your area of interest (this is what is defined in the offset option) in order to avoid boundary artefact in the estimated variance.\n\n\n\n\n\nDefine the SPDE representation of the spatial GF\nTo define the SPDE representation of the spatial GF we use the function inla.spde2.pcmatern. This takes as input the mesh we have defined and the PC-priors definition for \\(\\rho\\) and \\(\\sigma\\) (the range and the marginal standard deviation of the field).\nPC priors Gaussian Random field are defined in (Fuglstad et al. 2018). From a practical perspective for the range \\(\\rho\\) you need to define two paramters \\(\\rho_0\\) and \\(p_{\\rho}\\) such that you believe it is reasonable that\n\\[\nP(\\rho&lt;\\rho_0)=p_{\\rho}\n\\]\nwhile for the marginal variance \\(\\sigma\\) you need to define two parameters \\(\\sigma_0\\) and \\(p_{\\sigma}\\) such that you believe it is reasonable that\n\\[\nP(\\sigma&gt;\\sigma_0)=p_{\\sigma}\n\\]\n\n\n\n\n\n\n Question\n\n\n\nTake a look at the code below and select which of the following statements about the specified Matern PC priors are true.\n\nspde_model &lt;- inla.spde2.pcmatern(mexdolphin$mesh,\n  prior.sigma = c(2, 0.01),\n  prior.range = c(50, 0.01)\n)\n\n\n there is probability of 0.01 that the spatial range is greater or equal than 50 the probability that the spatial range is smaller than 50 is very small the probability that the marginal standard deviation is smaller than 2 is very small there is probability of 0.99 that the marginal standard deviation is less or equal than 2\n\n\n\n\n\nDefine the components of the linear predictor\nWe have now defined a mesh and a SPDE representation of the spatial GF. We now need to define the model components.\nFirst, we need to define the detection function. Here, we will define a half-normal detection probability function. This must take distance as its first argument and the linear predictor of the sigma parameter as its second:\n\nhn &lt;- function(distance, sigma) {\n  exp(-0.5 * (distance / sigma)^2)\n}\n\nWe need to now separately define the components of the model including the SPDE model, the Intercept, the effect of depth and the detection function parameter sigma.\n\ncmp &lt;- ~ space(main = geometry, model = spde_model) +\n  sigma(1,\n    prec.linear = 1,\n    marginal = bm_marginal(qexp, pexp, dexp, rate = 1 / 8)\n  ) +\n  Intercept(1)\n\n\n\n\n\n\n\nNote\n\n\n\nTo control the prior distribution for the sigma parameter, we use a transformation mapper that converts a latent variable into an exponentially distributed variable with expectation 8 (this is a somewhat arbitrary value, but motivated by the maximum observation distance W)\nThe marginal argument in the sigma component specifies the transformation function taking N(0,1) to Exponential(1/8).\n\n\nThe formula, which describes how these components are combined to form the linear predictor\n\\[\\log \\color{red}{\\tilde{\\lambda}(s)} = \\overbrace{\\log \\lambda (s)}^{\\beta_0 + \\xi(s)} + \\overbrace{\\log \\color{red}{g(d(s))}}^{-0.5~d(\\mathbf{s})^2\\sigma^{-2}}\\]\n\neta &lt;- geometry + distance ~ space +\n  log(hn(distance, sigma)) +\n  Intercept + log(2) \n\nHere, the log(2) offset in the predictor takes care of the two-sided detections\n\n\nDefine the observation model\ninlabru has support for latent Gaussian Cox processes through the cp likelihood family. To fit a point process model recall that we need to approximate the integral in using a numerical integration scheme as:\n\\[\n\\approx\\exp\\left(-\\sum_{k=1}^{N_k}w_k\\lambda(s_k)\\right)\\prod_{i=1}^n \\lambda(\\mathbf{s}_i)\n\\]\nThus, we first create our integration scheme using the fm_int function by specifying integration domains for the spatial and distance dimensions.\nHere we use the same points to define the SPDE approximation and to approximate the integral in Equation 1, so that the integration weight and SPDE weights are consistent with each other. We also need to explicitly integrate over the distance dimension so we use the fm_mesh_1d() to create mesh over the samplers (which are the transect lines in this dataset, so we need to tell inlabru about the strip half-width).\n\n# build integration scheme\ndistance_domain &lt;-  fm_mesh_1d(seq(0, 8,\n                              length.out = 30))\nips = fm_int(list(geometry = mexdolphin$mesh,\n                  distance = distance_domain),\n             samplers = mexdolphin$samplers)\n\nNow, we just need to supply the sf object as our data and the integration scheme ips:\n\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = mexdolphin$points,\n              ips = ips)\n\nThen we fit the model, passing both the components and the observaional model\n\nfit = bru(cmp, lik)\n\n\n\n\n\n\n\nNote\n\n\n\ninlabru supports a shortcut for defining the integration points using the domain and samplers argument of bru_obs(). This domain argument expects a list of named domains with inputs that are then internally passed to fm_int() to build the integration scheme. The samplers argument is used to define subsets of the domain over which the integral should be computed. An equivalent way to define the same model as above is:\n\nlik = bru_obs(formula = eta, \n              data = mexdolphin$points, \n              family = \"cp\",\n              domain = list(\n                geometry = mesh,\n                distance = fm_mesh_1d(seq(0, 8, length.out = 30))),\n              samplers = mexdolphin$samplers)",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#visualize-model-results",
    "href": "day5_practical_8.html#visualize-model-results",
    "title": "Practical 8",
    "section": "Visualize model Results",
    "text": "Visualize model Results\n\nPosterior summaries\nWe can use the fit$summary.fixed and summary.hyperpar to obtain posterior summaries of the model parameters.\n\n\n\n\n\n\n\n\n\nmean\n0.025quant\n0.975quant\n\n\n\n\nsigma\n−0.05\n−0.46\n0.36\n\n\nIntercept\n−8.16\n−9.29\n−7.34\n\n\nRange for space\n295.48\n110.54\n673.68\n\n\nStdev for space\n0.81\n0.42\n1.39\n\n\n\n\n\n\n\nLook at the SPDE parameter posteriors as follows:\n\nplot( spde.posterior(fit, \"space\", what = \"range\")) +\nplot( spde.posterior(fit, \"space\", what = \"log.variance\"))  \n\n\n\n\n\n\n\n\n\n\nModel predictions\nWe now want to extract the estimated posterior mean and sd of spatial GF. To do this we first need to define a grid of points where we want to predict. We do this using the function fm_pixel() which creates a regular grid of points covering the mesh\n\npxl &lt;- fm_pixels(mexdolphin$mesh, dims = c(200, 100), mask = mexdolphin$ppoly)\n\nthen compute the prediction for both the spatial GF and the linear predictor (spatial GF + intercept)\n\npr.int = predict(fit, pxl, ~data.frame(spatial = space,\n                                      lambda = exp(Intercept + space)))\n\nFinally, we can plot the maps of the spatial effect\n\nggplot() + geom_sf(data = pr.int$spatial,aes(color = mean)) + scale_color_scico() + ggtitle(\"Posterior mean\")\n\n\n\n\n\n\n\nggplot() + geom_sf(data = pr.int$spatial,aes(color = sd)) + scale_color_scico() + ggtitle(\"Posterior sd\")\n\n\n\n\n\n\n\n\nNote The posterior sd is lowest at the observation points. Note how the posterior sd is inflated around the border, this is the “border effect” due to the SPDE representation.\n\n\n\n\n\n\n Task\n\n\n\nUsing the predictions stored in pr.int, produce a map of the posterior mean intensity.\n\n\nTake hint\n\nRecall that the predicted intensity is given by \\(\\lambda(s) = \\exp(\\beta_0+\\xi(s))\\)\n\n\n\n\nClick here to see the solution\n\n\nCode\nggplot() + \n  geom_sf(data = pr.int$lambda,aes(color = mean)) +\n  scale_color_scico(palette = \"imola\") +\n  ggtitle(\"Posterior mean\")\n\n\n\n\n\n\n\n\n\n\n\n\nWe can predict the detection function in a similar fashion.Here, we should make sure that it doesn’t try to evaluate the effects of components that can’t be evaluated using the given input data.\n\ndistdf &lt;- data.frame(distance = seq(0, 8, length.out = 100))\ndfun &lt;- predict(fit, distdf, ~ hn(distance, sigma))\nplot(dfun)",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#abundance-estimates",
    "href": "day5_practical_8.html#abundance-estimates",
    "title": "Practical 8",
    "section": "Abundance estimates",
    "text": "Abundance estimates\nThe mean expected number of animals can be computed by integrating the intensity over the region of interest as follows:\n\npredpts &lt;- fm_int(mexdolphin$mesh, mexdolphin$ppoly)\nLambda &lt;- predict(fit, predpts, ~ sum(weight * exp(space + Intercept)))\nLambda\n\n     mean       sd   q0.025     q0.5   q0.975   median sd.mc_std_err\n1 244.311 59.99578 147.5474 235.0969 389.1917 235.0969      5.021626\n  mean.mc_std_err\n1        7.003903\n\n\nTo fully propagate the uncertainty on the expected number animals we can draw Monte Carlo samples from the fitted model as follows (this could take a couple of minutes):\n\nNs &lt;- seq(50, 450, by = 1)\nNest &lt;- predict(fit, predpts,\n  ~ data.frame(\n    N = Ns,\n    density = dpois(\n      Ns,\n      lambda = sum(weight * exp(space + Intercept))\n    )\n  ),\n  n.samples = 2000\n)\n\nWe can compare this with a simpler “plug-in” approximation:\n\nNest &lt;- dplyr::bind_rows(\n  cbind(Nest, Method = \"Posterior\"),\n  data.frame(\n    N = Nest$N,\n    mean = dpois(Nest$N, lambda = Lambda$mean),\n    mean.mc_std_err = 0,\n    Method = \"Plugin\"\n  )\n)\n\nThen, we can visualize the result as follows:\n\nggplot(data = Nest) +\n  geom_line(aes(x = N, y = mean, colour = Method)) +\n  geom_ribbon(\n    aes(\n      x = N,\n      ymin = mean - 2 * mean.mc_std_err,\n      ymax = mean + 2 * mean.mc_std_err,\n      fill = Method,\n    ),\n    alpha = 0.2\n  ) +\n  geom_line(aes(x = N, y = mean, colour = Method)) +\n  ylab(\"Probability mass function\")",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#model-checks",
    "href": "day5_practical_8.html#model-checks",
    "title": "Practical 8",
    "section": "Model checks",
    "text": "Model checks\nLastly, we can assess the goodness-of-fit of the models by comparing the observed counts across different distance bins and the expected counts and their associated uncertainty:\n\nbc &lt;- bincount(\n  result = fit,\n  observations = mexdolphin$points$distance,\n  breaks = seq(0, max(mexdolphin$points$distance), length.out = 9),\n  predictor = distance ~ hn(distance, sigma)\n)\nattributes(bc)$ggp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nFit a model using a hazard detection function instead and compare the GoF of this model with that from the half-normal detection model. Recall that the hazard detection function is given by:\n\\[\ng(\\mathbf{s}|\\sigma) = 1 - \\exp(-(d(\\mathbf{s})/\\sigma)^{-1})\n\\]\n\n\nTake hint\n\nThe hazard function can be codes as:\n\nhr &lt;- function(distance, sigma) {\n  1 - exp(-(distance / sigma)^-1)\n}\n\nYou can use the same prior for the sigma parameter as for the half-Normal model (such parameters aren’t always comparable, but in this example it’s a reasonable choice). You can also use the lgcp function as a shortcut to fit the model (type ?lgcp for further details).\n\n\n\n\nClick here to see the solution\n\n\nCode\nformula1 &lt;- geometry + distance ~ space +\n  log(hr(distance, sigma)) +\n  Intercept + log(2)\n\n# here we use the shorcut to specify the model\nfit1 &lt;- lgcp(\n  components = cmp,\n  mexdolphin$points,\n  samplers = mexdolphin$samplers,\n  domain = list(\n    geometry = mexdolphin$mesh,\n    distance = fm_mesh_1d(seq(0, 8, length.out = 30))\n  ),\n  formula = formula1\n)\n\nbc1 &lt;- bincount(\n  result = fit1,\n  observations = mexdolphin$points$distance,\n  breaks = seq(0, max(mexdolphin$points$distance), length.out = 9),\n  predictor = distance ~ hn(distance, sigma)\n)\nattributes(bc1)$ggp",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#sec-prep",
    "href": "day5_practical_8.html#sec-prep",
    "title": "Practical 8",
    "section": "Data Preparation",
    "text": "Data Preparation\nThe following example use the gorillas dataset available in the inlabru library.\nThe data give the locations of Gorilla’s nests in an area: ::: {.cell layout-align=“center”}\ngorillas_sf &lt;- inlabru::gorillas_sf\nnests &lt;- gorillas_sf$nests\nboundary &lt;- gorillas_sf$boundary\n\nggplot() + geom_sf(data = nests) +\n  geom_sf(data = boundary, alpha = 0)\n\n\n\n\nLocation of gorilla nests\n\n\n\n:::\nThe dataset also contains covariates in the form or raster data. We consider two of them here: ::: {.cell}\ngcov = gorillas_sf_gcov()\nelev_cov &lt;- gcov$elevation\ndist_cov &lt;-  gcov$waterdist\n:::\n\n\n\n\n\nCovariates\n\n\n\n\nNote: the covariates have been expanded to cover all the nodes in the mesh.\n\nTo obtain the count data, we rasterize the species counts to match the spatial resolution of the covariates available. Then we aggregate the pixels to a rougher resolution (5x5 pixels in the original covariate raster dimensions). Finally, we mask regions outside the study area.\nIn addition we compute the area of each grid cell.\n\n# Rasterize data\ncounts_rstr &lt;-\n  terra::rasterize(vect(nests), gcov, fun = sum, background = 0) %&gt;%\n  terra::aggregate(fact = 5, fun = sum) %&gt;%\n  mask(vect(sf::st_geometry(boundary)))\nplot(counts_rstr)\n\n\n\n\nCounts of gorilla nests\n\n\n\n# compute cell area\ncounts_rstr &lt;- counts_rstr %&gt;%\n  cellSize(unit = \"km\") %&gt;%\n  c(counts_rstr)\n\nTo create our dataset of counts, we extract also the coordinate of center point of each raster pixel. In addition we create a column with presences and one with the pixel area\n\ncounts_df &lt;- crds(counts_rstr, df = TRUE, na.rm = TRUE) %&gt;%\n  bind_cols(values(counts_rstr, mat = TRUE, na.rm = TRUE)) %&gt;%\n  rename(count = sum) %&gt;%\n  mutate(present = (count &gt; 0) * 1L) %&gt;%\n  st_as_sf(coords = c(\"x\", \"y\"), crs = st_crs(nests))\n\nWe then aggregate the covariates to the same resolution as the nest counts and scale them.\n\nelev_cov1 &lt;- elev_cov %&gt;% \n  terra::aggregate(fact = 5, fun = mean) %&gt;% scale()\ndist_cov1 &lt;- dist_cov %&gt;% \n  terra::aggregate(fact = 5, fun = mean) %&gt;% scale()\n\n\n\n\n\n\n\n\n\nFigure 1: Covariates\n\n\n\n\n\n\nMesh building\nWe now define the mesh and the spde object.\n\n\nmesh &lt;- fm_mesh_2d(\n  loc = st_as_sfc(counts_df),\n  max.edge = c(0.5, 1),\n  crs = st_crs(counts_df)\n)\n\nmatern &lt;- inla.spde2.pcmatern(mesh,\n  prior.sigma = c(1, 0.01),\n  prior.range = c(5, 0.01)\n)\n\n\n\n\n\n\nMesh over the count locations\n\n\n\n\nIn our dataset, the number of zeros is quite substantial, and our model may struggle to account for them adequately. To address this, we should select a model capable of handling an “inflated” number of zeros, exceeding what a standard Poisson model would imply. For this purpose, we opt for a “zero-inflated Poisson model,” commonly abbreviated as ZIP.",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#sec-zip",
    "href": "day5_practical_8.html#sec-zip",
    "title": "Practical 8",
    "section": "Zero-Inflated model (Type1)",
    "text": "Zero-Inflated model (Type1)\nWe fit now a Zero-Inflated model to our data.\nThe Type 1 Zero-inflated Poisson model is defined as follows:\n\\[\n\\text{Prob}(y\\vert\\dots)=\\pi\\times 1_{y=0}+(1-\\pi)\\times \\text{Poisson}(y)\n\\]\nHere, \\(\\pi=\\text{logit}^{-1}(\\theta)\\)\nThe expected value and variance for the counts are calculated as:\n\\[\n\\begin{gathered}\nE(count)=(1-\\pi)\\lambda \\\\\nVar(count)= (1-\\pi)(\\lambda+\\pi \\lambda^2)\n\\end{gathered}\n\\tag{2}\\]\nThis model has two parameters:\n\nThe probability of excess zero \\(\\pi\\) - This is a hyperparameter and therefore it is constant\nThe mean of the Poisson distribution \\(\\lambda\\). This is linked to the linear predictor as: \\[\n\\eta = E\\log(\\lambda) = \\log(E) + \\beta_0 + \\beta_1\\text{Elevation} + \\beta_2\\text{Distance } + u\n\\] where \\(\\log(E)\\) is an offset (the area of the pixel) that accounts for the size of the cell.\n\n\n\n\n\n\n\n Task\n\n\n\nFit a zero-inflated model to the data (zeroinflatedpoisson1) by completing the following code: ::: {.cell}\ncmp = ~ Intercept(1) + elevation(...) + distance(...) + space(...)\n\nlik = bru_obs(...,\n    E = area)\n\nfit_zip &lt;- bru(cmp, lik)\n\n\n\n\nTake hint\n\nThe E = area is an offset that adjusts for the size of each cell.\n\n\n\n\nClick here to see the solution\n\n\nCode\n\ncmp = ~ Intercept(1) + elevation(elev_cov1, model = \"linear\") + distance(dist_cov1, model = \"linear\") + space(geometry, model = matern)\n\n\n\nlik = bru_obs(formula = count ~ .,\n    family = \"zeroinflatedpoisson1\", \n    data = counts_df,\n    E = area)\n\nfit_zip &lt;- bru(cmp, lik)\n\n\n\n:::\nOnce the model is fitted we can look at the results\n\n\n\n\n\n\n Task\n\n\n\nCheck what the estimated excess zero probaility is.\nUse the predict() function to look at the estimated \\(\\lambda(s)\\) and mean count in Equation 2\n\n\nTake hint\n\nTo get the right name for the hyperparameters to use in the predict() function, you can use the function bru_names().\n\n\n\n\nClick here to see the solution\n\n\nCode\n# to check the estimated excess zero probability:\n# fit_zip$summary.hyperpar\n\npred_zip &lt;- predict(\n  fit_zip, \n  counts_df,\n  ~ {\n    pi &lt;- zero_probability_parameter_for_zero_inflated_poisson_1\n    lambda &lt;- area * exp( distance + elevation + space + Intercept)\n    expect &lt;- (1-pi) * lambda\n    variance &lt;- (1-pi) * (lambda + pi * lambda^2)\n    list(\n      lambda = lambda,\n      expect = expect,\n      variance = variance\n    )\n  },\n  n.samples = 2500\n)\n\n\n\n\n\n\n\n\n\n\nEstimated \\(\\lambda\\) (left) and expected counts (right) with zero inflated model",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#sec-zap",
    "href": "day5_practical_8.html#sec-zap",
    "title": "Practical 8",
    "section": "Hurdle model (Type0)",
    "text": "Hurdle model (Type0)\nWe now fit a hurdle model to the same data.\nIn the zeroinflatedpoisson0 model is defined by the following observation probability model\n\\[\n\\text{Prob}(y\\vert\\dots)=\\pi\\times 1_{y=0}+(1-\\pi)\\times \\text{Poisson}(y\\vert y&gt;0)\n\\]\nwhere \\(\\pi\\) is the probability of zero.\nThe expectation and variance of the counts are as follows:\n\\[\n\\begin{aligned}\nE(\\text{count})&=\\frac{1}{1-\\exp(-\\lambda)}\\pi\\lambda \\\\\nVar(\\text{count})&=  E(\\text{count}) \\left(1-\\exp(-\\lambda) E(\\text{count})\\right)\n\\end{aligned}\n\\tag{3}\\]\n\n\n\n\n\n\n Task\n\n\n\nFit a hurdle model to the data using the zeroinflatedpoisson0 likelihood\n\n\nTake hint\n\nYou do not need to redefine the components as the linear predictor is not changing.\n\n\n\n\nClick here to see the solution\n\n\nCode\nlik = bru_obs(formula = count ~ .,\n    family = \"zeroinflatedpoisson0\", \n    data = counts_df,\n    E = area)\n\nfit_zap &lt;- bru(cmp, lik)\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nAs before, check what the estimated probability of zero is and use predict() to obtain a map of the estimated mean counts in Equation 3 over the domain.\n\n\nTake hint\n\n\n\n\n\nClick here to see the solution\n\n\nCode\n\npred_zap &lt;- predict( fit_zap, counts_df,\n  ~ {\n    pi &lt;- zero_probability_parameter_for_zero_inflated_poisson_0\n    lambda &lt;- area * exp( distance + elevation + space + Intercept)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    list(\n      lambda = lambda,\n      expect = expect\n    )\n  },\n  n.samples = 2500\n)\n\n\n\n\n\n\n\n\n\n\nEstimated \\(\\lambda\\) (left) and expected counts (right) with hurdle model",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#sec-two-lik",
    "href": "day5_practical_8.html#sec-two-lik",
    "title": "Practical 8",
    "section": "Hurdle model using two likelihoods",
    "text": "Hurdle model using two likelihoods\nHere the model is the same as in Section 2.3, but this time we also want to model \\(\\pi\\) using covariates and random effects. Therefore we define a second linear predictor \\[\n\\eta^2 =\\beta_0^2 + \\beta_1^2\\text{Elevation} +  \\beta_2^2\\text{Distance} + u^2\n\\] Note here we have defined the two linear predictor to use the same covariates, but this is not necessary, they can be totally independent.\nTo fit this model we have to define two likelihoods: - One will account for the presence-absence process and has a Binomial model - One will account for the counts and has a truncated Poisson model\n\n\n\n\n\n\n Task\n\n\n\nComplete the following code to fit a hurdle model based on two likelihoods:\n\n# define components\ncmp &lt;- ~\n  Intercept_count(1) +\n    elev_count(elev_cov1, model = \"linear\") +\n    dist_count(dist_cov1, model = \"linear\") +\n    space_count(geometry, model = matern) +\n    Intercept_presence(1) +\n    elev_presence(elev_cov1, model = \"linear\") +\n    dist_presence(dist_cov1, model = \"linear\") +\n    space_presence(geometry, model = matern) \n\n# positive count model\npos_count_obs &lt;- bru_obs(formula = ...,\n      family = ...,\n      data = counts_df[counts_df$present &gt; 0, ],\n      E = area)\n  \n# presence model\npresence_obs &lt;- bru_obs(formula ...,\n  family = ...,\n  data = counts_df,\n)\n\n# fit the model\nfit_zap2 &lt;- bru(...)\n\n\n\nTake hint\n\nAdd hint details here…\n\n\n\n\nClick here to see the solution\n\n\nCode\ncmp &lt;- ~\n  Intercept_count(1) +\n    elev_count(elev_cov1, model = \"linear\") +\n    dist_count(dist_cov1, model = \"linear\") +\n    space_count(geometry, model = matern) +\n    Intercept_presence(1) +\n    elev_presence(elev_cov1, model = \"linear\") +\n    dist_presence(dist_cov1, model = \"linear\") +\n    space_presence(geometry, model = matern) \n\n\npos_count_obs &lt;- bru_obs(formula = count ~ Intercept_count + elev_count + \n                                   dist_count + space_count,\n      family = \"nzpoisson\",\n      data = counts_df[counts_df$present &gt; 0, ],\n      E = area)\n  \n\npresence_obs &lt;- bru_obs(formula = present ~ Intercept_presence + elev_presence + dist_presence +\n                          space_presence,\n  family = \"binomial\",\n  data = counts_df,\n)\n\nfit_zap2 &lt;- bru(\n  cmp,\n  presence_obs,\n  pos_count_obs\n)",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#sec-two-lik-share",
    "href": "day5_practical_8.html#sec-two-lik-share",
    "title": "Practical 8",
    "section": "Hurdle model using two likelihoods and a shared component",
    "text": "Hurdle model using two likelihoods and a shared component\nNote that in the model above, there is no direct link between the parameters of the two observation parts, and we could estimate them separately. However, the two likelihoods could share some of the components; for example the space_count component could be used for both predictors. This would be possible using the copy argument.\nWe would then need to define one component as space(geometry, model = matern) and then a copy of it as space_copy(geometry, copy = \"space\", fixed = FALSE).\nThe results from the model in ?@sec-sec-two-lik show that the estimated covariance parameters for the two fields are very different, so it is probably not sensible to share the same component between the two parts. We do it anyway to show an example:\n\ncmp &lt;- ~\n  Intercept_count(1) +\n    elev_count(elev_cov1, model = \"linear\") +\n    dist_count(dist_cov1, model = \"linear\") +\n    Intercept_presence(1) +\n    elev_presence(elev_cov1, model = \"linear\") +\n    dist_presence(dist_cov1, model = \"linear\") +\n    space(geometry, model = matern) +\n  space_copy(geometry, copy = \"space\", fixed = FALSE)\n\n\npos_count_obs &lt;- bru_obs(formula = count ~ Intercept_count + elev_count + dist_count + space,\n      family = \"nzpoisson\",\n      data = counts_df[counts_df$present &gt; 0, ],\n      E = area)\n\npresence_obs &lt;- bru_obs(formula = present ~ Intercept_presence + elev_presence + dist_presence + space_copy,\n  family = \"binomial\",\n  data = counts_df)\n\nfit_zap3 &lt;- bru(\n  cmp,\n  presence_obs,\n  pos_count_obs)",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#comparing-models",
    "href": "day5_practical_8.html#comparing-models",
    "title": "Practical 8",
    "section": "Comparing models",
    "text": "Comparing models\nWe have fitted four different models. Now we want to compare them and see how they fit the data.\n\nComparing model predictions\nWe first want to compare the estimated surfaces of expected counts. To do this we want to produce the estimated expected counts, similar to what we did in Section 2.2 and Section 2.3 for all four models and plot them together:\n\npred_zip &lt;- predict(\n  fit_zip, \n  counts_df,\n  ~ {\n    pi &lt;- zero_probability_parameter_for_zero_inflated_poisson_1\n    lambda &lt;- area * exp( distance + elevation + space + Intercept)\n    expect &lt;- (1-pi) * lambda\n    variance &lt;- (1-pi) * (lambda + pi * lambda^2)\n    list(\n      expect = expect\n    )\n  },n.samples = 2500)\n\npred_zap &lt;- predict( fit_zap, counts_df,\n  ~ {\n    pi &lt;- zero_probability_parameter_for_zero_inflated_poisson_0\n    lambda &lt;- area * exp( distance + elevation + space + Intercept)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    list(\n      expect = expect)\n  },n.samples = 2500)\n\ninv.logit = function(x) (exp(x)/(1+exp(x)))\n\npred_zap2 &lt;- predict( fit_zap2, counts_df,\n  ~ {\n    pi &lt;- inv.logit(Intercept_presence + elev_presence + dist_presence + space_presence)\n    lambda &lt;- area * exp( dist_count + elev_count + space_count + Intercept_count)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    list(\n      expect = expect)\n  },n.samples = 2500)\n\npred_zap3 &lt;- predict( fit_zap3, counts_df,\n  ~ {\n    pi &lt;- inv.logit(Intercept_presence + elev_presence + dist_presence + space_copy)\n    lambda &lt;- area * exp( dist_count + elev_count + space + Intercept_count)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    list(\n      expect = expect)\n  },n.samples = 2500)\n\n\n\n\n  data.frame(x = st_coordinates(counts_df)[,1],\n             y = st_coordinates(counts_df)[,2],\n    zip = pred_zip$expect$mean,\n         hurdle = pred_zap$expect$mean,\n         hurdle2 = pred_zap2$expect$mean,\n         hurdle3 = pred_zap3$expect$mean)  %&gt;%\n  pivot_longer(-c(x,y)) %&gt;%\n  ggplot() + geom_tile(aes(x,y, fill = value)) + facet_wrap(.~name) +\n    theme_map + scale_fill_scico(direction = -1)\n\n\n\n\nEstimated expected counts for all four models\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nCreate plots of the estimated variance of the counts.\n\n\nTake hint\n\nThe fomulas for the variances are in Equation 2 and Equation 3.\n\n\n\n\nClick here to see the solution\n\n\nCode\npred_zip &lt;- predict(\n  fit_zip, \n  counts_df,\n  ~ {\n    pi &lt;- zero_probability_parameter_for_zero_inflated_poisson_1\n    lambda &lt;- area * exp( distance + elevation + space + Intercept)\n    variance &lt;- (1-pi) * (lambda + pi * lambda^2)\n    list( variance = variance)\n  },n.samples = 2500)\n\npred_zap &lt;- predict( fit_zap, counts_df,\n  ~ {\n    pi &lt;- zero_probability_parameter_for_zero_inflated_poisson_0\n    lambda &lt;- area * exp( distance + elevation + space + Intercept)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    variance = expect *(1-exp(-lambda) * expect)\n    list(variance = variance)\n  },\n  n.samples = 2500)\n\ninv.logit = function(x) (exp(x)/(1+exp(x)))\n\npred_zap2 &lt;- predict( fit_zap2, counts_df,\n  ~ {\n    pi &lt;- inv.logit(Intercept_presence + elev_presence + dist_presence + space_presence)\n    lambda &lt;- area * exp( dist_count + elev_count + space_count + Intercept_count)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    variance = expect *(1-exp(-lambda) * expect)\n    list(variance = variance)\n  },\n  n.samples = 2500)\n\npred_zap3 &lt;- predict( fit_zap3, counts_df,\n  ~ {\n    pi &lt;- inv.logit(Intercept_presence + elev_presence + dist_presence + space_copy)\n    lambda &lt;- area * exp( dist_count + elev_count + space + Intercept_count)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    variance = expect *(1-exp(-lambda) * expect)\n    list(variance = variance)\n  },\n  n.samples = 2500)\n\n\n\n\n  data.frame(x = st_coordinates(counts_df)[,1],\n             y = st_coordinates(counts_df)[,2],\n    zip = pred_zip$variance$mean,\n         hurdle = pred_zap$variance$mean,\n         hurdle2 = pred_zap2$variance$mean,\n         hurdle3 = pred_zap3$variance$mean)  %&gt;%\n  pivot_longer(-c(x,y)) %&gt;%\n  ggplot() + geom_tile(aes(x,y, fill = value)) + facet_wrap(.~name) +\n    theme_map + scale_fill_scico(direction = -1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing scores\nWe can compare model using the scores that the bru() function computes since we have set, at the beginning. the options to ::: {.cell}\nbru_options_set(control.compute = list(dic = TRUE,\n                                       waic = TRUE,\n                                       mlik = TRUE,\n                                       cpo = TRUE))\n:::\nLets use these scores to compare the models.\n\n\n\n\n\n\n Task\n\n\n\nExtract the DIC, WAIC and MLIK values for the four models and compare them\n\n\n\nClick here to see the solution\n\n\nCode\ndata.frame( Model = c(\"ZIP\", \"HURDLE\", \"HURDLE_2\",\"HURDLE_3\" ),\n  DIC = c(fit_zip$dic$dic, fit_zap$dic$dic, WAIC = fit_zap2$dic$dic, fit_zap3$dic$dic),\n            WAIC = c(fit_zip$waic$waic, fit_zap$waic$waic, fit_zap2$waic$waic, fit_zap3$waic$waic),\n            MLIK = c(fit_zip$mlik[1], fit_zap$mlik[1], fit_zap2$mlik[1], fit_zap3$mlik[1]))\n#&gt;      Model      DIC     WAIC      MLIK\n#&gt; 1      ZIP 1214.136 1223.716 -686.9499\n#&gt; 2   HURDLE 1886.065 1909.721 -994.3807\n#&gt; 3 HURDLE_2 1268.319 1285.521 -734.3697\n#&gt; 4 HURDLE_3 1883.922 3895.375 -858.1406\n\n\n\n\n\nFrom the table above we can see that the model that best balances complexity and fit is the zero inflated one (ZIP).",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "slides/slides_2.html#outline",
    "href": "slides/slides_2.html#outline",
    "title": "Lecture 1",
    "section": "Outline",
    "text": "Outline\n\nWhat are INLA and inlabru?\nWhy the Bayesian framework?\nWhich model are inlabru-friendly?\nWhat are Latent Gaussian Models?\nHow are they implemented in inlabru?"
  },
  {
    "objectID": "slides/slides_2.html#what-is-inla-what-is-inlabru",
    "href": "slides/slides_2.html#what-is-inla-what-is-inlabru",
    "title": "Lecture 1",
    "section": "What is INLA? What is inlabru?",
    "text": "What is INLA? What is inlabru?\nThe short answer:\n\nINLA is a fast method to do Bayesian inference with latent Gaussian models and inlabru is an R-package that implements this method with a flexible and simple interface.\n\nThe (much) longer answer:\n\nRue, H., Martino, S. and Chopin, N. (2009), Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71: 319-392.\nVan Niekerk, J., Krainski, E., Rustand, D., & Rue, H. (2023). A new avenue for Bayesian inference with INLA. Computational Statistics & Data Analysis, 181, 107692.\nLindgren, F., Bachl, F., Illian, J., Suen, M. H., Rue, H., & Seaton, A. E. (2024). inlabru: software for fitting latent Gaussian models with non-linear predictors. arXiv preprint arXiv:2407.00791.\nLindgren, F., Bolin, D., & Rue, H. (2022). The SPDE approach for Gaussian and non-Gaussian fields: 10 years and still running. Spatial Statistics, 50, 100599."
  },
  {
    "objectID": "slides/slides_2.html#where",
    "href": "slides/slides_2.html#where",
    "title": "Lecture 1",
    "section": "Where?",
    "text": "Where?\n\n\n\n Website-tutorials\n\n\ninlabru https://inlabru-org.github.io/inlabru/\nR-INLA https://www.r-inla.org/home\n\n\n\n\n\n\n Discussion forums\n\n\ninlabru https://github.com/inlabru-org/inlabru/discussions\nR-INLA https://groups.google.com/g/r-inla-discussion-group\n\n\n\n\n\n\n Books\n\n\n\nBlangiardo, M., & Cameletti, M. (2015). Spatial and spatio-temporal Bayesian models with R-INLA. John Wiley & Sons.\nGómez-Rubio, V. (2020). Bayesian inference with INLA. Chapman and Hall/CRC.\nKrainski, E., Gómez-Rubio, V., Bakka, H., Lenzi, A., Castro-Camilo, D., Simpson, D., … & Rue, H. (2018). Advanced spatial modeling with stochastic partial differential equations using R and INLA. Chapman and Hall/CRC.\nWang, X., Yue, Y. R., & Faraway, J. J. (2018). Bayesian regression modeling with INLA. Chapman and Hall/CRC."
  },
  {
    "objectID": "slides/slides_2.html#so-why-should-you-use-inlabru",
    "href": "slides/slides_2.html#so-why-should-you-use-inlabru",
    "title": "Lecture 1",
    "section": "So… Why should you use inlabru?",
    "text": "So… Why should you use inlabru?\n\n\nWhat type of problems can we solve?\nWhat type of models can we use?\nWhen can we use it?"
  },
  {
    "objectID": "slides/slides_2.html#so-why-should-you-use-inlabru-1",
    "href": "slides/slides_2.html#so-why-should-you-use-inlabru-1",
    "title": "Lecture 1",
    "section": "So… Why should you use inlabru?",
    "text": "So… Why should you use inlabru?\n\nWhat type of problems can we solve?\nWhat type of models can we use?\nWhen can we use it?\n\n\nTo give proper answers to these questions, we need to start at the very beginning .."
  },
  {
    "objectID": "slides/slides_2.html#the-core",
    "href": "slides/slides_2.html#the-core",
    "title": "Lecture 1",
    "section": "The core",
    "text": "The core\n\nWe have observed something."
  },
  {
    "objectID": "slides/slides_2.html#the-core-1",
    "href": "slides/slides_2.html#the-core-1",
    "title": "Lecture 1",
    "section": "The core",
    "text": "The core\n\nWe have observed something.\nWe have questions."
  },
  {
    "objectID": "slides/slides_2.html#the-core-2",
    "href": "slides/slides_2.html#the-core-2",
    "title": "Lecture 1",
    "section": "The core",
    "text": "The core\n\nWe have observed something.\nWe have questions.\nWe want answers!"
  },
  {
    "objectID": "slides/slides_2.html#how-do-we-find-answers",
    "href": "slides/slides_2.html#how-do-we-find-answers",
    "title": "Lecture 1",
    "section": "How do we find answers?",
    "text": "How do we find answers?\nWe need to make choices:\n\n\n\nBayesian or frequentist?\nHow do we model the data?\nHow do we compute the answer?"
  },
  {
    "objectID": "slides/slides_2.html#how-do-we-find-answers-1",
    "href": "slides/slides_2.html#how-do-we-find-answers-1",
    "title": "Lecture 1",
    "section": "How do we find answers?",
    "text": "How do we find answers?\nWe need to make choices:\n\nBayesian or frequentist?\nHow do we model the data?\nHow do we compute the answer?\n\nThese questions are not independent."
  },
  {
    "objectID": "slides/slides_2.html#bayesian-or-frequentist",
    "href": "slides/slides_2.html#bayesian-or-frequentist",
    "title": "Lecture 1",
    "section": "Bayesian or frequentist?",
    "text": "Bayesian or frequentist?\nIn this course we embrace the Bayesian perspective\n\nThere are no “true but unknown” parameters !"
  },
  {
    "objectID": "slides/slides_2.html#bayesian-or-frequentist-1",
    "href": "slides/slides_2.html#bayesian-or-frequentist-1",
    "title": "Lecture 1",
    "section": "Bayesian or frequentist?",
    "text": "Bayesian or frequentist?\nIn this course we embrace the Bayesian perspective\n\nThere are no “true but unknown” parameters !\nEvery parameter is described by a probability distribution!"
  },
  {
    "objectID": "slides/slides_2.html#bayesian-or-frequentist-2",
    "href": "slides/slides_2.html#bayesian-or-frequentist-2",
    "title": "Lecture 1",
    "section": "Bayesian or frequentist?",
    "text": "Bayesian or frequentist?\nIn this course we embrace the Bayesian perspective\n\nThere are no “true but unknown” parameters !\nEvery parameter is described by a probability distribution!\nEvidence from the data is used to update the belief we had before observing the data!"
  },
  {
    "objectID": "slides/slides_2.html#some-more-details-i",
    "href": "slides/slides_2.html#some-more-details-i",
    "title": "Lecture 1",
    "section": "Some more details I",
    "text": "Some more details I\nWe define linear predictor the mean (or a function of the mean) of our observations given the model components.\n\nIn this case \\(E(y_i|\\beta_0, \\beta_i) = \\eta_i = \\beta_0 + \\beta_1 x_i\\)"
  },
  {
    "objectID": "slides/slides_2.html#some-more-details-i-1",
    "href": "slides/slides_2.html#some-more-details-i-1",
    "title": "Lecture 1",
    "section": "Some more details I",
    "text": "Some more details I\nWe define linear predictor the mean (or a function of the mean) of our observations given the model components.\n\nIn this case \\(E(y_i|\\beta_0, \\beta_i) =\\eta_i =  \\color{red}{\\boxed{\\beta_0}} +  \\color{red}{\\boxed{\\beta_1 x_i}}\\)\nThis model as two components !"
  },
  {
    "objectID": "slides/slides_2.html#some-more-details-ii",
    "href": "slides/slides_2.html#some-more-details-ii",
    "title": "Lecture 1",
    "section": "Some more details II",
    "text": "Some more details II\nGiven the linear predictor \\(\\eta\\) the observations a independent on each other!\n\nThis means that all dependencies in the observations are accounted for by the components!"
  },
  {
    "objectID": "slides/slides_2.html#some-more-details-ii-1",
    "href": "slides/slides_2.html#some-more-details-ii-1",
    "title": "Lecture 1",
    "section": "Some more details II",
    "text": "Some more details II\nGiven the linear predictor \\(\\eta\\) the observations a independent on each other!\n\nThe observation model (likelihood) can be written as: \\[\n\\pi(\\mathbf{y}|\\eta,\\sigma^2) = \\prod_{i = 1}^n\\pi(y_i|\\eta_i,\\sigma^2)\n\\]"
  },
  {
    "objectID": "slides/slides_2.html#lets-formalize-this-a-bit",
    "href": "slides/slides_2.html#lets-formalize-this-a-bit",
    "title": "Lecture 1",
    "section": "Let’s formalize this a bit…",
    "text": "Let’s formalize this a bit…\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\\\\nE(y_i|\\eta_i, \\sigma^2) & = \\eta_i\n\\end{aligned}\n\\] Note: We assume that, given the linear predictor \\(\\eta\\), the data are independent on each other! Data dependence is expressed through the components if the linear predictor."
  },
  {
    "objectID": "slides/slides_2.html#lets-formalize-this-a-bit-1",
    "href": "slides/slides_2.html#lets-formalize-this-a-bit-1",
    "title": "Lecture 1",
    "section": "Let’s formalize this a bit…",
    "text": "Let’s formalize this a bit…\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\(y_i|\\eta_i,\\sigma^2\\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\)\nA model for the linear predictor \\[\nE(y_i|\\eta_i,\\sigma^2) = \\eta_i = \\beta_0 + \\beta_1x_i\n\\]"
  },
  {
    "objectID": "slides/slides_2.html#lets-formalize-this-a-bit-2",
    "href": "slides/slides_2.html#lets-formalize-this-a-bit-2",
    "title": "Lecture 1",
    "section": "Let’s formalize this a bit…",
    "text": "Let’s formalize this a bit…\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\(y_i|\\eta_i,\\sigma^2\\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\)\nA model for the linear predictor\n\n\\[\nE(y_i|\\eta_i,\\sigma^2) = \\eta_i = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_1x_i} }\n\\]\nNote 1: These are the components of our model! These explain the dependence structure of the data."
  },
  {
    "objectID": "slides/slides_2.html#lets-formalize-this-a-bit-3",
    "href": "slides/slides_2.html#lets-formalize-this-a-bit-3",
    "title": "Lecture 1",
    "section": "Let’s formalize this a bit…",
    "text": "Let’s formalize this a bit…\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\(y_i|\\eta_i,\\sigma^2\\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\)\nA model for the linear predictor \\(\\eta_i = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_1x_i} }\\)\nA prior for the model components \\(\\textbf{u}\\) \\[\n\\mathbf{u} = \\{\\beta_0, \\beta_1\\}\\sim\\mathcal{N}(0,\\mathbf{Q}^{-1})\n\\] Note: These have always a Gaussian prior and are use to explain the dependencies among data!"
  },
  {
    "objectID": "slides/slides_2.html#lets-formalize-this-a-bit-4",
    "href": "slides/slides_2.html#lets-formalize-this-a-bit-4",
    "title": "Lecture 1",
    "section": "Let’s formalize this a bit…",
    "text": "Let’s formalize this a bit…\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\(y_i|\\eta_i,\\sigma^2\\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\)\nA model for the linear predictor \\(\\eta_i = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_1x_i} }\\)\nA prior for the model components \\(\\mathbf{u} = \\{\\beta_0, \\beta_1\\}\\sim\\mathcal{N}(0,\\mathbf{Q}^{-1})\\)\nA prior for the non-gaussian parameters \\(\\theta\\) \\[\n\\theta = \\sigma^2\n\\]"
  },
  {
    "objectID": "slides/slides_2.html#latent-gaussian-models-lgm",
    "href": "slides/slides_2.html#latent-gaussian-models-lgm",
    "title": "Lecture 1",
    "section": "Latent Gaussian Models (LGM)",
    "text": "Latent Gaussian Models (LGM)\n\n\n\nThe observation model: \\[\n\\pi(\\mathbf{y}|\\eta,\\theta) = \\prod_{i=1}^{n}\\pi(y_i|\\eta_i,\\theta)\n\\]\nLinear predictor \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\)\nLatent Gaussian field \\(\\pi(\\mathbf{u}|\\theta)\\)\nThe hyperparameters: \\(\\pi(\\theta)\\)\n\n\n\nStage 1 The data generating process"
  },
  {
    "objectID": "slides/slides_2.html#latent-gaussian-models-lgm-1",
    "href": "slides/slides_2.html#latent-gaussian-models-lgm-1",
    "title": "Lecture 1",
    "section": "Latent Gaussian Models (LGM)",
    "text": "Latent Gaussian Models (LGM)\n\n\n\nThe observation model: \\[\n\\pi(\\mathbf{y}|\\eta,\\theta) = \\prod_{i=1}^{n}\\pi(y_i|\\eta_i,\\theta)\n\\]\nLinear predictor \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\)\nLatent Gaussian field \\(\\pi(\\mathbf{u}|\\theta)\\)\nThe hyperparameters: \\(\\pi(\\theta)\\)\n\n\n\nStage 1 The data generating process\nStage 2 The dependence structure"
  },
  {
    "objectID": "slides/slides_2.html#latent-gaussian-models-lgm-2",
    "href": "slides/slides_2.html#latent-gaussian-models-lgm-2",
    "title": "Lecture 1",
    "section": "Latent Gaussian Models (LGM)",
    "text": "Latent Gaussian Models (LGM)\n\n\n\nThe observation model: \\[\n\\pi(\\mathbf{y}|\\eta,\\theta) = \\prod_{i=1}^{n}\\pi(y_i|\\eta_i,\\theta)\n\\]\nLinear predictor \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\)\nLatent Gaussian field \\(\\pi(\\mathbf{u}|\\theta)\\)\nThe hyperparameters: \\(\\pi(\\theta)\\)\n\n\n\nStage 1 The data generating process\nStage 2 The dependence structure\nStage 3 The hyperparameters"
  },
  {
    "objectID": "slides/slides_2.html#latent-gaussian-models-lgm-3",
    "href": "slides/slides_2.html#latent-gaussian-models-lgm-3",
    "title": "Lecture 1",
    "section": "Latent Gaussian Models (LGM)",
    "text": "Latent Gaussian Models (LGM)\n\n\n\n\nThe observation model: \\[\n\\pi(\\mathbf{y}|\\eta,\\theta) = \\prod_{i=1}^{n}\\pi(y_i|\\eta_i,\\theta)\n\\]\nLinear predictor \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\)\nLatent Gaussian field \\(\\pi(\\mathbf{u}|\\theta)\\)\nThe hyperparameters: \\(\\pi(\\theta)\\)\n\n\n\n\n\nStage 1 The data generating process\nStage 2 The dependence structure\nStage 3 The hyperparameters\n\n\n\n\nQ: What are we interested in?"
  },
  {
    "objectID": "slides/slides_2.html#the-posterior-distribution",
    "href": "slides/slides_2.html#the-posterior-distribution",
    "title": "Lecture 1",
    "section": "The posterior distribution",
    "text": "The posterior distribution\n\n\n\n\n\n\n\nposterior\n\n\n\nA\n\nPrior\n belief\n\n\n\nC\n\nBayes Theorem\n &\n Bayesian Computations\n\n\n\nA-&gt;C\n\n\n\n\n\nB\n\nObservation\n model\n\n\n\nB-&gt;C\n\n\n\n\n\nD\n\nPosterior\n distribution\n\n\n\nC-&gt;D\n\n\n\n\n\n\n\n\n\n\n\\[\n\\color{red}{\\pi(\\mathbf{u},\\theta|\\mathbf{y})}\\propto \\color{blue}{\\pi(\\mathbf{y}|\\mathbf{u},\\theta)}\\color{green}{\\pi(\\mathbf{u}|\\theta)\\pi(\\theta)}\n\\]"
  },
  {
    "objectID": "slides/slides_2.html#the-posterior-distribution-1",
    "href": "slides/slides_2.html#the-posterior-distribution-1",
    "title": "Lecture 1",
    "section": "The posterior distribution",
    "text": "The posterior distribution\n\n\n\n\n\n\n\nposterior\n\n\n\nC\n\nBayes Theorem\n &\n Bayesian Computations\n\n\n\nD\n\nPosterior\n distribution\n\n\n\nC-&gt;D\n\n\n\n\n\nE\n\nBayesian Computation are hard!!\n Here is where\n INLA\n comes in!!!\n\n\n\nE-&gt;C\n\n\n\n\n\nA\n\nPrior\n belief\n\n\n\nA-&gt;C\n\n\n\n\n\nB\n\nObservation\n model\n\n\n\nB-&gt;C"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression",
    "href": "slides/slides_2.html#inlabru-for-linear-regression",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\beta_0 + \\beta_i x_i\n\\end{aligned}\n\\]\n\n\nlibrary(inlabru)\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression-1",
    "href": "slides/slides_2.html#inlabru-for-linear-regression-1",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_i x_i}}\n\\end{aligned}\n\\]\n\n\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression-2",
    "href": "slides/slides_2.html#inlabru-for-linear-regression-2",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\color{red}{\\boxed{\\beta_0 + \\beta_i x_i}}\n\\end{aligned}\n\\]\n\n\nlibrary(inlabru)\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression-3",
    "href": "slides/slides_2.html#inlabru-for-linear-regression-3",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\n\\color{red}{\\boxed{y_i|\\eta_i, \\sigma^2}} & \\color{red}{\\boxed{\\sim \\mathcal{N}(\\eta_i,\\sigma^2)}}\\\\\n\\eta_i & = \\beta_0 + \\beta_i x_i\n\\end{aligned}\n\\]\n\n\nlibrary(inlabru)\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression-4",
    "href": "slides/slides_2.html#inlabru-for-linear-regression-4",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\beta_0 + \\beta_i x_i\n\\end{aligned}\n\\]\n\n\nlibrary(inlabru)\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression-5",
    "href": "slides/slides_2.html#inlabru-for-linear-regression-5",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression"
  },
  {
    "objectID": "slides/slides_2.html#real-datasets-are-more-complicated",
    "href": "slides/slides_2.html#real-datasets-are-more-complicated",
    "title": "Lecture 1",
    "section": "Real datasets are more complicated!",
    "text": "Real datasets are more complicated!\nData can have several dependence structures: temporal, spatial,…\nUsing a Bayesian framework:\n\nBuild (hierarchical) models to account for potentially complicated dependency structures in the data.\nAttribute uncertainty to model parameters and latent variables using priors.\n\nTwo main challenges:\n\nNeed computationally efficient methods to calculate posteriors (this is where INLA helps!).\nSelect priors in a sensible way (we’ll talk about this)"
  },
  {
    "objectID": "slides/slides_2.html#the-good-news",
    "href": "slides/slides_2.html#the-good-news",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! 😃\n\nStage 1: What is the distribution of the responses?\nStage 2: What are the model components? and what is their distribution?\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?"
  },
  {
    "objectID": "slides/slides_2.html#the-good-news-1",
    "href": "slides/slides_2.html#the-good-news-1",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! 😃\n\nStage 1: What is the distribution of the responses?\n\nGaussian response? (temperature, rainfall, fish weight …)\nCount data? (people infected with a disease in each area)\nPoint pattern? (locations of trees in a forest)\nBinary data? (yes/no response, binary image)\nSurvival data? (recovery time, time to death)\n… (many more examples!!)\n\nStage 2: What are the model components? and what is their distribution?\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?"
  },
  {
    "objectID": "slides/slides_2.html#the-good-news-2",
    "href": "slides/slides_2.html#the-good-news-2",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! 😃\n\nStage 1: What is the distribution of the responses?\n\nWe assume data to be conditionally independent given the model components and some hyperparameters\nThis means that all dependencies in data are explained in Stage\n\n\n\n\nStage 2: What are the model components? and what is their distribution?\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?"
  },
  {
    "objectID": "slides/slides_2.html#the-good-news-3",
    "href": "slides/slides_2.html#the-good-news-3",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! 😃\n\nStage 1: What is the distribution of the responses?\nStage 2: What are the model components? and what is their distribution?\n\nHere we can have:\n\nFixed effects for covariates\nUnstructured random effects (individual effects, group effects)\nStructured random effects (AR(1), regional effects, )\n…\n\nThese are linked to the responses in the likelihood through linear predictors.\n\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?"
  },
  {
    "objectID": "slides/slides_2.html#the-good-news-4",
    "href": "slides/slides_2.html#the-good-news-4",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! 😃\n\nStage 1: What is the distribution of the responses?\nStage 2: What are the model components? and what is their distribution?\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?\nThe likelihood and the latent model typically have hyperparameters that control their behavior.\nThey can include:\n\nVariance of observation noise\nDispersion parameter in the negative binomial model\nVariance of unstructured effects\n…"
  },
  {
    "objectID": "slides/slides_2.html#the-second-good-news",
    "href": "slides/slides_2.html#the-second-good-news",
    "title": "Lecture 1",
    "section": "The second good news!",
    "text": "The second good news!\nNo matter how complicated is your model, the inlabru workflow is always the same 😃\n\n# Define model components\ncomps &lt;- component_1(...) + \n  component_2(...) + ...\n\n# Define the model predictor\npred &lt;- linear_function(component_1, \n                            component_2, ...)\n\n# Build the observation model\nlik &lt;- bru_obs(formula = pred,\n               family = ... ,\n               data = ... ,\n                ...)\n\n# Fit the model\nfit &lt;- bru(comps, lik, ...)"
  },
  {
    "objectID": "slides/slides_2.html#the-second-good-news-1",
    "href": "slides/slides_2.html#the-second-good-news-1",
    "title": "Lecture 1",
    "section": "The second good news!",
    "text": "The second good news!\nNo matter how complicated is your model, the inlabru workflow is always the same 😃\n\n# Define model components\ncomps &lt;- component_1(...) + \n  component_2(...) + ...\n\n# Define the model predictor\npred &lt;- linear_function(component_1, \n                            component_2, ...)\n\n# Build the observation model\nlik &lt;- bru_obs(formula = pred,\n               family = ... ,\n               data = ... ,\n                ...)\n\n# Fit the model\nfit &lt;- bru(comps, lik, ...)\n\nNOTE we will see later that this function can also be non-linear….😁"
  },
  {
    "objectID": "slides/slides_2.html#the-tokyo-rainfall-data",
    "href": "slides/slides_2.html#the-tokyo-rainfall-data",
    "title": "Lecture 1",
    "section": "The Tokyo rainfall data",
    "text": "The Tokyo rainfall data\nOne example with time series: Rainfall over 1 mm in the Tokyo area for each calendar day during two years (1983-84) are registered."
  },
  {
    "objectID": "slides/slides_2.html#the-model",
    "href": "slides/slides_2.html#the-model",
    "title": "Lecture 1",
    "section": "The model",
    "text": "The model\nStage 1 The observation model\n\\[\ny_t|\\eta_t\\sim\\text{Bin}(n_t, p_t),\\qquad \\eta_t = \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\] \\[\nn_t = \\left\\{\n\\begin{array}{lr}\n1, & \\text{for}\\; 29\\; \\text{February}\\\\\n2, & \\text{other days}\n\\end{array}\\right.\n\\] \\[\ny_t =\n\\begin{cases}\n\\{0,1\\}, & \\text{for}\\; 29\\; \\text{February}\\\\\n\\{0,1,2\\}, & \\text{other days}\n\\end{cases}\n\\]\n\nthe likelihood has no hyperparameters"
  },
  {
    "objectID": "slides/slides_2.html#the-model-1",
    "href": "slides/slides_2.html#the-model-1",
    "title": "Lecture 1",
    "section": "The model",
    "text": "The model\nStage 1 The observation model\n\\[\ny_t|\\eta_t\\sim\\text{Bin}(n_t, p_t),\\qquad \\eta_t = \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\]\nStage 2 The latent field \\[\n\\eta_t = \\beta_0 + f(\\text{time}_t)\n\\]\n\nprobability of rain depends on on the day of the year \\(t\\)\n\\(\\beta_0\\) is an intercept\n\\(f(\\text{time}_t)\\) is a RW2 model (this is just a smoother). The smoothness is controlled by a hyperparameter \\(\\tau_f\\)"
  },
  {
    "objectID": "slides/slides_2.html#the-model-2",
    "href": "slides/slides_2.html#the-model-2",
    "title": "Lecture 1",
    "section": "The model",
    "text": "The model\nStage 1 The observation model\n\\[\ny_t|\\eta_t\\sim\\text{Bin}(n_t, p_t),\\qquad \\eta_t = \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\]\nStage 2 The latent field \\[\n\\eta_t = \\beta_0 + f(\\text{time}_t)\n\\]\nStage 3 The hyperparameters\n\nThe structured time effect is controlled by one parameter \\(\\tau_f\\).\nWe assign a prior to \\(\\tau_f\\) to finalize the model."
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-time-series",
    "href": "slides/slides_2.html#inlabru-for-time-series",
    "title": "Lecture 1",
    "section": "inlabru for time series",
    "text": "inlabru for time series\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim \\text{Binomial}(n_t,p_t)\\\\\n\\text{logit}(p_t) = \\eta_i & = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{f(\\text{time}_t)}}\n\\end{aligned}\n\\]\n\n\nTokyo[1:3,]\n\n  y n time\n1 0 2    1\n2 0 2    2\n3 1 2    3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-time-series-1",
    "href": "slides/slides_2.html#inlabru-for-time-series-1",
    "title": "Lecture 1",
    "section": "inlabru for time series",
    "text": "inlabru for time series\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim \\text{Binomial}(n_t,p_t)\\\\\n\\text{logit}(p_t) = \\color{red}{\\boxed{\\eta_i}} & = \\color{red}{\\boxed{\\beta_0 + f(\\text{time}_t)}}\n\\end{aligned}\n\\]\n\n\nTokyo[1:3,]\n\n  y n time\n1 0 2    1\n2 0 2    2\n3 1 2    3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-time-series-2",
    "href": "slides/slides_2.html#inlabru-for-time-series-2",
    "title": "Lecture 1",
    "section": "inlabru for time series",
    "text": "inlabru for time series\n\n\nThe Model\n\\[\n\\begin{aligned}\n\\color{red}{\\boxed{y_t|\\eta_t}} & \\color{red}{\\boxed{\\sim \\text{Binomial}(n_t,p_t)}}\\\\\n\\text{logit}(p_t) = \\eta_i & = \\beta_0 + f(\\text{time}_t)\n\\end{aligned}\n\\]\n\n\nTokyo[1:3,]\n\n  y n time\n1 0 2    1\n2 0 2    2\n3 1 2    3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-time-series-3",
    "href": "slides/slides_2.html#inlabru-for-time-series-3",
    "title": "Lecture 1",
    "section": "inlabru for time series",
    "text": "inlabru for time series\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim \\text{Binomial}(n_t,p_t)\\\\\n\\text{logit}(p_t) = \\eta_i & = \\beta_0 + f(\\text{time}_t)\n\\end{aligned}\n\\]\n\n\nTokyo[1:3,]\n\n  y n time\n1 0 2    1\n2 0 2    2\n3 1 2    3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#example-disease-mapping",
    "href": "slides/slides_2.html#example-disease-mapping",
    "title": "Lecture 1",
    "section": "Example: disease mapping",
    "text": "Example: disease mapping\nWe observed larynx cancer mortality counts for males in 544 district of Germany from 1986 to 1990 and want to make a model.\n\n\n\n\\(y_i\\): The count at location \\(i\\).\n\\(E_i\\): An offset; expected number of cases in district \\(i\\).\n\\(c_i\\): A covariate (level of smoking consumption) at \\(i\\)\n\\(\\boldsymbol{s}_i\\): spatial location \\(i\\) ."
  },
  {
    "objectID": "slides/slides_2.html#bayesian-disease-mapping",
    "href": "slides/slides_2.html#bayesian-disease-mapping",
    "title": "Lecture 1",
    "section": "Bayesian disease mapping",
    "text": "Bayesian disease mapping\n\n\n\nStage 1: We assume the responses are Poisson distributed: \\[          \ny_i \\mid \\eta_i \\sim \\text{Poisson}(E_i\\exp(\\eta_i)))\n\\]\n\n\n\n\n\n\nStage 2: \\(\\eta_i\\) is a linear function of three components: an intercept, a covariate \\(c_i\\), a spatially structured effect \\(\\omega\\) likelihood by \\[\n\\eta_i = \\beta_0 + \\beta_1\\ c_i + \\omega_i\n\\]\n\n\n\n\n\n\nStage 3:\n\n\\(\\tau_{\\omega}\\): Precisions parameter for the random effects\n\n\n\n\n\nThe latent field is \\(\\boldsymbol{u} = (\\beta_0, \\beta_1, \\omega_1, \\omega_2,\\ldots, \\omega_n)\\), the hyperparameters are \\(\\boldsymbol{\\theta} = (\\tau_{\\omega})\\), and must be given a prior."
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-disease-mapping",
    "href": "slides/slides_2.html#inlabru-for-disease-mapping",
    "title": "Lecture 1",
    "section": "inlabru for disease mapping",
    "text": "inlabru for disease mapping\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_i|\\eta_t & \\sim \\text{Poisson}(E_i\\lambda_i)\\\\\n\\text{log}(\\lambda_i) = \\eta_i & = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_1\\ c_i}} + \\color{red}{\\boxed{\\omega_i}}\n\\end{aligned}\n\\]\n\n\ng = system.file(\"demodata/germany.graph\",\n                package=\"INLA\")\nGermany[1:3,]\n\n  region         E  Y  x region.struct\n1      1  7.965008  8 56             1\n2      2 22.836219 22 65             2\n3      3 22.094716 19 50             3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + beta1(x, model = \"linear\") +\n  space(region, model = \"besag\", graph = g)\n\n# define model predictor\neta = Y ~ beta0 + beta1 + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"poisson\",\n              E = E,\n              data = Germany)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-disease-mapping-1",
    "href": "slides/slides_2.html#inlabru-for-disease-mapping-1",
    "title": "Lecture 1",
    "section": "inlabru for disease mapping",
    "text": "inlabru for disease mapping\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_i|\\eta_t & \\sim \\text{Poisson}(E_i\\lambda_i)\\\\\n\\text{log}(\\lambda_i) = \\color{red}{\\boxed{\\eta_i}} & = \\color{red}{\\boxed{\\beta_0 + \\beta_1\\ c_i + \\omega_i}}\n\\end{aligned}\n\\]\n\n\ng = system.file(\"demodata/germany.graph\",\n                package=\"INLA\")\nGermany[1:3,]\n\n  region         E  Y  x region.struct\n1      1  7.965008  8 56             1\n2      2 22.836219 22 65             2\n3      3 22.094716 19 50             3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + beta1(x, model = \"linear\") +\n  space(region, model = \"bym2\", graph = g)\n\n# define model predictor\neta = Y ~ beta0 + beta1 + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"poisson\",\n              E = E,\n              data = Germany)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-disease-mapping-2",
    "href": "slides/slides_2.html#inlabru-for-disease-mapping-2",
    "title": "Lecture 1",
    "section": "inlabru for disease mapping",
    "text": "inlabru for disease mapping\n\n\nThe Model\n\\[\n\\begin{aligned}\n\\color{red}{\\boxed{y_i|\\eta_t}} & \\color{red}{\\boxed{\\sim \\text{Poisson}(E_i\\lambda_i)}}\\\\\n\\text{log}(\\lambda_i) = \\eta_i & = \\beta_0 + \\beta_1\\ c_i + \\omega_i\n\\end{aligned}\n\\]\n\n\ng = system.file(\"demodata/germany.graph\",\n                package=\"INLA\")\nGermany[1:3,]\n\n  region         E  Y  x region.struct\n1      1  7.965008  8 56             1\n2      2 22.836219 22 65             2\n3      3 22.094716 19 50             3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + beta1(x, model = \"linear\") +\n  space(region, model = \"bym2\", graph = g)\n\n# define model predictor\neta = Y ~ beta0 + beta1 + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"poisson\",\n              E = E,\n              data = Germany)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-disease-mapping-3",
    "href": "slides/slides_2.html#inlabru-for-disease-mapping-3",
    "title": "Lecture 1",
    "section": "inlabru for disease mapping",
    "text": "inlabru for disease mapping"
  },
  {
    "objectID": "slides/slides_2.html#bayesian-geostatistics",
    "href": "slides/slides_2.html#bayesian-geostatistics",
    "title": "Lecture 1",
    "section": "Bayesian Geostatistics",
    "text": "Bayesian Geostatistics\nEncounter probability of Pacific Cod (Gadus macrocephalus) from a trawl survey.\n\n\n\n\n\n\n\n\\(y(s)\\) Presence of absence in location \\(s\\)"
  },
  {
    "objectID": "slides/slides_2.html#bayesian-geostatistics-1",
    "href": "slides/slides_2.html#bayesian-geostatistics-1",
    "title": "Lecture 1",
    "section": "Bayesian Geostatistics",
    "text": "Bayesian Geostatistics\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim\\text{Binom}(1, p(s))\n\\]\nStage 2 Latent field model \\[\n\\eta(s) = \\text{logit}(p(s)) = \\beta_0 + f( x(s)) + \\omega(s)\n\\]\nStage 3 Hyperparameters"
  },
  {
    "objectID": "slides/slides_2.html#bayesian-geostatistics-2",
    "href": "slides/slides_2.html#bayesian-geostatistics-2",
    "title": "Lecture 1",
    "section": "Bayesian Geostatistics",
    "text": "Bayesian Geostatistics\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim\\text{Binom}(1, p(s))\n\\]\nStage 2 Latent field model \\[\n\\eta(s) = \\text{logit}(p(s)) = \\beta_0 + f( x(s)) + \\omega(s)\n\\]\n\nA global intercept \\(\\beta_0\\)\nA smooth effect of covariate \\(x(s)\\) (depth)\nA Gaussian field \\(\\omega(s)\\) (will discuss this later..)\n\nStage 3 Hyperparameters"
  },
  {
    "objectID": "slides/slides_2.html#bayesian-geostatistics-3",
    "href": "slides/slides_2.html#bayesian-geostatistics-3",
    "title": "Lecture 1",
    "section": "Bayesian Geostatistics",
    "text": "Bayesian Geostatistics\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim\\text{Binom}(1, p(s))\n\\]\nStage 2 Latent field model \\[\n\\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\beta_1 x(s) + \\omega(s)\n\\]\nStage 3 Hyperparameters\n\nPrecision for the smooth function \\(f(\\cdot)\\)\nRange and sd in the Gaussian field \\(\\sigma_{\\omega}, \\tau_{\\omega}\\)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-geostatistics",
    "href": "slides/slides_2.html#inlabru-for-geostatistics",
    "title": "Lecture 1",
    "section": "inlabru for geostatistics",
    "text": "inlabru for geostatistics\n\n\nThe Model\n$$\n\\[\\begin{aligned}\ny(s)|\\eta(s) & \\sim\\text{Binom}(1, p(s))\\\\\n\\eta(s) &  = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{ f(x(s))}} + \\color{red}{\\boxed{ \\omega(s)}}\\\\\n\n\\end{aligned}\\]\n$$\n\n\ndf %&gt;% select(depth, present) %&gt;% print(n = 3)\n\nSimple feature collection with 2143 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 343.0617 ymin: 5635.893 xmax: 579.3681 ymax: 5839.019\nProjected CRS: +proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\n# A tibble: 2,143 × 3\n  depth present            geometry\n  &lt;dbl&gt;   &lt;dbl&gt;        &lt;POINT [km]&gt;\n1   201       1 (446.4752 5793.426)\n2   212       1 (446.4594 5800.136)\n3   220       0 (448.5987 5801.687)\n# ℹ 2,140 more rows\n\n\n\nThe code\n\n# define model component\ncmp = ~ -1 + Intercept(1) +  depth_smooth(log(depth), model='rw2') + \n  space(geometry, model = spde_model)\n\n# define model predictor\neta = present ~ Intercept + depth_smooth + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              data = df,\n              family = \"binomial\")\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-geostatistics-1",
    "href": "slides/slides_2.html#inlabru-for-geostatistics-1",
    "title": "Lecture 1",
    "section": "inlabru for geostatistics",
    "text": "inlabru for geostatistics\n\n\nThe Model\n$$\n\\[\\begin{aligned}\ny(s)|\\eta(s) & \\sim\\text{Binom}(1, p(s))\\\\\n\\color{red}{\\boxed{\\eta(s)}} &  = \\color{red}{\\boxed{\\beta_0 +  f(x(s)) +  \\omega(s)}}\\\\\n\n\\end{aligned}\\]\n$$\n\n\ndf %&gt;% select(depth, present) %&gt;% print(n = 3)\n\nSimple feature collection with 2143 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 343.0617 ymin: 5635.893 xmax: 579.3681 ymax: 5839.019\nProjected CRS: +proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\n# A tibble: 2,143 × 3\n  depth present            geometry\n  &lt;dbl&gt;   &lt;dbl&gt;        &lt;POINT [km]&gt;\n1   201       1 (446.4752 5793.426)\n2   212       1 (446.4594 5800.136)\n3   220       0 (448.5987 5801.687)\n# ℹ 2,140 more rows\n\n\n\nThe code\n\n# define model component\ncmp = ~ -1 + Intercept(1) +  depth_smooth(log(depth), model='rw2') + \n  space(geometry, model = spde_model)\n\n# define model predictor\neta = present ~ Intercept + depth_smooth + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              data = df,\n              family = \"binomial\")\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-geostatistics-2",
    "href": "slides/slides_2.html#inlabru-for-geostatistics-2",
    "title": "Lecture 1",
    "section": "inlabru for geostatistics",
    "text": "inlabru for geostatistics\n\n\nThe Model\n$$\n\\[\\begin{aligned}\n\\color{red}{\\boxed{y(s)|\\eta(s)}} & \\sim \\color{red}{\\boxed{\\text{Binom}(1, p(s))}}\\\\\n\\eta(s) &  = \\beta_0 +  f(x(s)) +  \\omega(s)\\\\\n\n\\end{aligned}\\]\n$$\n\n\ndf %&gt;% select(depth, present) %&gt;% print(n = 3)\n\nSimple feature collection with 2143 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 343.0617 ymin: 5635.893 xmax: 579.3681 ymax: 5839.019\nProjected CRS: +proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\n# A tibble: 2,143 × 3\n  depth present            geometry\n  &lt;dbl&gt;   &lt;dbl&gt;        &lt;POINT [km]&gt;\n1   201       1 (446.4752 5793.426)\n2   212       1 (446.4594 5800.136)\n3   220       0 (448.5987 5801.687)\n# ℹ 2,140 more rows\n\n\n\nThe code\n\n# define model component\ncmp = ~ -1 + Intercept(1) +  depth_smooth(log(depth), model='rw2') + \n  space(geometry, model = spde_model)\n\n# define model predictor\neta = present ~ Intercept + depth_smooth + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              data = df,\n              family = \"binomial\")\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-geostatistics-3",
    "href": "slides/slides_2.html#inlabru-for-geostatistics-3",
    "title": "Lecture 1",
    "section": "inlabru for geostatistics",
    "text": "inlabru for geostatistics"
  },
  {
    "objectID": "slides/slides_2.html#take-home-message",
    "href": "slides/slides_2.html#take-home-message",
    "title": "Lecture 1",
    "section": "Take home message!",
    "text": "Take home message!\n\nMany of the model you have used (and some you have never used but will learn about) are just special cases of the large class of Latent Gaussian models\ninlabru provides an efficient and unified way to fit all these models!"
  },
  {
    "objectID": "slides/slides_5.html#outline",
    "href": "slides/slides_5.html#outline",
    "title": "Lecture 5",
    "section": "Outline",
    "text": "Outline\n\nWhy spatial modelling?\nWhy is spatial modelling computationally expensive?\nDifferent data types?\nModelling in discrete space – areal data\nModelling in continuous space – geo-referenced data\nModelling continuous space – spatial point process data"
  },
  {
    "objectID": "slides/slides_5.html#spatial-modelling",
    "href": "slides/slides_5.html#spatial-modelling",
    "title": "Lecture 5",
    "section": "Spatial modelling",
    "text": "Spatial modelling\nmany natural processes take place in space large amounts of data collected in space; increased resolution large, complex data sets\nHOWEVER:\n\nspatial statistical analysis is often not complex enough\ninaccessible to practitioners as literature written for statisticians\ndevelopment of methodology often not linked to applications (unrealistic assumptions)\ndifficult to apply (unless you are an expert statistician and programmer)\n\nwe will see that inlabru can help with this… why do we need spatial models in the first place?"
  },
  {
    "objectID": "slides/slides_5.html#an-example",
    "href": "slides/slides_5.html#an-example",
    "title": "Lecture 5",
    "section": "an example",
    "text": "an example\nglobal pm 2.5\nexposure to air pollution; particulate matter &lt; 2.5 microns in diameter (PM 2.5)\n\nlinked to poor health outcomes\nresponsible for three million deaths worldwide each year\nmaybe observations not independent?\nsparsely measured\nheterogeneous spatial coverage\n\nWe need to take account of spatial dependence… i.e. account for autocorrelation… and complexity"
  },
  {
    "objectID": "slides/slides_5.html#spatial-modelling-1",
    "href": "slides/slides_5.html#spatial-modelling-1",
    "title": "Lecture 5",
    "section": "Spatial modelling",
    "text": "Spatial modelling\naccounting for spatial dependence\n\nstandard statistical modelling usually assumes independent observations\ndistributional assumptions that are made are only true if the independence assumption hold\nspatio-temporal data, however, are often not independent, but are spatially auto correlated\nindependence assumptions are violated here\ntwo observations taken in close proximity are very similar\ndo not provide two (independent) pieces of information"
  },
  {
    "objectID": "slides/slides_5.html#spatial-modelling-2",
    "href": "slides/slides_5.html#spatial-modelling-2",
    "title": "Lecture 5",
    "section": "Spatial modelling",
    "text": "Spatial modelling\naccounting for spatial dependence - ignoring this = pretend we have as much independent information as we have observations - pretend we have more information than we actually have - spurious inference and ultimately wrong conclusions…\nspatio-temporal models have special model components that explicitly model the dependence structure we need to:\n\nsay what the dependence in our data looks like in general: choose a specific class of spatial models, and\nestimate its specific properties for a specific dataset"
  },
  {
    "objectID": "slides/slides_5.html#spatial-modelling-computations",
    "href": "slides/slides_5.html#spatial-modelling-computations",
    "title": "Lecture 5",
    "section": "Spatial modelling – computations",
    "text": "Spatial modelling – computations\n\ncomputationally expensive\nin the past: often MCMC, takes forever"
  },
  {
    "objectID": "slides/slides_5.html#types-of-spatial-data",
    "href": "slides/slides_5.html#types-of-spatial-data",
    "title": "Lecture 5",
    "section": "Types of spatial data",
    "text": "Types of spatial data\nWe can distinguish three types of spatial data\nDiscrete space: - data on a spatial grid (areal data)\nContinuous space: - geostatistical (geo-referenced) data - spatial point data"
  },
  {
    "objectID": "slides/slides_5.html#discrete-space-areal-data",
    "href": "slides/slides_5.html#discrete-space-areal-data",
    "title": "Lecture 5",
    "section": "Discrete space: areal data",
    "text": "Discrete space: areal data\n\ndata on a (regular or irregular) spatial grid\nexamples: number of individuals in a region, average rainfall in a province\n(originally geostatistical or point data; gridded for practical reasons)\n\nObserved response(s): Measurement over each grid cell (e.g. number of individuals in cell; rainfall in province)"
  },
  {
    "objectID": "slides/slides_5.html#continuous-space-geostatistical-data",
    "href": "slides/slides_5.html#continuous-space-geostatistical-data",
    "title": "Lecture 5",
    "section": "Continuous space: geostatistical data",
    "text": "Continuous space: geostatistical data\n\nphenomenon that is continuous in space\nexamples: nutrient levels in soil, salinity in the sea measurements at a given set of locations that are determined by surveyor\n\nObserved response(s): measurement(s) taken at given locations"
  },
  {
    "objectID": "slides/slides_5.html#continuous-space-spatial-point-patterns",
    "href": "slides/slides_5.html#continuous-space-spatial-point-patterns",
    "title": "Lecture 5",
    "section": "Continuous space: spatial point patterns",
    "text": "Continuous space: spatial point patterns\n\npatterns formed by locations of objects (individuals) in space (typically 2D)\nexamples: locations of trees in a forest, groups of animals, earthquakes\n\nObserved response(s): x,y coordinates of points (individuals/groups) sometimes also properties of individuals/groups (“marks”)"
  },
  {
    "objectID": "slides/slides_5.html#point-patterns-vs.-geostatistical-data",
    "href": "slides/slides_5.html#point-patterns-vs.-geostatistical-data",
    "title": "Lecture 5",
    "section": "point patterns vs. geostatistical data",
    "text": "point patterns vs. geostatistical data\npoint patterns: - data format : x,y coordinates optional : properties of objects represented by the points (“marks”)\ngeostatistical data: - data format : x,y coordinates not optional : measurement taken in these locations\nThese seem rather similar…"
  },
  {
    "objectID": "slides/slides_5.html#point-patterns-vs.-geostatistical-data-1",
    "href": "slides/slides_5.html#point-patterns-vs.-geostatistical-data-1",
    "title": "Lecture 5",
    "section": "point patterns vs. geostatistical data",
    "text": "point patterns vs. geostatistical data\npoint patterns : - data format : x,y coordinates\noptional : properties of objects represented by the points (“marks”)\n\naim : modelling the locations of objects in continuous space\nlocations are being modelled and are considered random\nmarks only take values in locations where there is a point\n\ngeostatistical data : - data format : x,y coordinates\nnot optional : measurement taken in these locations\n\naim : modelling continuous process observed in finite number of locations\nlocations have typically been deliberately chosen and are fixed\ncontinuous spatial field takes on values in whole subset of \\(\\mathbf R^ 2\\)"
  },
  {
    "objectID": "slides/slides_5.html#take-home-message",
    "href": "slides/slides_5.html#take-home-message",
    "title": "Lecture 5",
    "section": "Take home message!",
    "text": "Take home message!\n\nall spatial models we discuss here are also special cases of the large class of Latent Gaussian models\nto account for spatial dependence in the data, different types of spatial terms have to be included in the model for different spatial data structures, but they are all approximated by an SPDE model\ninlabru provides an efficient and unified way to fit all these models!"
  },
  {
    "objectID": "slides/slides_12.html#motivation",
    "href": "slides/slides_12.html#motivation",
    "title": "Lecture ??",
    "section": "Motivation",
    "text": "Motivation\n\n\nCount data are often modeled with Poisson or Negative Binomial models.\n\nThese assume zeros occur naturally from the count process.\n\nExample: for Poisson is \\(P(Y=0) = \\exp(\\lambda)\\)\n\nBut in real data, we often see too many zeros → zero inflation.\nExamples:\n\nMany people have 0 doctor visits per year.\n\nMost customers have 0 claims.\n\nMany species are absent from samples."
  },
  {
    "objectID": "slides/slides_12.html#two-main-solutions",
    "href": "slides/slides_12.html#two-main-solutions",
    "title": "Lecture ??",
    "section": "Two main solutions",
    "text": "Two main solutions\nBoth handle excess zeros but use different assumptions about how zeros are generated.\n\nZero-Inflated Models\nHurdle Models"
  },
  {
    "objectID": "slides/slides_12.html#two-main-solutions-1",
    "href": "slides/slides_12.html#two-main-solutions-1",
    "title": "Lecture ??",
    "section": "Two main solutions",
    "text": "Two main solutions\nBoth handle excess zeros but use different assumptions about how zeros are generated.\n\nZero-Inflated Models\n\nTwo sources of zero: inflation process or count process\n\nHurdle Models"
  },
  {
    "objectID": "slides/slides_12.html#two-main-solutions-2",
    "href": "slides/slides_12.html#two-main-solutions-2",
    "title": "Lecture ??",
    "section": "Two main solutions",
    "text": "Two main solutions\nBoth handle excess zeros but use different assumptions about how zeros are generated:\n\nZero-Inflated Models\n\nTwo sources of zero: inflation process or count process\n\nHurdle Models\n\nOnle one source of zeros"
  },
  {
    "objectID": "slides/slides_12.html#zero-inflated-models",
    "href": "slides/slides_12.html#zero-inflated-models",
    "title": "Lecture ??",
    "section": "Zero-Inflated Models",
    "text": "Zero-Inflated Models\nAssume two processes:\n\nA binary process → decides if the observation is a certain zero.\nA count process → generates counts, including zeros.\n\n\\[\n\\begin{aligned}\n\\text{P}(Y=0) & = \\pi + (1-\\pi)f(0) & \\\\\n\\text{P}(Y=y) & = (1-\\pi)f(y)& y=1,2,\\dots\n\\end{aligned}\n\\]\n\nNote In zero-inflated models \\(f(y)\\) is typically discrete (ex Poisson, Binomial, …)"
  },
  {
    "objectID": "slides/slides_12.html#hurdle-models",
    "href": "slides/slides_12.html#hurdle-models",
    "title": "Lecture ??",
    "section": "Hurdle Models",
    "text": "Hurdle Models\nAlso have two parts, but different logic:\n\nA binary model determines if the observation crosses the hurdle (i.e. zero vs. positive).\nA truncated count model models only positive counts.\n\nZeros come only from the first process.\n\\[\n\\begin{aligned}\n\\text{P}(Y=0) & = \\pi & \\\\\n\\text{P}(Y=y) & = \\frac{f(y)}{1-f(0)}& y=1,2,\\dots\n\\end{aligned}\n\\]\n\nNote In hurdle models \\(f_Y(y)\\) can also be continuous (f.ex Gamma). In these cases: \\[\n\\begin{aligned}\n\\text{P}(Y=0) & = \\pi & \\\\\nf_Y(y) & = f(y)& y&gt;0\n\\end{aligned}\n\\] where \\(f(y)\\) is scaled so that it integrates to \\(1-\\pi\\)"
  },
  {
    "objectID": "slides/slides_12.html#comparison-zero-inflated-vs.-hurdle-models",
    "href": "slides/slides_12.html#comparison-zero-inflated-vs.-hurdle-models",
    "title": "Lecture ??",
    "section": "Comparison: Zero-Inflated vs. Hurdle Models",
    "text": "Comparison: Zero-Inflated vs. Hurdle Models\n\n\n\n\n\n\n\n\nFeature\nZero-Inflated\nHurdle\n\n\n\n\nZeros come from\nTwo sources (inflation + count)\nOne source (hurdle only)\n\n\nPositive part\nIncludes zeros\nTruncated at zero\n\n\nCan use continuous distributions?\n❌ Typically count only\n✅ Yes (Gamma, Lognormal)"
  },
  {
    "objectID": "slides/slides_12.html#zero-inflated-models-in-inlabru-type-1",
    "href": "slides/slides_12.html#zero-inflated-models-in-inlabru-type-1",
    "title": "Lecture ??",
    "section": "Zero Inflated models in inlabru (Type 1)",
    "text": "Zero Inflated models in inlabru (Type 1)\n\nFour zero-inflated models are implemented\n\nPoisson (zeroinflatedpoisson1)\nBinomial (zeroinflatedbinomial1)\nNegative Binomial (zeroinflatednbinomial1)\nBetaBinomial (zeroinflatedbinomial1)\n\n\nTo get details about the distributions you can type\n\nINLA::inla.doc(\"zero inflated\")\n\n\nThe probability of the inflation process \\(\\pi\\) is a hyperparameter therefore cannot be modeled with covariates"
  },
  {
    "objectID": "slides/slides_12.html#example-number-of-fishes-caught-by-fishermen-at-a-state-park.",
    "href": "slides/slides_12.html#example-number-of-fishes-caught-by-fishermen-at-a-state-park.",
    "title": "Lecture ??",
    "section": "Example: number of fishes caught by fishermen at a state park.",
    "text": "Example: number of fishes caught by fishermen at a state park.\n\n\n  nofish persons child count\n1      1       1     0     0\n2      0       1     0     0\n3      0       1     0     0"
  },
  {
    "objectID": "slides/slides_12.html#example-two-models",
    "href": "slides/slides_12.html#example-two-models",
    "title": "Lecture ??",
    "section": "Example: two models",
    "text": "Example: two models\n\\[\n\\begin{eqnarray}\n\\text{Model 1:   }\\ & Y\\sim \\text{Poisson}(\\lambda)\\\\\n\\text{Model 2:   }\\ & Y\\sim \\text{NegBinomial}(n,p)\\\\\n\\text{Linear predictor:   }\\ & \\eta = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "slides/slides_12.html#example-two-models-1",
    "href": "slides/slides_12.html#example-two-models-1",
    "title": "Lecture ??",
    "section": "Example: two models",
    "text": "Example: two models\n\\[\n\\begin{eqnarray}\n\\text{Model 1:   }\\ & Y\\sim \\text{Poisson}(\\lambda)\\\\\n\\text{Model 2:   }\\ & Y\\sim \\text{NegBinomial}(n,p)\\\\\n\\text{Linear predictor:   }\\ & \\eta = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\n\\end{eqnarray}\n\\]\nNegative Binomial distribution\n\\[\n\\text{Prob}(Y=y) = \\frac{\\Gamma(y+n)}{\\Gamma(n)\\Gamma(y+1)}p^n(1-p)^y\n\\] with \\[\nE(Y) = \\mu = n\\frac{1-p}{p} \\qquad \\text{Var}(Y) = \\mu(1+\\frac{\\mu}{n})\n\\] and linear predictor \\(\\eta = log(\\mu)\\)"
  },
  {
    "objectID": "slides/slides_12.html#example-implementation",
    "href": "slides/slides_12.html#example-implementation",
    "title": "Lecture ??",
    "section": "Example: Implementation",
    "text": "Example: Implementation\n\n\nThe Model \\[\n\\begin{aligned}\n\\text{Model 1 }:  y_i|\\eta_i & \\sim \\pi1_{(y=0)} + (1-\\pi)\\text{Pois}(\\lambda(\\eta_i))\\\\\n\\text{Model 2 }:  y_i|\\eta_i & \\sim \\pi1_{(y=0)} + (1-\\pi)\\text{NegBin}(n(\\eta_i),p(\\eta_i))\\\\\n\\text{Linear Predictor }: \\eta_i & = \\color{red}{\\boxed{\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}}}\n\\end{aligned}\n\\]\n\n\nzinb[1:3,]\n\n  nofish persons child count\n1      1       1     0     0\n2      0       1     0     0\n3      0       1     0     0\n\n\n\nThe code\n\n# define model components\ncmp = ~ Intercept(1) + fishing(nofish, model = \"linear\") + \n  persons(persons, model = \"linear\") + \n  child(child, model = \"linear\")\n\n# define model predictor\nformula = count ~ .\n\n# build the observation model\n# Poisson model\nlik_pois = bru_obs(formula,\n                   family = \"zeroinflatedpoisson1\",\n                   data = zinb) \n\n# Negative binomial model\nlik_nbin = bru_obs(formula,\n                   family = \"zeroinflatednbinomial1\",\n                   data = zinb) \n\n# fit the model\nfit_pois = bru(cmp, lik_pois)\nfit_nbin = bru(cmp, lik_nbin)"
  },
  {
    "objectID": "slides/slides_12.html#example-implementation-1",
    "href": "slides/slides_12.html#example-implementation-1",
    "title": "Lecture ??",
    "section": "Example: Implementation",
    "text": "Example: Implementation\n\n\nThe Model \\[\n\\begin{aligned}\n\\text{Model 1 }:  \\color{red}{\\boxed{y_i|\\eta_i}} & \\color{red}{\\boxed{\\sim \\pi1_{(y=0)} + (1-\\pi)\\text{Pois}(\\lambda(\\eta_i))}}\\\\\n\\text{Model 2 }:  y_i|\\eta_i & \\sim \\pi1_{(y=0)} + (1-\\pi)\\text{NegBin}(n(\\eta_i),p(\\eta_i))\\\\\n\\text{Linear Predictor }: \\eta_i & = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}\n\\end{aligned}\n\\]\n\n\nzinb[1:3,]\n\n  nofish persons child count\n1      1       1     0     0\n2      0       1     0     0\n3      0       1     0     0\n\n\n\nThe code\n\n# define model components\ncmp = ~ Intercept(1) + fishing(nofish, model = \"linear\") + \n  persons(persons, model = \"linear\") + \n  child(child, model = \"linear\")\n\n# define model predictor\nformula = count ~ .\n\n# build the observation model\n# Poisson model\nlik_pois = bru_obs(formula,\n                   family = \"zeroinflatedpoisson1\",\n                   data = zinb) \n\n# Negative binomial model\nlik_nbin = bru_obs(formula,\n                   family = \"zeroinflatednbinomial1\",\n                   data = zinb) \n\n# fit the model\nfit_pois = bru(cmp, lik_pois)\nfit_nbin = bru(cmp, lik_nbin)"
  },
  {
    "objectID": "slides/slides_12.html#example-implementation-2",
    "href": "slides/slides_12.html#example-implementation-2",
    "title": "Lecture ??",
    "section": "Example: Implementation",
    "text": "Example: Implementation\n\n\nThe Model \\[\n\\begin{aligned}\n\\text{Model 1 }:  y_i|\\eta_i & \\sim \\pi1_{(y=0)} + (1-\\pi)\\text{Pois}(\\lambda(\\eta_i))\\\\\n\\text{Model 2 }:  \\color{red}{\\boxed{y_i|\\eta_i}} & \\color{red}{\\boxed{\\sim \\pi1_{(y=0)} + (1-\\pi)\\text{NegBin}(n(\\eta_i),p(\\eta_i))}}\\\\\n\\text{Linear Predictor }: \\eta_i & = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}\n\\end{aligned}\n\\]\n\n\nzinb[1:3,]\n\n  nofish persons child count\n1      1       1     0     0\n2      0       1     0     0\n3      0       1     0     0\n\n\n\nThe code\n\nbru_options_set(control.compute = list(dic = T, waic = T, mlik = T))\n# define model components\ncmp = ~ Intercept(1) + fishing(nofish, model = \"linear\") + \n  persons(persons, model = \"linear\") + \n  child(child, model = \"linear\")\n\n# define model predictor\nformula = count ~ .\n\n# build the observation model\n# Poisson model\nlik_pois = bru_obs(formula,\n                   family = \"zeroinflatedpoisson1\",\n                   data = zinb) \n\n# Negative binomial model\nlik_nbin = bru_obs(formula,\n                   family = \"zeroinflatednbinomial1\",\n                   data = zinb) \n\n# fit the model\nfit_pois = bru(cmp, lik_pois)\nfit_nbin = bru(cmp, lik_nbin)"
  },
  {
    "objectID": "slides/slides_12.html#results---fixed-effects",
    "href": "slides/slides_12.html#results---fixed-effects",
    "title": "Lecture ??",
    "section": "Results - Fixed effects",
    "text": "Results - Fixed effects\n\n\n[1] \"Poisson Model\"\n\n\n           mean   sd 0.025quant 0.975quant\nIntercept  0.22 0.15      -0.08       0.51\nfishing   -0.90 0.12      -1.13      -0.67\npersons    0.65 0.05       0.56       0.74\nchild     -0.95 0.10      -1.15      -0.76\n\n\n[1] \"Negative Binomial Model\"\n\n\n           mean   sd 0.025quant 0.975quant\nIntercept -0.80 0.31      -1.42      -0.18\nfishing   -0.59 0.25      -1.08      -0.10\npersons    0.94 0.12       0.71       1.17\nchild     -1.65 0.19      -2.03      -1.27"
  },
  {
    "objectID": "slides/slides_12.html#results---hyperparameters-and-scores",
    "href": "slides/slides_12.html#results---hyperparameters-and-scores",
    "title": "Lecture ??",
    "section": "Results - Hyperparameters and scores",
    "text": "Results - Hyperparameters and scores\nHyperparameters\n\n\n[1] \"Poisson Model\"\n\n\n                                                       mean   sd\nzero-probability parameter for zero-inflated poisson_1 0.48 0.04\n\n\n[1] \"Negative Binomial Model\"\n\n\n                                                         mean   sd\nsize for nbinomial_1 zero-inflated observations          0.56 0.10\nzero-probability parameter for zero-inflated nbinomial_1 0.06 0.06\n\n\nScores\n\n\n  Score Poisson    Nbin\n1   DIC 1111.36  795.44\n2  Mlik -579.97 -424.72\n3  WAIC 1207.12  798.86\n\n\n\nThere seems to be more overdispersion than excess zero..the negative binomial model better fits the data estimateing a small excess of zero."
  },
  {
    "objectID": "slides/slides_12.html#comparing-estimated-distribution-for-a-specific-data-point",
    "href": "slides/slides_12.html#comparing-estimated-distribution-for-a-specific-data-point",
    "title": "Lecture ??",
    "section": "Comparing estimated distribution for a specific data point",
    "text": "Comparing estimated distribution for a specific data point\n\\[\n\\begin{eqnarray}\n\\text{Model 1:   }\\ & P(Y=0) &= \\pi + (1-\\pi)\\ \\text{Poisson}(\\lambda)\\\\\n\\text{Model 2:   }\\ & P(Y=0)&= \\pi + (1-\\pi)\\ \\text{NegBinom}(n,p)\\\\\n\\end{eqnarray}\n\\] We check the distribution for one person, non-fisher and no children"
  },
  {
    "objectID": "slides/slides_12.html#comparing-estimated-distribution---implementation",
    "href": "slides/slides_12.html#comparing-estimated-distribution---implementation",
    "title": "Lecture ??",
    "section": "Comparing estimated distribution - Implementation",
    "text": "Comparing estimated distribution - Implementation\n\ndf_pred = data.frame(nofish = 0, persons = 1, child = 0)\n\nprob_pois = predict(fit_pois, df_pred,\n        formula = ~ {\n          pi0 &lt;- zero_probability_parameter_for_zero_inflated_poisson_1\n          lambda = exp(Intercept +  fishing + persons + child)\n          yy = 0:20\n          list(\n            prob0 =  pi0 * c(1,rep(0,20)) + (1-pi0) * dpois(yy, lambda = lambda))\n          } )\n\nprob_nbin = predict(fit_nbin, df_pred,\n        formula = ~ {\n          pi0 &lt;- zero_probability_parameter_for_zero_inflated_nbinomial_1\n          size = exp(size_for_nbinomial_1_zero_inflated_observations)\n          mu = exp(Intercept +  fishing + persons + child)\n          p = size/(size + mu)\n          yy = 0:20\n          list(prob0 = pi0 * c(1,rep(0,20)) + (1-pi0) * dnbinom(yy,size = size, mu = mu))\n        })"
  },
  {
    "objectID": "slides/slides_12.html#comparing-estimated-distribution---implementation-1",
    "href": "slides/slides_12.html#comparing-estimated-distribution---implementation-1",
    "title": "Lecture ??",
    "section": "Comparing estimated distribution - Implementation",
    "text": "Comparing estimated distribution - Implementation\nNote: To get the names to input in predict use the bru_standardise_names() function:\n\ninlabru::bru_standardise_names(fit_pois)"
  },
  {
    "objectID": "slides/slides_12.html#hurdle-models-in-inlabru",
    "href": "slides/slides_12.html#hurdle-models-in-inlabru",
    "title": "Lecture ??",
    "section": "Hurdle Models in inlabru",
    "text": "Hurdle Models in inlabru\nThere are two ways to implement hurdle models in inlabru\n\nStrategy 1 Use a modified version of the same distributions defined for the zero-inflated models\nStrategy 2 Use a “two-likelihood” trick"
  },
  {
    "objectID": "slides/slides_12.html#strategy-1---implemented-models",
    "href": "slides/slides_12.html#strategy-1---implemented-models",
    "title": "Lecture ??",
    "section": "Strategy 1 - Implemented models",
    "text": "Strategy 1 - Implemented models\nAvailable models (Type 0)\n\nPoisson (zeroinflatedpoisson0)\nBinomial (zeroinflatedbinomial0)\nNegative Binomial (zeroinflatednbinomial0)\nBetaBinomial (zeroinflatedbinomial0)\n\n\nAdvantages\n\nIt works exactly as the zero-inflated models\nNo need for extra coding\n\n\n\nDisadvantages\n\nCan only be used for the models that are already implemented\nThe probability of zero (\\(\\pi\\)) is fixed and cannot have covariates"
  },
  {
    "objectID": "slides/slides_12.html#example-strategy-1-implementation",
    "href": "slides/slides_12.html#example-strategy-1-implementation",
    "title": "Lecture ??",
    "section": "Example Strategy 1: Implementation",
    "text": "Example Strategy 1: Implementation\n\n\nThe Model \\[\n\\begin{aligned}\n\\text{Model 1 }:  y_i|\\eta_i & \\sim \\pi1_{(y=0)} + (1-\\pi)\\text{TruncPois}(\\lambda(\\eta_i))\\\\\n\\text{Model 2 }:  y_i|\\eta_i & \\sim \\pi1_{(y=0)} + (1-\\pi)\\text{TruncNegBin}(n(\\eta_i),p(\\eta_i))\\\\\n\\text{Linear Predictor }: \\eta_i & = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}\n\\end{aligned}\n\\]\n\n\nzinb[1:3,]\n\n  nofish persons child count\n1      1       1     0     0\n2      0       1     0     0\n3      0       1     0     0\n\n\n\nThe code\n\n# define model components\ncmp = ~ Intercept(1) + fishing(nofish, model = \"linear\") + \n  persons(persons, model = \"linear\") + \n  child(child, model = \"linear\")\n\n# define model predictor\nformula = count ~ .\n\n# build the observation model\n# Poisson model\nlik_pois = bru_obs(formula,\n                   family = \"zeroinflatedpoisson0\",\n                   data = zinb) \n\n# Negative binomial model\nlik_nbin = bru_obs(formula,\n                   family = \"zeroinflatednbinomial0\",\n                   data = zinb) \n\n# fit the model\nfit_pois0 = bru(cmp, lik_pois)\nfit_nbin0 = bru(cmp, lik_nbin)"
  },
  {
    "objectID": "slides/slides_12.html#example-strategy-1-results",
    "href": "slides/slides_12.html#example-strategy-1-results",
    "title": "Lecture ??",
    "section": "Example Strategy 1: Results",
    "text": "Example Strategy 1: Results"
  },
  {
    "objectID": "slides/slides_12.html#strategy-2---two-likelihood-trick",
    "href": "slides/slides_12.html#strategy-2---two-likelihood-trick",
    "title": "Lecture ??",
    "section": "Strategy 2 - Two-likelihood trick",
    "text": "Strategy 2 - Two-likelihood trick\nAdvantages\n\nMore flexible model\nCan model also the probability of zero \\(\\pi\\)\nCan also be used for continuous distributions with mass at 0\n\n\nDisadvantage\n\nSome more coding"
  },
  {
    "objectID": "slides/slides_12.html#hurdle-models-com",
    "href": "slides/slides_12.html#hurdle-models-com",
    "title": "Lecture ??",
    "section": "Hurdle models com",
    "text": "Hurdle models com\nThe Hurdle Model handles this by splitting the data-generating process into two parts:\n\nZero-hurdle component: This part is a binary model that estimates the probability of a zero count versus a non-zero count.\nPositive component: This part models the distribution of the positive (non-zero) values.\n\n\nThis idea can be used in inlabru by using two likelihood\n\nA binomial likelihood to model the 0/1 process\nA continuous positive density or a truncated count distribution for the positive part"
  },
  {
    "objectID": "slides/slides_12.html#example---gamma-hurdle-model",
    "href": "slides/slides_12.html#example---gamma-hurdle-model",
    "title": "Lecture ??",
    "section": "Example - Gamma-Hurdle model",
    "text": "Example - Gamma-Hurdle model\nDaily precipitation in Parana state\n\n\n\\[\nz_{st} = \\left\\{\n\\begin{eqnarray}\n0 & \\text{ if } &\\text{ no rain on day } t \\text{ at station } s\\\\\n1 & \\text{ if } &\\text{  rain on day } t \\text{ at station } s\n\\end{eqnarray}\n\\right.\n\\]\n\\[\ny_{st} = \\left\\{\n\\begin{eqnarray}\n\\text{NA} \\text{ if } && \\text{ no rain on day } t  \\text{ at station } s\\\\\n\\text{rain amout} \\text{ if } && \\text{  rain on day } t  \\text{ at station } s\n\\end{eqnarray}\n\\right.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nz_{st}\\sim \\text{Binom}(p_{st}, n_{st} = 1); \\qquad y_{st}|z_{st}=1\\sim\\text{Gamma}(a,b) \\ \\text{with}\\  E(y_{st}) = \\mu_{st} =  a/b\n\\] Where \\[\n\\begin{eqnarray}\n\\eta_t^1 & = \\text{logit}(p_t)  & = \\beta_0^B+u^B(s) \\\\\n\\eta_t^2 & = \\log(\\mu_t)& = \\beta_0^G+u^G(s)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "slides/slides_12.html#example---gamma-hurdle-model-1",
    "href": "slides/slides_12.html#example---gamma-hurdle-model-1",
    "title": "Lecture ??",
    "section": "Example - Gamma-Hurdle model",
    "text": "Example - Gamma-Hurdle model\nImplementation\n\nCreate response for binary and Gamma process\n\n\ndf = df %&gt;%\n  mutate(z = ifelse(value==0,0,1),\n         y = ifelse(value==0,NA, value),)\n\n\nDefine components, observation models and run\n\n\ncmp = ~ -1 + Intercept_z(1) + Intercept_y(1) +\n  space_z(geometry, model = spde) +\n  space_y(geometry, model = spde) +\n  local_z(station, model = \"iid\")\n\nlik_z = bru_obs(formula = z ~ Intercept_z + space_z + local_z,\n                data = df %&gt;% filter(time==1),\n                family = \"binomial\")\n\nlik_y = bru_obs(formula = y ~ Intercept_y + space_y,\n                data =  df %&gt;% filter(time==1),\n                family = \"gamma\")\n\nfit = bru(cmp, lik_z, lik_y)"
  },
  {
    "objectID": "slides/slides_12.html#example---gamma-hurdle-model-2",
    "href": "slides/slides_12.html#example---gamma-hurdle-model-2",
    "title": "Lecture ??",
    "section": "Example - Gamma-Hurdle model",
    "text": "Example - Gamma-Hurdle model\nResults"
  },
  {
    "objectID": "slides/slides_12.html#example---gamma-hurdle-model-3",
    "href": "slides/slides_12.html#example---gamma-hurdle-model-3",
    "title": "Lecture ??",
    "section": "Example - Gamma-Hurdle model",
    "text": "Example - Gamma-Hurdle model\nWe can:\n\nuse other components in both linear predictors \\(\\eta^1\\) and \\(\\eta^2\\)\nhave shared components between the two linear predictor\nuse other positive-continuous distribution instead of the Gamma (for example log-Normal, Weibull, etc.)"
  },
  {
    "objectID": "slides/slides_12.html#example---poisson--hurdle-model",
    "href": "slides/slides_12.html#example---poisson--hurdle-model",
    "title": "Lecture ??",
    "section": "Example - Poisson- Hurdle model",
    "text": "Example - Poisson- Hurdle model\nLet’s look again at the fishes examples. We now want to include covariates also in the model for the zero probability:\n\\[\n\\begin{eqnarray}\nP(Y = 0) = &\\pi\\\\\nP(Y=y) = & (1-\\pi) \\frac{f_Y(y)}{f_Y(0)},& \\qquad y =1,2,\\dots\n\\end{eqnarray}\n\\] We use the same “trick” as before and we now have \\[\n\\begin{eqnarray}\nz\\sim\\text{Binomial}(\\pi,n); &\\qquad \\eta_1 = \\text{logit}(\\pi) = \\beta^1X\\\\\nz\\sim\\text{Truncated Poisson}(\\lambda); &\\qquad \\eta_2 =\\log(\\lambda) =  \\beta^2X\\\\\n\\end{eqnarray}\n\\] In inlabru the truncated Poisson distribution is called nzpoisson"
  },
  {
    "objectID": "slides/slides_12.html#example---poisson--hurdle-model-1",
    "href": "slides/slides_12.html#example---poisson--hurdle-model-1",
    "title": "Lecture ??",
    "section": "Example - Poisson- Hurdle model",
    "text": "Example - Poisson- Hurdle model\nImplementation\n\nDefine response for binary and truncated poisson process\n\n\nzinb  = zinb %&gt;%\n  mutate(z = ifelse(count==0,1,0),\n         y = ifelse(count==0,NA, count))\n\n\nDefine components, observation models and run\n\n\ncmp = ~ -1 + Intercept_z(1) + Intercept_y(1) +\n  nofish_z(nofish, model = \"linear\") + persons_z(persons, model = \"linear\") +\n    nofish_y(nofish, model = \"linear\") + persons_y(persons, model = \"linear\")\n\nlik_z = bru_obs(formula = z~ Intercept_z  + nofish_z + persons_z,\n                data = zinb,\n                family = \"binomial\")\n\nlik_y = bru_obs(formula = y~ Intercept_y  + nofish_y + persons_y,\n                data = zinb,\n                family = \"nzpoisson\")\n\nfit = bru(cmp, lik_z, lik_y)"
  },
  {
    "objectID": "slides/slides_12.html#example---poisson--hurdle-model-2",
    "href": "slides/slides_12.html#example---poisson--hurdle-model-2",
    "title": "Lecture ??",
    "section": "Example - Poisson- Hurdle model",
    "text": "Example - Poisson- Hurdle model\nResults\n\nround(fit$summary.fixed,2)\n\n             mean   sd 0.025quant 0.5quant 0.975quant  mode kld\nIntercept_z  0.70 0.33       0.04     0.70       1.35  0.70   0\nnofish_z     0.23 0.28      -0.33     0.23       0.78  0.23   0\npersons_z   -0.18 0.12      -0.41    -0.18       0.04 -0.18   0\nIntercept_y  0.35 0.15       0.05     0.35       0.65  0.35   0\nnofish_y    -0.83 0.12      -1.07    -0.83      -0.59 -0.83   0\npersons_y    0.53 0.04       0.44     0.53       0.61  0.53   0"
  },
  {
    "objectID": "slides/slides_12.html#example-binomial-hurdle-model",
    "href": "slides/slides_12.html#example-binomial-hurdle-model",
    "title": "Lecture ??",
    "section": "Example: Binomial-Hurdle model",
    "text": "Example: Binomial-Hurdle model\n\nIn inlabru there is no implementation for the truncated binomial (or negative binomial) distribution\nWe can “trick” inlabru by using one of the implemented Type 0 models \\[\ny_i|\\eta_i  \\sim \\pi1_{(y=0)} + (1-\\pi)\\frac{f(y)}{f(0)}\n\\] with a fixed and small \\(\\pi\\) thus creating a truncated distribution!"
  },
  {
    "objectID": "slides/slides_12.html#example-binomial-hurdle-model-1",
    "href": "slides/slides_12.html#example-binomial-hurdle-model-1",
    "title": "Lecture ??",
    "section": "Example: Binomial-Hurdle model",
    "text": "Example: Binomial-Hurdle model\nImplementation\n\nDefine response and the components\n\n\n# Define response for binary and truncated neg. binomial\nzinb  = zinb %&gt;% mutate(z = ifelse(count==0,1,0),\n                        y = ifelse(count==0,NA, count))\n# define model components\ncmp = ~ -1 + Intercept_z(1) + Intercept_y(1) + \n  fishing_z(nofish, model = \"linear\") + fishing_y(nofish, model = \"linear\") + \n  persons_z(persons, model = \"linear\") + persons_y(persons, model = \"linear\") + \n  child_z(child, model = \"linear\") + child_y(child, model = \"linear\")\n\n\n\nBuild the likelihood for the binomial part\n\n\n# build the observation model\n# Poisson model\nlik_binom = bru_obs(z ~ Intercept_z + fishing_z + persons_z + child_z ,\n                   family = \"binomial\",\n                   data = zinb) \n\n\n\n\nDefine the type 0 Negbinomial with \\(\\pi\\) fixed to a small value\n\n\n# Negative binomial model\nlik_trunc_nbin = bru_obs(y ~ Intercept_y + fishing_y + persons_y + child_y ,\n                   family = \"zeroinflatednbinomial0\",\n                   data = zinb,\n                   control.family = list(hyper = list(theta = list(initial = -20, \n                                                                   fixed = TRUE))))\n\n\n\n\nRun the model\n\n\nfit_nbin = bru(cmp, lik_binom, lik_trunc_nbin)"
  },
  {
    "objectID": "slides/slides_12.html#summary---treating-many-zero-with-inlabru",
    "href": "slides/slides_12.html#summary---treating-many-zero-with-inlabru",
    "title": "Lecture ??",
    "section": "Summary - Treating many zero with inlabru",
    "text": "Summary - Treating many zero with inlabru\nThere are mainly two types of models to treat excess of zeros\n\nZero inflated models\nHurdle models"
  },
  {
    "objectID": "slides/slides_12.html#summary---treating-many-zero-with-inlabru-1",
    "href": "slides/slides_12.html#summary---treating-many-zero-with-inlabru-1",
    "title": "Lecture ??",
    "section": "Summary - Treating many zero with inlabru",
    "text": "Summary - Treating many zero with inlabru\nThere are mainly two types of models to treat excess of zeros\n\nZero inflated models\n\nZeros come from two sources\nCan only be used for counts (Poisson, Binomial,…)\nFour likelihood are implemented in inlabru (Type1)\nThe probability of zero-inflation is constant\n\nHurdle models"
  },
  {
    "objectID": "slides/slides_12.html#summary---treating-many-zero-with-inlabru-2",
    "href": "slides/slides_12.html#summary---treating-many-zero-with-inlabru-2",
    "title": "Lecture ??",
    "section": "Summary - Treating many zero with inlabru",
    "text": "Summary - Treating many zero with inlabru\nThere are mainly two types of models to treat excess of zeros\n\nZero inflated models\nHurdle models\n\nOnly one source of zeros\nPositive values can be both discrete and continuous\nTwo ways to imlement in inlabru"
  },
  {
    "objectID": "slides/slides_12.html#summary---treating-many-zero-with-inlabru-3",
    "href": "slides/slides_12.html#summary---treating-many-zero-with-inlabru-3",
    "title": "Lecture ??",
    "section": "Summary - Treating many zero with inlabru",
    "text": "Summary - Treating many zero with inlabru\nThere are mainly two types of models to treat excess of zeros\n\nZero inflated models\nHurdle models\n\nOnly one source of zeros\nPositive values can be both discrete and continuous\nTwo ways to imlement in inlabru\n\nUse the implemented Type0 likelihoods\n\n\n\nProbability of zero is constant\n\n\nUse the “two-likelihood” approach\n\n\nProbability of zero can be modelled"
  }
]