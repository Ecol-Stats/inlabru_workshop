[
  {
    "objectID": "slides/slides_1.html#course-structure-day-1",
    "href": "slides/slides_1.html#course-structure-day-1",
    "title": "inlabru workshop",
    "section": "Course Structure: Day 1",
    "text": "Course Structure: Day 1\n\n\n\n\n\n\n\n\n\nTime\nTopic\nContent\nExcercises\n\n\n\n\nXXXX am\nCore concepts\n\nLGM and INLA\ninlabru workflow\nModel selection\n\n\n\n\nXXXX am\nTemporal Models\n\nDiscrete time models\nContinuous time models"
  },
  {
    "objectID": "slides/slides_1.html#course-structure-day-2",
    "href": "slides/slides_1.html#course-structure-day-2",
    "title": "inlabru workshop",
    "section": "Course Structure: Day 2",
    "text": "Course Structure: Day 2\n\n\n\n\n\n\n\n\n\nTime\nTopic\nContent\nExcercises\n\n\n\n\nXXXX am\nIntroduction to Spatial Modelling\n\nTypes of spatial data\nSpatial data wrangling and manipulation in R (e.g, terra & sf)\nAreal processes\n\n\n\n\nXXXX am\nModelling geostatistical data\n\nSPDE & the mesh\nGeostatistical Data\nSpatial predictions"
  },
  {
    "objectID": "slides/slides_1.html#course-structure-day-3",
    "href": "slides/slides_1.html#course-structure-day-3",
    "title": "inlabru workshop",
    "section": "Course Structure: Day 3",
    "text": "Course Structure: Day 3\n\n\n\n\n\n\n\n\n\nTime\nTopic\nContent\nExcercises\n\n\n\n\nXXXX am\nSpatial Point processes\n\nSpatial point process\nDistance sampling\n\n\n\n\nXXXX am\nSpatiotemporal Models\n\nSeparable time-space models\nnon-separable space-time models"
  },
  {
    "objectID": "slides/slides_1.html#course-structure-day-4",
    "href": "slides/slides_1.html#course-structure-day-4",
    "title": "inlabru workshop",
    "section": "Course Structure: Day 4",
    "text": "Course Structure: Day 4\n\n\n\n\n\n\n\n\n\nTime\nTopic\nContent\nExcercises\n\n\n\n\nXXXX am\nMultilikelihood and Non-linear models\n\niterated inla\nlogistic growth\nCorregionalization models"
  },
  {
    "objectID": "day4_practical_6.html",
    "href": "day4_practical_6.html",
    "title": "Practical 6",
    "section": "",
    "text": "Aim of this practical:\nIn this practical we are going to look at some model comparison and validation techniques.\nDownload Practical 6 R script",
    "crumbs": [
      "Home",
      "Practical 6"
    ]
  },
  {
    "objectID": "day4_practical_6.html#model-checking-for-linear-models",
    "href": "day4_practical_6.html#model-checking-for-linear-models",
    "title": "Practical 6",
    "section": "Model Checking for Linear Models",
    "text": "Model Checking for Linear Models\nIn this exercise we will:\n\nLearn about some model assessments techniques available in INLA\nConduct posterior predictive model checking\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nRecall a simple linear regression model with Gaussian observations\n\\[\ny_i\\sim\\mathcal{N}(\\mu_i, \\sigma^2), \\qquad i = 1,\\dots,N\n\\]\nwhere \\(\\sigma^2\\) is the observation error, and the mean parameter \\(\\mu_i\\) is linked to the linear predictor through an identity function:\n\\[\n\\eta_i = \\mu_i = \\beta_0 + \\beta_1 x_i\n\\] where \\(x_i\\) is a covariate and \\(\\beta_0, \\beta_1\\) are parameters to be estimated.\n\nSimulate example data\nWe simulate data from a simple linear regression model\n\n\nCode\nbeta = c(2,0.5)\nsd_error = 0.1\n\nn = 100\nx = rnorm(n)\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error)\n\ndf = data.frame(y = y, x = x)  \n\n\n\n\nFitting the linear regression model with inlabru\nNow we fit a simple linear regression model in inalbru by defining (1) the model components, (2) the linear predictor and (3) the likelihood.\n\n# Model components\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n# Linear predictor\nformula = y ~ Intercept + beta_1\n# Observational model likelihood\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n# Fit the Model\nfit.lm = bru(cmp, lik)\n\n\n\nResiduals analysis\nA common way for model diagnostics in regression analysis is by checking residual plots. In a Bayesian setting residuals can be defined in multiple ways depending on how you account for posterior uncertainty. Here, we will adopt a Bayesian approach by generating samples from the posterior distribution of the model parameters and then draw samples from the residuals defined as:\n\\[\nr_i = y_i - x_i^T\\beta\n\\]\nWe can use the predict function to achieve this:\n\nres_samples &lt;- predict(\n  fit.lm,         # the fitted model\n  df,             # the original data set\n  ~ data.frame(   \n    res = y-(beta_0 + beta_1)  # compute the residuals\n  ),\n  n.samples = 1000   # draw 1000 samples\n)\n\nThe resulting data frame contains the posterior draw of the residuals mean for which we can produce some diagnostics plots , e.g.\n\n\nResiduals checks for Linear Model\nggplot(res_samples,aes(y=mean,x=1:100))+geom_point() +\nggplot(res_samples,aes(y=mean,x=x))+geom_point()\n\n\n\n\n\nBayesian residual plots: the left panel is the residual index plot; the right panel is the plot of the residual versus the covariate x\n\n\n\n\nWe can also compare these against the theoretical quantiles of the Normal distribution as follows:\n\n\nQQPlot for Linear Model\narrange(res_samples, mean) %&gt;%\n  mutate(theortical_quantiles = qnorm(1:100 / (1+100))) %&gt;%\n  ggplot(aes(x=theortical_quantiles,y= mean)) + \n  geom_ribbon(aes(ymin = q0.025, ymax = q0.975), fill = \"grey70\")+\n  geom_abline(intercept = mean(res_samples$mean),\n              slope = sd(res_samples$mean)) +\n  geom_point() +\n  labs(x = \"Theoretical Quantiles (Normal)\",\n       y= \"Sample Quantiles (Residuals)\") \n\n\n\n\n\n\n\n\n\n\n\nPosterior Predictive Checks\nNow, instead of generating samples from the mean, we will account for the observational process uncertainty by:\n\nSampling \\(y^{1k}_i\\sim\\pi(y_i|\\mathbf{y})\\) \\(k = 1,\\dots,M;~i = 1,\\ldots,100\\) using generate() (here we will draw \\(M=500\\) samples)\n\n\nsamples =  generate(fit.lm, df,\n  formula = ~ {\n    mu &lt;- (beta_0 + beta_1)\n    sd &lt;- sqrt(1 / Precision_for_the_Gaussian_observations)\n    rnorm(100, mean = mu, sd = sd)\n  },\n  n.samples = 500\n) \n\n\nComparing some summaries of the simulated data with the one of the observed one\n\nHere we compare (i) the estimated posterior densities \\(\\hat{\\pi}^k(y|\\mathbf{y})\\) with the estimated data density and (ii) the samples means and 95% credible intervals against the observations.\n\n# Tidy format for plotting\nsamples_long = data.frame(samples) %&gt;% \n  mutate(id = 1:100) %&gt;% # i-th observation\n  pivot_longer(-id)\n\n# compute the mean and quantiles for the samples\ndraws_summaries = data.frame(mean_samples = apply(samples,1,mean),\nq25 = apply(samples,1,function(x)quantile(x,0.025)),  \nq975 = apply(samples,1,function(x)quantile(x,0.975)),\nobservations = df$y)  \n\np1 = ggplot() + geom_density(data = samples_long, \n                        aes(value, group = name),  color = \"#E69F00\") +\n  geom_density(data = df, aes(y))  +\n  xlab(\"\") + ylab(\"\") \n\np2 = ggplot(draws_summaries,aes(y=mean_samples,x=observations))+\n  geom_errorbar(aes(ymin = q25,\n                   ymax = q975), \n               alpha = 0.5, color = \"grey50\")+\ngeom_point()+geom_abline(slope = 1,intercept = 0,lty=2)+labs()\n\np1 +p2",
    "crumbs": [
      "Home",
      "Practical 6"
    ]
  },
  {
    "objectID": "day4_practical_6.html#sec-linmodel",
    "href": "day4_practical_6.html#sec-linmodel",
    "title": "Practical 6",
    "section": "GLM model checking",
    "text": "GLM model checking\nIn this exercise we will:\n\nLearn about some model assessments techniques available in INLA\nConduct posterior predictive model checking using CPO and PIT\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nIn this exercise, we will use data on horseshoe crabs (Limulus polyphemus) where the number of satellites males surrounding a breeding female are counted along with the female’s color and carapace width.\n Download data set \nA possible model to study the factors that affect the number of satellites for female crabs is\n\\[\n\\begin{aligned}\ny_i&\\sim\\mathrm{Poisson}(\\mu_i), \\qquad i = 1,\\dots,N \\\\\n\\eta_i &= \\mu_i = \\beta_0 + \\beta_1 x_i + \\ldots\n\\end{aligned}\n\\]\nWe can explore the conditional means and variances given the female’s color:\n\ncrabs &lt;- read.csv(\"datasets/crabs.csv\")\n\n# conditional means and variances\ncrabs %&gt;%\n  summarise( Mean = mean(satell ),\n             Variance = var(satell),\n                     .by = color)\n\n   color     Mean  Variance\n1 medium 3.294737 10.273908\n2   dark 2.227273  6.737844\n3  light 4.083333  9.719697\n4 darker 2.045455 13.093074\n\n\nThe mean of the number of satellites vary by color which gives a good indication that color might be useful for predicting satellites numbers. However, notice that the mean is lower than its variance suggesting that overdispersion might be present and that a negative binomial model would be more appropriate for the data (we will cover this later).\nFitting the model\nFirst, lets begin fitting the Poisson model above using the carapace’s color and width as predictors. Since, color is a categorical variable in our model we need to create a dummy variable for it. We can use the model.matrix function to help us constructing the design matrix and then append this to our data:\n\ncrabs_df = model.matrix( ~  color , crabs) %&gt;%\n  as.data.frame() %&gt;%\n  select(-1) %&gt;%        # drop intercept\n  bind_cols(crabs) %&gt;%  # append to original data\n  select(-color)        # remove original color categorical variable\n\nThe new data set crabs_df contains a dummy variable for the different color categories (dark being the reference category). Then we can fit the model in inlabru as follows:\n\ncmp =  ~ -1 + beta0(1) +  colordarker +\n       colorlight + colormedium +\n       w(weight, model = \"linear\")\n\nlik =  bru_obs(formula = satell ~.,\n            family = \"poisson\",\n            data = crabs_df)\n\nfit_pois = bru(cmp, lik)\n\nsummary(fit_pois)\n\ninlabru version: 2.13.0\nINLA version: 25.08.21-1\nComponents:\nbeta0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\ncolordarker: main = linear(colordarker), group = exchangeable(1L), replicate = iid(1L), NULL\ncolorlight: main = linear(colorlight), group = exchangeable(1L), replicate = iid(1L), NULL\ncolormedium: main = linear(colormedium), group = exchangeable(1L), replicate = iid(1L), NULL\nw: main = linear(weight), group = exchangeable(1L), replicate = iid(1L), NULL\nObservation models:\n  Family: 'poisson'\n    Tag: &lt;No tag&gt;\n    Data class: 'data.frame'\n    Response class: 'integer'\n    Predictor: satell ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[beta0, colordarker, colorlight, colormedium, w], latent[]\nTime used:\n    Pre = 0.337, Running = 0.331, Post = 0.0715, Total = 0.74 \nFixed effects:\n              mean    sd 0.025quant 0.5quant 0.975quant   mode kld\nbeta0       -0.501 0.196     -0.885   -0.501     -0.117 -0.501   0\ncolordarker -0.008 0.180     -0.362   -0.008      0.345 -0.008   0\ncolorlight   0.445 0.176      0.101    0.445      0.790  0.445   0\ncolormedium  0.248 0.118      0.017    0.248      0.479  0.248   0\nw            0.001 0.000      0.000    0.001      0.001  0.001   0\n\nMarginal log-Likelihood:  -489.43 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\n\nModel assessment and model choice\nNow that we have fitted the model we would like to carry some model assessments. In a Bayesian setting, this is often based on posterior predictive checks. To do so, we will use the CPO and PIT - two commonly used Bayesian model assessment criteria based on the posterior predictive distribution.\n\n\n\n\n\n\nPosterior predictive model checking\n\n\n\nThe posterior predictive distribution for a predicted value \\(\\hat{y}\\) is\n\\[\n\\pi(\\hat{y}|\\mathbf{y}) = \\int_\\theta \\pi(\\hat{y}|\\theta)\\pi(\\theta|\\mathbf{y})d\\theta.\n\\]\nThe probability integral transform (PIT) introduced by Dawid (1984) is defined for each observation as:\n\\[\n\\mathrm{PIT}_i = \\pi(\\hat{y}_i \\leq y_i |\\mathbf{y}{-i})\n\\]\nThe PIT evaluates how well a model’s predicted values match the observed data distribution. It is computed as the cumulative distribution function (CDF) of the observed data evaluated at each predicted value. If the model is well-calibrated, the PIT values should be approximately uniformly distributed. Deviations from this uniform distribution may indicate issues with model calibration or overfitting.\nAnother metric we could used to asses the model fit is the conditional predictive ordinate (CPO) introduced by Pettit (1990), and deﬁned as:\n\\[\n\\text{CPO}_i = \\pi(y_i| \\mathbf{y}{-i})\n\\]\nThe CPO measures the density of the observed value of \\(y_i\\) when model is fit using all data but \\(y_i\\). CPO provides a measure of how well the model predicts each individual observation while taking into account the rest of the data and the model. Large values indicate a better fit of the model to the data, while small values indicate a bad fitting of the model\n\n\nTo compute PIT and CPO we can either:\n\nask inlabru to compute them by set options = list(control.compute = list(cpo = TRUE)) in the bru() function arguments.\nset this as default in inlabru global option using the bru_options_set function.\n\nHere we will do the later and re-run the model\n\nbru_options_set(control.compute = list(cpo = TRUE))\n\nfit_pois = bru(cmp, lik)\n\nNow we can produce histograms and QQ plots to assess for uniformity in the PIT values which can be accessed through inlabru_model$cpo$pit :\n\nPlotR Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfit_pois$cpo$pit %&gt;%\n  hist(main = \"Histogram of PIT values\")\n\nqqplot(qunif(ppoints(length(fit_pois$cpo$pit))),\n       fit_pois$cpo$pit,\n       main = \"Q-Q plot for Unif(0,1)\",\n       xlab = \"Theoretical Quantiles\",\n       ylab = \"Sample Quantiles\")\n\nqqline(fit_pois$cpo$pit,\n       distribution = function(p) qunif(p),\n       prob = c(0.1, 0.9))\n\n\n\n\nBoth Q-Q plots and histogram of the PIT values suggest a not so great model fit. For the CPO values, usually the following summary of the CPO is often used:\n\\[\n-\\sum_{i=1}^n \\log (\\text{CPO}\\_i)\n\\]\nThis quantities is useful when comparing different models - a smaller values indicate a better model fit. CPO values can be accessed by typing inlabru_model$cpo$cpo.\n\n\n\n\n\n\n Task\n\n\n\nThe model assessment above suggests that a Poisson model might not be the most appropriate model, likely due to the overdispersion we detected previously. Fit a Negative binomial to relax the Poisson model assumption that the conditional mean and variance are equal. Then, compute the CPO summary statistic and PIT QQ plot to decide which model gives the better fit.\n\n\nTake hint\n\nTo specify a negative binomial model you only need to change the family distribution to family =  \"nbinomial\".\n\n\n\n\nClick here to see the solution\n\npar(mfrow=c(1,2))\n\n# Fit the negative binomial model\n\nlik_nbinom =  bru_obs(formula = satell ~.,\n            family = \"nbinomial\",\n            data = crabs_df)\n\nfit_nbinom = bru(cmp, lik_nbinom)\n\n# PIT checks\n\nfit_nbinom$cpo$pit %&gt;%\n  hist(main = \"Histogram of PIT values\")\n\nqqplot(qunif(ppoints(length(fit_nbinom$cpo$pit))),\n       fit_nbinom$cpo$pit,\n       main = \"Q-Q plot for Unif(0,1)\",\n       xlab = \"Theoretical Quantiles\",\n       ylab = \"Sample Quantiles\")\n\nqqline(fit_nbinom$cpo$pit,\n       distribution = function(p) qunif(p),\n       prob = c(0.1, 0.9))\n\n\n\n\n\n\n\n# CPO comparison\n\ndata.frame( CPO = c(-sum(log(fit_pois$cpo$cpo)),\n                    -sum(log(fit_nbinom$cpo$cpo))),\n          Model = c(\"Poisson\",\"Negative Binomial\"))\n\n       CPO             Model\n1 465.4061           Poisson\n2 379.3340 Negative Binomial\n\n# Overall, we can see that the negative binomial model provides a better fit to the data.",
    "crumbs": [
      "Home",
      "Practical 6"
    ]
  },
  {
    "objectID": "day2_practical_4.html",
    "href": "day2_practical_4.html",
    "title": "Practical 4",
    "section": "",
    "text": "Aim of this practical:\nwe are going to learn:\nDownload Practical 3 R script\nIn this practical we will:",
    "crumbs": [
      "Home",
      "Practical 4"
    ]
  },
  {
    "objectID": "day2_practical_4.html#sec-areal_data",
    "href": "day2_practical_4.html#sec-areal_data",
    "title": "Practical 4",
    "section": "Areal (lattice) data",
    "text": "Areal (lattice) data\nAreal data our measurements are summarised across a set of discrete, non-overlapping spatial units such as postcode areas, health board or pixels on a satellite image. In consequence, the spatial domain is a countable collection of (regular or irregular) areal units at which variables are observed. Many public health studies use data aggregated over groups rather than data on individuals - often this is for privacy reasons, but it may also be for convenience.\nIn the next example we are going to explore data on respiratory hospitalisations for Greater Glasgow and Clyde between 2007 and 2011. The data are available from the CARBayesdata R Package:\n\nlibrary(CARBayesdata)\n\ndata(pollutionhealthdata)\ndata(GGHB.IZ)\n\nThe pollutionhealthdata contains the spatiotemporal data on respiratory hospitalisations, air pollution concentrations and socio-economic deprivation covariates for the 271 Intermediate Zones (IZ) that make up the Greater Glasgow and Clyde health board in Scotland. Data are provided by the Scottish Government and the available variables are:\n\nIZ: unique identifier for each IZ.\nyear: the year were the measruments were taken\nobserved: observed numbers of hospitalisations due to respiratory disease.\nexpected: expected numbers of hospitalisations due to respiratory disease computed using indirect standardisation from Scotland-wide respiratory hospitalisation rates.\npm10: Average particulate matter (less than 10 microns) concentrations.\njsa: The percentage of working age people who are in receipt of Job Seekers Allowance\nprice: Average property price (divided by 100,000).\n\nThe GGHB.IZ data is a Simple Features (sf) object containing the spatial polygon information for the set of 271 Intermediate Zones (IZ), that make up of the Greater Glasgow and Clyde health board in Scotland ( Figure 1 ).\n\n\n\n\n\n\n\n\nFigure 1: Greater Glasgow and Clyde health board represented by 271 Intermediate Zones\n\n\n\n\nLet’s start by loading useful libraries:\n\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(scico)\n\nThe sf package allows us to work with vector data which is used to represent points, lines, and polygons. It can also be used to read vector data stored as a shapefiles.\nFirst, lets combine both data sets based on the Intermediate Zones (IZ) variable using the merge function from base R:\n\nresp_cases &lt;- merge(GGHB.IZ, pollutionhealthdata, by = \"IZ\")\n\nIn epidemiology, disease risk is usually estimated using Standardized Mortality Ratios (SMR). The SMR for a given spatial areal unit \\(i\\) is defined as the ratio between the observed ( \\(Y_i\\) ) and expected ( \\(E_i\\) ) number of cases:\n\\[\nSMR_i = \\dfrac{Y_i}{E_i}\n\\]\nA value \\(SMR &gt; 1\\) indicates that there are more observed cases than expected which corresponds to a high risk area. On the other hand, if \\(SMR&lt;1\\) then there are fewer observed cases than expected, suggesting a low risk area.\nWe can manipulate sf objects the same way we manipulate standard data frame objects via the dplyr package. Lets use the pipeline command %&gt;% and the mutate function to calculate the yearly SMR values for each IZ:\n\nlibrary(dplyr)\nresp_cases &lt;- resp_cases %&gt;% \n  mutate(SMR = observed/expected, .by = year )\n\nNow we use ggplot to visualize our data by adding a geom_sf layer and coloring it according to our variable of interest (i.e., SMR). We can further use facet_wrap to create a layer per year and chose an appropriate color palette using the scale_fill_scico from the scico package:\n\nggplot()+\n  geom_sf(data=resp_cases,aes(fill=SMR))+\n  facet_wrap(~year)+scale_fill_scico(palette = \"roma\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nProduce a map that shows the spatial distribution of each of the following variables for the year 2011:\n\nAverage particulate matter pm10\nAverage property price price\nPercentage of working age people who are in receipt of Job Seekers Allowance jsa\n\n\n\nhint\n\nYou can use the filter function from dplyr to subset the data according to the year of interest.\n\n\n\n\nClick here to see the solution\n\n# Library for plotting multiple maps together\n\nlibrary(patchwork)\n\n# subset data set for 2011\n\nresp_cases_2011 &lt;- resp_cases %&gt;% filter(year ==2011)\n\n# pm10 plot\n\npm10_plot &lt;- ggplot()+\n  geom_sf(data=resp_cases_2011,aes(fill=pm10))+\n  scale_fill_scico(palette = \"navia\")\n\n# property price\n\nprice_plot &lt;- ggplot()+\n  geom_sf(data=resp_cases_2011,aes(fill=price))+\n  facet_wrap(~year)+scale_fill_scico(palette = \"bilbao\")\n\n#  percentage jsa\n\njsa_plot &lt;- ggplot()+\n  geom_sf(data=resp_cases_2011,aes(fill=jsa))+\n  facet_wrap(~year)+scale_fill_scico(palette = \"lapaz\") \n\n# plot maps together\n\npm10_plot + price_plot + jsa_plot + plot_layout(ncol=3)\n\n\n\n\n\n\n\n\n\n\n\nAs with the other types of spatial modelling, our goal is to observe and explain spatial variation in our data. Generally, we are aiming to produce a smoothed map which summarises the spatial patterns we observe in our data.\nA key aspect of any spatial analysis is that observations closer together in space are likely to have more in common than those further apart. This can lead us towards approaches similar to those used in time series, where we consider the spatial closeness of our regions in terms of a neighbourhood structure.\nThe function poly2nb() of the spdep package can be used to construct a list of neighbors based on areas with contiguous boundaries (e.g., using Queen contiguity).\n\nlibrary(spdep)\n\nW.nb &lt;- poly2nb(GGHB.IZ,queen = TRUE)\nW.nb\n\nNeighbour list object:\nNumber of regions: 271 \nNumber of nonzero links: 1424 \nPercentage nonzero weights: 1.938971 \nAverage number of links: 5.254613 \n2 disjoint connected subgraphs\n\n\nThe warning tell us that the neighbourhood is comprised of two interconnected regions. By looking at the neighbourhood graph below, we can see that these are the North and South Glasgow regions which are separated by the River Clyde.\n\nplot(st_geometry(GGHB.IZ), border = \"lightgray\")\nplot.nb(W.nb, st_geometry(GGHB.IZ), add = TRUE)\n\n\n\n\n\n\n\n\nYou could use the snap argument within poly2nb to set a distance at which the different regions centroids are consider neighbours. To do so we first need to be aware about the spatial units of the spatial coordinate reference system (CRS). We can check this as follows:\n\nst_crs(GGHB.IZ)$units\n\n[1] \"m\"\n\n\nThen, we could set a distance of 250m to join the IZ centroids that are are less than 250m apart.\n\nW.nb250 &lt;- poly2nb(GGHB.IZ,snap=250)\nW.nb250\n\nNeighbour list object:\nNumber of regions: 271 \nNumber of nonzero links: 1758 \nPercentage nonzero weights: 2.393758 \nAverage number of links: 6.487085 \n\n\n\nplot(st_geometry(GGHB.IZ), border = \"lightgray\")\nplot.nb(W.nb250, st_geometry(GGHB.IZ), add = TRUE)\n\n\n\n\n\n\n\n\nOnce we have identified a set of neighbours using our chosen method, we can use this to account for correlation.\nMoran’s \\(I\\) is a measure of global spatial autocorrelation, and can be considered an extension of the Pearson correlation coefficient. For a set of data \\(Z_1, \\ldots, Z_m\\) measured on regions \\(B_1, \\ldots B_m\\), with neighbourhood matrix \\(W\\), we can compute Moran’s I as:\n\\[\nI = \\frac{m}{\\sum_{i=1}^m \\sum_{j=1}^m w_{ij}}\\frac{\\sum_{i=1}^m \\sum_{j=1}^m w_{ij} (Z_i - \\bar{Z})(Z_j - \\bar{Z})}{\\sum_{i=1}^m (Z_i - \\bar{Z})^2}\n\\]\nThis is basically a function of differences in values between neighbouring areas. By far the most common approach is to use a binary neighbourhood matrix, \\(W\\), denoted by\n\\[ w_{ij} = \\begin{cases} 1 & \\text{if areas } (B_i, B_j) \\text{ are neighbours.}\\\\ 0 & \\text{otherwise.} \\end{cases} \\]\nBinary matrices are used for their simplicity. Fitting spatial models often requires us to invert \\(W\\), and this is less computationally intensive for sparse matrices.\nMoran’s \\(I\\) ranges between -1 and 1, and can be interpreted in a similar way to a standard correlation coefficient.\n\n\\(I=1\\) implies that we have perfect spatial correlation.\n\\(I=0\\) implies that we have complete spatial randomness.\n\\(I=-1\\) implies that we have perfect dispersion (negative correlation).\n\nOur observed \\(I\\) is a point estimate, and we may also wish to assess whether it is significantly different from zero. We can test for a statistically significant spatial correlation using a permutation test, with hypotheses:\n\\[\n\\begin{aligned}\nH_0&: \\text{ negative or no spatial association } (I \\leq 0)\\\\\nH_1&: \\text{ positive spatial association } (I &gt; 0)\n\\end{aligned}\n\\]\nWe can use moran.test() to test this hypothesis by setting alternative = \"greater\". To do so, we need to supply list containing the neighbors via the nb2listw() function from the spdep package. Lets assess now the spatial autocorrelation of the SMR in 2011:\n\n# subset the data\nresp_cases_2011 &lt;- resp_cases %&gt;% filter(year ==2011)\n\n# neighbors list \nnbw &lt;- nb2listw(W.nb, style = \"W\")\n\n# Global Moran's I\ngmoran &lt;- moran.test(resp_cases_2011$SMR, nbw,\n                     alternative = \"greater\")\ngmoran\n\n\n    Moran I test under randomisation\n\ndata:  resp_cases_2011$SMR  \nweights: nbw    \n\nMoran I statistic standard deviate = 11.42, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.439899780      -0.003703704       0.001508809 \n\n\n\n\n\n\n\n\n Question\n\n\n\nWhat do we conclude from the Moran’s I test?\n\n\nAnswer\n\nSince have set the alternative hypothesis to be $ I &gt; 0 $ and have a p-value \\(&lt;0.05\\), we then reject the null hypothesis and conclude there is evidence for positive spatial autocorrelation.\n\n\n\nA local version of Moran’s I can also be used to measure the similarity between each IZ via the localmoran function:\n\nlmoran &lt;- localmoran(resp_cases_2011$SMR, nbw, alternative = \"two.sided\")\n\nIn this case we set alternative = \"two.sided\" to test whether there is any evidence of spatial autocorrelation in our data:\n\\[\n\\begin{aligned}\nH_0&: \\text{ no spatial association } (I=0)\\\\\nH_1&: \\text{ some spatial association } (I \\neq 0)\n\\end{aligned}\n\\]\nWe can obtain the \\(Z\\)-scores from the test (Z.Ii) where any values smaller than –1.96 indicate negative spatial autocorrelation, and z-score values greater than 1.96 indicate positive spatial autocorrelation:\n\nresp_cases_2011_m &lt;- resp_cases_2011 %&gt;% mutate(Zscores = lmoran[,\"Z.Ii\"])\n\nresp_cases_2011_m &lt;- resp_cases_2011_m %&gt;% \n  mutate (SAC = case_when(\n  Zscores &gt; qnorm(0.975) ~ \" M &gt; 1\" ,\n  Zscores &lt; -1*qnorm(0.975) ~ \" M &lt; 1\",\n  .default = \"M = 0\"),\n  SAC=as.factor(SAC)\n)\n\nWe can visualize these results using either ggplot as we did before, or via the mapview library which contains interactive features:\n\nlibrary(mapview)\nmapview(resp_cases_2011_m, \n        zcol = \"SAC\",\n        layer.name = \"SAC\",\n        col.regions=c(\"turquoise\",\"orange\",\"grey40\"))\n\n\n\n\n\nIn this practical we will:\n\nExplore tools for geostatistical spatial data wrangling and visualization.\nCompute a variogram to assess for spatial autocorrelation in our data.\n\nFirst, lets load some useful libraries for data wrangling and visualization\n\n# For plotting\nlibrary(mapview)\nlibrary(ggplot2)\nlibrary(scico) # for colouring palettes\n\n# Data manipulation\nlibrary(dplyr)",
    "crumbs": [
      "Home",
      "Practical 4"
    ]
  },
  {
    "objectID": "day2_practical_4.html#georeferenced-data",
    "href": "day2_practical_4.html#georeferenced-data",
    "title": "Practical 4",
    "section": "Georeferenced data",
    "text": "Georeferenced data\nTobler’s first law of geography states that:\n“Everything is related to everything else, but near things are more related than distant things”\nSpatial patterns are fundamental in environmental and ecological data. In many ecological and environmental settings, measurements from fixed sampling units, aiming to quantify spatial variation and interpolate values at unobserved sites.\nGeorefernced data are the most common form of spatial data found in environmental setting. In these data we regularly take measurements of a spatial ecological or environmental process at a set of fixed locations. This could be data from transects (e.g, where the height of trees is recorded), samples taken across a region (e.g., water depth in a lake) or from monitoring stations as part of a network (e.g., air pollution). In each of these cases, our goal is to estimate the value of our variable across the entire space.\nLet \\(D\\) be our two-dimensional region of interest. In principle, there is aninfinite number of locations within \\(D\\), each of which can be represented by mathematical coordinates (e.g., latitude and longitude). We then can identify any individual location as \\(s_i = (x_i, y_i)\\), where \\(x_i\\) and \\(y_i\\) are their coordinates.\nWe can treat our variable of interest as a random variable, \\(Z\\) which can be observed at any location as \\(Z(\\mathbf{s}_i)\\).\nOur geostatistical process can therefore be written as: \\[\\{Z(\\mathbf{s}); \\mathbf{s} \\in D\\}\\]\nIn practice, our data are observed in a finite number of locations, \\(m\\), and can be denoted as:\n\\[z = \\{z(\\mathbf{s}_1), \\ldots z(\\mathbf{s}_m) \\}\\]\nIn the next example, we will explore data on the Pacific Cod (Gadus macrocephalus) from a trawl survey in Queen Charlotte Sound. The pcod dataset is available from the sdmTMB package and contains the presence/absence records of the Pacific Cod during each surveys along with the biomass density of Pacific cod in the area swept (kg/Km\\(^2\\)). The qcs_grid data contain the depth values stored as \\(2\\times 2\\) km grid for Queen Charlotte Sound.\n\nlibrary(sdmTMB)\n\npcod_df = sdmTMB::pcod \nqcs_grid = sdmTMB::qcs_grid\n\n\nGeoreferrenced data\nLet’s create an initial sf spatial object using the standard geographic coordinate system (EPSG:4326). This correctly defines the point locations based on latitude and longitude.\n\nlibrary(sf)\npcod_sf =   st_as_sf(pcod_df, coords = c(\"lon\",\"lat\"), crs = 4326)\n\nNow we can transform to the standard UTM Zone 9N projection (EPSG:32609) which uses meters:\n\npcod_sf_proj &lt;- st_transform(pcod_sf, crs = 32609)\nst_crs(pcod_sf_proj)$units\n\n[1] \"m\"\n\n\nWe can change the spatial units to km to better reflect the scale of our ecological study and to make resulting distance/area values more intuitive to interpret:\n\npcod_sf_proj = st_transform(pcod_sf_proj,\n                            gsub(\"units=m\",\"units=km\",\n                                 st_crs(pcod_sf_proj)$proj4string)) \nst_crs(pcod_sf_proj)$units\n\n[1] \"km\"\n\n\nInstead of first setting an EPSG code and then transforming, we can define the target Coordinate Reference System (CRS) directly using a proj4string. This allows us to customize non-standard parameters in a single step, in this case, explicitly setting the projection units to kilometers (+units=km).\n\npcod_sf = st_transform(pcod_sf,\n                       crs = \"+proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\" )\nst_crs(pcod_sf)$units\n\n[1] \"km\"\n\n\nSpatial sf objects can be manipulated the same way we manipulate standard data frame objects via the dplyr package. For example, you can select a specific year using the filter function from dplyr. Let’s map the present/absence of the Pacific Cod in 2017 using the mapview function:\n\npcod_sf %&gt;% \n  filter(year== 2017) %&gt;%\n  mutate(present = as.factor(present)) %&gt;%\nmapview(zcol = \"present\",\n        layer.name = \"Occupancy status of Pacific Cod in 2017\")\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nUse ggplot and the sf library to map the biomass density of the pacific cod across years.\n\n\nhint\n\nYou can plot ansf object by adding a geom_sf layer to a ggplot object. You can also use the facet_wrap argument to plot an arrange of plots according to a grouping variable.\n\n\n\n\nClick here to see the solution\n\nggplot()+ \n  geom_sf(data=pcod_sf,aes(color=density))+ \n  facet_wrap(~year)+\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nRaster Data\nEnvironmental data are typically stored in raster format, which represents spatially continuous phenomena by dividing a region into a grid of equally-sized cells, each storing a value for the variable of interest. In R, the terra package is a modern and powerful tool for efficiently working with raster data. The function rast(), can be used both to read raster files from standard formats (e.g., .tif or .tiff) and to create a new raster object from a data frame. For instance, the following code creates a raster from the qcs_grid grid data for Queen Charlotte Sound.\n\nlibrary(terra)\ndepth_r &lt;- rast(qcs_grid, type = \"xyz\")\ndepth_r\n\nclass       : SpatRaster \nsize        : 102, 121, 3  (nrow, ncol, nlyr)\nresolution  : 2, 2  (x, y)\nextent      : 341, 583, 5635, 5839  (xmin, xmax, ymin, ymax)\ncoord. ref. :  \nsource(s)   : memory\nnames       :    depth, depth_scaled, depth_scaled2 \nmin values  :  12.0120,    -6.000040,  4.892624e-08 \nmax values  : 805.7514,     3.453937,  3.600048e+01 \n\n\nThe raster object contains three layers corresponding to the (i) depth values, (ii) the scaled depth values and (iii) the squared depth values.\nNotice that there are no CRS associated with the raster. Thus, we can assign appropriate CRS using the crs function. Additionally, we also want the raster CRS to match the CRS in the survey data (recall that we have previously reprojected our data to utm coordinates). We can assign an appropiate CRS that matches the CRS of the sf object as follows:\n\ncrs(depth_r) &lt;- crs(pcod_sf)\n\nWe can use the tidyterra package to plot raster data using ggplot by adding a geom_spatraster function and then select an appropriate fill and color palettes:\n\nlibrary(tidyterra)\n\n\nAttaching package: 'tidyterra'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nggplot()+ \n  geom_spatraster(data=depth_r$depth)+\n  geom_sf(data=pcod_sf,aes(color=factor(present))) +\n  facet_wrap(~year)+\n    scale_color_manual(name=\"Occupancy status for the Pacific Cod\",\n                     values = c(\"black\",\"orange\"),\n                     labels= c(\"Absence\",\"Presence\"))+\n  scale_fill_scico(name = \"Depth\",\n                   palette = \"nuuk\",\n                   na.value = \"transparent\" )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nMap the scaled depth and the presence/absence records of the Pacific cod for 2003 to 2005 only.\n\n\nhint\n\nThe different layers of a raster can be accessed using the $ symbol.\n\n\n\n\nClick here to see the solution\n\nggplot()+ \n  geom_spatraster(data=depth_r$depth_scaled)+\n  geom_sf(data=pcod_sf %&gt;% filter(year %in% 2003:2005),\n          aes(color=factor(present)))+ \n  facet_wrap(~year)+\n  scale_color_manual(name=\"Occupancy status for the Pacific Cod\",\n                     values = c(\"black\",\"orange\"),\n                     labels= c(\"Absence\",\"Presence\"))+\n    scale_fill_scico(name = \"Scaled Depth\",\n                     palette = \"davos\",\n                     na.value = \"transparent\" )\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutocorrelation and Variograms\nSpatial statistics quantifies the fundamental principle that nearby things are closely related. This spatial dependence means that observation are not independent, as assumed by most classical statistical models, but are instead correlated relative to their proximity. While this correlation can be a valuable source of information, it must be explicitly accounted for to avoid wrong inference and incorrect conclusions.\nThe first step is to assess whether there is any evidence of spatial dependency in our data. Spatial dependence in georeferenced data can be explored by a function known as a variogram \\(2\\gamma(\\cdot)\\) (or semivariogram \\(\\gamma(\\cdot)\\)). The variogram is similar in many ways to the autocorrelation function used in time series modelling. In simple terms, it is a function which measures the difference in the spatial process between a pair of locations a fixed distance apart.\nThe variogram measures the variance of the difference in the process \\(Z(\\cdot)\\) at two spatial locations \\(\\mathbf{s}\\) and \\(\\mathbf{s+h}\\) and is defined as :\n\\[\\mathrm{Var}[Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h})] = E[(Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h}))^2] = 2\\gamma_z(\\mathbf{h}).\\]\nHere, \\(2\\gamma_z(\\mathbf{h})\\) is the variogram, but in practice we use the semi-variogram, \\(\\gamma_z(\\mathbf{h})\\). We use the semi-variogram because our points come in pairs, and the semi-variance is equivalent to the variance per point at a given lag.\n\nWhen the variance of the difference \\(Z(\\mathbf{s}) - Z(\\mathbf{t})\\) is relatively small, then \\(Z(\\mathbf{s})\\) and \\(Z(\\mathbf{t})\\) are similar (spatially correlated).\nWhen the variance of the difference \\(Z(\\mathbf{s}) - Z(\\mathbf{t})\\) is relatively large, then \\(Z(\\mathbf{s})\\) and \\(Z(\\mathbf{t})\\) are less similar (closer to independence).\n\nThe variogram is a function of the underlying geostatistical process \\(Z\\). In practice, we only have access to \\(m\\) realisations of this process, and therefore we have to estimate the variogram. This is known as the empirical variogram.\nWe obtain this by computing the semi-variance for all possible pairs of observations: \\(\\gamma(\\mathbf{s}, \\mathbf{t}) = 0.5(Z(\\mathbf{s}) - Z(\\mathbf{t}))^2\\).\n\n\n\n\n\n\n Example\n\n\n\nTo illustrate how an empirical variogram is computed, consider the biomass density of the Pacific Cod in 2017 for the two highlighted locations below.\n\n\n\n\n\n\n\n\n\n\nWe can first compute the distance between the two locations using the standard Euclidean distance formula as\n\n\\[h = \\sqrt{(441.6 -481)^2 + (5743.4-5748.2)^2} \\approx  39 ~\\text{Km}\\]\n\nNext, we compute the semi-variance between the points using their observed values as \\[\\gamma(\\mathbf{s}, \\mathbf{t}) = 0.5(Z(\\mathbf{s}) - Z(\\mathbf{t}))^2 = 0.5(149.5 - 40.64)^2 = 5925.25\\]\nWe repeat this process for every possible pair of points, and plot \\(h\\) against \\(\\gamma(\\mathbf{s}, \\mathbf{t})\\) for each.\n\n\n\nTo make the variogram easier to use and interpret, we divide the distances into a set of discrete bins, and compute the average semi-variance in each. We compute this binned empirical variogram as:\n\\[\\gamma(\\mathbf{h}) = \\frac{1}{2N(h_k)}\\sum_{(\\mathbf{s},\\mathbf{t}) \\in N(h_k)}[z(\\mathbf{s}) - z(\\mathbf{t})]^2\\]\nWe can calculate the binned- empirical variogram for the data using variogram function from the gstat library. This plot shows the semi-variances for each pair of points. Lets compute a variogram for the biomass density of the Pacific Cod in 2017:\n\nlibrary(gstat)\n\npcod_sf_subset &lt;- pcod_sf %&gt;% filter(year ==2017)\n\nvgm1 &lt;- variogram(density~1, pcod_sf_subset)\nplot(vgm1)\n\n\n\n\n\n\n\n\nAssessing spatial dependence\nWe can construct null envelope based on permutations of the data values across the locations, i.e. envelopes built under the assumption of no spatial correlation. By overlapping these envelopes with the empirical variograms we can determine whether there is some spatial dependence in our data ,e.g. if our observed variograms falls outside of the envelopes constructed under spatial randomness.\nWe can construct permutation envelopes on the gstat empirical variogram using the envelope function from the variosig R package. Then we can visualize the results using the envplot function:\n\nlibrary(variosig)\n\nvarioEnv &lt;- envelope(vgm1,\n                     data = pcod_sf_subset,\n                     locations = st_coordinates(pcod_sf_subset),\n                     formula = density ~ 1,\n                     nsim = 499)\n\nenvplot(varioEnv)\n\n\n\n\n\n\n\n\n[1] \"There are 1 out of 15 variogram estimates outside the 95% envelope.\"\n\n\nIn this practical we will:\n\nExplore tools for spatial point pattern data wrangling and visualization.\n\nFirst, lets load some useful libraries:\n\n# For plotting\nlibrary(ggplot2)\nlibrary(scico) # for colouring palettes\n\n# Data manipulation\nlibrary(dplyr)",
    "crumbs": [
      "Home",
      "Practical 4"
    ]
  },
  {
    "objectID": "day2_practical_4.html#spatial-point-processes-data",
    "href": "day2_practical_4.html#spatial-point-processes-data",
    "title": "Practical 4",
    "section": "Spatial Point processes data",
    "text": "Spatial Point processes data\nIn point processes we measure the locations where events occur and the coordinates of such occurrences are our data.\nPoint process models are probabilistic models that describe the likelihood of patterns of points that represent the random location of some event. A spatial point process is a set of locations that have been generated by some form of stochastic (random) mechanism. In other words, the point process is a random variable operating in continuous space, and we observe realisations of this variable as point patterns across space (and/or time).\nConsider a fixed geographical region \\(A\\). The set of locations at which events occur are denoted \\(\\mathbf{s} = s_1,\\ldots,s_n\\). We let \\(N(A)\\) be the random variable which represents the number of events in region \\(A\\).\nOur primary interest is in measuring where events occur, so the locations are our data. We typically assume that a spatial point pattern is generated by an unique point process over the whole study area. This means that the delimitation of the study area will affect the observed point patters.\nThe observed distribution of points can be described based on the intensity of points within a delimited region. We can define the (first order) intensity of a point process as the expected number of events per unit area. This can also be thought of as a measure of the density of our points. In some cases, the intensity will be constant over space (homogeneous), while in other cases it can vary by location (inhomogeneous or heterogenous).\nIn the next example, we will explore tools for visualizing spatial point patterns. Specifically, we will map the spatial distribution of the Ringlet butterfly in Scotland’s Cairngorms National Park (CNP).\n\nBNM citizen science program\nCitizen science initiatives have become an important source of information in ecological research, offering large volumes of species distribution data collected by volunters for multiple taxonomic groups across wide spatial and temporal scales\nButterflies for the New Millennium (BNM) is a large-scale monitoring scheme launched in the earlies 70’s to keep track of butterflies’ populations in the UK. With over 12 million butterflies sighting and more than 10,000 volunteers, this recording scheme has proven to be a successful program that has been used to assess long-term changes in the distributions of UK butterfly species.\nHere we will focus on the distribution of the Ringlet butterfly species, which holds particular significance in environmental studies as one of the Habitat specialists species (UK Government, 2024). The data set consists of Ringlet butterfly presence-only records collected by volunteers in Scotland’s Cairngorms National Park (CNP).\nReading shapefiles into R\nFirst, we load the geographical region of interest which can be downloaded here (i.e., CNP boundaries). We can use thre st_read function from the sf library to load the .shp file by specifying the directory where you downloaded the files:\n\nlibrary(sf)\nshp_SGC &lt;-  st_read(\"datasets/SG_CairngormsNationalPark/SG_CairngormsNationalPark_2010.shp\",quiet =T)\n\nThen, we can use appropriate CRS for the UK (i.e., EPSG code: 27700) :\n\nshp_SGC &lt;- shp_SGC %&gt;% st_transform(crs = 27700)\nst_crs(shp_SGC)$units\n\n[1] \"m\"\n\n\nNotice that the spatial resolution is in meters. Let’s change the spatial units to km to make resulting distance/area values more intuitive to interpret:\n\nshp_SGC &lt;- st_transform(shp_SGC,gsub(\"units=m\",\"units=km\",st_crs(shp_SGC)$proj4string)) \nst_crs(shp_SGC)$units\n\n[1] \"km\"\n\n\nWe can then plot the CNP boundary as follows:\n\nggplot()+\n  geom_sf(data=shp_SGC)\n\n\n\n\n\n\n\n\nCreating sf spatial objects in R\nNow we will read the Ringlet butterfly records which can be downloaded below:\n Download data set \n\nringlett &lt;- read.csv(\"datasets/bnm_ringlett.csv\")\nhead(ringlett)\n\n         y         x\n1 57.58752 -2.712498\n2 54.97742 -3.274879\n3 54.89929 -3.771451\n4 55.40323 -5.737059\n5 54.91438 -3.959336\n6 55.87255 -4.167174\n\n\nThe data set contains the longitude latitude where an observation was made. We can convert this into a spatial sf object using the st_as_sf function by declaring the columns in our data that contain the spatial coordinates:\n\nringlett_sf &lt;- ringlett %&gt;% st_as_sf(coords = c(\"x\",\"y\"),crs = \"+proj=longlat +datum=WGS84\") \n\n\n\n\n\n\n\n Task\n\n\n\nWe have set standard WGS84 coordinates for the Ringlet butterfly occurrence records. Set the CRS to match the CRS used in shapefile. Then, produce a map of the CNP region with the projected observations overlayed.\n\n\nTake hint\n\nYou can use the st_transform() function to change the coordinates of an sf object (type ?st_transform for more details)/\n\n\n\n\nClick here to see the solution\n\n\nCode\nringlett_sf &lt;- ringlett_sf %&gt;%\n  st_transform(st_crs(shp_SGC))\n\nggplot()+\n  geom_sf(data=shp_SGC)+\n  geom_sf(data=ringlett_sf)\n\n\n\n\n\n\n\n\n\n\n\n\nWe can subset two sf objects with the same CRS in the same way as we subset a data frame in R. For example, if we want to subset the Ringlet butterfly occurrence records to those contained only within the CNP, we can type the following:\n\nringlett_CNP &lt;- ringlett_sf[shp_SGC,] # crop to mainland\n\nIf we plot the ringlett_CNP object along with the CNP boundary, we should then obtain a map of the occurrence records within the park:\n\nggplot()+\n  geom_sf(data=shp_SGC)+\n  geom_sf(data=ringlett_CNP)\n\n\n\n\n\n\n\n\nReading Raster Data\nWe can use the terra R package to read raster files. The Scotland_elev.tiff raster contains the output of a digital elevation model for Scotland:\n Download raster data \nOnce you download the raster you can read it using the rast function after specifying the path where the file has been stored. Then, we assign the same CRS as our data.\n\nlibrary(terra)\nelevation_r &lt;- rast(\"datasets/Scotland_elev.tiff\")\ncrs(elevation_r) = crs(shp_SGC)\nplot(elevation_r)\n\n\n\n\n\n\n\n\nWe can apply different R functions to our rasters. For example, we can scale the elevation values as follows:\n\nelevation_r &lt;- elevation_r %&gt;% scale()\n\nLastly, we can crop the raster to the boundaries of our region of interest. Let’s crop the elevation raster to the CNP area using the crop function:\n\nelev_CNP &lt;- terra::crop(elevation_r,shp_SGC,mask=T)\nplot(elev_CNP)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nUsing tidyterra and ggplot, produce a map of the elevation profile in the CNP and overlay the spatial point pattern of the Ringlet butterfly occurrence records. Use an appropriate colouring scheme for the elevation values. Do you see any pattern?\n\n\nTake hint\n\nYou can use the geom_spatraster() to add a raster layer to a ggplot object. Furthermore the scico library contains a nice range of coloring palettes you can choose, type scico_palette_show() to see the color palettes that are available.\n\n\n\n\nClick here to see the solution\n\n\nCode\nlibrary(scico)\n\nggplot()+ \n  tidyterra::geom_spatraster(data=elev_CNP)+\n  geom_sf(data=ringlett_CNP)+\n  scale_fill_scico(name = \"Elevation scaled\",\n                   palette = \"devon\",\n                   na.value = \"transparent\" )",
    "crumbs": [
      "Home",
      "Practical 4"
    ]
  },
  {
    "objectID": "day1_practical_2.html",
    "href": "day1_practical_2.html",
    "title": "Practical 2",
    "section": "",
    "text": "Aim of this practical:\nwe are going to learn:\nDownload Practical 2 R script",
    "crumbs": [
      "Home",
      "Practical 2"
    ]
  },
  {
    "objectID": "day1_practical_2.html#setting-priors-and-model-checking-for-linear-models",
    "href": "day1_practical_2.html#setting-priors-and-model-checking-for-linear-models",
    "title": "Practical 2",
    "section": "Setting priors and model checking for Linear Models",
    "text": "Setting priors and model checking for Linear Models\nIn this exercise we will:\n\nLearn how to set priors for linear effects \\(\\beta_0\\) and \\(\\beta_1\\)\nLearn how to set the priors for the hyperparameter \\(\\tau = 1/\\sigma^2\\).\nVisualize marginal posterior distributions\n\nStart by loading useful libraries:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nRecall a simple linear regression model with Gaussian observations \\[\ny_i\\sim\\mathcal{N}(\\mu_i, \\sigma^2), \\qquad i = 1,\\dots,N\n\\]\nwhere \\(\\sigma^2\\) is the observation error, and the mean parameter \\(\\mu_i\\) is linked to the linear predictor through an identity function: \\[\n\\eta_i = \\mu_i = \\beta_0 + \\beta_1 x_i\n\\] where \\(x_i\\) is a covariate and \\(\\beta_0, \\beta_1\\) are parameters to be estimated. In INLA, we assume that the model is a latent Gaussian model, i.e., we have to assign \\(\\beta_0\\) and \\(\\beta_1\\) a Gaussian prior. For the precision hyperparameter \\(\\tau = 1/\\sigma^2\\) a typical prior choice is a \\(\\text{Gamma}(a,b)\\) prior.\nIn R-INLA, the default choice of priors for each \\(\\beta\\) is\n\\[\n\\beta \\sim \\mathcal{N}(0,10^3).\n\\]\nand the prior for the variance parameter in terms of the log precision is\n\\[ \\log(\\tau) \\sim \\mathrm{logGamma}(1,5 \\times 10^{-5}) \\]\n\n\n\n\n\n\nNote\n\n\n\nIf your model uses the default intercept construction (i.e., Intercept(1) in the linear predictor) INLA will assign a default \\(\\mathcal{N} (0,0)\\) prior to it.\n\n\nLets see how can we change the default priors using some simulated data\n\nSimulate example data\nWe simulate data from a simple linear regression model\n\n\nCode\nbeta = c(2,0.5)\nsd_error = 0.1\n\nn = 100\nx = rnorm(n)\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error)\n\ndf = data.frame(y = y, x = x)  \n\n\n\n\nFitting the linear regression model with inlabru\nNow we fit a simple linear regression model in inalbru by defining (1) the model components, (2) the linear predictor and (3) the likelihood.\n\n# Model components\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n# Linear predictor\nformula = y ~ Intercept + beta_1\n# Observational model likelihood\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n# Fit the Model\nfit.lm = bru(cmp, lik)\n\n\n\nChange the prior distributions\nUntil now, we have used the default priors for both the precision \\(\\tau\\) and the fixed effects \\(\\beta_0\\) and \\(\\beta_1\\). Let’s see how to customize these.\nTo check which priors are used in a fitted model one can use the function inla.prior.used()\n\ninla.priors.used(fit.lm)\n\nsection=[family]\n    tag=[INLA.Data1] component=[gaussian]\n        theta1:\n            parameter=[log precision]\n            prior=[loggamma]\n            param=[1e+00, 5e-05]\nsection=[linear]\n    tag=[beta_0] component=[beta_0]\n        beta:\n            parameter=[beta_0]\n            prior=[normal]\n            param=[0.000, 0.001]\n    tag=[beta_1] component=[beta_1]\n        beta:\n            parameter=[beta_1]\n            prior=[normal]\n            param=[0.000, 0.001]\n\n\nFrom the output we see that the precision for the observation \\(\\tau\\sim\\text{Gamma}(1e+00,5e-05)\\) while \\(\\beta_0\\) and \\(\\beta_1\\) have precision 0.001, that is variance \\(1/0.001\\).\nChange the precision for the linear effects\nThe precision for linear effects is set in the component definition. For example, if we want to increase the precision to 0.01 for \\(\\beta_0\\) we define the relative components as:\n\ncmp1 =  ~-1 +  beta_0(1, prec.linear = 0.01) + beta_1(x, model = \"linear\")\n\n\n\n\n\n\n\n Task\n\n\n\nRun the model again using 0.1 as default precision for both the intercept and the slope parameter.\n\n\n\nClick here to see the solution\n\ncmp2 =  ~ -1 + \n          beta_0(1, prec.linear = 0.1) + \n          beta_1(x, model = \"linear\", prec.linear = 0.1)\n\nlm.fit2 = bru(cmp2, lik) \n\n\nNote that we can use the same observation model as before since both the formula and the dataset are unchanged.\n\n\nChange the prior for the precision of the observation error \\(\\tau\\)\nPriors on the hyperparameters of the observation model must be passed by defining argument hyper within control.family in the call to the bru_obs() function.\n\n# First we define the logGamma (0.01,0.01) prior \n\nprec.tau &lt;- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(0.01, 0.01))) # prior values\n\nlik2 =  bru_obs(formula = y ~.,\n                family = \"gaussian\",\n                data = df,\n                control.family = list(hyper = prec.tau))\n\nfit.lm2 = bru(cmp2, lik2) \n\nThe names of the priors available in R-INLA can be seen with names(inla.models()$prior)\n\n\nVisualizing the posterior marginals\nPosterior marginal distributions of the ﬁxed effects parameters and the hyperparameters can be visualized using the plot() function by calling the name of the component. For example, if want to visualize the posterior density of the intercept \\(\\beta_0\\) we can type:\n\n\nCode\nplot(fit.lm, \"beta_0\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nPlot the posterior marginals for \\(\\beta_1\\) and for the precision of the observation error \\(\\pi(\\tau|y)\\)\n\n\nTake hint\n\nSee the summary() output to check the names for the different model components.\n\n\n\n\nClick here to see the solution\n\n\nCode\nplot(fit.lm, \"beta_1\") +\nplot(fit.lm, \"Precision for the Gaussian observations\")",
    "crumbs": [
      "Home",
      "Practical 2"
    ]
  },
  {
    "objectID": "day1_practical_2.html#sec-llm_fish",
    "href": "day1_practical_2.html#sec-llm_fish",
    "title": "Practical 2",
    "section": "Linear Mixed Model for fish weight-length relationship",
    "text": "Linear Mixed Model for fish weight-length relationship\nIn this exercise we will:\n\nPlot random effects of a LMM\nCompute posterior densities and summaries for the variance components\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nIn this exercise, we will use a subset of the Pygmy Whitefish (Prosopium coulterii) dataset from the FSAdata R package, containing biological data collected in 2001 from Dina Lake, British Columbia.\n Download data set \nThe data set contains the following information:\n\nnet_noUnique net identification number\nwt Fish weight (g)\ntl Total fish length (cm)\nsex Sex code (F=Female, M = Male)\n\nWe can visualize the distribution of the response (weight) across the nets split by sex as follows:\n\nPygmyWFBC &lt;- read.csv(\"datasets/PygmyWFBC.csv\")\n\nggplot(PygmyWFBC, aes(x = factor(net_no), y = wt,fill = sex)) + \n  geom_boxplot() + \n  labs(y=\"Weight (g)\",x = \"Net no.\")\n\n\n\n\n\n\n\n\nSuppose we are interested in modelling the weight-length relationship for captured fish. The exploratory plot suggest some important variability in this relationship, potentially attributable to differences among sampling nets deployed across various sites in the Dina Lake.\nTo account for this between-net variability, we model net as a random effect using the following linear mixed model:\n\\[\n\\begin{aligned}\ny_{ij} &\\sim\\mathcal{N}(\\mu_{ij}, \\sigma_e^2), \\qquad i = 1,\\dots,a \\qquad j = 1,\\ldots,n \\\\\n\\eta_{ij} &= \\mu_{ij} = \\beta_0 + \\beta_1 \\times \\text{length}_j + \\beta_2 \\times \\mathbb{I}(\\mathrm{Sex}_{ij}=\\mathrm{M}) +  u_i \\\\\nu_i &\\sim \\mathcal{N}(0,\\sigma^2_u)\n\\end{aligned}\n\\]\nwhere:\n\n\\(y_{ij}\\) is the weight of the \\(j\\)-th fish from net \\(i\\)\n\\(\\text{length}_{ij}\\) is the corresponding fish length\n\\(\\mathbb{I}(\\text{Sex}_{ij} = \\text{M})\\) is an indicator/dummy such that for the ith net \\[\n\\mathbb{I}(\\mathrm{Sex}_{ij}) \\begin{cases}1 & \\text{if the } j \\text{th fish is Male} \\\\0 & \\text{otherwise} \\end{cases}\n\\]\n\\(u_i\\) represents the random intercept for net \\(i\\)\n\\(\\sigma_u^2\\) and \\(\\sigma_\\epsilon^2\\) are the between-net and residual variances, respectively\n\nTo run this model ininlabru we first need to create our sex dummy variable :\n\nPygmyWFBC$sex_M &lt;- ifelse(PygmyWFBC$sex==\"F\",0,1)\n\ninlabru will treat 0 as the reference category (i.e., the intercept \\(\\beta_0\\) will represent the baseline weight for females ). Now we can define the model component, the likelihood and fit the model.\n\ncmp =  ~ -1 + sex_M +  beta_0(1)  + beta_1(tl, model = \"linear\") +   net_eff(net_no, model = \"iid\")\n\nlik =  bru_obs(formula = wt ~ .,\n            family = \"gaussian\",\n            data = PygmyWFBC)\n\nfit = bru(cmp, lik)\n\nsummary(fit)\n\ninlabru version: 2.13.0\nINLA version: 25.08.21-1\nComponents:\nsex_M: main = linear(sex_M), group = exchangeable(1L), replicate = iid(1L), NULL\nbeta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\nbeta_1: main = linear(tl), group = exchangeable(1L), replicate = iid(1L), NULL\nnet_eff: main = iid(net_no), group = exchangeable(1L), replicate = iid(1L), NULL\nObservation models:\n  Family: 'gaussian'\n    Tag: &lt;No tag&gt;\n    Data class: 'data.frame'\n    Response class: 'numeric'\n    Predictor: wt ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[sex_M, beta_0, beta_1, net_eff], latent[]\nTime used:\n    Pre = 0.369, Running = 0.397, Post = 0.162, Total = 0.927 \nFixed effects:\n          mean    sd 0.025quant 0.5quant 0.975quant    mode kld\nsex_M   -1.106 0.218     -1.534   -1.106     -0.678  -1.106   0\nbeta_0 -15.816 0.870    -17.516  -15.819    -14.098 -15.819   0\nbeta_1   2.555 0.072      2.414    2.555      2.696   2.555   0\n\nRandom effects:\n  Name    Model\n    net_eff IID model\n\nModel hyperparameters:\n                                         mean    sd 0.025quant 0.5quant\nPrecision for the Gaussian observations 0.475 0.044      0.393    0.473\nPrecision for net_eff                   2.146 1.311      0.569    1.839\n                                        0.975quant mode\nPrecision for the Gaussian observations      0.567 0.47\nPrecision for net_eff                        5.521 1.32\n\nMarginal log-Likelihood:  -467.54 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\nFor interpretability, we could have centered the predictors, but our primary focus here is on estimating the variance components of the mixed model.\nWe can plot the posterior density of the nets random intercept as follows:\n\nplot(fit,\"net_eff\")\n\n\n\n\n\n\n\n\nFor theoretical and computational purposes, INLA works with the precision which is the inverse of the variance. To obtain the posterior summaries on the SDs scale we can sample from the posterior distribution for the precision while back-transforming the samples and then computing the summary statistics. Transforming the samples is necessary because some quantities such as the mean and mode are not invariant to monotone transformation; alternatively we can use some of the in-built R-INLA functions to achieve this (see supplementary note).\nWe use the inla.hyperpar.sample function to draw samples from the approximated joint posterior for the hyperparameters, then invert them to get variances and lastly compute the mean, std. dev., quantiles, etc.\n\nsampvars &lt;- 1/inla.hyperpar.sample(1000,fit,improve.marginals = T)\n\ncolnames(sampvars) &lt;- c(\"Error variance\",\"Between-net Variance\")\n\napply(sampvars,2,\n      function(x) c(\"mean\"=mean(x),\n                    \"std.dev\" = sd(x),\n                    quantile(x,c(0.025,0.5,0.975))))\n\n        Error variance Between-net Variance\nmean         2.1248605            0.6492665\nstd.dev      0.1981528            0.4112481\n2.5%         1.7665997            0.1837601\n50%          2.1144330            0.5431690\n97.5%        2.5407998            1.7322448\n\n\n\n\n\n\n\n\n Task\n\n\n\nAnother useful quantity we can compute is the intraclass correlation coefﬁcient (ICC) which help us determine how much the response varies within groups compared to between groups. The intraclass correlation coefﬁcient is defined as:\n\\[\n\\text{ICC} = \\frac{\\sigma^2_u}{\\sigma^2_u + \\sigma^2_e}\n\\]\nCompute the median, and quantiles for the ICC using the posterior samples we draw for \\(\\sigma^2_e\\) and \\(\\sigma^2_u\\).\n\n\nTake hint\n\nThe rowSums function can be used to compute \\(\\sigma^2_{u,s} + \\sigma^2_{e,s}\\) for the \\(s\\)th posterior draw.\n\n\n\n\nClick here to see the solution\n\n\nCode\nsampicc &lt;- sampvars[,2]/(rowSums(sampvars))\nquantile(sampicc, c(0.025,0.5,0.975))\n\n\n     2.5%       50%     97.5% \n0.0798669 0.2044105 0.4486958 \n\n\n\n\n\n\n\n\n\n\n\nSupplementary Material\n\n\n\nThe marginal densities for the hyper parameters can be also found by callinginlabru_model$marginals.hyperpar. We can then apply a transformation using the inla.tmarginal function to transform the precision posterior distributions.\n\nvar_e &lt;- fit$marginals.hyperpar$`Precision for the Gaussian observations` %&gt;%\n  inla.tmarginal(function(x) 1/x,.) \n\nvar_u &lt;- fit$marginals.hyperpar$`Precision for net_eff` %&gt;%\n  inla.tmarginal(function(x) 1/x,.) \n\nThe marginal densities for the hyper parameters can be found with inlabru_model$marginals.hyperpar, then we can apply a transformation using the inla.tmarginal function to transform the precision posterior distributions. Then, we can compute posterior summaries using inla.zmarginal function as follows:\n\npost_var_summaries &lt;- cbind( inla.zmarginal(var_e,silent = T),\n                             inla.zmarginal(var_u,silent = T))\ncolnames(post_var_summaries) &lt;- c(\"sigma_e\",\"sigma_u\")\npost_var_summaries\n\n           sigma_e   sigma_u  \nmean       2.124441  0.6486006\nsd         0.1980652 0.4115367\nquant0.025 1.764596  0.1816268\nquant0.25  1.985364  0.3691971\nquant0.5   2.113691  0.5415111\nquant0.75  2.251954  0.8051964\nquant0.975 2.541942  1.738421",
    "crumbs": [
      "Home",
      "Practical 2"
    ]
  },
  {
    "objectID": "day1_practical_2.html#hierarchical-generalised-additive-mixed-models-with-inlabru",
    "href": "day1_practical_2.html#hierarchical-generalised-additive-mixed-models-with-inlabru",
    "title": "Practical 2",
    "section": "Hierarchical generalised additive mixed models with inlabru",
    "text": "Hierarchical generalised additive mixed models with inlabru\nIn this excercise we will:\n\nFit an hierarchical generalised additive mixed models\nFit a model with a global smooth term\nFit a model with global and group-level smooth terms\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nThe oceans represent Earth’s largest habitat, with life distributed unevenly across depths primarily due to variations in light, temperature, and pressure. Biomass generally decreases with depth, though complex factors like water density layers create non-linear patterns. A significant portion of deep-sea organisms exhibit bioluminescence, which scientists measure using specialized equipment like free-fall camera systems to profile vertical distribution.\nIn this exercise, we analyze the ISIT dataset, which contains bioluminescence measurements from the northeast Atlantic Ocean. This dataset was previously examined in Zuur et al. (2009) and Gillibrand et al. (2007) and consists of observations collected across a depth gradient (0–4,800 m) during spring and summer cruises in 2001–2002 using an ISIT free-fall profiler.\n Download data set \nThe focus of this excersice will be on characterizing seasonal variation in the relationship between bioluminescent source density (sources m\\(^{2}\\)) and depth. We begin by exploring distribution patterns of pelagic bioluminescence through source-depth profiles, with each profile representing measurements from an individual sampling station. These profiles will be grouped by month to examine temporal patterns in the water column’s bioluminescent structure.\n\nPlotR-Code\n\n\n\n\n\n\n\nSource–depth proﬁles per month. Each line represents a station.\n\n\n\n\n\n\n\nicit &lt;- read.csv(\"datasets/ISIT.csv\")\n\nicit$Month &lt;- as.factor(icit$Month)\nlevels(icit$Month) &lt;- month.abb[unique(icit$Month)]\n\nggplot(icit,aes(x=SampleDepth,y= Sources,\n                group=as.factor(Station),\n                colour=as.factor(Station)))+\n  geom_line()+\n  facet_wrap(~Month)+\n  theme(legend.position = \"none\")\n\n\n\n\nAs expected, there seems to be a non-linear depth effect with some important variability across months.\n\nFitting a global smoother\nWe could begin analysing these data with a global smoother and a random intercept for each month. Thus, a possible model is of the form:\n\\[\nS_{is} = \\beta_0 + f(\\text{Depth})_s + \\text{Month}_i +  \\epsilon_{is} ~~\\text{such that}~ \\epsilon \\sim \\mathcal{N}(0,\\sigma^2_e);~ \\text{Month} \\sim \\mathrm{N}(0,\\sigma^2_m).\n\\]\nwhere the source during month \\(i\\) at depth \\(s\\), \\(S_{is}\\), are modelled as smoothing function of depth and a month effect. The model has one smoothing curve for all months and can be fitted in inlabru as follows:\n\nicit$Month_id &lt;- as.numeric(icit$Month) # numeric index for the i-th month\n\ncmp_g =  ~ -1+ beta_0(1) + \n  smooth_g(SampleDepth, model = \"rw1\") + \n  month_reff(Month_id, model = \"iid\") \n\nlik =  bru_obs(formula = Sources ~.,\n               family = \"gaussian\",\n               data = icit)\n\nfit_g = bru(cmp_g, lik)\n\nsummary(fit_g)\n\ninlabru version: 2.13.0\nINLA version: 25.08.21-1\nComponents:\nbeta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\nsmooth_g: main = rw1(SampleDepth), group = exchangeable(1L), replicate = iid(1L), NULL\nmonth_reff: main = iid(Month_id), group = exchangeable(1L), replicate = iid(1L), NULL\nObservation models:\n  Family: 'gaussian'\n    Tag: &lt;No tag&gt;\n    Data class: 'data.frame'\n    Response class: 'numeric'\n    Predictor: Sources ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[beta_0, smooth_g, month_reff], latent[]\nTime used:\n    Pre = 0.397, Running = 0.832, Post = 0.252, Total = 1.48 \nFixed effects:\n         mean    sd 0.025quant 0.5quant 0.975quant   mode kld\nbeta_0 10.017 1.614      6.692   10.025     13.291 10.023   0\n\nRandom effects:\n  Name    Model\n    smooth_g RW1 model\n   month_reff IID model\n\nModel hyperparameters:\n                                          mean    sd 0.025quant 0.5quant\nPrecision for the Gaussian observations  0.024 0.001      0.021    0.024\nPrecision for smooth_g                  21.173 5.428     12.387   20.541\nPrecision for month_reff                 0.138 0.089      0.030    0.117\n                                        0.975quant   mode\nPrecision for the Gaussian observations      0.026  0.024\nPrecision for smooth_g                      33.606 19.363\nPrecision for month_reff                     0.366  0.078\n\nMarginal log-Likelihood:  -2217.28 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\nWe can plot the smoother marginal effect as follows:\n\n\nGlobal smoother marginal effect\ndata.frame(fit_g$summary.random$smooth_g) %&gt;% \n  ggplot() + \n  geom_ribbon(aes(ID,ymin = X0.025quant, ymax= X0.975quant), alpha = 0.5) + \n  geom_line(aes(ID,mean)) + \n  xlab(\"covariate\") + ylab(\"smooth effect\")\n\n\n\n\n\n\n\n\n\nYou might want to have a smoother function by placing a RW2 prior. Unfortunately, this assumes that all the knots are regularly spaced and some depth values are too close to be used for building the RW2 priors. For the case, it is possible to use function inla.group() to bin data into groups according to the values of the covariate:\n\nicit$depth_grouped &lt;- inla.group(icit$SampleDepth,n=50)\n\n\n\n\n\n\n\n Task\n\n\n\nRe-run the global smoother model using a RW2 prior for the depth smoother and compare your results with the RW1 model.\n\n\nTake hint\n\nUse the depth_grouped covariate to define the smoother.\n\n\n\n\nClick here to see the solution\n\ncmp_rw2 =  ~ -1+ beta_0(1) + \n  smooth_g(depth_grouped, model = \"rw2\") + \n  month_reff(Month_id, model = \"iid\") \n\nlik =  bru_obs(formula = Sources ~.,\n               family = \"gaussian\",\n               data = icit)\n\nfit_rw2 = bru(cmp_rw2, lik)\n\ndata.frame(fit_rw2$summary.random$smooth_g) %&gt;% \n  ggplot() + \n  geom_ribbon(aes(ID,ymin = X0.025quant, ymax= X0.975quant), alpha = 0.5) + \n  geom_line(aes(ID,mean)) + \n  xlab(\"covariate\") + ylab(\"Global smooth effect\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting group-level smoothers\nHere we fit a model where each month is allowed to have its own smoother for depth, i.e., \\(f_i(\\text{Depth})_s\\). The model structure is given by:\n\\[\nS_{is} = \\beta_0 + f_i(\\text{Depth})_s + \\text{Month}_i +  \\epsilon_{is}.\n\\]\nNotice the only different between the global smoother model (Model G) and the group level model (Model GS) is the indexing of the smooth function for depth. We can fit a group-level smoother using the group argument within the model component as follows:\n\ncmp_gs =  ~ -1+ beta_0(1) +\n  smooth_g(SampleDepth, model = \"rw1\") + \n  month_reff(Month_id, model = \"iid\")+\n  smooth_loc(SampleDepth, model = \"rw1\", group = Month_id)\n\nThen, we simply run the model (since the observational model has not changed -only the model components have):\n\nfit_gs = bru(cmp_gs, lik) \n\nLastly, we can generate model predictions using the predict function.\n\npred_gs = predict(fit_gs, icit, ~ (beta_0 + smooth_g+month_reff+smooth_loc))\n\nThen, we plot the predicted mean values with their corresponding 95% CrIs.\n\n\nGlobal + group smoother predictions\nggplot(pred_gs,aes(y=mean,x=SampleDepth))+\n  geom_ribbon(aes(SampleDepth,ymin = q0.025, ymax= q0.975), alpha = 0.5,fill=\"tomato\") +\n  geom_line()+\n  geom_point(aes(x=SampleDepth,y=Sources ),alpha=0.25,col=\"grey40\")+\n  facet_wrap(~Month)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nRe-fit the model GS without the global smoother. By omitting the global smoother, we do not longer force group-level smooths to follow a shared pattern, which is useful when groups may differ substantially from a common trend.\n\n\nTake hint\n\nYou only need to modify the model components in cmp_gs\nAdd hint details here…\n\n\n\n\nClick here to see the solution\n\n\nCode\ncmp_s =  ~ -1+ beta_0(1) +\n  month_reff(Month_id, model = \"iid\")+\n  smooth_loc(SampleDepth, model = \"rw1\", group = Month_id)\n\nfit_s = bru(cmp_s, lik) \n\npred_s = predict(fit_s, icit, ~ (beta_0 +month_reff+smooth_loc))\n\nggplot(pred_s,aes(y=mean,x=SampleDepth))+\n  geom_ribbon(aes(SampleDepth,ymin = q0.025, ymax= q0.975), alpha = 0.5,fill=\"tomato\") +\n  geom_line()+\n  geom_point(aes(x=SampleDepth,y=Sources ),alpha=0.25,col=\"grey40\")+\n  facet_wrap(~Month)",
    "crumbs": [
      "Home",
      "Practical 2"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "inlabru Workshop",
    "section": "",
    "text": "Welcome to the course!\n\nWelcome to the inlabru workshop!\nThe goal for the workshop is to introduce Bayesian statistics implemented using the inlabru package.\nThe workshop is intended for … No knowledge of R-INLA is required.\nWorkshop materials in the github repository inlabru-workshop\n\n\n\nLearning Objectives for the workshop\nAt the end of the workshop, participants will be able to:\n\nTEST\nILO2\nILO3\n\nIntended audience and level: The tutorial is intended for … No knowledge of R-INLA is required.\n\n\nSchedule Program\n\nDay 1Day 2Day 3Day 4Day 5\n\n\n\n\n\nTime\nTopic\n\n\n\n\n10:00 - 10:30\nICES informative session\n\n\n10:30 - 11:30\nSession 1: Introduction to inlabru\n\n\n11:30 - 13:00\nPractical Session 1\n\n\n13:00 - 14:30\nLunch break 🍽️\n\n\n14:30 - 15:30\nSession 2: Latent Gaussian Models and INLA\n\n\n15:30 - 15:45\nCoffee Break ☕\n\n\n15:45 - 16:45\nPractical Session 2\n\n\n16:45 - 17:00\nwrap-up and outlook\n\n\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 10:00\nSession 3: Temporal modelling and smoothing part 1\n\n\n10:00 - 10:30\nSnack 🥙\n\n\n10:30 - 11:30\nSession 4: Temporal modelling and smoothing part 2\n\n\n11:30 - 13:00\nPractical Session 3\n\n\n13:00 - 14:30\nLunch break 🍽️\n\n\n14:30 - 15:30\nSession 5: Introduction to Spatial Statistics\n\n\n15:35 - 15:45\nCoffee Break ☕\n\n\n15:45 - 16:45\nPractical Session 4\n\n\n16:45 - 17:00\nwrap-up and outlook\n\n\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 10:00\nSession 6: Areal Processes\n\n\n10:00 - 10:30\nSnack 🥙\n\n\n10:30 - 11:30\nSession 7: Geostatistics\n\n\n11:30 - 13:00\nPractical Session 5\n\n\n13:00 - 14:30\nLunch break 🍽️\n\n\n14:30 - 15:30\nSession 8: Spatial Point processes\n\n\n15:35 - 15:45\nCoffee Break ☕\n\n\n15:45 - 16:45\nPractical Session 5 continued\n\n\n16:45 - 17:00\nwrap-up and outlook\n\n\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 10:00\nSession 9: Spatiotemporal models\n\n\n10:00 - 10:30\nSnack 🥙\n\n\n10:30 - 11:30\nSession 10: Model comparison and evaluation\n\n\n11:30 - 13:00\nPractical Session 6\n\n\n13:00 - 14:30\nLunch break 🍽️\n\n\n14:30 - 15:30\nSession 11: Multilikelihoods/joint likelihood\n\n\n15:35 - 15:45\nCoffee Break ☕\n\n\n15:45 - 16:45\nPractical Session 7\n\n\n16:45 - 17:00\nwrap-up and outlook\n\n\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 10:00\nSession 12: Zero inflated models\n\n\n10:00 - 10:30\nSnack 🥙\n\n\n10:30 - 11:30\nSession 13: Complex observational processes: Distance Sampling\n\n\n11:30 - 13:00\nPractical Session 8\n\n\n13:00 - 13:15\nCoffee Break ☕\n\n\n13:15 - 14:00\nClosing session\n\n\n\n\n\n\n\n\nIn preparation for the workshop\nParticipants are required to follow the next steps before the day of the workshop:\n\nInstall R-INLA\nInstall inlabru (available from CRAN)\n\n\n# Enable universe(s) by inlabru-org\noptions(repos = c(\n  inlabruorg = \"https://inlabru-org.r-universe.dev\",\n  INLA = \"https://inla.r-inla-download.org/R/testing\",\n  CRAN = \"https://cloud.r-project.org\"\n))\n\n# Install some packages\ninstall.packages(\"inlabru\")\n\n\nMake sure you have the latest R-INLA, inlabru and R versions installed.\nInstall the following libraries:\n\n\n\ninstall.packages(c(\n  \"CARBayesdata\",\n  \"DAAG\",\n  \"dplyr\",\n  \"FSAdata\",\n  \"ggplot2\",\n  \"gstat\",\n  \"gt\",\n  \"lubridate\",\n  \"magrittr\",\n  \"mapview\",\n  \"patchwork\",\n  \"scico\",\n  \"sdmTMB\",\n  \"sf\",\n  \"spatstat\",\n  \"spdep\",\n  \"terra\",\n  \"tidyr\",\n  \"tidyterra\",\n  \"tidyverse\",\n  \"variosig\"\n))",
    "crumbs": [
      "Home",
      "`inlabru` Workshop"
    ]
  },
  {
    "objectID": "day1_practical.html",
    "href": "day1_practical.html",
    "title": "Practical 1",
    "section": "",
    "text": "Aim of this practical:\nIn this first practical we are going to look at some simple models\nwe are going to learn:\nDownload Practical 1 R script",
    "crumbs": [
      "Home",
      "Practical 1"
    ]
  },
  {
    "objectID": "day1_practical.html#sec-linmodel",
    "href": "day1_practical.html#sec-linmodel",
    "title": "Practical 1",
    "section": "Linear Model",
    "text": "Linear Model\nIn this practical we will:\n\nSimulate Gaussian data\nLearn how to fit a linear model with inlabru\nGenerate predictions from the model\n\nStart by loading useful libraries:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n# load some libraries to generate nice plots\nlibrary(scico)\n\nAs our first example we consider a simple linear regression model with Gaussian observations \\[\ny_i\\sim\\mathcal{N}(\\mu_i, \\sigma^2), \\qquad i = 1,\\dots,N\n\\]\nwhere \\(\\sigma^2\\) is the observation error, and the mean parameter \\(\\mu_i\\) is linked to the linear predictor (\\(\\eta_i\\)) through an identity function: \\[\n\\eta_i = \\mu_i = \\beta_0 + \\beta_1 x_i\n\\] where \\(x_i\\) is a covariate and \\(\\beta_0, \\beta_1\\) are parameters to be estimated. We assign \\(\\beta_0\\) and \\(\\beta_1\\) a vague Gaussian prior.\nTo finalize the Bayesian model we assign a \\(\\text{Gamma}(a,b)\\) prior to the precision parameter \\(\\tau = 1/\\sigma^2\\) and two independent Gaussian priors with mean \\(0\\) and precision \\(\\tau_{\\beta}\\) to the regression parameters \\(\\beta_0\\) and \\(\\beta_1\\) (we will use the default prior settings in INLA for now).\n\n\n\n\n\n\n Question\n\n\n\nWhat is the dimension of the hyperparameter vector and latent Gaussian field?\n\n\nAnswer\n\nThe hyperparameter vector has dimension 1, \\(\\pmb{\\theta} = (\\tau)\\) while the latent Gaussian field \\(\\pmb{u} = (\\beta_0, \\beta_1)\\) has dimension 2, \\(0\\) mean, and sparse precision matrix:\n\\[\n\\pmb{Q} = \\begin{bmatrix}\n\\tau_{\\beta_0} & 0\\\\\n0 & \\tau_{\\beta_1}\n\\end{bmatrix}\n\\] Note that, since \\(\\beta_0\\) and \\(\\beta_1\\) are fixed effects, the precision parameters \\(\\tau_{\\beta_0}\\) and \\(\\tau_{\\beta_1}\\) are fixed.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can write the linear predictor vector \\(\\pmb{\\eta} = (\\eta_1,\\dots,\\eta_N)\\) as\n\\[\n\\pmb{\\eta} = \\pmb{A}\\pmb{u} = \\pmb{A}_1\\pmb{u}_1 + \\pmb{A}_2\\pmb{u}_2 = \\begin{bmatrix}\n1 \\\\\n1\\\\\n\\vdots\\\\\n1\n\\end{bmatrix} \\beta_0 + \\begin{bmatrix}\nx_1 \\\\\nx_2\\\\\n\\vdots\\\\\nx_N\n\\end{bmatrix} \\beta_1\n\\]\nOur linear predictor consists then of two components: an intercept and a slope.\n\n\n\nSimulate example data\nFirst, we simulate data from the model\n\\[\ny_i\\sim\\mathcal{N}(\\eta_i,0.1^2), \\ i = 1,\\dots,100\n\\]\nwith\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_i\n\\]\nwhere \\(\\beta_0 = 2\\), \\(\\beta_1 = 0.5\\) and the values of the covariate \\(x\\) are generated from an Uniform(0,1) distribution. The simulated response and covariate data are then saved in a data.frame object.\n\n\nSimulate Data from a LM\nbeta = c(2,0.5)\nsd_error = 0.1\n\nn = 100\nx = rnorm(n)\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error)\n\ndf = data.frame(y = y, x = x)  \n\n\n\n\nFitting a linear regression model with inlabru\n\nDefining model components\nThe model has two parameters to be estimated \\(\\beta_1\\) and \\(\\beta_2\\). We need to define the two corresponding model components:\n\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n\nThe cmp object is here used to define model components. We can give them any useful names we like, in this case, beta_0 and beta_1.\n\n\n\n\n\n\nNote\n\n\n\nNote that we have excluded the default Intercept term in the model by typing -1 in the model components. However, inlabru has automatic intercept that can be called by typing Intercept() , which is one of inlabru special names and it is used to define a global intercept, e.g.\n\ncmp =  ~  Intercept(1) + beta_1(x, model = \"linear\")\n\n\n\nObservation model construction\nThe next step is to construct the observation model by defining the model likelihood. The most important inputs here are the formula, the family and the data.\nThe formula defines how the components should be combined in order to define the model predictor.\n\nformula = y ~ beta_0 + beta_1\n\n\n\n\n\n\n\nNote\n\n\n\nIn this case we can also use the shortcut formula = y ~ .. This will tell inlabru that the model is linear and that it is not necessary to linearize the model and assess convergence.\n\n\nThe likelihood is defined using the bru_obs() function as follows:\n\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n\nFit the model\nWe fit the model using the bru() functions which takes as input the components and the observation model:\n\nfit.lm = bru(cmp, lik)\n\nExtract results\nThe summary() function will give access to some basic information about model fit and estimates\n\nsummary(fit.lm)\n## inlabru version: 2.13.0\n## INLA version: 25.08.21-1\n## Components:\n## beta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\n## beta_1: main = linear(x), group = exchangeable(1L), replicate = iid(1L), NULL\n## Observation models:\n##   Family: 'gaussian'\n##     Tag: &lt;No tag&gt;\n##     Data class: 'data.frame'\n##     Response class: 'numeric'\n##     Predictor: y ~ .\n##     Additive/Linear: TRUE/TRUE\n##     Used components: effects[beta_0, beta_1], latent[]\n## Time used:\n##     Pre = 1.05, Running = 0.526, Post = 0.109, Total = 1.68 \n## Fixed effects:\n##         mean   sd 0.025quant 0.5quant 0.975quant  mode kld\n## beta_0 1.988 0.01      1.969    1.988      2.006 1.988   0\n## beta_1 0.496 0.01      0.477    0.496      0.515 0.496   0\n## \n## Model hyperparameters:\n##                                           mean    sd 0.025quant 0.5quant\n## Precision for the Gaussian observations 111.64 15.79      82.89   110.89\n##                                         0.975quant   mode\n## Precision for the Gaussian observations     144.66 109.41\n## \n## Marginal log-Likelihood:  71.42 \n##  is computed \n## Posterior summaries for the linear predictor and the fitted values are computed\n## (Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\nWe can see that both the intercept and slope and the error precision are correctly estimated.\n\n\nGenerate model predictions\n\nNow we can take the fitted bru object and use the predict function to produce predictions for \\(\\mu\\) given a new set of values for the model covariates or the original values used for the model fit\n\nnew_data = data.frame(x = c(df$x, runif(10)),\n                      y = c(df$y, rep(NA,10)))\npred = predict(fit.lm, new_data, ~ beta_0 + beta_1,\n               n.samples = 1000)\n\nThe predict function generate samples from the fitted model. In this case we set the number of samples to 1000.\n\nPlotR Code\n\n\n\n\n\n\n\nData and 95% credible intervals\n\n\n\n\n\n\n\n\nCode\npred %&gt;% ggplot() + \n  geom_point(aes(x,y), alpha = 0.3) +\n  geom_line(aes(x,mean)) +\n  geom_line(aes(x, q0.025), linetype = \"dashed\")+\n  geom_line(aes(x, q0.975), linetype = \"dashed\")+\n  xlab(\"Covariate\") + ylab(\"Observations\")\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nGenerate predictions for a new observation with \\(x_0 = 0.45\\)\n\n\nTake hint\n\nYou can create a new data frame containing the new observation \\(x_0\\) and then use the predict function.\n\n\n\n\nClick here to see the solution\n\n\nCode\nnew_data = data.frame(x = 0.45)\npred = predict(fit.lm, new_data, ~ beta_0 + beta_1,\n               n.samples = 1000)",
    "crumbs": [
      "Home",
      "Practical 1"
    ]
  },
  {
    "objectID": "day1_practical.html#linear-mixed-model",
    "href": "day1_practical.html#linear-mixed-model",
    "title": "Practical 1",
    "section": "Linear Mixed Model",
    "text": "Linear Mixed Model\nIn this practical we will:\n\nUnderstand the basic structure of a Linear Mixed Model (LLM)\nSimulate data from a LMM\nLearn how to fit a LMM with inlabru and predict from the model.\n\nConsider the a simple linear regression model except with the addition that the data that comes in groups. Suppose that we want to include a random effect for each group \\(j\\) (equivalent to adding a group random intercept). The model is then: \\[\ny_{ij}  = \\beta_0 + \\beta_1 x_i + u_j + \\epsilon_{ij} ~~~  \\text{for}~i = 1,\\ldots,N~ \\text{and}~ j = 1,\\ldots,m.\n\\]\nHere the random group effect is given by the variable \\(u_j \\sim \\mathcal{N}(0, \\tau^{-1}_u)\\) with \\(\\tau_u = 1/\\sigma^2_u\\) describing the variability between groups (i.e., how much the group means differ from the overall mean). Then, \\(\\epsilon_j \\sim \\mathcal{N}(0, \\tau^{-1}_\\epsilon)\\) denotes the residuals of the model and \\(\\tau_\\epsilon = 1/\\sigma^2_\\epsilon\\) captures how much individual observations deviate from their group mean (i.e., variability within group).\nThe model design matrix for the random effect has one row for each observation (this is equivalent to a random intercept model). The row of the design matrix associated with the \\(ij\\)-th observation consists of zeros except for the element associated with \\(u_j\\), which has a one.\n\\[\n\\pmb{\\eta} = \\pmb{A}\\pmb{u} = \\pmb{A}_1\\pmb{u}_1 + \\pmb{A}_2\\pmb{u}_2 + \\pmb{A}_3\\pmb{u}_3\n\\]\n\n\n\n\n\n\nSupplementary material: LMM as a LGM\n\n\n\nIn matrix form, the linear mixed model for the j-th group can be written as:\n\\[ \\overbrace{\\mathbf{y}_j}^{ N \\times 1} = \\overbrace{X_j}^{ N \\times 2} \\underbrace{\\beta}_{1\\times 1} + \\overbrace{Z_j}^{n_j \\times 1} \\underbrace{u_j}_{1\\times1} + \\overbrace{\\epsilon_j}^{n_j \\times 1}, \\]\nIn a latent Gaussian model (LGM) formulation the mixed model predictor for the i-th observation can be written as :\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_i + \\sum_k^K f_k(u_j)\n\\]\nwhere \\(f_k(u_j) = u_j\\) since there’s only one random effect per group (i.e., a random intercept for group \\(j\\)). The fixed effects \\((\\beta_0,\\beta_1)\\) are assigned Gaussian priors (e.g., \\(\\beta \\sim \\mathcal{N}(0,\\tau_\\beta^{-1})\\)). The random effects \\(\\mathbf{u} = (u_1,\\ldots,u_m)^T\\) follow a Gaussian density \\(\\mathcal{N}(0,\\mathbf{Q}_u^{-1})\\) where \\(\\mathbf{Q}_u = \\tau_u\\mathbf{I}_m\\) is the precision matrix for the random intercepts. Then, the components for the LGM are the following:\n\nLatent field given by\n\\[\n\\begin{bmatrix} \\beta \\\\\\mathbf{u}\n\\end{bmatrix} \\sim \\mathcal{N}\\left(\\mathbf{0},\\begin{bmatrix}\\tau_\\beta^{-1}\\mathbf{I}_2&\\mathbf{0}\\\\\\mathbf{0} &\\tau_u^{-1}\\mathbf{I}_m\\end{bmatrix}\\right)\n\\]\nLikelihood:\n\\[\ny_i \\sim \\mathcal{N}(\\eta_i,\\tau_{\\epsilon}^{-1})\n\\]\nHyperparameters:\n\n\\(\\tau_u\\sim\\mathrm{Gamma}(a,b)\\)\n\\(\\tau_\\epsilon \\sim \\mathrm{Gamma}(c,d)\\)\n\n\n\n\n\nSimulate example data\n\nset.seed(12)\nbeta = c(1.5,1)\nsd_error = 1\ntau_group = 1\n\nn = 100\nn.groups = 5\nx = rnorm(n)\nv = rnorm(n.groups, sd = tau_group^{-1/2})\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error) +\n  rep(v, each = 20)\n\ndf = data.frame(y = y, x = x, j = rep(1:5, each = 20))  \n\nNote that inlabru expects an integer indexing variable to label the groups.\n\n\nCode\nggplot(df) +\n  geom_point(aes(x = x, colour = factor(j), y = y)) +\n  theme_classic() +\n  scale_colour_discrete(\"Group\")\n\n\n\n\n\nData for the linear mixed model example with 5 groups\n\n\n\n\n\n\nFitting a LMM in inlabru\n\nDefining model components and observational model\nIn order to specify this model we must use the group argument to tell inlabru which variable indexes the groups. The model = \"iid\" tells INLA that the groups are independent from one another.\n\n# Define model components\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\") +\n  u(j, model = \"iid\")\n\nThe group variable is indexed by column j in the dataset. We have chosen to name this component v() to connect with the mathematical notation that we used above.\n\n# Construct likelihood\nlik =  like(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n\nWarning: `like()` was deprecated in inlabru 2.12.0.\nℹ Please use `bru_obs()` instead.\n\n\nFitting the model\nThe model can be fitted exactly as in the previous examples by using the bru function with the components and likelihood objects.\n\nfit = bru(cmp, lik)\nsummary(fit)\n## inlabru version: 2.13.0\n## INLA version: 25.08.21-1\n## Components:\n## beta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\n## beta_1: main = linear(x), group = exchangeable(1L), replicate = iid(1L), NULL\n## u: main = iid(j), group = exchangeable(1L), replicate = iid(1L), NULL\n## Observation models:\n##   Family: 'gaussian'\n##     Tag: &lt;No tag&gt;\n##     Data class: 'data.frame'\n##     Response class: 'numeric'\n##     Predictor: y ~ .\n##     Additive/Linear: TRUE/TRUE\n##     Used components: effects[beta_0, beta_1, u], latent[]\n## Time used:\n##     Pre = 0.736, Running = 0.725, Post = 0.335, Total = 1.8 \n## Fixed effects:\n##         mean    sd 0.025quant 0.5quant 0.975quant  mode kld\n## beta_0 2.108 0.438      1.229    2.108      2.986 2.108   0\n## beta_1 1.172 0.120      0.936    1.172      1.407 1.172   0\n## \n## Random effects:\n##   Name     Model\n##     u IID model\n## \n## Model hyperparameters:\n##                                          mean    sd 0.025quant 0.5quant\n## Precision for the Gaussian observations 0.995 0.144      0.738    0.986\n## Precision for u                         1.613 1.060      0.369    1.356\n##                                         0.975quant  mode\n## Precision for the Gaussian observations       1.30 0.971\n## Precision for u                               4.35 0.918\n## \n## Marginal log-Likelihood:  -179.93 \n##  is computed \n## Posterior summaries for the linear predictor and the fitted values are computed\n## (Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\n\nModel predictions\nTo compute model predictions we can create a data.frame containing a range of values of covariate where we want the response to be predicted for each group. Then we simply call the predict function while spe\n\n\nLMM fitted values\n# New data\nxpred = seq(range(x)[1], range(x)[2], length.out = 100)\nj = 1:n.groups\npred_data = expand.grid(x = xpred, j = j)\npred = predict(fit, pred_data, formula = ~ beta_0 + beta_1 + u) \n\n\npred %&gt;%\n  ggplot(aes(x=x,y=mean,color=factor(j)))+\n  geom_line()+\n  geom_ribbon(aes(x,ymin = q0.025, ymax= q0.975,fill=factor(j)), alpha = 0.5) + \n  geom_point(data=df,aes(x=x,y=y,colour=factor(j)))+\n  facet_wrap(~j)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Question\n\n\n\nSuppose that we are also interested in including random slopes into our model. Assuming intercept and slopes are independent, can your write down the linear predictor and the components of this model as a LGM?\n\n\nGive me a hint\n\nIn general, the mixed model predictor can decomposed as:\n\\[ \\pmb{\\eta} = X\\beta + Z\\mathbf{u} \\]\nWhere \\(X\\) is a \\(n \\times p\\) design matrix and \\(\\beta\\) the corresponding p-dimensional vector of fixed effects. Then \\(Z\\) is a \\(n\\times q_J\\) design matrix for the \\(q_J\\) random effects and \\(J\\) groups; \\(\\mathbf{v}\\) is then a \\(q_J \\times 1\\) vector of \\(q\\) random effects for the \\(J\\) groups. In a latent Gaussian model (LGM) formulation this can be written as:\n\\[ \\eta_i = \\beta_0 + \\sum\\beta_j x_{ij} + \\sum_k f(k) (u_{ij}) \\]\n\n\n\nSee Solution\n\n\nThe linear predictor is given by\n\\[\n\\eta_i = \\beta_0 + \\beta_1x_i + u_{0j} + u_{1j}x_i\n\\]\nLatent field defined by:\n\n\\(\\beta \\sim \\mathcal{N}(0,\\tau_\\beta^{-1})\\)\n\\(\\mathbf{u}_j = \\begin{bmatrix}u_{0j} \\\\ u_{1j}\\end{bmatrix}, \\mathbf{u}_j \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{Q}_u^{-1})\\) where the precision matrix is a block-diagonal matrix with entries \\(\\mathbf{Q}_u= \\begin{bmatrix}\\tau_{u_0} & {0} \\\\{0} & \\tau_{u_1}\\end{bmatrix}\\)\n\nThe hyperparameters are then:\n\n\\(\\tau_{u_0},\\tau_{u_1} \\text{and}~\\tau_\\epsilon\\)\n\n\nTo fit this model in inlabru we can simply modify the model components as follows:\n\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\") +\n  u0(j, model = \"iid\") + u1(j,x, model = \"iid\")",
    "crumbs": [
      "Home",
      "Practical 1"
    ]
  },
  {
    "objectID": "day1_practical.html#sec-genlinmodel",
    "href": "day1_practical.html#sec-genlinmodel",
    "title": "Practical 1",
    "section": "Generalized Linear Model",
    "text": "Generalized Linear Model\nIn this practical we will:\n\nSimulate non-Gaussian data\nLearn how to fit a generalised linear model with inlabru\nGenerate predictions from the model\n\nA generalised linear model allows for the data likelihood to be non-Gaussian. In this example we have a discrete response variable which we model using a Poisson distribution. Thus, we assume that our data \\[\ny_i \\sim \\text{Poisson}(\\lambda_i)\n\\] with rate parameter \\(\\lambda_i\\) which, using a log link, has associated predictor \\[\n\\eta_i = \\log \\lambda_i = \\beta_0 + \\beta_1 x_i\n\\] with parameters \\(\\beta_0\\) and \\(\\beta_1\\), and covariate \\(x\\). This is identical in form to the predictor in Section 1. The only difference is now we must specify a different data likelihood.\n\nSimulate example data\nThis code generates 100 samples of covariate x and data y.\n\nset.seed(123)\nn = 100\nbeta = c(1,1)\nx = rnorm(n)\nlambda = exp(beta[1] + beta[2] * x)\ny = rpois(n, lambda  = lambda)\ndf = data.frame(y = y, x = x)  \n\n\n\nFitting a GLM in inlabru\n\nDefine model components and likelihood\nSince the predictor is the same as Section 1, we can use the same component definition:\n\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n\nHowever, when building the observation model likelihood we must now specify the Poisson likelihood using the family argument (the default link function for this family is the \\(\\log\\) link).\n\nlik =  bru_obs(formula = y ~.,\n            family = \"poisson\",\n            data = df)\n\nFit the model\nOnce the likelihood object is constructed, fitting the model is exactly the same process as in Section 1.\n\nfit_glm = bru(cmp, lik)\n\nAnd model summaries can be viewed using\n\nsummary(fit_glm)\n\ninlabru version: 2.13.0\nINLA version: 25.08.21-1\nComponents:\nbeta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\nbeta_1: main = linear(x), group = exchangeable(1L), replicate = iid(1L), NULL\nObservation models:\n  Family: 'poisson'\n    Tag: &lt;No tag&gt;\n    Data class: 'data.frame'\n    Response class: 'integer'\n    Predictor: y ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[beta_0, beta_1], latent[]\nTime used:\n    Pre = 0.628, Running = 0.434, Post = 0.118, Total = 1.18 \nFixed effects:\n        mean    sd 0.025quant 0.5quant 0.975quant  mode kld\nbeta_0 0.915 0.071      0.775    0.915      1.054 0.915   0\nbeta_1 1.048 0.056      0.938    1.048      1.157 1.048   0\n\nMarginal log-Likelihood:  -204.02 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\n\n\nGenerate model predictions\n\nTo generate new predictions we must provide a data frame that contains the covariate values for \\(x\\) at which we want to predict.\nThis code block generates predictions for the data we used to fit the model (contained in df$x) as well as 10 new covariate values sampled from a uniform distribution runif(10).\n\n# Define new data, set to NA the values for prediction\n\nnew_data = data.frame(x = c(df$x, runif(10)),\n                      y = c(df$y, rep(NA,10)))\n\n# Define predictor formula\npred_fml &lt;- ~ exp(beta_0 + beta_1)\n\n# Generate predictions\npred_glm &lt;- predict(fit_glm, new_data, pred_fml)\n\nSince we used a log link (which is the default for family = \"poisson\"), we want to predict the exponential of the predictor. We specify this using a general R expression using the formula syntax.\n\n\n\n\n\n\nNote\n\n\n\nNote that the predict function will call the component names (i.e. the “labels”) that were decided when defining the model.\n\n\nSince the component definition is looking for a covariate named \\(x\\), all we need to provide is a data frame that contains one, and the software does the rest.\n\nPlotR Code\n\n\n\n\n\n\n\nData and 95% credible intervals\n\n\n\n\n\n\n\npred_glm %&gt;% ggplot() + \n  geom_point(aes(x,y), alpha = 0.3) +\n  geom_line(aes(x,mean)) +\n    geom_ribbon(aes(x = x, ymax = q0.975, ymin = q0.025),fill = \"tomato\", alpha = 0.3)+\n  xlab(\"Covariate\") + ylab(\"Observations (counts)\")\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nSuppose a binary response such that\n\\[\n    \\begin{aligned}\ny_i &\\sim \\mathrm{Bernoulli}(\\psi_i)\\\\\n\\eta_i &= \\mathrm{logit}(\\psi_i) = \\alpha_0 +\\alpha_1 \\times w_i\n\\end{aligned}\n\\] Using the following simulated data, use inlabru to fit the logistic regression above. Then, plot the predictions for the data used to fit the model along with 10 new covariate values\n\nset.seed(123)\nn = 100\nalpha = c(0.5,1.5)\nw = rnorm(n)\npsi = plogis(alpha[1] + alpha[2] * w)\ny = rbinom(n = n, size = 1, prob =  psi) # set size = 1 to draw binary observations\ndf_logis = data.frame(y = y, w = w)  \n\nHere we use the logit link function \\(\\mathrm{logit}(x) = \\log\\left(\\frac{x}{1-x}\\right)\\) (plogis() function in R) to link the linear predictor to the probabilities \\(\\psi\\).\n\n\nTake hint\n\nYou can set family = \"binomial\" for binary responses and the plogis() function for computing the predicted values.\n\n\n\n\n\n\nNote\n\n\n\nThe Bernoulli distribution is equivalent to a \\(\\mathrm{Binomial}(1, \\psi)\\) pmf. If you have proportional data (e.g. no. successes/no. trials) you can specify the number of events as your response and then the number of trials via the Ntrials = n argument of the bru_obs function (where n is the known vector of trials in your data set).\n\n\n\n\n\n\nClick here to see the solution\n\n\nCode\n# Model components\ncmp_logis =  ~ -1 + alpha_0(1) + alpha_1(w, model = \"linear\")\n# Model likelihood\nlik_logis =  bru_obs(formula = y ~.,\n            family = \"binomial\",\n            data = df_logis)\n# fit the model\nfit_logis &lt;- bru(cmp_logis,lik_logis)\n\n# Define data for prediction\nnew_data = data.frame(w = c(df_logis$w, runif(10)),\n                      y = c(df_logis$y, rep(NA,10)))\n# Define predictor formula\npred_fml &lt;- ~ plogis(alpha_0 + alpha_1)\n\n# Generate predictions\npred_logis &lt;- predict(fit_logis, new_data, pred_fml)\n\n# Plot predictions\npred_logis %&gt;% ggplot() + \n  geom_point(aes(w,y), alpha = 0.3) +\n  geom_line(aes(w,mean)) +\n    geom_ribbon(aes(x = w, ymax = q0.975, ymin = q0.025),fill = \"tomato\", alpha = 0.3)+\n  xlab(\"Covariate\") + ylab(\"Observations\")",
    "crumbs": [
      "Home",
      "Practical 1"
    ]
  },
  {
    "objectID": "day1_practical.html#sec-gam_ex",
    "href": "day1_practical.html#sec-gam_ex",
    "title": "Practical 1",
    "section": "Generalised Additive Model",
    "text": "Generalised Additive Model\nIn this practical we will:\n\nLearn how to fit a GAM with inlabru\nGenerate predictions from the model\n\nGeneralised Additive Models (GAMs) are very similar to linear models, but with an additional basis set that provides flexibility.\nAdditive models are a general form of statistical model which allows us to incorporate smooth functions alongside linear terms. A general expression for the linear predictor of a GAM is given by\n\\[\n\\eta_i = g(\\mu_i) = \\beta_0 + \\sum_{j=1}^L f_j(x_{ij})\n\\]\nwhere the mean \\(\\pmb{\\mu} = E(\\mathbf{y}|\\mathbf{x}_1,\\ldots,\\mathbf{x}_L)\\) and \\(g()\\) is a link function (notice that the distribution of the response and the link between the predictors and this distribution can be quite general). The term \\(f_j()\\) is a smooth function for the j-th explanatory variable that can be represented as\n\\[\nf(x_i) = \\sum_{k=0}^q\\beta_k b_k(x_i)\n\\]\nwhere \\(b_k\\) denote the basis functions and \\(\\beta_K\\) are their coefficients.\nIncreasing the number of basis functions leads to a more wiggly line. Too few basis functions might make the line too smooth, too many might lead to overfitting.To avoid this, we place further constraints on the spline coefficients which leads to constrained optimization problem where the objective function to be minimized is given by:\n\\[\n\\mathrm{min}\\sum_i(y_i-f(x_i))^2 + \\lambda(\\sum_kb^2_k)\n\\] The first term measures how close the function \\(f()\\) is to the data while the second term \\(\\lambda(\\sum_kb^2_k)\\), penalizes the roughness in the function. Here, \\(\\lambda &gt;0\\) is known as the smoothing parameter because it controls the degree of smoothing (i.e. the trade-off between the two terms). In a Bayesian setting,including the penalty term is equivalent to setting a specific prior on the coefficients of the covariates.\nIn this exercise we will set a random walk prior of order 1 on \\(f\\), i.e. \\(f(x_i)-f(x_i-1) \\sim \\mathcal{N}(0,\\sigma^2_f)\\) where \\(\\sigma_f^2\\) is the smoothing parameter such that small values give large smoothing. Notice that we will assume \\(x_i\\)’s are equally spaced for now (we will cover a stochastic differential equation approach that relaxes this assumption later on in the course).\n\nSimulate Data\nLets generate some data so evaluate how RW models perform when estimating a smooth curve. The data are simulated from the following model:\n\\[\ny_i = 1 + \\mathrm{cos}(x) + \\epsilon_i, ~ \\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2_\\epsilon)\n\\] where \\(\\sigma_\\epsilon^2 = 0.25\\)\n\nn = 100\nx = rnorm(n)\neta = (1 + cos(x))\ny = rnorm(n, mean =  eta, sd = 0.5)\n\ndf = data.frame(y = y, \n                x_smooth = inla.group(x)) # equidistant x's \n\n\n\nFitting a GAM in inlabru\nNow lets fit a flexible model by setting a random walk of order 1 prior on the coefficients. This can be done bye specifying model = \"rw1\" in the model component (similarly,a random walk of order 2 can be placed by setting model = \"rw2\" )\n\ncmp =  ~ Intercept(1) + \n  smooth(x_smooth, model = \"rw1\")\n\nNow we define the observational model:\n\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n\nWe then can fit the model:\n\nfit = bru(cmp, lik)\nfit$summary.fixed\n\n              mean         sd 0.025quant 0.5quant 0.975quant     mode\nIntercept 1.298799 0.06506721    1.17164 1.298565   1.427294 1.298569\n                   kld\nIntercept 9.965794e-09\n\n\nThe posterior summary regarding the estimated function using RW1 can be accessed through fit$summary.random$smooth, the output includes the value of \\(x_i\\) (ID) as well as the posterior mean, standard deviation, quantiles and mode of each \\(f(x_i)\\). We can use this information to plot the posterior mean and associated 95% credible intervals.\n\nR plotR Code\n\n\n\n\n\n\n\nSmooth effect of the covariate\n\n\n\n\n\n\n\ndata.frame(fit$summary.random$smooth) %&gt;% \n  ggplot() + \n  geom_ribbon(aes(ID,ymin = X0.025quant, ymax= X0.975quant), alpha = 0.5) + \n  geom_line(aes(ID,mean)) + \n  xlab(\"covariate\") + ylab(\"\")\n\n\n\n\n\n\nModel Predictions\nWe can obtain the model predictions using the predict function.\n\npred = predict(fit, df, ~ (Intercept + smooth))\n\nThe we can plot them together with the true curve and data points:\n\n\nCode\npred %&gt;% ggplot() + \n  geom_point(aes(x_smooth,y), alpha = 0.3) +\n  geom_line(aes(x_smooth,1+cos(x_smooth)),col=2)+\n  geom_line(aes(x_smooth,mean)) +\n  geom_line(aes(x_smooth, q0.025), linetype = \"dashed\")+\n  geom_line(aes(x_smooth, q0.975), linetype = \"dashed\")+\n  xlab(\"Covariate\") + ylab(\"Observations\")\n\n\n\n\n\nData and 95% credible intervals\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nFit a flexible model using a random walk of order 2 (RW2) and compare the results with the ones above.\n\n\nTake hint\n\nYou can set model = \"rw2\" for assigning a random walk 2 prior.\n\n\n\n\nClick here to see the solution\n\n\nCode\ncmp_rw2 =  ~ Intercept(1) + \n  smooth(x_smooth, model = \"rw2\")\nlik_rw2 =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\nfit_rw2 = bru(cmp_rw2, lik_rw2)\n\n# Plot the fitted functions\nggplot() + \n  geom_line(data= fit$summary.random$smooth,aes(ID,mean,colour=\"RW1\"),lty=2) + \n  geom_line(data= fit_rw2$summary.random$smooth,aes(ID,mean,colour=\"RW2\")) + \n  xlab(\"covariate\") + ylab(\"\") + scale_color_discrete(name=\"Model\")\n\n\n\n\n\n\n\n\n\n\nWe see that the RW1 fit is too wiggly while the RW2 is smoother and seems to have better fit.",
    "crumbs": [
      "Home",
      "Practical 1"
    ]
  },
  {
    "objectID": "day2_practical_3.html",
    "href": "day2_practical_3.html",
    "title": "Practical 3",
    "section": "",
    "text": "Aim of this practical:\nwe are going to learn:\nDownload Practical 3 R script",
    "crumbs": [
      "Home",
      "Practical 3"
    ]
  },
  {
    "objectID": "day2_practical_3.html#ar1-models-in-inlabru",
    "href": "day2_practical_3.html#ar1-models-in-inlabru",
    "title": "Practical 3",
    "section": "AR(1) models in inlabru",
    "text": "AR(1) models in inlabru\nIn this exercise we will:\n\nSimulate a time series with autocorrelated errors.\nFit an AR(1) process with inlabru\nVisualize model predictions.\nForecasting for future observations\n\nStart by loading useful libraries:\n\nlibrary(tidyverse)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nTime series analysis is particularly valuable for modelling data with temporal dependence or autocorrelation, where observations taken at nearby time points tend to be more similar than those further apart.\nA time series process is a stochastic process \\(\\{X_t~|~t \\in T\\}\\), which is a collection of random variables that are ordered in time where \\(T\\) is the index set that determines the set of discrete and equally spaced time points at which the process is defined and observations are made.\nAutoregressive processes allow us to account for the time dependence by regressing \\(X_t\\) on past values \\(X_{t-1},\\ldots,X_{t-p}\\) with associated coefficients \\(\\phi_k\\) for each lag \\(k = 1,\\ldots,p\\). Thus an autoregressive process of order \\(p\\), denoted AR(\\(p\\)) , is given by:\n\\[\nX_t = \\phi_1 X_{t-1} + \\ldots + \\phi_p X_{t-p} + \\varepsilon_t; ~~ \\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2_e)\n\\]\nConsider now an univariate time series \\(y_t\\) which evolves over time according to some autoregressive stochastic process. For example, a time series where the system follows an AR(1) process can be defined as:\n\\[\n\\begin{aligned}\ny_t &\\sim \\mathcal{N}(\\mu_t,\\tau_y^{-1})\\\\\n\\eta_t &= g^{-1}(\\mu_t) = \\alpha + u_t \\\\\nu_t &= \\phi u_{t-1} + \\delta_t ; ~~ \\delta_t \\sim \\mathcal{N}(0,\\tau_u^{-1}); ~~ t &gt; 1 \\\\\nu_1 &= \\mathcal{N}(0,\\kappa^{-1})\\\\\n\\kappa &= \\tau_u (1-\\phi^2)\n\\end{aligned}\n\\]\nThe response \\(y_t\\) is assumed to be normal distributed with mean \\(\\alpha + u_t\\) and precision error \\(\\tau_y\\) ( here \\(g(\\cdot)\\) is just the identity link function that maps the linear predictor to the mean of the process). Then, the process \\(u_t\\) follows an AR(1) process where \\(u_1\\) is drawn from a stationary normal distribution such that \\(\\kappa\\) denotes the marginal precision for state \\(u_t\\)\nThe covariance matrix is then given by:\n\\[\n\\Sigma = \\frac{\\tau^{-1}_u}{1-\\phi^2}\n\\begin{bmatrix}\n1 & \\phi & \\phi^2 & \\ldots & \\phi^{n-1}\\\\\n\\phi & 1 & \\phi & \\ldots & \\phi^{n-2} \\\\\n\\phi^2 & \\phi & 1 & \\ldots & \\phi^{n-3} \\\\\n\\phi^{n-1} & \\phi^{n-2} & \\phi^{n-3} & \\ldots & 1\n\\end{bmatrix}\n\\]\nNotice that conditionally on \\(u_t\\), the observed data \\(y_y\\) are independent from\\(y_{t-1},y_{t-2}.y_{t-3},\\ldots\\), also the conditional distribution of \\(u_t\\) is a markov chain such that \\(\\pi(u_t|u_{t-1},u_{t-2},u_{t-3}) = \\pi(u_t|u_{t-1})\\). Thus, each time point is only conditionally dependent on the two closest time points:\n\\[\nu_t|\\mathbf{u}_{-t} \\sim \\mathcal{N}\\left(\\frac{\\phi}{1-\\phi^2}(u_{t-1}+u_{t+1}),\\frac{\\tau_u^{-1}}{1-\\phi^2}\\right)\n\\]\n\nSimulate example data\nFirst, we simulate data from the model:\n\\[\n\\begin{aligned}\ny_t &= \\alpha + u_t + \\varepsilon_t;~ \\varepsilon_t \\sim \\mathcal{N}(0,\\tau_y^{-1})\\\\\nu_t &= \\phi y_{t-1} + \\delta_t; ~ \\delta_t \\sim \\mathcal{N}(0,\\tau_u^{-1})\n\\end{aligned}\n\\]\n\nset.seed(123)\n\nphi = 0.8\ntau_u = 10\nmarg.prec = tau_u * (1-phi^2) # ar1 in INLA is parametrized as marginal variance\nu_t =  as.vector(arima.sim(list(order = c(1,0,0), ar = phi), \n                          n = 100,\n                          sd=sqrt(1/tau_u)))\na = 1\ntau_e = 5\nepsilon_t = rnorm(100, sd = sqrt(1/tau_e))\ny = a + u_t + epsilon_t\n\n\nts_dat &lt;- data.frame(y =y , x= 1:100)\n\n\n\n\n\n\n\n\n\n\n\n\nFitting an AR(1) model with inlabru\nModel components\nFirst, we define the model components, notice that the latent field is defined by two components: the intercept \\(\\alpha\\) and the autoregressive random effects \\(u_t\\):\n\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(x,model = \"ar1\")\n\nThe we can define the formula for the linear predictor and specify the observational model\n\n# Model formula\nformula = y ~ alpha + ut\n# Observational model\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = ts_dat)\n\nLastly, we fit the model using the bru function and compare the model estimates with the true mdeol parameters we simulate our data from.\n\n# fit the model\nfit.ar1 = bru(cmp, lik)\n\n# compare against the true values\n\ndata.frame(\n  true = c(a,tau_e,marg.prec,phi),\n  rbind(fit.ar1$summary.fixed[,c(1,3,5)],\n        fit.ar1$summary.hyperpar[,c(1,3,5)])\n        ) %&gt;% round(2)\n\n                                        true mean X0.025quant X0.975quant\nalpha                                    1.0 1.00        0.69        1.28\nPrecision for the Gaussian observations  5.0 4.91        3.11        7.29\nPrecision for ut                         3.6 7.96        2.91       18.10\nRho for ut                               0.8 0.80        0.54        0.94\n\n\nModel predictions\nHere, we will predict the mean of our time series along with 95% credible intervals. Note that this interval are for the mean and not for new observations, we will cover forecasting new observations next.\n\npred_ar1 = predict(fit.ar1, ts_dat, ~ alpha + ut)\n\nggplot(pred_ar1,aes(y=mean,x=x))+\n  geom_line()+\n    geom_ribbon(aes(x = x, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(aes(y=y,x=x))\n\n\n\n\n\n\n\n\n\n\nForecasting\nA common goal in time series modelling is forecasting into the future. Forecasting can be treated as a missing data problem where future values of the response variable are missing. Let \\(y_m\\) be the missing response, then, by fitting a statistical model to the observed data \\(\\mathbf{y}_{obs}\\), we condition on its parameters to obtain the posterior predictive distribution:\n\\[\n\\pi(y_{m} \\mid \\mathbf{y}_{obs}) = \\int \\pi(y_{m}, \\theta \\mid \\mathbf{y}_{obs})  d\\theta = \\int \\pi(y_{m} \\mid \\mathbf{y}_{obs}, \\theta) \\pi(\\theta \\mid \\mathbf{y}_{obs})  d\\theta\n\\]\nThis distribution, which integrates over all parameter uncertainty, provides the complete probabilistic forecast for the missing values. INLA will automatically compute the predictive distributions for all missing values in the response. To do so, we can augment our data set by including the new time points at which the prediction will be made and setting the response value to NA for these new time points:\n\nts.forecast &lt;- rbind(ts_dat, \n  data.frame(y = rep(NA, 50), x = 101:150))\n\nNext, we fit the ar1 model to the new dataset so that the predictive distributions are computed:\n\ncmp =  ~ -1 + alpha(1) + ut(x,model = \"ar1\")\n\npred_lik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = ts.forecast)\n\nfit.forecast = bru(cmp, pred_lik)\n\nLastly, we can draw samples from the posterior predictive distribution using the predict function and visualize our forecast as follows:\n\npred_forecast = predict(fit.forecast, ts.forecast, ~ alpha + ut)\n\np1= ggplot(pred_forecast,aes(y=mean,x=x))+\n  geom_line()+\n    geom_ribbon(aes(x = x, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(data=ts_dat, aes(y=y,x=x))",
    "crumbs": [
      "Home",
      "Practical 3"
    ]
  },
  {
    "objectID": "day2_practical_3.html#modelling-great-lakes-water-level",
    "href": "day2_practical_3.html#modelling-great-lakes-water-level",
    "title": "Practical 3",
    "section": "Modelling Great Lakes water level",
    "text": "Modelling Great Lakes water level\nIn this exercise we will:\n\nFit an AR(1) process with inlabru to model lakes water levels\nChange the default priors for the observational error\nSet penalized complexity priors for the correlation and precision parameters of the latent effects.\nFit a RW(1) model\nFit a an AR(1) model with group-level correlation\n\nStart by loading useful libraries:\n\nlibrary(tidyverse) \nlibrary(INLA) \nlibrary(ggplot2)\nlibrary(patchwork) \nlibrary(inlabru)\nlibrary(DAAG)\n\nIn this exercise we will look at greatLakes dataset from the DAAG package. The data set contains the water level heights for the lakes Erie, Michigan/Huron, Ontario and St Clair from 1918 to 2009.\nLets begin by loading and formatting the data into a tidy format.\n\ndata(\"greatLakes\")\n\ngreatLakes.df = data.frame(as.matrix(greatLakes),\n                           year = time(greatLakes)) %&gt;%\n  pivot_longer(cols = c(\"Erie\",\"michHuron\",\"Ontario\",\"StClair\"),\n               names_to = \"Lakes\",\n               values_to = \"height\" ) \n\n\n\n\n\n\n\n\n\n\n\nFitting an AR(1) model in inlabru\nWe will focus on the Erin lake for now. Lets begin by fitting an AR(1) model of the form:\n\\[\n\\begin{aligned}\n\\text{height}_t &= \\alpha + u_t +\\varepsilon_t~; ~~ \\varepsilon_t\\sim \\mathcal{N}(0,\\tau_e^{-1}) \\\\\nu_t &= \\phi u_{t-1} + \\delta_t~~~ ; ~~ \\delta_t \\sim \\mathcal{N}(0,\\tau_u^{-1}); ~~ t &gt; 1 \\\\\nx_1 &= \\mathcal{N}(0,\\kappa^{-1})\\\\\n\\kappa &= \\tau_u (1-\\phi^2)\n\\end{aligned}\n\\]\nWhere \\(\\alpha\\) is the intercept, \\(\\phi\\) is the correlation term, \\(\\varepsilon\\) is the observational Gaussian error with mean zero and precision \\(\\tau_e\\) and \\(\\kappa\\) is the marginal precision for the state \\(u_t\\) for \\(t= 1,\\ldots,92\\).\nFirst we make a subset of the dataset and create a time index \\(T\\):\n\ngreatLakes.df$t.idx &lt;- greatLakes.df$year-1917\n\nErie.df = greatLakes.df %&gt;% filter(Lakes == \"Erie\")\n\n\n\n\n\n\n\n Task\n\n\n\nFit an AR(1) model to the Erie lake data using inlabru, then plot the model fitted values showing 95% credible intervals.\n\n\nTake hint\n\nRemember this is done by (1) defining the model components, (2) the formula and (3) the observational model. Then you can use the predict function to compute the predicted values for the mean along with 95% credible intervals.\n\n\n\n\nClick here to see the solution\n\n\nCode\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(t.idx,model = \"ar1\")\n# Model formula\nformula = height ~ alpha + ut\n\n\n# Observational model\nlik =  bru_obs(formula = height   ~.,\n            family = \"gaussian\",\n            data = Erie.df )\n\n# fit the model\nfit.Erie_ar1 = bru(cmp, lik)\n\n# Model predictions \n\npred_ar1.Erie = predict(fit.Erie_ar1, Erie.df, ~ alpha + ut)\n\n# plot model fitted values\nggplot(pred_ar1.Erie,aes(y=mean,x=year))+\n  geom_line()+\n    geom_ribbon(aes(x = year, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(aes(y=height,x=year))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Question\n\n\n\nAre there any issues with the fitted model, and if so, how do you think we should address them?\n\n\nAnswer\n\nIt is clear that the model overfits the data, leading to poor predictive performance. Thus, we need to introduce some prior information on the what we expect the variation of the process to be.\n\n\n\nPriors\nLet review INLA’ prior parametrization for autoregressive models:\nLet \\(\\pmb{\\theta} = \\{\\theta_y,\\theta_u,\\theta_\\phi\\}\\) be INLA’s internal representation of the hyperparameters such that:\n\\(\\theta_y = \\log(\\tau^2_y)\\)\n\\(\\theta_u = \\log(\\kappa) = \\log\\left(\\tau_u[1-\\phi^2]\\right)\\)\n\\(\\theta_\\phi = \\log \\left(\\frac{1+\\phi}{1-\\phi}\\right)\\)\nThe default priors for \\(\\{\\theta_y,\\theta_u\\}\\) are \\(\\text{log-gamma} (1, 5\\times 10^{-5} )\\) priors with default initial values set to 4 in each case. Then, Gaussian priors \\(\\alpha \\sim \\mathcal{N}(0,\\tau_y = 0.001)\\) and \\(\\theta_\\phi \\sim \\mathcal{N}(0, \\tau_y= 0.15)\\) are used for the intercept and correlation parameter respectively.\n\n\n\n\n\n\nNote\n\n\n\nSpecifically for AR(1) correlation parameter \\(\\phi\\), INLA uses the following logit transformation on \\(\\theta_\\phi\\):\n\\[ \\phi = \\frac{2\\exp(\\theta_\\phi)}{1+ \\exp(\\theta_\\phi)} -1. \\]\n\n\nSetting priors and PC-priors\nLets now set a Gamma prior with parameters 1 and 1, so that the precision of the Gaussian osbervational error is centered at 1 with a variance of 1. Additionally we will set Penalized Complexity (PC) priors according to the following probability statements:\n\n\\(P(\\sigma &gt; 1) = 0.01\\)\n\\(P(\\phi &gt; 0.5) = 0.3\\)\n\nNotice that the PC prior for the precision \\(\\tau_u\\) is defined on the standard deviation \\(\\sigma_u = \\tau_u^{-1/2}\\)\n\npc_prior &lt;- list(theta = list(prior = \"pc.prec\", param = c(1, 0.01)),\n                 rho = list(prior = \"pc.cor0\", param = c(0.5, 0.3))) \n\nprec.tau_e &lt;- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(1, 1))) # prior values\n\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(t.idx, model = \"ar1\",  hyper = pc_prior)\n# Model formula\nformula = height ~ alpha + ut\n\n\n# Observational model\nlik =  bru_obs(formula = height  ~.,\n            family = \"gaussian\",\n            data = Erie.df,\n            control.family = list(hyper = prec.tau_e))\n\n# fit the model\nfit.Erie_ar1 = bru(cmp, lik)\n\n\n\n\n\n\n\n Question\n\n\n\nWhat is the posterior mean for the correlation parameter \\(\\rho\\)? \n\n\n\n\n\n\n\n\n Task\n\n\n\nPlot the fitted values of the model, has the overfitting problem being alleviated?\n\n\n\nClick here to see the solution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting a RW(1) model\nNow we fit a random walk of order 1 to the Erie lake data:\n\\[\n\\begin{aligned}\ny_t &= \\alpha + u_t + \\varepsilon_t, ~ t = 1,\\ldots,92 \\\\\n\\varepsilon_t & \\sim \\mathcal{N}(0,\\tau_e) \\\\\nu_t - u_{t-1} &\\sim \\mathcal{N}(0,\\tau_u),~ t = 2,\\ldots,92 \\\\\n\\end{aligned}\n\\]\nFirs we define model priors:\n\npc_prior &lt;- list(theta = list(prior = \"pc.prec\", param = c(1, 0.01))) \n\nprec.tau_e &lt;- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(1, 1))) # prior values\n\nNow we define model components:\n\ncmp_rw =  ~ -1 + alpha(1) + \n  ut(t.idx ,\n     constr=FALSE,\n     model = \"rw1\",\n     hyper=pc_prior,\n     scale.model = TRUE)\n\nNotice that we have set scale.model = TRUE to scale the latent effects. This is particularly important when Intrinsic Gaussian Markov random fields (IGMRFs) are used as priors (e.g., random walk models or some spatial models) for the latent effects. By defining scale.model = TRUE, the rw1-model is scaled to have a generalized variance equal to one. By scaling scaling the models we ensure that a fixed hyperprior for the precision parameter has a similar interpretation for different types of IGMRFs, making precision estimates comparable between different models. Scaling also allows estimates to be less sensitive to re-scaling covariates in the linear predictor and makes the precision invariant to changes in the shape and size of the latent effect (see Sørbye (2014) for further details) .\nWe can now fit the model with the updated components and plot the predicted values\n\n# Model formula\nformula = height ~ alpha + ut\n# Observational model\nlik =  bru_obs(formula = height  ~.,\n            family = \"gaussian\",\n            data = Erie.df,\n            control.family = list(hyper = prec.tau_e))\n# fit the model\nfit.Erie_rw1 = bru(cmp_rw, lik)\n# Model predictions\npred_rw1.Erie = predict(fit.Erie_rw1, Erie.df, ~ alpha + ut)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Question\n\n\n\nTake a look at the model summaries using the summary function, do you see anything odd?\n\n\nAnswer\n\nThe intercept has zero mean and a very large variance. This is because we have not imposed a sum-to-zero constraint on the model random effects (constr=FALSE). Without this constraint, intrinsic models are non-identifiable. The intercept and the random effects are confounded, For example, you could add a constant value to every random effect and subtract it from the intercept without changing the model’s predictions. Take for instance for any constant \\(c\\), the following models are identical:\n\\(y_t = \\alpha + u_{t} + \\varepsilon_t\\) \\(y_t = (\\alpha - c) + (u_{t} + c) + \\varepsilon_t\\)\nThus, you need to set constr=FALSE so that \\(\\sum_t u_t=0\\) to ensure identifiability of \\(\\alpha\\)\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nFit an RW(1) model to the Erie data but now set constr=TRUE to impose a sum-to-zero constraint on the random effect. Then compare your results with the unconstrained model.\n\n\n\nClick here to see the solution\n\n\nCode\n# Model components\ncmp_rw =  ~ -1 + alpha(1) + \n  ut(t.idx ,\n     constr=TRUE,\n     model = \"rw1\",\n     hyper=pc_prior,\n     scale.model = TRUE)\n\nfit.Erie_rw1_constr = bru(cmp_rw, lik)\n\nfit.Erie_rw1_constr$summary.fixed\n\n\n          mean        sd 0.025quant 0.5quant 0.975quant     mode          kld\nalpha 174.1381 0.0237476   174.0914 174.1381   174.1848 174.1381 1.935519e-08\n\n\n\n\n\n\n\nGroup-level effects\nNow we will model the height water levels for all four lakes by grouping the random effects. This will allow a within-lakes correlation to be included. In the next example, we allow for correlated effects using an ar1 model for the years and iid random effects on the lakes. First we create a lakes id and set the priors for our model:\n\ngreatLakes.df$lake_id &lt;- as.numeric(as.factor(greatLakes.df$Lakes))\n\npc_prior &lt;- list(theta = list(prior = \"pc.prec\", param = c(1, 0.01)),\n                 rho = list(prior = \"pc.cor0\", param = c(0.5, 0.3))) \n\nprec.tau_e &lt;- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(1, 10))) # prior values\n\nNow we define the model components. The lakes IDs that define the group are passed with parameter group argument and the iid model and other parameters are passed through the control.group parameter.\n\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(year,model = \"ar1\",\n                            hyper = pc_prior,\n                            group =lake_id,\n                            control.group = \n                            list(model = \"iid\", \n                                 scale.model = TRUE))\n\nWe fit the model in a similar fashion as we did before:\n\n# Model formula\nformula = height ~ alpha + ut\n\n\n# Observational model\nlik =  bru_obs(formula = height  ~.,\n            family = \"gaussian\",\n            data = greatLakes.df,\n            control.family = list(hyper = prec.tau_e))\n\n# fit the model\nfit.all_lakes_ar1 = bru(cmp, lik)\n\n# Model predictions\npred_ar1.all = predict(fit.all_lakes_ar1, greatLakes.df, ~ alpha + ut)\n\nLastly we can visualize group-level model predictions as follows:\n\nggplot(pred_ar1.all,aes(y=mean,x=year))+\n  geom_line()+\n    geom_ribbon(aes(x = year, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(aes(y=height,x=year)) + facet_wrap(~Lakes,scales = \"free\")",
    "crumbs": [
      "Home",
      "Practical 3"
    ]
  },
  {
    "objectID": "day2_practical_3.html#non-gaussian-data",
    "href": "day2_practical_3.html#non-gaussian-data",
    "title": "Practical 3",
    "section": "Non-Gaussian data",
    "text": "Non-Gaussian data\nIn the next example we will use the Toyo data set to illustrate how temporal models can be fit to non-Gaussian data.\nThe Tokyo data set available in INLA contains the recorded days of rain above 1 mm in Tokyo for 2 years, 1983:84. The data set contains the following variables:\n\ny : number of days with rain\nn : total number of days\ntime : day of the year\n\n\nlibrary(INLA)\nlibrary(inlabru)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n\ndata(\"Tokyo\")\n\nA possible observational model for these data is\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim\\text{Bin}(n_t, p_t) \\\\\n\\eta_t &= \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\end{aligned}\n\\]\n\\[\nn_t = \\left\\{\n\\begin{array}{lr}\n1, & \\text{for}\\; 29\\; \\text{February}\\\\\n2, & \\text{other days}\n\\end{array}\\right.\n\\]\n\\[\ny_t =\n\\begin{cases}\n\\{0,1\\}, & \\text{for}\\; 29\\; \\text{February}\\\\\n\\{0,1,2\\}, & \\text{other days}\n\\end{cases}\n\\]\nThen, the latent field is given by\n\\[\n\\eta_t = \\beta_0 + f(\\text{time}_t)\n\\]\n\nWhere the probability of rain depends on on the day of the year \\(t\\)\n\\(\\beta_0\\) is an intercept\n\\(f(\\text{time}_t)\\) is a temporal model, e.g., a RW2 model (this is just a smoother).\n\nThe smoothness is controlled by a hyperparameter \\(\\tau_f\\) . Thus, we assign a prior to \\(\\tau_f\\) to finalize the model.\nWe can fit the model as follows:\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)\n\nNotice that we have set cyclic = TRUE as this is a cyclic effect. Finally, we can produce model predictions in a similar fashion as we did before:\n\npTokyo = predict(fit, Tokyo, ~ plogis(beta0 + time_effect))\n\nggplot(data=pTokyo , aes(x= time, y= y) ) +\n  geom_point() + \n  ylab(\"\") + xlab(\"\") +\n  # Custom the Y scales:\n  scale_y_continuous(\n    # Features of the first axis\n    name = \"\",\n    # Add a second axis and specify its features\n    sec.axis = sec_axis( transform=~./2, name=\"Probability\")\n  )  + geom_line(aes(y=mean*2,x=time)) +\n  geom_ribbon(aes( ymin = q0.025*2, \n                             ymax = q0.975*2), alpha = 0.5)",
    "crumbs": [
      "Home",
      "Practical 3"
    ]
  },
  {
    "objectID": "day3_practical_5.html",
    "href": "day3_practical_5.html",
    "title": "Practical 5",
    "section": "",
    "text": "Aim of this practical:\nIn this first practical we are going to fit spatial modes for Areal, Geostatistical and Point-Process Data.\nDownload Practical 5 R script",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#the-data",
    "href": "day3_practical_5.html#the-data",
    "title": "Practical 5",
    "section": "The data",
    "text": "The data\nWe consider data on respiratory hospitalizations for Greater Glasgow and Clyde in 2007. The data are available from the CARBayesdata R Package:\n\nlibrary(CARBayesdata)\n\ndata(pollutionhealthdata)\ndata(GGHB.IZ)\n\nThe pollutionhealthdata contains the spatiotemporal data on respiratory hospitalizations, air pollution concentrations and socio-economic deprivation covariates for the 271 Intermediate Zones (IZ) that make up the Greater Glasgow and Clyde health board in Scotland. Data are provided by the Scottish Government and the available variables are:\n\nIZ: unique identifier for each IZ.\nyear: the year when the measurements were taken\nobserved: observed numbers of hospitalizations due to respiratory disease.\nexpected: expected numbers of hospitalizations due to respiratory disease computed using indirect standardisation from Scotland-wide respiratory hospitalization rates.\npm10: Average particulate matter (less than 10 microns) concentrations.\njsa: The percentage of working age people who are in receipt of Job Seekers Allowance\nprice: Average property price (divided by 100,000).\n\nThe GGHB.IZ data is a Simple Features (sf) object containing the spatial polygon information for the set of 271 Intermediate Zones (IZ), that make up of the Greater Glasgow and Clyde health board in Scotland ( Figure 1 ).\n\n\n\n\n\n\n\n\nFigure 1: Greater Glasgow and Clyde health board represented by 271 Intermediate Zones\n\n\n\n\nWe first merge the two dataset and select only one year of data, compute the SME and plot the observed\n\nresp_cases &lt;- merge(GGHB.IZ %&gt;%\n                      mutate(space = 1:dim(GGHB.IZ)[1]),\n                             pollutionhealthdata, by = \"IZ\") %&gt;%\n  filter(year == 2007) %&gt;%\n    mutate(SMR = observed/expected)\n\nggplot() + geom_sf(data = resp_cases, aes(fill = SMR)) + scale_fill_scico(direction = -1)\n\n\n\n\n\n\n\n\nThen we compute the adjacency matrix using the functions poly2nb() and nb2mat() in the spdep library. We then convert the adjacency matrix into the precision matrix \\(\\mathbf{Q}\\) of the CAR model. Remember this matrix has, on the diagonal the number of e\n\nlibrary(spdep)\n\nW.nb &lt;- poly2nb(GGHB.IZ,queen = TRUE)\nR &lt;- nb2mat(W.nb, style = \"B\", zero.policy = TRUE)\n\ndiag = apply(R,1,sum)\nQ = -R\ndiag(Q) = diag",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#the-model",
    "href": "day3_practical_5.html#the-model",
    "title": "Practical 5",
    "section": "The model",
    "text": "The model\nWe fit a first model to the data where we consider a Poisson model for the observed cases.\nStage 1 Model for the response \\[\ny_i|\\eta_i\\sim\\text{Poisson}(E_i\\lambda_i)\n\\] where \\(E_i\\) are the expected cases for area \\(i\\).\nStage 2 Latent field model \\[\n\\eta_i = \\text{log}(\\lambda_i) = \\beta_0 + \\omega_i + z_i\n\\] where\n\n\\(\\beta_0\\) is a common intercept\n\\(\\mathbf{\\omega} = (\\omega_1, \\dots, \\omega_k)\\) is a conditional Autorgressive model (CAR) with precision matrix \\(\\tau_1\\mathbf{Q}\\)\n\\(\\mathbf{z} = (z_1, \\dots, z_k)\\) is an unstrictured random effect with precision \\(\\tau_2\\)\n\nStage 3 Hyperparameters\nThe hyperparameters of the model are \\(\\tau_1\\) and \\(\\tau_2\\)\nNOTE In this case the linear predictor \\(\\eta\\) consists of three components!!\n\n\n\n\n\n\n Task\n\n\n\nFit the above model in using inlabru by completing the following code:\n\ncmp = ~ Intercept(1) + space(...) + iid(...)\n\nformula = ...\n\n\nlik = bru_obs(formula = formula, \n              family = ...,\n              E = ...,\n              data = ...)\n\nfit = bru(cmp, lik)\n\n\n\nAnswer\n\n\ncmp = ~ Intercept(1) + space(space, model = \"besag\", graph = Q) + iid(space, model = \"iid\")\n\nformula = observed ~ Intercept + space + iid\n\nlik = bru_obs(formula = formula, \n              family = \"poisson\",\n              E = expected,\n              data = resp_cases)\n\nfit = bru(cmp, lik)\n\n\n\n\nAfter fitting the model we want to extract results.\n\n\n\n\n\n\n Question\n\n\n\n\nWhat is the estimated value for \\(\\beta_0\\)? \nLook at the estimated values of the hyperparameters using fit$summary.hyperpar , which of the two spatial components (structured or unstructured) explains more of the variability in the counts? structuredunstructured\n\n\n\nWe now look at the predictions over space.\n\n\n\n\n\n\n Task\n\n\n\nComplete the code below to produce prediction of the linear predictor \\(\\eta_i\\) and of the risk \\(\\lambda_i\\) and of the expected cases \\(E_i\\exp(\\lambda_i)\\) over the whole space of interest. Then plot the mean and sd of the resulting surfaces.\n\npred = predict(fit, resp_cases, ~data.frame(log_risk = ...,\n                                             risk = exp(...),\n                                             cases = ...\n                                             ),\n               n.samples = 1000)\n\n\n\nShow Answer\n\n\n# produce predictions\npred = predict(fit, resp_cases, ~data.frame(log_risk = Intercept + space,\n                                             risk = exp(Intercept + space),\n                                             cases = expected * exp(Intercept + space)\n                                             ),\n               n.samples = 1000)\n# plot the predictions\n\np1 = ggplot() + geom_sf(data = pred$log_risk, aes(fill = mean)) + scale_fill_scico(direction = -1) + ggtitle(\"mean log risk\")\np2 = ggplot() + geom_sf(data = pred$log_risk, aes(fill = sd)) + scale_fill_scico(direction = -1) + ggtitle(\"sd log risk\")\np1 + p2\n\n\n\n\n\n\n\np1 = ggplot() + geom_sf(data = pred$risk, aes(fill = mean)) + scale_fill_scico(direction = -1) + ggtitle(\"mean  risk\")\np2 = ggplot() + geom_sf(data = pred$risk, aes(fill = sd)) + scale_fill_scico(direction = -1) + ggtitle(\"sd  risk\")\np1 + p2\n\n\n\n\n\n\n\np1 = ggplot() + geom_sf(data = pred$cases, aes(fill = mean)) + scale_fill_scico(direction = -1)+ ggtitle(\"mean  expected counts\")\np2 = ggplot() + geom_sf(data = pred$cases, aes(fill = sd)) + scale_fill_scico(direction = -1)+ ggtitle(\"sd  expected counts\")\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\nFinally we want to compare our observations \\(y_i\\) with the predicted means of the Poisson distribution \\(E_i\\exp(\\lambda_i)\\)\n\npred$cases %&gt;% ggplot() + geom_point(aes(observed, mean)) + \n  geom_errorbar(aes(observed, ymin = q0.025, ymax = q0.975)) +\n  geom_abline(intercept = 0, slope = 1)\n\n\n\n\n\n\n\n\nNote: Here we are predicting the mean of counts, not the counts!!! Predicting counts is the theme of the next task!",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#getting-prediction-densities",
    "href": "day3_practical_5.html#getting-prediction-densities",
    "title": "Practical 5",
    "section": "Getting prediction densities",
    "text": "Getting prediction densities\nPosterior predictive distributions, that is \\(\\pi(y_i^{\\text{new}}|\\mathbf{y})\\) are of interest in many applied problems. The bru() function does not return predictive densities. In the previous step we have computed predictions for the expected counts \\(\\pi(E_i\\lambda_i|\\mathbf{y})\\). The predictive distribution is then: \\[\n\\pi(y_i^{\\text{new}}|\\mathbf{y}) = \\int \\pi(y_i|E_i\\lambda_i)\\pi(E_i\\lambda_i|\\mathbf{y})\\ dE_i\\lambda_i\n\\] where, in our case, \\(\\pi(y_i|E_i\\lambda_i)\\) is Poisson with mean \\(E_i\\lambda_i\\). We can achieve this using the following algorith:\n\nSimulate \\(n\\) replicates of \\(g^k = E_i\\lambda_i\\) for \\(k = 1,\\dots,n\\) using the function generate() which takes the same input as predict()\nFor each of the \\(k\\) replicates simulate a new value \\(y_i^{new}\\) using the function rpois()\nSummarise the \\(n\\) samples of \\(y_i^{new}\\) using, for example the mean and the 0.025 and 0.975 quantiles.\n\nHere is the code:\n\n# simulate 1000 realizations of E_i\\lambda_i\nexpected_counts = generate(fit, resp_cases, \n                           ~ expected * exp(Intercept + space),\n                           n.samples = 1000)\n\n\n# simulate poisson data\naa = rpois(271*1000, lambda = as.vector(expected_counts))\nsim_counts = matrix(aa, 271, 1000)\n\n# summarise the samples with posterior means and quantiles\npred_counts = data.frame(observed = resp_cases$observed,\n                         m = apply(sim_counts,1,mean),\n                         q1 = apply(sim_counts,1,quantile, 0.025),\n                         q2 = apply(sim_counts,1,quantile, 0.975),\n                         vv = apply(sim_counts,1,var)\n                         )\n\n\n\n\n\n\n\n Task\n\n\n\nPlot the observations against the predicted new counts and the predicted expected counts. Include the uncertainty and compare the two.\n\n\nTake hint\n\n\n\n\n\nClick here to see the solution\n\n\nCode\nggplot() + \n  geom_point(data = pred_counts, aes(observed, m, color = \"Pred_obs\")) + \n  geom_errorbar(data = pred_counts, aes(observed, ymin = q1, ymax = q2, color = \"Pred_obs\")) +\n  geom_point(data = pred$cases, aes(observed, mean, color = \"Pred_means\")) + \n  geom_errorbar(data = pred$cases, aes(observed, ymin = q0.025, ymax = q0.975, color = \"Pred_means\")) +\n  \n  geom_abline(intercept = 0, slope =1)",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#the-data-1",
    "href": "day3_practical_5.html#the-data-1",
    "title": "Practical 5",
    "section": "The data",
    "text": "The data\nIn this practical, we will explore data on the Pacific Cod (Gadus macrocephalus) from a trawl survey in Queen Charlotte Sound. The pcod dataset is available from the sdmTMB package and contains the presence/absence records of the Pacific Cod during each surveys along with the biomass density of Pacific cod in the area swept (kg/Km\\(^2\\)). The qcs_grid data contain the depth values stored as \\(2\\times 2\\) km grid for Queen Charlotte Sound.\nThe dataset contains presence/absence data from 2003 to 2017. In this practical we only consider year 2003.\nWe first load the dataset and select the year of interest\n\nlibrary(sdmTMB)\n\npcod_df = sdmTMB::pcod %&gt;% filter(year==2003)\nqcs_grid = sdmTMB::qcs_grid\n\nThen, we create ab sf object and assign the rough coordinate reference to it:\n\npcod_sf =   st_as_sf(pcod_df, coords = c(\"lon\",\"lat\"), crs = 4326)\npcod_sf = st_transform(pcod_sf,\ncrs = \"+proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\" )\n\nWe convert the covariate into a raster and assign the same coordinate reference:\n\ndepth_r &lt;- rast(qcs_grid, type = \"xyz\")\ncrs(depth_r) &lt;- crs(pcod_sf)\n\nFinally we can plot our dataset. Note that to plot the raster we need to upload also the tidyterra library.\n\nggplot()+ \n  geom_spatraster(data=depth_r$depth)+\n  geom_sf(data=pcod_sf,aes(color=factor(present))) +\n    scale_color_manual(name=\"Occupancy status for the Pacific Cod\",\n                     values = c(\"black\",\"orange\"),\n                     labels= c(\"Absence\",\"Presence\"))+\n  scale_fill_scico(name = \"Depth\",\n                   palette = \"nuuk\",\n                   na.value = \"transparent\" ) + xlab(\"\") + ylab(\"\")",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#the-model-1",
    "href": "day3_practical_5.html#the-model-1",
    "title": "Practical 5",
    "section": "The model",
    "text": "The model\nWe first fit a simple model where we consider the observation as Bernoulli and where the linear predictor contains only one intercept and the GR field defined through the SPDE approach. The model is defined as:\nStage 1 Model for the response\n\\[\ny(s)|\\eta(s)\\sim\\text{Binom}(1, p(s))\n\\] Stage 2 Latent field model\n\\[\n\\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s)\n\\]\nwith\n\\[\n\\omega(s)\\sim \\text{  GF with range } \\rho\\  \\text{ and maginal variance }\\ \\sigma^2\n\\]\nStage 3 Hyperparameters\nThe hyperparameters of the model are \\(\\rho\\) and \\(\\sigma\\)\nNOTE In this case the linear predictor \\(\\eta\\) consists of two components!!\n\nThe workflow\nWhen fitting a geostatistical model we need to fulfill the following tasks:\n\nBuild the mesh\nDefine the SPDE representation of the spatial GF. This includes defining the priors for the range and sd of the spatial GF\nDefine the components of the linear predictor. This includes the spatial GF and all eventual covariates\nDefine the observation model using the bru_obs() function\nRun the model using the bru() function\n\n\n\n1. Building the mesh\nThe first task, when dealing with geostatistical models in inlabru is to build the mesh that covers the area of interest. For this purpose we use the function fm_messh_2d.\nOne way to build the mesh is to start from the locations where we have observations, these are contained in the dataset pcod_sf.\n\nmesh = fm_mesh_2d(loc = pcod_sf,           # Build the mesh\n                  max.edge = c(10,20),     # The largest allowed triangle edge length.\n                  offset = c(5,50))       # The automatic extension distance\nggplot() + gg(mesh) + geom_sf(data= pcod_sf, aes(color = factor(present)), size = 0.1) + xlab(\"\") + ylab(\"\")\n\n\n\n\n\n\n\n\nAs you can see from the plot above, some of the locations are very close to each other, this causes some very small triangles. This can be avoided using the option cutoff = which collapses the locations that are closer than a cutoff (those points are collapsed in the mesh construction but, of course, not when it come to estimaation.)\n\nmesh = fm_mesh_2d(loc = pcod_sf,           # Build the mesh\n                  cutoff = 2,\n                  max.edge = c(10,20),     # The largest allowed triangle edge length.\n                  offset = c(5,50))       # The automatic extension distance\nggplot() + gg(mesh) + geom_sf(data= pcod_sf, aes(color = factor(present)), size = 0.1) + xlab(\"\") + ylab(\"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nLook at the documentation for the fm_mesh_2d function typing\n\n?fm_mesh_2d\n\nplay around with the different options and create different meshes.\nThe rule of thumb is that your mesh should be:\n\nfine enough to well represent the spatial variability of your process, but not too fine in order to avoid computation burden\nthe triangles should be regular, avoid long and thin triangles.\nThe mesh should contain a buffer around your area of interest (this is what is defined in the offset option) in order to avoid boundary artefact in the estimated variance.\n\n\n\n\n\n2. Define the SPDE representation of the spatial GF\nTo define the SPDE representation of the spatial GF we use the function inla.spde2.pcmatern. This takes as input the mesh we have defined and the PC-priors definition for \\(\\rho\\) and \\(\\sigma\\) (the range and the marginal standard deviation of the field).\nPC priors Gaussian Random field are defined in (Fuglstad et al. 2018). From a practical perspective for the range \\(\\rho\\) you need to define two paramters \\(\\rho_0\\) and \\(p_{\\rho}\\) such that you believe it is reasonable that\n\\[\nP(\\rho&lt;\\rho_0)=p_{\\rho}\n\\]\nwhile for the margianal variance \\(\\sigma\\) you need to define two parameters \\(\\sigma_0\\) and \\(p_{\\sigma}\\) such that you believe it is reasonable that\n\\[\nP(\\sigma&lt;\\sigma_0)=p_{\\sigma}\n\\]\nYou can use the following function to compute and plot the prior distributions for the range and sd of the Matern field.\n\ndens_prior_range = function(rho_0, p_alpha)\n{\n  # compute the density of the PC prior for the\n  # range rho of the Matern field\n  # rho_0 and p_alpha are defined such that\n  # P(rho&lt;rho_0) = p_alpha\n  rho = seq(0, rho_0*10, length.out =100)\n  alpha1_tilde = -log(p_alpha) * rho_0\n  dens_rho =  alpha1_tilde / rho^2 * exp(-alpha1_tilde / rho)\n  return(data.frame(x = rho, y = dens_rho))\n}\n\ndens_prior_sd = function(sigma_0, p_sigma)\n{\n  # compute the density of the PC prior for the\n  # sd sigma of the Matern field\n  # sigma_0 and p_sigma are defined such that\n  # P(sigma&gt;sigma_0) = p_sigma\n  sigma = seq(0, sigma_0*10, length.out =100)\n  alpha2_tilde = -log(p_sigma)/sigma_0\n  dens_sigma = alpha2_tilde* exp(-alpha2_tilde * sigma) \n  return(data.frame(x = sigma, y = dens_sigma))\n}\n\nHere are some alternatives for defining priors for our model\n\nspde_model1 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(.1, 0.5),\n                                  prior.range = c(30, 0.5))\nspde_model2 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(10, 0.5),\n                                  prior.range = c(1000, 0.5))\nspde_model3 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n\nAnd here we plot the different priors for the range:\n\nggplot() + \n  geom_line(data = dens_prior_range(30,.5), aes(x,y, color = \"model1\")) +\n  geom_line(data = dens_prior_range(1000,.5), aes(x,y, color = \"model2\")) +\n  geom_line(data = dens_prior_range(100,.5), aes(x,y, color = \"model3\")) \n\n\n\n\n\n\n\n\nand for the sd:\n\nggplot() + \n  geom_line(data = dens_prior_sd(1,.5), aes(x,y, color = \"model1\")) +\n  geom_line(data = dens_prior_sd(10,.5), aes(x,y, color = \"model2\")) +\n  geom_line(data = dens_prior_sd(.1,.5), aes(x,y, color = \"model3\")) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Question\n\n\n\nConsider the pcod_sf, the spatial extension and type of the data…is some of the previous choices more reasonable than other? spde_model1spde_model2spde_model3\nNOTE Remember that a prior should be reasonable..but the model should not totally depend on it.\n\n\n\n\n3. Define the components of the linear predictor\nWe have now defined a mesh and a SPDE representation of the spatial GF. We now need to define the model components:\n\ncmp = ~ Intercept(1) + space(geometry, model = spde_model3)\n\nNOTE since the dataframe we use (pcod_sf) is an sf object the input in the space() component is the geometry of the dataset.\n\n\n4. Define the observation model\nOur data are Bernoulli distributed so we can define the observation model as:\n\nformula = present ~ Intercept  + space\n\nlik = bru_obs(formula = formula, \n              data = pcod_sf, \n              family = \"binomial\")\n\n\n\n5. Run the model\nFinally we are ready to run the model\n\nfit1 = bru(cmp,lik)",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#extract-results",
    "href": "day3_practical_5.html#extract-results",
    "title": "Practical 5",
    "section": "Extract results",
    "text": "Extract results\n\nHyperparameters\n\n\n\n\n\n\n Task\n\n\n\nPlot the posterior densities for the range \\(\\rho\\) and the standard deviation \\(\\sigma\\) alog with the prior for both parameters.\n\n\nTake hint\n\nPosterior marginals for the hyperparameters can be extracted from the fitted model as:\n\n\n\nClick here to see the solution\n\n\nCode\nfit1$marginals.hyperpar$'name of the parameter'\n\n\n\n\n\n\n\nClick here to see the solution\n\n\nCode\n# Extract marginal for the range\n\nggplot() + \n  geom_line(data = fit1$marginals.hyperpar$`Range for space`,\n            aes(x,y, color = \"Posterior\")) +\n  geom_line(data = dens_prior_range(100,.5),\n            aes(x,y, color = \"Prior\"))\n\n\nggplot() + \n  geom_line(data = fit1$marginals.hyperpar$`Stdev for space`,\n            aes(x,y, color = \"Posterior\")) +\n  geom_line(data = dens_prior_sd(1,.5), aes(x,y, color = \"Prior\"))",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#spatial-prediction",
    "href": "day3_practical_5.html#spatial-prediction",
    "title": "Practical 5",
    "section": "Spatial prediction",
    "text": "Spatial prediction\nWe now want to extract the estimated posterior mean and sd of spatial GF. To do this we first need to define a grid of points where we want to predict. We do this using the function fm_pixel() which creates a regular grid of points covering the mesh\n\npxl = fm_pixels(mesh)\n\nthen compute the prediction for both the spatial GF and the linear predictor (spatial GF + intercept)\n\npreds = predict(fit1, pxl, ~data.frame(spatial = space,\n                                      total = Intercept + space))\n\nFinally, we can plot the maps\n\nggplot() + geom_sf(data = preds$spatial,aes(color = mean)) + scale_color_scico() + ggtitle(\"Posterior mean\")\n\n\n\n\n\n\n\nggplot() + geom_sf(data = preds$spatial,aes(color = sd)) + scale_color_scico() + ggtitle(\"Posterior sd\")\n\n\n\n\n\n\n\n\nNote The posterior sd is lowest at the observation points. Note how the posterior sd is inflated around the border, this is the “border effect” due to the SPDE representation.\nInstead of predicting over a grid covering the whole mesh, we can limit our predictions to the points where the covariate is defined. We can do this by defining a sf object using coordinates in the object depth_r.\n\npxl1 = data.frame(crds(depth_r), \n                  as.data.frame(depth_r$depth)) %&gt;% \n       filter(!is.na(depth)) %&gt;%\nst_as_sf(coords = c(\"x\",\"y\"))\n\n\n\n\n\n\n\n Task\n\n\n\nProduce prediction over pxl1 unsing the same techniques as before. Plot your results.\n\n\nTake hint\n\nAdd hint details here…\n\n\n\n\nClick here to see the solution\n\n\nCode\npred_pxl1 = predict(fit1, pxl1, ~data.frame(spatial = space,\n                                      total = Intercept + space))\n\nggplot() + geom_sf(data = pred_pxl1$total,aes(color = mean)) + scale_color_scico() + ggtitle(\"Posterior mean\")\n\n\n\n\n\n\n\n\n\nCode\nggplot() + geom_sf(data = pred_pxl1$total,aes(color = sd)) + scale_color_scico() + ggtitle(\"Posterior sd\")\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of computing the posterior mean and standard deviations of the estimated surface, one can also simulate possible realizations of such surface. This will give the user a better idea of the type of realized surfaces one can expect. We can do this using the function generate().\n\n# we simulate 4 samples from the \ngens = generate(fit1, pxl1, ~ (Intercept + space),\n                n.samples = 4)\n\npp = cbind(pxl1, gens)\n\npp %&gt;% select(-depth) %&gt;%\n  pivot_longer(-geometry) %&gt;%\n    ggplot() + \n      geom_sf(aes(color = value)) +\n      facet_wrap(.~name) +\n        scale_color_scico(direction = -1) +\n        ggtitle(\"Sample from the fitted model\")",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#an-alternative-model",
    "href": "day3_practical_5.html#an-alternative-model",
    "title": "Practical 5",
    "section": "An alternative model",
    "text": "An alternative model\nWe now want to check if the depth covatiate has an influende on the probability of presence. We do this in two different models\n\nModel 1 The depth enters the model in a linear way. The linear predictor is then defined as:\n\n\\[\n  \\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s) + \\beta_1\\ \\text{depth}(s)\n\\]\n\nModel 1 The depth enters the model in a non linear way. The linear predictor is then defined as:\n\n\\[\n  \\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s) +  f(\\text{depth}(s))\n\\] where \\(f(.)\\) is a smooth function. We will use a RW2 model for this.\n\n\n\n\n\n\n Task\n\n\n\nFit model 1. Define components, observation model and use the bru() function to estimate the parameters.\nNote Use the scaled version of the covariate stored in depth_r$depth_scaled.\nWhat is the liner effect of depth on the logit probability?\n\n\nTake hint\n\nAdd hint details here…\n\n\n\n\nClick here to see the solution\n\n\nCode\ncmp = ~ Intercept(1) + space(geometry, model = spde_model3) +\n        covariate(depth_r$depth_scaled, model = \"linear\")\n\nformula = present ~ Intercept  + space + covariate\n\nlik = bru_obs(formula = formula, \n              data = pcod_sf, \n              family = \"binomial\")\n\n\nfit2 = bru(cmp, lik)\n\n\n\n\n\nWe now want to fit Model 2 where we allow the effect of depth to be non-linear. To use the RW2 model we need to group the values of depth into distinct classe. To do this we use the function inla.group() which, by default, creates 20 groups. The we can fit the model as usual\n\n# create the grouped variable\ndepth_r$depth_group = inla.group(values(depth_r$depth_scaled))\n\n# run the model\ncmp = ~ Intercept(1) + space(geometry, model = spde_model3) +\n        covariate(depth_r$depth_group, model = \"rw2\")\n\nformula = present ~ Intercept  + space + covariate\n\nlik = bru_obs(formula = formula, \n              data = pcod_sf, \n              family = \"binomial\")\n\n\nfit3 = bru(cmp, lik)\n\n# plot the estimated effect of depth\n\nfit3$summary.random$covariate %&gt;% \n  ggplot() + geom_line(aes(ID,mean)) + \n                                  geom_ribbon(aes(ID, ymin = `0.025quant`, \n                                                      ymax = `0.975quant`), alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nCreate a map of predicted probability from Model 3. You can use a inverse logit function defined as\n\ninv_logit = function(x) (1+exp(-x))^(-1)\n\n\n\nTake hint\n\nThe predict() function can take as input also functions of elements of the components you want to consider\n\n\n\n\nClick here to see the solution\n\n\nCode\ninv_logit = function(x) (1+exp(-x))^(-1)\n\npred3  = predict(fit3, pxl1, ~inv_logit(Intercept + space + covariate) )\n\npred3 %&gt;% ggplot() + \n      geom_sf(aes(color = mean)) +\n        scale_color_scico(direction = -1) +\n        ggtitle(\"Sample from the fitted model\")",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#the-data-2",
    "href": "day3_practical_5.html#the-data-2",
    "title": "Practical 5",
    "section": "The data",
    "text": "The data\nIn this practical we consider the data clmfires in the spatstat library.\nThis dataset is a record of forest fires in the Castilla-La Mancha region of Spain between 1998 and 2007. This region is approximately 400 by 400 kilometres. The coordinates are recorded in kilometres. For more info about the data you can type:\n\n?clmfires\n\nWe first read the data and transform them into an sf object. We also create a polygon that represents the border of the Castilla-La Mancha region. We select the data for year 2004 and only those fires caused by lightning.\n\ndata(\"clmfires\")\npp = st_as_sf(as.data.frame(clmfires) %&gt;%\n                mutate(x = x, \n                       y = y),\n              coords = c(\"x\",\"y\"),\n              crs = NA) %&gt;%\n  filter(cause == \"lightning\",\n         year(date) == 2004)\n\npoly = as.data.frame(clmfires$window$bdry[[1]]) %&gt;%\n  mutate(ID = 1)\n\nregion = poly %&gt;% \n  st_as_sf(coords = c(\"x\", \"y\"), crs = NA) %&gt;% \n  dplyr::group_by(ID) %&gt;% \n  summarise(geometry = st_combine(geometry)) %&gt;%\n  st_cast(\"POLYGON\") \n  \nggplot() + geom_sf(data = region, alpha = 0) + geom_sf(data = pp)  \n\n\n\n\n\n\n\nFigure 2: Distribution of the observed forest fires caused by lightning in Castilla-La Mancha in 2004",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#HPP",
    "href": "day3_practical_5.html#HPP",
    "title": "Practical 5",
    "section": "Fit a homogeneous Poisson Process",
    "text": "Fit a homogeneous Poisson Process\nAs a first exercise we are going to fit a homogeneous Poisson process (HPP) to the data. This is a model that assume constant intensity over the whole space so our linear predictor is then:\n\\[\n\\eta(s) = \\log\\lambda(s) = \\beta_0 , \\ \\mathbf{s}\\in\\Omega\n\\]\nso the likelihood can be written as:\n\\[\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n& = \\exp\\left( -\\int_{\\Omega}\\exp(\\beta_0)ds\\right)\\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n\\end{aligned}\n\\]\nwhere \\(|\\Omega|\\) is the area of the domain of interest.\nWe need to approximate the integral using a numerical integration scheme as:\n\\[\n\\approx\\exp\\left(-\\sum_{k=1}^{N_k}w_k\\lambda(s_k)\\right)\\prod_{i=1}^n \\lambda(\\mathbf{s}_i)\n\\]\nWhere \\(N_k\\) is the number of integration points \\(s_1,\\dots,s_{N_k}\\) and \\(w_1,\\dots,w_{N_k}\\) are the integration weights.\nIn this case, since the intensity is constant, the integration scheme is really simple: it is enough to consider one random point inside the domain with weight equal to the area of the domain.\n\n# define integration scheme\n\nips = st_sf(\ngeometry = st_sample(region, 1)) # some random location inside the domain\nips$weight = st_area(region) # integration weight is the area of the domain\n\ncmp = ~ 0 + beta_0(1)\n\nformula = geometry ~ beta_0\n\nlik = bru_obs(data = pp,\n              family = \"cp\",\n              formula = formula,\n              ips = ips)\nfit1 = bru(cmp, lik)\n\n\n\n\n\n\n\n Task\n\n\n\n\nPlot the estimated posterior distribution of the intensity\nCompare the estimated expected number of fires on the whole domain with the observed ones.\n\n\n\nTake hint\n\nRemember that in the inlabru framework we model the log intensity \\(\\eta = log(\\lambda)\\)\n\n\n\n\nClick here to see the solution\n\n\nCode\n# 1) The estimated posterior distribution of the  intensity is\n\npost_int = inla.tmarginal(function(x) exp(x), fit1$marginals.fixed$beta_0)\npost_int %&gt;% ggplot() + geom_line(aes(x,y))\n\n\n\n\n\n\n\n\n\nCode\n# 2) To compute the expected number of points in the area we need to multiply the\n# estimated intensity by the area of the domain.\n# In the same plot we also show the number of observed fires as a vertical line.\n\npost_int = inla.tmarginal(function(x) st_area(region)* exp(x), fit1$marginals.fixed$beta_0)\npost_int %&gt;% ggplot() + geom_line(aes(x,y)) +\n  geom_vline(xintercept = dim(pp)[1])",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#NHPP",
    "href": "day3_practical_5.html#NHPP",
    "title": "Practical 5",
    "section": "Fit an Inhomogeneous Poisson Process",
    "text": "Fit an Inhomogeneous Poisson Process\nThe model above has the clear disadvantages that assumes a constant intensity and from Figure 2 we clearly see that this is not the case.\nThe library spatstat contains also some covariates that can help explain the fires distribution. Figure @fit-altitude shows the location of fires together with the (scaled) altitude.\n\n#|label: fig-altitude\n#|fig-cap: \"Distribution of the observed forest fires and scaled altitude\"\n#| \nelev_raster = rast(clmfires.extra[[2]]$elevation)\nelev_raster = scale(elev_raster)\nggplot() + \n  geom_spatraster(data = elev_raster) + \n  geom_sf(data = pp) +\n  scale_fill_scico()\n\n\n\n\n\n\n\n\nWe are now going to use the altitude as a covariate to explain the variability of the intensity \\(\\lambda(s)\\) over the domain of interest.\nOur model is \\[\n\\log\\lambda(s) = \\beta_0 + \\beta_1x(s)\n\\] where \\(x(s)\\) is the altitude at location \\(s\\).\nThe likelihood becomes:\n\\[\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n& = \\exp \\left( -\\int_\\Omega \\exp(\\beta_0 + \\beta_1x(s)) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n\\end{aligned}\n\\]\nNow we need to choose an integration scheme to solve the integral.\nIn this case we will take a simple grid based approach where each quadrature location has an equal weight. Our grid consists of \\(N_k = 1000\\) points and the weights are all equal to \\(|\\Omega|/N_k\\).\n\n#|label: fig-int2\n#|fig-cap: \"Integration scheme.\"\n\nn.int = 1000\nips = st_sf(geometry = st_sample(region,\n            size = n.int,\n            type = \"regular\"))\n\nips$weight = st_area(region) / n.int\nggplot() + geom_sf(data = ips, aes(color = weight)) + geom_sf(data= region, alpha = 0)\n\n\n\n\n\n\n\n\nOBS: The implicit assumption here is that the intensity is constant inside each grid box, and so is the covariate!!\nWe can now fit the model:\n\ncmp = ~ Intercept(1) + elev(elev_raster, model = \"linear\")\nformula = geometry ~ Intercept + elev\nlik = bru_obs(data = pp,\n              family = \"cp\",\n              formula = formula,\n              ips = ips)\nfit2 = bru(cmp, lik)\n\n\n\n\n\n\n\n Task\n\n\n\nWhat is the effect of the altitude on the (log) intensity of the process?\n\n\nTake hint\n\nYou can look at the summary for the fixed effects\n\n\n\n\nClick here to see the solution\n\n\nCode\nfit2$summary.fixed\n\n\n                mean         sd 0.025quant   0.5quant 0.975quant       mode kld\nIntercept -6.5875947 0.10093471 -6.7854231 -6.5875947 -6.3897663 -6.5875947   0\nelev       0.6369707 0.06737839  0.5049115  0.6369707  0.7690299  0.6369707   0\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n⚠️ WARNING!!⚠️ When fitting a Point process, the integration scheme has to be fine enough to capture the spatial variability of the covariate!!\n\n\n\n\n\n\n\n\n Task\n\n\n\nRerun the model with the altitude as covariate, but this time change the integration scheme as follows:\n\nn.int2 = 50\n\nips2 = st_sf(geometry = st_sample(region,\n            size = n.int2,\n            type = \"regular\"))\nips2$weight = st_area(region) / n.int2\n\nWhat happens to the effect of the covariate?\n\n\nTake hint\n\nRe-run the model changing the integration scheme in the ips input of the bru_obs() function.\n\n\n\n\nClick here to see the solution\n\n\nCode\nlik_bis = bru_obs(data = pp,\n              family = \"cp\",\n              formula = formula,\n              ips = ips2)\n\nfit2bis = bru(cmp, lik_bis)\n\n# you can the check the differences between the two models\nrbind(fit2$summary.fixed,\nfit2bis$summary.fixed)\n\n\n                 mean         sd 0.025quant   0.5quant 0.975quant       mode\nIntercept  -6.5875947 0.10093471 -6.7854231 -6.5875947 -6.3897663 -6.5875947\nelev        0.6369707 0.06737839  0.5049115  0.6369707  0.7690299  0.6369707\nIntercept1 -6.5388807 0.10327882 -6.7413035 -6.5388807 -6.3364579 -6.5388807\nelev1       0.5042466 0.07192471  0.3632768  0.5042466  0.6452165  0.5042466\n           kld\nIntercept    0\nelev         0\nIntercept1   0\nelev1        0\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nNow we want to predict the log-intensity over the whole domain. Use the grid from the elevation raster to predict the intensity over the domain.\n\nest_grid = st_as_sf(data.frame(crds(elev_raster)), coords = c(\"x\",\"y\"))\nest_grid  = st_intersection(est_grid, region)\n\n\n\n\nClick here to see the solution\n\n\nCode\npreds2 = predict(fit2, est_grid, ~ data.frame(log_scale = Intercept + elev,\n\n                                              lin_scale = exp(Intercept + elev)))\n# then visualize it like\npreds2$log_scale %&gt;% \n  ggplot() +\n  geom_sf(aes(color = mean)) +\n  scale_color_scico()\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we want to use the fitted model to estimate the total number of fires over the whole region. To do this we first have to fine the expected number of fires as:\n\\[\nE(N_{\\Omega}) = \\int_{\\Omega}\\exp(\\lambda(s))\\ ds\n\\]\nThen simulate possible realizations of \\(N_{\\Omega}\\) to include also the likelihood variability in our estimate:\n\nN_fires = generate(fit2, ips,\n                      formula = ~ {\n                        lambda = sum(weight * exp(elev + Intercept))\n                        rpois(1, lambda)},\n                    n.samples = 2000)\n\nggplot(data = data.frame(N = as.vector(N_fires))) +\n  geom_histogram(aes(x = N),\n                 colour = \"blue\",\n                 alpha = 0.5,\n                 bins = 20) +\n  geom_vline(xintercept = nrow(pp),\n             colour = \"red\") +\n  theme_minimal() +\n  xlab(expression(Lambda))",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#LGCP",
    "href": "day3_practical_5.html#LGCP",
    "title": "Practical 5",
    "section": "Fit a Log-Gaussian Cox Process",
    "text": "Fit a Log-Gaussian Cox Process\nFinally we want to fit a LGCP with log intensity:\n\\[\n\\log(s) = \\beta_0 + \\beta_1x + u(s)\n\\]\nwhere \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) the effect of (standarized) altitude \\(x(s)\\) as before and \\(u(s)\\) is a Gaussian Random field defined through the SPDE approach.\n\nDefine the mesh\nThe first step, as any time we use the SPDE approach is to defie the mesh and the priors for the marginal variance and range:\n\nmesh = fm_mesh_2d(boundary = region,\n                  max.edge = c(5, 10),\n                  cutoff = 4, crs = NA)\n\nggplot() + gg(mesh) + geom_sf(data = pp)\n\n\n\n\n\n\n\nspde_model =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n\nWe can then define the integration weight. Here we use the same points to define the SPDE approximation and to approximate the integral in the likelihood. We will see later that this does not have to be like this, BUT integration weight and SPDE weights have to be consistent with each other!\n\nips = fm_int(mesh, samplers = region)\n\nggplot() + geom_sf(data = ips, aes(color = weight)) +\n  gg(mesh) +\n   scale_color_scico()\n\n\n\n\n\n\n\n\n\n\nRun the model\n\n\n\n\n\n\n Task\n\n\n\nSet up the components and the formula for the model above by completing the code below and run the model.\n\ncmp = ~ ...\n\nformula = geometry ~ ...\n\nlik = bru_obs(\"cp\",\n              formula = formula,\n              data = pp,\n              ips = ...)\n\nfit3 = bru(cmp, lik)\n\n\n\nTake hint\n\nThe model has three components: intercept, linear effect of altitude and the spatial GRF\n\n\n\n\nClick here to see the solution\n\n\nCode\ncmp = ~ Intercept(1) + space(geometry, model = spde_model) + elev(elev_raster, model = \"linear\")\n\nformula = geometry ~ Intercept + space + elev\n\nlik = bru_obs(\"cp\",\n              formula = formula,\n              data = pp,\n              ips = ips)\n\nfit3 = bru(cmp, lik)\n\n\n\n\n\nNote when running the model above you will get a warning:\n\n\nWarning in bru_log_warn(msg): Model input 'elev_raster' for 'elev' returned some NA values.\nAttempting to fill in spatially by nearest available value.\nTo avoid this basic covariate imputation, supply complete data.\n\n\nIt means that the bru() function cannot find the covariate values for some of the mesh nodes. This is a common situation. As the warning says, the bru() function automatically imputes the value of the covarite using the nearest nodes. This increases the running time of the bru() function, so one solution is to impute the values of the covariate over the whole mesh ‘before’ running the bru() function.\nHere, we notice that there is a single point for which elevation values are missing (see Figure 3 the red point that lies outside the raster extension ).\n\n\n\n\n\n\n\n\nFigure 3: Integration scheme for numerical approximation of the stochastic integral in La Mancha Region\n\n\n\n\n\nTo solve this, we can increase the raster extension so it covers all both data-points and quadrature locations as well. Then, we can use the bru_fill_missing() function to input the missing values with the nearest-available-value. We can achieve this using the following code:\n\n# Extend raster ext by 30 % of the original raster so it covers the whole mesh\nre &lt;- extend(elev_raster, ext(elev_raster)*1.3)\n# Convert to an sf spatial object\nre_df &lt;- re %&gt;% stars::st_as_stars() %&gt;%  st_as_sf(na.rm=F)\n# fill in missing values using the original raster \nre_df$lyr.1 &lt;- bru_fill_missing(elev_raster,re_df,re_df$lyr.1)\n# rasterize\nelev_rast_p &lt;- stars::st_rasterize(re_df) %&gt;% rast()\nggplot() + geom_spatraster(data = elev_rast_p) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe bru_fill_missing() function was added mainly to handle very local infilling on domain boundaries. For properly missing data, one should consider doing a proper model of the spatial field instead.",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day3_practical_5.html#results",
    "href": "day3_practical_5.html#results",
    "title": "Practical 5",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\n Task\n\n\n\nPlot the estimated mean and standard deviation of the spatial GF and the log-intensity over the domain of interest\n\n\nTake hint\n\nUse the fm_pixels() and predict() functions.\n\n\n\n\nClick here to see the solution\n\n\nCode\npxl = fm_pixels(mesh, mask= region, dims = c(200,200))\npreds = predict(fit3, pxl, ~data.frame(spde = space,\n                                       log_int = Intercept + space + elev))\n\n#and plot as \nlibrary(scico)\nlibrary(patchwork)\n\nggplot(data= preds$spde) + \n  geom_sf(aes(color = mean)) + \n  scale_color_scico() +\n ggtitle(\"spde mean\") +\nggplot(data=preds$spde ) +\n  geom_sf(aes(color = sd)) +\n  scale_color_scico() +\n ggtitle(\"spde sd\") +\n\nggplot(data=preds$log_int) + \n  geom_sf(aes(color = mean)) + \n  scale_color_scico() +\n ggtitle(\"log-int mean\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=preds$log_int) + \n  geom_sf(aes(color = sd)) +\n  scale_color_scico() +\n ggtitle(\"log-int sd\") +\n  plot_layout(ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of just looking at the posterior mean and standard deviation, it can be usefull to look at simulated fields from the posterior distribution. This is because the mean field is, by definition, smoother than any realization of the field. So looking at simulation can give us a better idea of how the field might look like. We can do this using the generate() function:\n\nsim_fields = generate(fit3, pxl, ~data.frame(spde = space,\n                                       log_int = Intercept + space + elev),\n                     n.samples = 4)\n\ncbind(pxl,sapply(sim_fields, function(x) x$spde)) %&gt;%\n  pivot_longer(-geometry) %&gt;%\n  ggplot() + geom_sf(aes(color = value)) + \n  facet_wrap(.~name) + scale_color_scico() +\n  ggtitle(\"simulated spatial fields\")\n\n\n\n\n\n\n\ncbind(pxl,sapply(sim_fields, function(x) x$log_int)) %&gt;%\n  pivot_longer(-geometry) %&gt;%\n  ggplot() + geom_sf(aes(color = value)) + \n  facet_wrap(.~name) + scale_color_scico() + \n  ggtitle(\"simulated log intensity\")",
    "crumbs": [
      "Home",
      "Practical 5"
    ]
  },
  {
    "objectID": "day5_practical_8.html",
    "href": "day5_practical_8.html",
    "title": "Practical 8",
    "section": "",
    "text": "Aim of this practical:\nIn this practical we are going to look at some model comparison and validation techniques.\nDownload Practical 8 R script",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#the-data",
    "href": "day5_practical_8.html#the-data",
    "title": "Practical 8",
    "section": "The data",
    "text": "The data\nIn the next exercise, we will explore data from a combination of several NOAA shipboard surveys conducted on pan-tropical spotted dolphins in the Gulf of Mexico. The data set is available in inlabru (originally obtianed from the dsm R package) and contains the following information:\n\nA total of 47 observations of groups of dolphins were detected. The group size was recorded, as well as the Beaufort sea state at the time of the observation.\nTransect width is 16 km, i.e. maximal detection distance 8 km (transect half-width 8 km).\n\nWe can load and visualize the data as follows:\n\nmexdolphin &lt;- mexdolphin_sf\nmexdolphin$depth &lt;- mexdolphin$depth %&gt;% mutate(depth=scale(depth)%&gt;%c())\nmapviewOptions(basemaps = c( \"OpenStreetMap.DE\"))\n\nmapview(mexdolphin$points,zcol=\"size\")+\n  mapview(mexdolphin$samplers)+\n mapview(mexdolphin$ppoly )",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#the-workflow",
    "href": "day5_practical_8.html#the-workflow",
    "title": "Practical 8",
    "section": "The workflow",
    "text": "The workflow\nTo model the density of spotted dolphins we take a thinned point process model of the form:\n\\[\np(\\mathbf{y} | \\lambda)  \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) p(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) p(\\mathbf{s}_i))\n\\tag{1}\\]\nWhen fitting a distance sampling model we need to fulfill the following tasks:\n\nBuild the mesh\nDefine the SPDE representation of the spatial GF. This includes defining the priors for the range and sd of the spatial GF\nDefine the components of the linear predictor. This includes the spatial GF and all eventual covariates\nDefine the observation model using the bru_obs() function\nRun the model using the bru() function\n\n\nBuilding the mesh\nThe first task is to build the mesh that covers the area of interest. For this purpose we use the function fm_mesh_2d. To do so, we need to define the area of interest. We can either use a predefined boundary or create a non convex hull surrounding the location of the specie sightseeings\n\nnon-covex hulldomain boundary\n\n\n\nboundary0 = fm_nonconvex_hull(mexdolphin$points,convex = -0.1)\n\nmesh_0 = fm_mesh_2d(boundary = boundary0,\n                          max.edge = c(30, 150), # The largest allowed triangle edge length.\n                          cutoff = 15,\n                          crs = fm_crs(mexdolphin$points))\nggplot() + gg(mesh_0)\n\n\n\n\n\n\n\n\n\n\nThe mexdolphin object contains a predefined region of interest which can be accessed through mexdolphin$ppoly\n\nmesh_1 = fm_mesh_2d(boundary = mexdolphin$ppoly,\n                    max.edge = c(30, 150),\n                    cutoff = 15,\n                    crs = fm_crs(mexdolphin$points))\nggplot() + gg(mesh_1)\n\n\n\n\n\n\n\n\n\n\n\nKey parameters in mesh construction include: max.edge for maximum triangle edge lengths, offset for inner and outer extensions (to prevent edge effects), and cutoff to avoid overly small triangles in clustered areas.\n\n\n\n\n\n\nNote\n\n\n\nGeneral guidelines for creating the mesh\n\nCreate triangulation meshes with fm_mesh_2d()\nMove undesired boundary effects away from the domain of interest by extending to a smooth external boundary\nUse a coarser resolution in the extension to reduce computational cost (max.edge=c(inner, outer))\nUse a fine resolution (subject to available computational resources) for the domain of interest (inner correlation range) and filter out small input point clusters (0 &lt; cutoff &lt; inner)\nCoastlines and similar can be added to the domain specification in fm_mesh_2d() through the boundary argument.\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nLook at the documentation for the fm_mesh_2d function typing\n\n?fm_mesh_2d\n\nplay around with the different options and create different meshes. You can compare these against a pre-computed mesh available by typing plot(mexdolphin$mesh)\nThe rule of thumb is that your mesh should be:\n\nfine enough to well represent the spatial variability of your process, but not too fine in order to avoid computation burden\nthe triangles should be regular, avoid long and thin triangles.\nThe mesh should contain a buffer around your area of interest (this is what is defined in the offset option) in order to avoid boundary artefact in the estimated variance.\n\n\n\n\n\nDefine the SPDE representation of the spatial GF\nTo define the SPDE representation of the spatial GF we use the function inla.spde2.pcmatern. This takes as input the mesh we have defined and the PC-priors definition for \\(\\rho\\) and \\(\\sigma\\) (the range and the marginal standard deviation of the field).\nPC priors Gaussian Random field are defined in (Fuglstad et al. 2018). From a practical perspective for the range \\(\\rho\\) you need to define two paramters \\(\\rho_0\\) and \\(p_{\\rho}\\) such that you believe it is reasonable that\n\\[\nP(\\rho&lt;\\rho_0)=p_{\\rho}\n\\]\nwhile for the marginal variance \\(\\sigma\\) you need to define two parameters \\(\\sigma_0\\) and \\(p_{\\sigma}\\) such that you believe it is reasonable that\n\\[\nP(\\sigma&gt;\\sigma_0)=p_{\\sigma}\n\\]\n\n\n\n\n\n\n Question\n\n\n\nTake a look at the code below and select which of the following statements about the specified Matern PC priors are true.\n\nspde_model &lt;- inla.spde2.pcmatern(mexdolphin$mesh,\n  prior.sigma = c(2, 0.01),\n  prior.range = c(50, 0.01)\n)\n\n\n there is probability of 0.01 that the spatial range is greater or equal than 50 the probability that the spatial range is smaller than 50 is very small the probability that the marginal standard deviation is smaller than 2 is very small there is probability of 0.99 that the marginal standard deviation is less or equal than 2\n\n\n\n\n\nDefine the components of the linear predictor\nWe have now defined a mesh and a SPDE representation of the spatial GF. We now need to define the model components.\nFirst, we need to define the detection function. Here, we will define a half-normal detection probability function. This must take distance as its first argument and the linear predictor of the sigma parameter as its second:\n\nhn &lt;- function(distance, sigma) {\n  exp(-0.5 * (distance / sigma)^2)\n}\n\nWe need to now separately define the components of the model including the SPDE model, the Intercept, the effect of depth and the detection function parameter sigma.\n\ncmp &lt;- ~ space(main = geometry, model = spde_model) +\n  sigma(1,\n    prec.linear = 1,\n    marginal = bm_marginal(qexp, pexp, dexp, rate = 1 / 8)\n  ) +\n  Intercept(1)\n\n\n\n\n\n\n\nNote\n\n\n\nTo control the prior distribution for the sigma parameter, we use a transformation mapper that converts a latent variable into an exponentially distributed variable with expectation 8 (this is a somewhat arbitrary value, but motivated by the maximum observation distance W)\nThe marginal argument in the sigma component specifies the transformation function taking N(0,1) to Exponential(1/8).\n\n\nThe formula, which describes how these components are combined to form the linear predictor\n\\[\\log \\color{red}{\\tilde{\\lambda}(s)} = \\overbrace{\\log \\lambda (s)}^{\\beta_0 + \\xi(s)} + \\overbrace{\\log \\color{red}{g(d(s))}}^{-0.5~d(\\mathbf{s})^2\\sigma^{-2}}\\]\n\neta &lt;- geometry + distance ~ space +\n  log(hn(distance, sigma)) +\n  Intercept + log(2) \n\nHere, the log(2) offset in the predictor takes care of the two-sided detections\n\n\nDefine the observation model\ninlabru has support for latent Gaussian Cox processes through the cp likelihood family. To fit a point process model recall that we need to approximate the integral in using a numerical integration scheme as:\n\\[\n\\approx\\exp\\left(-\\sum_{k=1}^{N_k}w_k\\lambda(s_k)\\right)\\prod_{i=1}^n \\lambda(\\mathbf{s}_i)\n\\]\nThus, we first create our integration scheme using the fm_int function by specifying integration domains for the spatial and distance dimensions.\nHere we use the same points to define the SPDE approximation and to approximate the integral in Equation 1, so that the integration weight and SPDE weights are consistent with each other. We also need to explicitly integrate over the distance dimension so we use the fm_mesh_1d() to create mesh over the samplers (which are the transect lines in this dataset, so we need to tell inlabru about the strip half-width).\n\n# build integration scheme\ndistance_domain &lt;-  fm_mesh_1d(seq(0, 8,\n                              length.out = 30))\nips = fm_int(list(geometry = mexdolphin$mesh,\n                  distance = distance_domain),\n             samplers = mexdolphin$samplers)\n\nNow, we just need to supply the sf object as our data and the integration scheme ips:\n\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = mexdolphin$points,\n              ips = ips)\n\nThen we fit the model, passing both the components and the observaional model\n\nfit = bru(cmp, lik)\n\n\n\n\n\n\n\nNote\n\n\n\ninlabru supports a shortcut for defining the integration points using the domain and samplers argument of bru_obs(). This domain argument expects a list of named domains with inputs that are then internally passed to fm_int() to build the integration scheme. The samplers argument is used to define subsets of the domain over which the integral should be computed. An equivalent way to define the same model as above is:\n\nlik = bru_obs(formula = eta, \n              data = mexdolphin$points, \n              family = \"cp\",\n              domain = list(\n                geometry = mesh,\n                distance = fm_mesh_1d(seq(0, 8, length.out = 30))),\n              samplers = mexdolphin$samplers)",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#visualize-model-results",
    "href": "day5_practical_8.html#visualize-model-results",
    "title": "Practical 8",
    "section": "Visualize model Results",
    "text": "Visualize model Results\n\nPosterior summaries\nWe can use the fit$summary.fixed and summary.hyperpar to obtain posterior summaries of the model parameters.\n\n\n\n\n\n\n\n\n\nmean\n0.025quant\n0.975quant\n\n\n\n\nsigma\n−0.05\n−0.46\n0.36\n\n\nIntercept\n−8.16\n−9.29\n−7.34\n\n\nRange for space\n295.48\n110.54\n673.68\n\n\nStdev for space\n0.81\n0.42\n1.39\n\n\n\n\n\n\n\nLook at the SPDE parameter posteriors as follows:\n\nplot( spde.posterior(fit, \"space\", what = \"range\")) +\nplot( spde.posterior(fit, \"space\", what = \"log.variance\"))  \n\n\n\n\n\n\n\n\n\n\nModel predictions\nWe now want to extract the estimated posterior mean and sd of spatial GF. To do this we first need to define a grid of points where we want to predict. We do this using the function fm_pixel() which creates a regular grid of points covering the mesh\n\npxl &lt;- fm_pixels(mexdolphin$mesh, dims = c(200, 100), mask = mexdolphin$ppoly)\n\nthen compute the prediction for both the spatial GF and the linear predictor (spatial GF + intercept)\n\npr.int = predict(fit, pxl, ~data.frame(spatial = space,\n                                      lambda = exp(Intercept + space)))\n\nFinally, we can plot the maps of the spatial effect\n\nggplot() + geom_sf(data = pr.int$spatial,aes(color = mean)) + scale_color_scico() + ggtitle(\"Posterior mean\")\n\n\n\n\n\n\n\nggplot() + geom_sf(data = pr.int$spatial,aes(color = sd)) + scale_color_scico() + ggtitle(\"Posterior sd\")\n\n\n\n\n\n\n\n\nNote The posterior sd is lowest at the observation points. Note how the posterior sd is inflated around the border, this is the “border effect” due to the SPDE representation.\n\n\n\n\n\n\n Task\n\n\n\nUsing the predictions stored in pr.int, produce a map of the posterior mean intensity.\n\n\nTake hint\n\nRecall that the predicted intensity is given by \\(\\lambda(s) = \\exp(\\beta_0+\\xi(s))\\)\n\n\n\n\nClick here to see the solution\n\n\nCode\nggplot() + \n  geom_sf(data = pr.int$lambda,aes(color = mean)) +\n  scale_color_scico(palette = \"imola\") +\n  ggtitle(\"Posterior mean\")\n\n\n\n\n\n\n\n\n\n\n\n\nWe can predict the detection function in a similar fashion.Here, we should make sure that it doesn’t try to evaluate the effects of components that can’t be evaluated using the given input data.\n\ndistdf &lt;- data.frame(distance = seq(0, 8, length.out = 100))\ndfun &lt;- predict(fit, distdf, ~ hn(distance, sigma))\nplot(dfun)",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#abundance-estimates",
    "href": "day5_practical_8.html#abundance-estimates",
    "title": "Practical 8",
    "section": "Abundance estimates",
    "text": "Abundance estimates\nThe mean expected number of animals can be computed by integrating the intensity over the region of interest as follows:\n\npredpts &lt;- fm_int(mexdolphin$mesh, mexdolphin$ppoly)\nLambda &lt;- predict(fit, predpts, ~ sum(weight * exp(space + Intercept)))\nLambda\n\n      mean       sd   q0.025     q0.5   q0.975   median sd.mc_std_err\n1 244.5924 64.83581 158.7342 232.2831 418.7614 232.2831       6.79121\n  mean.mc_std_err\n1        7.841823\n\n\nTo fully propagate the uncertainty on the expected number animals we can draw Monte Carlo samples from the fitted model as follows (this could take a couple of minutes):\n\nNs &lt;- seq(50, 450, by = 1)\nNest &lt;- predict(fit, predpts,\n  ~ data.frame(\n    N = Ns,\n    density = dpois(\n      Ns,\n      lambda = sum(weight * exp(space + Intercept))\n    )\n  ),\n  n.samples = 2000\n)\n\nWe can compare this with a simpler “plug-in” approximation:\n\nNest &lt;- dplyr::bind_rows(\n  cbind(Nest, Method = \"Posterior\"),\n  data.frame(\n    N = Nest$N,\n    mean = dpois(Nest$N, lambda = Lambda$mean),\n    mean.mc_std_err = 0,\n    Method = \"Plugin\"\n  )\n)\n\nThen, we can visualize the result as follows:\n\nggplot(data = Nest) +\n  geom_line(aes(x = N, y = mean, colour = Method)) +\n  geom_ribbon(\n    aes(\n      x = N,\n      ymin = mean - 2 * mean.mc_std_err,\n      ymax = mean + 2 * mean.mc_std_err,\n      fill = Method,\n    ),\n    alpha = 0.2\n  ) +\n  geom_line(aes(x = N, y = mean, colour = Method)) +\n  ylab(\"Probability mass function\")",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#model-checks",
    "href": "day5_practical_8.html#model-checks",
    "title": "Practical 8",
    "section": "Model checks",
    "text": "Model checks\nLastly, we can assess the goodness-of-fit of the models by comparing the observed counts across different distance bins and the expected counts and their associated uncertainty:\n\nbc &lt;- bincount(\n  result = fit,\n  observations = mexdolphin$points$distance,\n  breaks = seq(0, max(mexdolphin$points$distance), length.out = 9),\n  predictor = distance ~ hn(distance, sigma)\n)\nattributes(bc)$ggp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nFit a model using a hazard detection function instead and compare the GoF of this model with that from the half-normal detection model. Recall that the hazard detection function is given by:\n\\[\ng(\\mathbf{s}|\\sigma) = 1 - \\exp(-(d(\\mathbf{s})/\\sigma)^{-1})\n\\]\n\n\nTake hint\n\nThe hazard function can be codes as:\n\nhr &lt;- function(distance, sigma) {\n  1 - exp(-(distance / sigma)^-1)\n}\n\nYou can use the same prior for the sigma parameter as for the half-Normal model (such parameters aren’t always comparable, but in this example it’s a reasonable choice). You can also use the lgcp function as a shortcut to fit the model (type ?lgcp for further details).\n\n\n\n\nClick here to see the solution\n\n\nCode\nformula1 &lt;- geometry + distance ~ space +\n  log(hr(distance, sigma)) +\n  Intercept + log(2)\n\n# here we use the shorcut to specify the model\nfit1 &lt;- lgcp(\n  components = cmp,\n  mexdolphin$points,\n  samplers = mexdolphin$samplers,\n  domain = list(\n    geometry = mexdolphin$mesh,\n    distance = fm_mesh_1d(seq(0, 8, length.out = 30))\n  ),\n  formula = formula1\n)\n\nbc1 &lt;- bincount(\n  result = fit1,\n  observations = mexdolphin$points$distance,\n  breaks = seq(0, max(mexdolphin$points$distance), length.out = 9),\n  predictor = distance ~ hn(distance, sigma)\n)\nattributes(bc1)$ggp",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#sec-prep",
    "href": "day5_practical_8.html#sec-prep",
    "title": "Practical 8",
    "section": "Data Preparation",
    "text": "Data Preparation\nThe following example use the gorillas dataset available in the inlabru library.\nThe data give the locations of Gorilla’s nests in an area: ::: {.cell layout-align=“center”}\ngorillas_sf &lt;- inlabru::gorillas_sf\nnests &lt;- gorillas_sf$nests\nboundary &lt;- gorillas_sf$boundary\n\nggplot() + geom_sf(data = nests) +\n  geom_sf(data = boundary, alpha = 0)\n\n\n\n\nLocation of gorilla nests\n\n\n\n:::\nThe dataset also contains covariates in the form or raster data. We consider two of them here: ::: {.cell}\ngcov = gorillas_sf_gcov()\nelev_cov &lt;- gcov$elevation\ndist_cov &lt;-  gcov$waterdist\n:::\n\n\n\n\n\nCovariates\n\n\n\n\nNote: the covariates have been expanded to cover all the nodes in the mesh.\n\nTo obtain the count data, we rasterize the species counts to match the spatial resolution of the covariates available. Then we aggregate the pixels to a rougher resolution (5x5 pixels in the original covariate raster dimensions). Finally, we mask regions outside the study area.\nIn addition we compute the area of each grid cell.\n\n# Rasterize data\ncounts_rstr &lt;-\n  terra::rasterize(vect(nests), gcov, fun = sum, background = 0) %&gt;%\n  terra::aggregate(fact = 5, fun = sum) %&gt;%\n  mask(vect(sf::st_geometry(boundary)))\nplot(counts_rstr)\n\n\n\n\nCounts of gorilla nests\n\n\n\n# compute cell area\ncounts_rstr &lt;- counts_rstr %&gt;%\n  cellSize(unit = \"km\") %&gt;%\n  c(counts_rstr)\n\nTo create our dataset of counts, we extract also the coordinate of center point of each raster pixel. In addition we create a column with presences and one with the pixel area\n\ncounts_df &lt;- crds(counts_rstr, df = TRUE, na.rm = TRUE) %&gt;%\n  bind_cols(values(counts_rstr, mat = TRUE, na.rm = TRUE)) %&gt;%\n  rename(count = sum) %&gt;%\n  mutate(present = (count &gt; 0) * 1L) %&gt;%\n  st_as_sf(coords = c(\"x\", \"y\"), crs = st_crs(nests))\n\nWe then aggregate the covariates to the same resolution as the nest counts and scale them.\n\nelev_cov1 &lt;- elev_cov %&gt;% \n  terra::aggregate(fact = 5, fun = mean) %&gt;% scale()\ndist_cov1 &lt;- dist_cov %&gt;% \n  terra::aggregate(fact = 5, fun = mean) %&gt;% scale()\n\n\n\n\n\n\n\n\n\nFigure 1: Covariates\n\n\n\n\n\n\nMesh building\nWe now define the mesh and the spde object.\n\n\nmesh &lt;- fm_mesh_2d(\n  loc = st_as_sfc(counts_df),\n  max.edge = c(0.5, 1),\n  crs = st_crs(counts_df)\n)\n\nmatern &lt;- inla.spde2.pcmatern(mesh,\n  prior.sigma = c(1, 0.01),\n  prior.range = c(5, 0.01)\n)\n\n\n\n\n\n\nMesh over the count locations\n\n\n\n\nIn our dataset, the number of zeros is quite substantial, and our model may struggle to account for them adequately. To address this, we should select a model capable of handling an “inflated” number of zeros, exceeding what a standard Poisson model would imply. For this purpose, we opt for a “zero-inflated Poisson model,” commonly abbreviated as ZIP.",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#sec-zip",
    "href": "day5_practical_8.html#sec-zip",
    "title": "Practical 8",
    "section": "Zero-Inflated model (Type1)",
    "text": "Zero-Inflated model (Type1)\nWe fit now a Zero-Inflated model to our data.\nThe Type 1 Zero-inflated Poisson model is defined as follows:\n\\[\n\\text{Prob}(y\\vert\\dots)=\\pi\\times 1_{y=0}+(1-\\pi)\\times \\text{Poisson}(y)\n\\]\nHere, \\(\\pi=\\text{logit}^{-1}(\\theta)\\)\nThe expected value and variance for the counts are calculated as:\n\\[\n\\begin{gathered}\nE(count)=(1-\\pi)\\lambda \\\\\nVar(count)= (1-\\pi)(\\lambda+\\pi \\lambda^2)\n\\end{gathered}\n\\tag{2}\\]\nThis model has two parameters:\n\nThe probability of excess zero \\(\\pi\\) - This is a hyperparameter and therefore it is constant\nThe mean of the Poisson distribution \\(\\lambda\\). This is linked to the linear predictor as: \\[\n\\eta = E\\log(\\lambda) = \\log(E) + \\beta_0 + \\beta_1\\text{Elevation} + \\beta_2\\text{Distance } + u\n\\] where \\(\\log(E)\\) is an offset (the area of the pixel) that accounts for the size of the cell.\n\n\n\n\n\n\n\n Task\n\n\n\nFit a zero-inflated model to the data (zeroinflatedpoisson1) by completing the following code: ::: {.cell}\ncmp = ~ Intercept(1) + elevation(...) + distance(...) + space(...)\n\nlik = bru_obs(...,\n    E = area)\n\nfit_zip &lt;- bru(cmp, lik)\n\n\n\n\nTake hint\n\nThe E = area is an offset that adjusts for the size of each cell.\n\n\n\n\nClick here to see the solution\n\n\nCode\n\ncmp = ~ Intercept(1) + elevation(elev_cov1, model = \"linear\") + distance(dist_cov1, model = \"linear\") + space(geometry, model = matern)\n\n\n\nlik = bru_obs(formula = count ~ .,\n    family = \"zeroinflatedpoisson1\", \n    data = counts_df,\n    E = area)\n\nfit_zip &lt;- bru(cmp, lik)\n\n\n\n:::\nOnce the model is fitted we can look at the results\n\n\n\n\n\n\n Task\n\n\n\nCheck what the estimated excess zero probaility is.\nUse the predict() function to look at the estimated \\(\\lambda(s)\\) and mean count in Equation 2\n\n\nTake hint\n\nTo get the right name for the hyperparameters to use in the predict() function, you can use the function bru_names().\n\n\n\n\nClick here to see the solution\n\n\nCode\n# to check the estimated excess zero probability:\n# fit_zip$summary.hyperpar\n\npred_zip &lt;- predict(\n  fit_zip, \n  counts_df,\n  ~ {\n    pi &lt;- zero_probability_parameter_for_zero_inflated_poisson_1\n    lambda &lt;- area * exp( distance + elevation + space + Intercept)\n    expect &lt;- (1-pi) * lambda\n    variance &lt;- (1-pi) * (lambda + pi * lambda^2)\n    list(\n      lambda = lambda,\n      expect = expect,\n      variance = variance\n    )\n  },\n  n.samples = 2500\n)\n\n\n\n\n\n\n\n\n\n\nEstimated \\(\\lambda\\) (left) and expected counts (right) with zero inflated model",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#sec-zap",
    "href": "day5_practical_8.html#sec-zap",
    "title": "Practical 8",
    "section": "Hurdle model (Type0)",
    "text": "Hurdle model (Type0)\nWe now fit a hurdle model to the same data.\nIn the zeroinflatedpoisson0 model is defined by the following observation probability model\n\\[\n\\text{Prob}(y\\vert\\dots)=\\pi\\times 1_{y=0}+(1-\\pi)\\times \\text{Poisson}(y\\vert y&gt;0)\n\\]\nwhere \\(\\pi\\) is the probability of zero.\nThe expectation and variance of the counts are as follows:\n\\[\n\\begin{aligned}\nE(\\text{count})&=\\frac{1}{1-\\exp(-\\lambda)}\\pi\\lambda \\\\\nVar(\\text{count})&=  E(\\text{count}) \\left(1-\\exp(-\\lambda) E(\\text{count})\\right)\n\\end{aligned}\n\\tag{3}\\]\n\n\n\n\n\n\n Task\n\n\n\nFit a hurdle model to the data using the zeroinflatedpoisson0 likelihood\n\n\nTake hint\n\nYou do not need to redefine the components as the linear predictor is not changing.\n\n\n\n\nClick here to see the solution\n\n\nCode\nlik = bru_obs(formula = count ~ .,\n    family = \"zeroinflatedpoisson0\", \n    data = counts_df,\n    E = area)\n\nfit_zap &lt;- bru(cmp, lik)\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nAs before, check what the estimated probability of zero is and use predict() to obtain a map of the estimated mean counts in Equation 3 over the domain.\n\n\nTake hint\n\n\n\n\n\nClick here to see the solution\n\n\nCode\n\npred_zap &lt;- predict( fit_zap, counts_df,\n  ~ {\n    pi &lt;- zero_probability_parameter_for_zero_inflated_poisson_0\n    lambda &lt;- area * exp( distance + elevation + space + Intercept)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    list(\n      lambda = lambda,\n      expect = expect\n    )\n  },\n  n.samples = 2500\n)\n\n\n\n\n\n\n\n\n\n\nEstimated \\(\\lambda\\) (left) and expected counts (right) with hurdle model",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#sec-two-lik",
    "href": "day5_practical_8.html#sec-two-lik",
    "title": "Practical 8",
    "section": "Hurdle model using two likelihoods",
    "text": "Hurdle model using two likelihoods\nHere the model is the same as in Section 2.3, but this time we also want to model \\(\\pi\\) using covariates and random effects. Therefore we define a second linear predictor \\[\n\\eta^2 =\\beta_0^2 + \\beta_1^2\\text{Elevation} +  \\beta_2^2\\text{Distance} + u^2\n\\] Note here we have defined the two linear predictor to use the same covariates, but this is not necessary, they can be totally independent.\nTo fit this model we have to define two likelihoods: - One will account for the presence-absence process and has a Binomial model - One will account for the counts and has a truncated Poisson model\n\n\n\n\n\n\n Task\n\n\n\nComplete the following code to fit a hurdle model based on two likelihoods:\n\n# define components\ncmp &lt;- ~\n  Intercept_count(1) +\n    elev_count(elev_cov1, model = \"linear\") +\n    dist_count(dist_cov1, model = \"linear\") +\n    space_count(geometry, model = matern) +\n    Intercept_presence(1) +\n    elev_presence(elev_cov1, model = \"linear\") +\n    dist_presence(dist_cov1, model = \"linear\") +\n    space_presence(geometry, model = matern) \n\n# positive count model\npos_count_obs &lt;- bru_obs(formula = ...,\n      family = ...,\n      data = counts_df[counts_df$present &gt; 0, ],\n      E = area)\n  \n# presence model\npresence_obs &lt;- bru_obs(formula ...,\n  family = ...,\n  data = counts_df,\n)\n\n# fit the model\nfit_zap2 &lt;- bru(...)\n\n\n\nTake hint\n\nAdd hint details here…\n\n\n\n\nClick here to see the solution\n\n\nCode\ncmp &lt;- ~\n  Intercept_count(1) +\n    elev_count(elev_cov1, model = \"linear\") +\n    dist_count(dist_cov1, model = \"linear\") +\n    space_count(geometry, model = matern) +\n    Intercept_presence(1) +\n    elev_presence(elev_cov1, model = \"linear\") +\n    dist_presence(dist_cov1, model = \"linear\") +\n    space_presence(geometry, model = matern) \n\n\npos_count_obs &lt;- bru_obs(formula = count ~ Intercept_count + elev_count + \n                                   dist_count + space_count,\n      family = \"nzpoisson\",\n      data = counts_df[counts_df$present &gt; 0, ],\n      E = area)\n  \n\npresence_obs &lt;- bru_obs(formula = present ~ Intercept_presence + elev_presence + dist_presence +\n                          space_presence,\n  family = \"binomial\",\n  data = counts_df,\n)\n\nfit_zap2 &lt;- bru(\n  cmp,\n  presence_obs,\n  pos_count_obs\n)",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#sec-two-lik-share",
    "href": "day5_practical_8.html#sec-two-lik-share",
    "title": "Practical 8",
    "section": "Hurdle model using two likelihoods and a shared component",
    "text": "Hurdle model using two likelihoods and a shared component\nNote that in the model above, there is no direct link between the parameters of the two observation parts, and we could estimate them separately. However, the two likelihoods could share some of the components; for example the space_count component could be used for both predictors. This would be possible using the copy argument.\nWe would then need to define one component as space(geometry, model = matern) and then a copy of it as space_copy(geometry, copy = \"space\", fixed = FALSE).\nThe results from the model in ?@sec-sec-two-lik show that the estimated covariance parameters for the two fields are very different, so it is probably not sensible to share the same component between the two parts. We do it anyway to show an example:\n\ncmp &lt;- ~\n  Intercept_count(1) +\n    elev_count(elev_cov1, model = \"linear\") +\n    dist_count(dist_cov1, model = \"linear\") +\n    Intercept_presence(1) +\n    elev_presence(elev_cov1, model = \"linear\") +\n    dist_presence(dist_cov1, model = \"linear\") +\n    space(geometry, model = matern) +\n  space_copy(geometry, copy = \"space\", fixed = FALSE)\n\n\npos_count_obs &lt;- bru_obs(formula = count ~ Intercept_count + elev_count + dist_count + space,\n      family = \"nzpoisson\",\n      data = counts_df[counts_df$present &gt; 0, ],\n      E = area)\n\npresence_obs &lt;- bru_obs(formula = present ~ Intercept_presence + elev_presence + dist_presence + space_copy,\n  family = \"binomial\",\n  data = counts_df)\n\nfit_zap3 &lt;- bru(\n  cmp,\n  presence_obs,\n  pos_count_obs)",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  },
  {
    "objectID": "day5_practical_8.html#comparing-models",
    "href": "day5_practical_8.html#comparing-models",
    "title": "Practical 8",
    "section": "Comparing models",
    "text": "Comparing models\nWe have fitted four different models. Now we want to compare them and see how they fit the data.\n\nComparing model predictions\nWe first want to compare the estimated surfaces of expected counts. To do this we want to produce the estimated expected counts, similar to what we did in Section 2.2 and Section 2.3 for all four models and plot them together:\n\npred_zip &lt;- predict(\n  fit_zip, \n  counts_df,\n  ~ {\n    pi &lt;- zero_probability_parameter_for_zero_inflated_poisson_1\n    lambda &lt;- area * exp( distance + elevation + space + Intercept)\n    expect &lt;- (1-pi) * lambda\n    variance &lt;- (1-pi) * (lambda + pi * lambda^2)\n    list(\n      expect = expect\n    )\n  },n.samples = 2500)\n\npred_zap &lt;- predict( fit_zap, counts_df,\n  ~ {\n    pi &lt;- zero_probability_parameter_for_zero_inflated_poisson_0\n    lambda &lt;- area * exp( distance + elevation + space + Intercept)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    list(\n      expect = expect)\n  },n.samples = 2500)\n\ninv.logit = function(x) (exp(x)/(1+exp(x)))\n\npred_zap2 &lt;- predict( fit_zap2, counts_df,\n  ~ {\n    pi &lt;- inv.logit(Intercept_presence + elev_presence + dist_presence + space_presence)\n    lambda &lt;- area * exp( dist_count + elev_count + space_count + Intercept_count)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    list(\n      expect = expect)\n  },n.samples = 2500)\n\npred_zap3 &lt;- predict( fit_zap3, counts_df,\n  ~ {\n    pi &lt;- inv.logit(Intercept_presence + elev_presence + dist_presence + space_copy)\n    lambda &lt;- area * exp( dist_count + elev_count + space + Intercept_count)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    list(\n      expect = expect)\n  },n.samples = 2500)\n\n\n\n\n  data.frame(x = st_coordinates(counts_df)[,1],\n             y = st_coordinates(counts_df)[,2],\n    zip = pred_zip$expect$mean,\n         hurdle = pred_zap$expect$mean,\n         hurdle2 = pred_zap2$expect$mean,\n         hurdle3 = pred_zap3$expect$mean)  %&gt;%\n  pivot_longer(-c(x,y)) %&gt;%\n  ggplot() + geom_tile(aes(x,y, fill = value)) + facet_wrap(.~name) +\n    theme_map + scale_fill_scico(direction = -1)\n\n\n\n\nEstimated expected counts for all four models\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nCreate plots of the estimated variance of the counts.\n\n\nTake hint\n\nThe fomulas for the variances are in Equation 2 and Equation 3.\n\n\n\n\nClick here to see the solution\n\n\nCode\npred_zip &lt;- predict(\n  fit_zip, \n  counts_df,\n  ~ {\n    pi &lt;- zero_probability_parameter_for_zero_inflated_poisson_1\n    lambda &lt;- area * exp( distance + elevation + space + Intercept)\n    variance &lt;- (1-pi) * (lambda + pi * lambda^2)\n    list( variance = variance)\n  },n.samples = 2500)\n\npred_zap &lt;- predict( fit_zap, counts_df,\n  ~ {\n    pi &lt;- zero_probability_parameter_for_zero_inflated_poisson_0\n    lambda &lt;- area * exp( distance + elevation + space + Intercept)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    variance = expect *(1-exp(-lambda) * expect)\n    list(variance = variance)\n  },\n  n.samples = 2500)\n\ninv.logit = function(x) (exp(x)/(1+exp(x)))\n\npred_zap2 &lt;- predict( fit_zap2, counts_df,\n  ~ {\n    pi &lt;- inv.logit(Intercept_presence + elev_presence + dist_presence + space_presence)\n    lambda &lt;- area * exp( dist_count + elev_count + space_count + Intercept_count)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    variance = expect *(1-exp(-lambda) * expect)\n    list(variance = variance)\n  },\n  n.samples = 2500)\n\npred_zap3 &lt;- predict( fit_zap3, counts_df,\n  ~ {\n    pi &lt;- inv.logit(Intercept_presence + elev_presence + dist_presence + space_copy)\n    lambda &lt;- area * exp( dist_count + elev_count + space + Intercept_count)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    variance = expect *(1-exp(-lambda) * expect)\n    list(variance = variance)\n  },\n  n.samples = 2500)\n\n\n\n\n  data.frame(x = st_coordinates(counts_df)[,1],\n             y = st_coordinates(counts_df)[,2],\n    zip = pred_zip$variance$mean,\n         hurdle = pred_zap$variance$mean,\n         hurdle2 = pred_zap2$variance$mean,\n         hurdle3 = pred_zap3$variance$mean)  %&gt;%\n  pivot_longer(-c(x,y)) %&gt;%\n  ggplot() + geom_tile(aes(x,y, fill = value)) + facet_wrap(.~name) +\n    theme_map + scale_fill_scico(direction = -1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing scores\nWe can compare model using the scores that the bru() function computes since we have set, at the beginning. the options to ::: {.cell}\nbru_options_set(control.compute = list(dic = TRUE,\n                                       waic = TRUE,\n                                       mlik = TRUE,\n                                       cpo = TRUE))\n:::\nLets use these scores to compare the models.\n\n\n\n\n\n\n Task\n\n\n\nExtract the DIC, WAIC and MLIK values for the four models and compare them\n\n\n\nClick here to see the solution\n\n\nCode\ndata.frame( Model = c(\"ZIP\", \"HURDLE\", \"HURDLE_2\",\"HURDLE_3\" ),\n  DIC = c(fit_zip$dic$dic, fit_zap$dic$dic, WAIC = fit_zap2$dic$dic, fit_zap3$dic$dic),\n            WAIC = c(fit_zip$waic$waic, fit_zap$waic$waic, fit_zap2$waic$waic, fit_zap3$waic$waic),\n            MLIK = c(fit_zip$mlik[1], fit_zap$mlik[1], fit_zap2$mlik[1], fit_zap3$mlik[1]))\n#&gt;      Model      DIC     WAIC      MLIK\n#&gt; 1      ZIP 1214.142 1223.723 -686.9495\n#&gt; 2   HURDLE 1886.065 1909.721 -994.3807\n#&gt; 3 HURDLE_2 1268.319 1285.521 -734.3697\n#&gt; 4 HURDLE_3 1973.401 4126.802 -858.2085\n\n\n\n\n\nFrom the table above we can see that the model that best balances complexity and fit is the zero inflated one (ZIP).",
    "crumbs": [
      "Home",
      "Practical 8"
    ]
  }
]