[
  {
    "objectID": "slides/slides_3.html#outline",
    "href": "slides/slides_3.html#outline",
    "title": "Lecture 2",
    "section": "Outline",
    "text": "Outline\n\nLatent Gaussian Models"
  },
  {
    "objectID": "slides/slides_3.html#repetition",
    "href": "slides/slides_3.html#repetition",
    "title": "Lecture 2",
    "section": "Repetition",
    "text": "Repetition\nEverything in R-INLA is based on so-called latent Gaussian models\n\n‚Äî A few hyperparameters \\(\\theta\\sim\\pi(\\theta)\\) control variances, range and so on\n‚Äî Given these hyperparameters we have an underlying Gaussian distribution \\(\\mathbf{u}|\\theta\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{Q}^{-1}(\\theta))\\) that we cannot directly observe\n‚Äî Instead we make indirect observations \\(\\mathbf{y}|\\mathbf{u},\\theta\\sim\\pi(\\mathbf{y}|\\mathbf{u},\\theta)\\) of the underlying latent Gaussian field"
  },
  {
    "objectID": "slides/slides_3.html#repetition-1",
    "href": "slides/slides_3.html#repetition-1",
    "title": "Lecture 2",
    "section": "Repetition",
    "text": "Repetition\nModels of this kind: \\[\n\\begin{aligned}\n\\mathbf{y}|\\mathbf{x},\\theta &\\sim \\prod_i \\pi(y_i|\\eta_i,\\theta)\\\\\n\\mathbf{\\eta} & = A_1\\mathbf{u}_1 + A_2\\mathbf{u}_2+\\dots + A_k\\mathbf{u}_k\\\\\n\\mathbf{u},\\theta &\\sim \\mathcal{N}(0,\\mathbf{Q}(Œ∏)^{‚àí1})\\\\\n\\theta & \\sim \\pi(\\theta)\n\\end{aligned}\n\\]\noccurs in many, seemingly unrelated, statistical models."
  },
  {
    "objectID": "slides/slides_3.html#examples",
    "href": "slides/slides_3.html#examples",
    "title": "Lecture 2",
    "section": "Examples",
    "text": "Examples\n\nGeneralised linear (mixed) models\nStochastic volatility\nGeneralised additive (mixed) models\nMeasurement error models\nSpline smoothing\nSemiparametric regression\nSpace-varying (semiparametric) regression models\nDisease mapping\nLog-Gaussian Cox-processes\nModel-based geostatistics (*)\nSpatio-temporal models\nSurvival analysis\n+++"
  },
  {
    "objectID": "slides/slides_3.html#main-characteristics",
    "href": "slides/slides_3.html#main-characteristics",
    "title": "Lecture 2",
    "section": "Main Characteristics",
    "text": "Main Characteristics\n\nLatent Gaussian model\nThe data are conditionally independent given the latent field\nThe predictor is linear1\nThe dimension of \\(\\mathbf{u}\\) can be big (\\(10^3-10^6\\))\nThe dimension of \\(\\theta\\) should be not too big.\n\n\n\n\n\n \n\n\n\n\n\nwe will see that this can be partly relaxed üòÉ"
  },
  {
    "objectID": "slides/slides_1.html#course-structure-day-1",
    "href": "slides/slides_1.html#course-structure-day-1",
    "title": "inlabru workshop",
    "section": "Course Structure: Day 1",
    "text": "Course Structure: Day 1\n\n\n\n\n\n\n\n\n\nTime\nTopic\nContent\nExcercises\n\n\n\n\nXXXX am\nCore concepts\n\nLGM and INLA\ninlabru workflow\nModel selection\n\n\n\n\nXXXX am\nTemporal Models\n\nDiscrete time models\nContinuous time models"
  },
  {
    "objectID": "slides/slides_1.html#course-structure-day-2",
    "href": "slides/slides_1.html#course-structure-day-2",
    "title": "inlabru workshop",
    "section": "Course Structure: Day 2",
    "text": "Course Structure: Day 2\n\n\n\n\n\n\n\n\n\nTime\nTopic\nContent\nExcercises\n\n\n\n\nXXXX am\nIntroduction to Spatial Modelling\n\nTypes of spatial data\nSpatial data wrangling and manipulation in R (e.g, terra & sf)\nAreal processes\n\n\n\n\nXXXX am\nModelling geostatistical data\n\nSPDE & the mesh\nGeostatistical Data\nSpatial predictions"
  },
  {
    "objectID": "slides/slides_1.html#course-structure-day-3",
    "href": "slides/slides_1.html#course-structure-day-3",
    "title": "inlabru workshop",
    "section": "Course Structure: Day 3",
    "text": "Course Structure: Day 3\n\n\n\n\n\n\n\n\n\nTime\nTopic\nContent\nExcercises\n\n\n\n\nXXXX am\nSpatial Point processes\n\nSpatial point process\nDistance sampling\n\n\n\n\nXXXX am\nSpatiotemporal Models\n\nSeparable time-space models\nnon-separable space-time models"
  },
  {
    "objectID": "slides/slides_1.html#course-structure-day-4",
    "href": "slides/slides_1.html#course-structure-day-4",
    "title": "inlabru workshop",
    "section": "Course Structure: Day 4",
    "text": "Course Structure: Day 4\n\n\n\n\n\n\n\n\n\nTime\nTopic\nContent\nExcercises\n\n\n\n\nXXXX am\nMultilikelihood and Non-linear models\n\niterated inla\nlogistic growth\nCorregionalization models"
  },
  {
    "objectID": "day1_practical_2.html",
    "href": "day1_practical_2.html",
    "title": "inlabru workshop",
    "section": "",
    "text": "Aim of this practical:",
    "crumbs": [
      "Home",
      "Practical 2"
    ]
  },
  {
    "objectID": "day1_practical_2.html#setting-priors-and-model-checking-for-linear-models",
    "href": "day1_practical_2.html#setting-priors-and-model-checking-for-linear-models",
    "title": "inlabru workshop",
    "section": "Setting priors and model checking for Linear Models",
    "text": "Setting priors and model checking for Linear Models\nIn this exercise we will:\n\nLearn how to set priors for linear effects \\(\\beta_0\\) and \\(\\beta_1\\)\nLearn how to set the priors for the hyperparameter \\(\\tau = 1/\\sigma^2\\).\nVisualize marginal posterior distributions\nPerform model checks for linear models\n\nStart by loading useful libraries:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nRecall a simple linear regression model with Gaussian observations \\[\ny_i\\sim\\mathcal{N}(\\mu_i, \\sigma^2), \\qquad i = 1,\\dots,N\n\\]\nwhere \\(\\sigma^2\\) is the observation error, and the mean parameter \\(\\mu_i\\) is linked to the linear predictor through an identity function: \\[\n\\eta_i = \\mu_i = \\beta_0 + \\beta_1 x_i\n\\] where \\(x_i\\) is a covariate and \\(\\beta_0, \\beta_1\\) are parameters to be estimated. In INLA, we assume that the model is a latent Gaussian model, i.e., we have to assign \\(\\beta_0\\) and \\(\\beta_1\\) a Gaussian prior. For the precision hyperparameter \\(\\tau = 1/\\sigma^2\\) a typical prior choice is a \\(\\text{Gamma}(a,b)\\) prior.\nIn R-INLA, the default choice of priors for each \\(\\beta\\) is\n\\[\n\\beta \\sim \\mathcal{N}(0,10^3).\n\\]\nand the prior for the variance parameter in terms of the log precision is\n\\[ \\log(\\tau) \\sim \\mathrm{logGamma}(1,5 \\times 10^{-5}) \\]\n\n\n\n\n\n\nNote\n\n\n\nIf your model uses the default intercept construction (i.e., Intercept(1) in the linear predictor) INLA will assign a default \\(\\mathcal{N} (0,0)\\) prior to it.\n\n\nLets see how can we change the default priors using some simulated data\n\nSimulate example data\nWe simulate data from a simple linear regression model\n\n\nCode\nbeta = c(2,0.5)\nsd_error = 0.1\n\nn = 100\nx = rnorm(n)\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error)\n\ndf = data.frame(y = y, x = x)  \n\n\n\n\nFitting the linear regression model with inlabru\nNow we fit a simple linear regression model in inalbru by defining (1) the model components, (2) the linear predictor and (3) the likelihood.\n\n# Model components\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n# Linear predictor\nformula = y ~ Intercept + beta_1\n# Observational model likelihood\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n# Fit the Model\nfit.lm = bru(cmp, lik)\n\n\n\nChange the prior distributions\nUntil now, we have used the default priors for both the precision \\(\\tau\\) and the fixed effects \\(\\beta_0\\) and \\(\\beta_1\\). Let‚Äôs see how to customize these.\nTo check which priors are used in a fitted model one can use the function inla.prior.used()\n\ninla.priors.used(fit.lm)\n\nsection=[family]\n    tag=[INLA.Data1] component=[gaussian]\n        theta1:\n            parameter=[log precision]\n            prior=[loggamma]\n            param=[1e+00, 5e-05]\nsection=[linear]\n    tag=[beta_0] component=[beta_0]\n        beta:\n            parameter=[beta_0]\n            prior=[normal]\n            param=[0.000, 0.001]\n    tag=[beta_1] component=[beta_1]\n        beta:\n            parameter=[beta_1]\n            prior=[normal]\n            param=[0.000, 0.001]\n\n\nFrom the output we see that the precision for the observation \\(\\tau\\sim\\text{Gamma}(1e+00,5e-05)\\) while \\(\\beta_0\\) and \\(\\beta_1\\) have precision 0.001, that is variance \\(1/0.001\\).\nChange the precision for the linear effects\nThe precision for linear effects is set in the component definition. For example, if we want to increase the precision to 0.01 for \\(\\beta_0\\) we define the relative components as:\n\ncmp1 =  ~-1 +  beta_0(1, prec.linear = 0.01) + beta_1(x, model = \"linear\")\n\n\n\n\n\n\n\n Task\n\n\n\nRun the model again using 0.1 as default precision for both the intercept and the slope parameter.\n\n\n\nClick here to see the solution\n\ncmp2 =  ~ -1 + \n          beta_0(1, prec.linear = 0.1) + \n          beta_1(x, model = \"linear\", prec.linear = 0.1)\n\nlm.fit2 = bru(cmp2, lik) \n\n\nNote that we can use the same observation model as before since both the formula and the dataset are unchanged.\n\n\nChange the prior for the precision of the observation error \\(\\tau\\)\nPriors on the hyperparameters of the observation model must be passed by defining argument hyper within control.family in the call to the bru_obs() function.\n\n# First we define the logGamma (0.01,0.01) prior \n\nprec.tau &lt;- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(0.01, 0.01))) # prior values\n\nlik2 =  bru_obs(formula = y ~.,\n                family = \"gaussian\",\n                data = df,\n                control.family = list(hyper = prec.tau))\n\nfit.lm2 = bru(cmp2, lik2) \n\nThe names of the priors available in¬†R-INLA¬†can be seen with names(inla.models()$prior)\n\n\nVisualizing the posterior marginals\nPosterior marginal distributions of the Ô¨Åxed effects parameters and the hyperparameters can be visualized using the plot() function by calling the name of the component. For example, if want to visualize the posterior density of the intercept \\(\\beta_0\\) we can type:\n\n\nCode\nplot(fit.lm, \"beta_0\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nPlot the posterior marginals for \\(\\beta_1\\) and for the precision of the observation error \\(\\pi(\\tau|y)\\)\n\n\nTake hint\n\nSee the summary() output to check the names for the different model components.\n\n\n\n\nClick here to see the solution\n\n\nCode\nplot(fit.lm, \"beta_1\") +\nplot(fit.lm, \"Precision for the Gaussian observations\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Checking\nA common way for model diagnostics in regression analysis is by checking residual plots. In a Bayesian setting residuals can be defined in multipleways depending on how you account for posterior uncertainty. Here, we will adopt a Bayesian approach by generating samples from the posterior distribution of the model parameters and then draw samples from the residuals defined as:\n\\[\nr_i = y_i - x_i^T\\beta\n\\]\nWe can use the predict function to achieve this:\n\nres_samples &lt;- predict(\n  fit.lm,         # the fitted model\n  df,             # the original data set\n  ~ data.frame(   \n    res = y-(beta_0 + beta_1)  # compute the residuals\n  ),\n  n.samples = 1000   # draw 1000 samples\n)\n\nThe resulting data frame contains the posterior draw of the residuals mean for which we can produce some diagnostics plots , e.g.\n\n\nResiduals checks for Linear Model\nggplot(res_samples,aes(y=mean,x=1:100))+geom_point() +\nggplot(res_samples,aes(y=mean,x=x))+geom_point()\n\n\n\n\n\nBayesian residual plots: the left panel is the residual index plot; the right panel is the plot of the residual versus the covariate x\n\n\n\n\nWe can also compare these against the theoretical quantiles of the Normal distribution as follows:\n\n\nQQPlot for Linear Model\narrange(res_samples, mean) %&gt;%\n  mutate(theortical_quantiles = qnorm(1:100 / (1+100))) %&gt;%\n  ggplot(aes(x=theortical_quantiles,y= mean)) + \n  geom_ribbon(aes(ymin = q0.025, ymax = q0.975), fill = \"grey70\")+\n  geom_abline(intercept = mean(res_samples$mean),\n              slope = sd(res_samples$mean)) +\n  geom_point() +\n  labs(x = \"Theoretical Quantiles (Normal)\",\n       y= \"Sample Quantiles (Residuals)\")",
    "crumbs": [
      "Home",
      "Practical 2"
    ]
  },
  {
    "objectID": "day1_practical_2.html#sec-llm_fish",
    "href": "day1_practical_2.html#sec-llm_fish",
    "title": "inlabru workshop",
    "section": "Linear Mixed Model for fish weight-length relationship",
    "text": "Linear Mixed Model for fish weight-length relationship\nIn this exercise we will:\n\nPlot random effects of a LMM\nCompute posterior densities and summaries for the variance components\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nIn this exercise, we will use a subset of the Pygmy Whitefish (Prosopium coulterii) dataset from the FSAdata R package, containing biological data collected in 2001 from Dina Lake, British Columbia.\n Download data set \nThe data set contains the following information:\n\nnet_noUnique net identification number\nwt Fish weight (g)\ntl Total fish length (cm)\nsex Sex code (F=Female, M = Male)\n\nWe can visualize the distribution of the response (weight) across the nets split by sex as follows:\n\nPygmyWFBC &lt;- read.csv(\"datasets/PygmyWFBC.csv\")\n\nggplot(PygmyWFBC, aes(x = factor(net_no), y = wt,fill = sex)) + \n  geom_boxplot() + \n  labs(y=\"Weight (g)\",x = \"Net no.\")\n\n\n\n\n\n\n\n\nSuppose we are interested in modelling the weight-length relationship for captured fish. The exploratory plot suggest some important variability in this relationship, potentially attributable to differences among sampling nets deployed across various sites in the Dina Lake.\nTo account for this between-net variability, we model net as a random effect using the following linear mixed model:\n\\[\n\\begin{aligned}\ny_{ij} &\\sim\\mathcal{N}(\\mu_{ij}, \\sigma_e^2), \\qquad i = 1,\\dots,a \\qquad j = 1,\\ldots,n \\\\\n\\eta_{ij} &= \\mu_{ij} = \\beta_0 + \\beta_1 \\times \\text{length}_j + \\beta_2 \\times \\mathbb{I}(\\mathrm{Sex}_{ij}=\\mathrm{M}) +  u_i \\\\\nu_i &\\sim \\mathcal{N}(0,\\sigma^2_u)\n\\end{aligned}\n\\]\nwhere:\n\n\\(y_{ij}\\) is the weight of the \\(j\\)-th fish from net \\(i\\)\n\\(\\text{length}_{ij}\\) is the corresponding fish length\n\\(\\mathbb{I}(\\text{Sex}_{ij} = \\text{M})\\) is an indicator/dummy such that for the ith net \\[\n\\mathbb{I}(\\mathrm{Sex}_{ij}) \\begin{cases}1 & \\text{if the } j \\text{th fish is Male} \\\\0 & \\text{otherwise} \\end{cases}\n\\]\n\\(u_i\\) represents the random intercept for net \\(i\\)\n\\(\\sigma_u^2\\) and \\(\\sigma_\\epsilon^2\\) are the between-net and residual variances, respectively\n\nTo run this model ininlabru we first need to create our sex dummy variable :\n\nPygmyWFBC$sex_M &lt;- ifelse(PygmyWFBC$sex==\"F\",0,1)\n\ninlabru will treat 0 as the reference category (i.e., the intercept \\(\\beta_0\\) will represent the baseline weight for females ). Now we can define the model component, the likelihood and fit the model.\n\ncmp =  ~ -1 + sex_M +  beta_0(1)  + beta_1(tl, model = \"linear\") +   net_eff(net_no, model = \"iid\")\n\nlik =  bru_obs(formula = wt ~ .,\n            family = \"gaussian\",\n            data = PygmyWFBC)\n\nfit = bru(cmp, lik)\n\nsummary(fit)\n\ninlabru version: 2.13.0\nINLA version: 25.08.21-1\nComponents:\nsex_M: main = linear(sex_M), group = exchangeable(1L), replicate = iid(1L), NULL\nbeta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\nbeta_1: main = linear(tl), group = exchangeable(1L), replicate = iid(1L), NULL\nnet_eff: main = iid(net_no), group = exchangeable(1L), replicate = iid(1L), NULL\nObservation models:\n  Family: 'gaussian'\n    Tag: &lt;No tag&gt;\n    Data class: 'data.frame'\n    Response class: 'numeric'\n    Predictor: wt ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[sex_M, beta_0, beta_1, net_eff], latent[]\nTime used:\n    Pre = 0.337, Running = 0.27, Post = 0.176, Total = 0.784 \nFixed effects:\n          mean    sd 0.025quant 0.5quant 0.975quant    mode kld\nsex_M   -1.106 0.218     -1.534   -1.106     -0.678  -1.106   0\nbeta_0 -15.817 0.870    -17.516  -15.820    -14.100 -15.820   0\nbeta_1   2.555 0.072      2.414    2.555      2.696   2.555   0\n\nRandom effects:\n  Name    Model\n    net_eff IID model\n\nModel hyperparameters:\n                                         mean    sd 0.025quant 0.5quant\nPrecision for the Gaussian observations 0.475 0.044      0.393    0.473\nPrecision for net_eff                   2.146 1.313      0.562    1.839\n                                        0.975quant mode\nPrecision for the Gaussian observations      0.568 0.47\nPrecision for net_eff                        5.523 1.32\n\nMarginal log-Likelihood:  -467.54 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\nFor interpretability, we could have centered the predictors, but our primary focus here is on estimating the variance components of the mixed model.\nWe can plot the posterior density of the nets random intercept as follows:\n\nplot(fit,\"net_eff\")\n\n\n\n\n\n\n\n\nFor theoretical and computational purposes, INLA works with the precision which is the inverse of the variance. To obtain the posterior summaries on the SDs scale we can sample from the posterior distribution for the precision while back-transforming the samples and then computing the summary statistics. Transforming the samples is necessary because some quantities such as the mean and mode are not invariant to monotone transformation; alternatively we can use some of the in-built R-INLA functions to achieve this (see supplementary note).\nWe use the inla.hyperpar.sample function to draw samples from the approximated joint posterior for the hyperparameters, then invert them to get variances and lastly compute the mean, std. dev., quantiles, etc.\n\nsampvars &lt;- 1/inla.hyperpar.sample(1000,fit,improve.marginals = T)\n\ncolnames(sampvars) &lt;- c(\"Error variance\",\"Between-net Variance\")\n\napply(sampvars,2,\n      function(x) c(\"mean\"=mean(x),\n                    \"std.dev\" = sd(x),\n                    quantile(x,c(0.025,0.5,0.975))))\n\n        Error variance Between-net Variance\nmean         2.1248349            0.6515836\nstd.dev      0.1981369            0.4171121\n2.5%         1.7661417            0.1836902\n50%          2.1146062            0.5428438\n97.5%        2.5402879            1.7523999\n\n\n\n\n\n\n\n\n Task\n\n\n\nAnother useful quantity we can compute is the intraclass correlation coefÔ¨Åcient (ICC) which help us determine how much the response varies within groups compared to between groups. The intraclass correlation coefÔ¨Åcient is defined as:\n\\[\n\\text{ICC} = \\frac{\\sigma^2_u}{\\sigma^2_u + \\sigma^2_e}\n\\]\nCompute the median, and quantiles for the ICC using the posterior samples we draw for \\(\\sigma^2_e\\) and \\(\\sigma^2_u\\).\n\n\nTake hint\n\nThe rowSums function can be used to compute \\(\\sigma^2_{u,s} + \\sigma^2_{e,s}\\) for the \\(s\\)th posterior draw.\n\n\n\n\nClick here to see the solution\n\n\nCode\nsampicc &lt;- sampvars[,2]/(rowSums(sampvars))\nquantile(sampicc, c(0.025,0.5,0.975))\n\n\n      2.5%        50%      97.5% \n0.07782059 0.20514207 0.45394700 \n\n\n\n\n\n\n\n\n\n\n\nSupplementary Material\n\n\n\nThe marginal densities for the hyper parameters can be also found by callinginlabru_model$marginals.hyperpar. We can then apply a transformation using the inla.tmarginal function to transform the precision posterior distributions.\n\nvar_e &lt;- fit$marginals.hyperpar$`Precision for the Gaussian observations` %&gt;%\n  inla.tmarginal(function(x) 1/x,.) \n\nvar_u &lt;- fit$marginals.hyperpar$`Precision for net_eff` %&gt;%\n  inla.tmarginal(function(x) 1/x,.) \n\nThe marginal densities for the hyper parameters can be found with inlabru_model$marginals.hyperpar, then we can apply a transformation using the inla.tmarginal function to transform the precision posterior distributions. Then, we can compute posterior summaries using inla.zmarginal function as follows:\n\npost_var_summaries &lt;- cbind( inla.zmarginal(var_e,silent = T),\n                             inla.zmarginal(var_u,silent = T))\ncolnames(post_var_summaries) &lt;- c(\"sigma_e\",\"sigma_u\")\npost_var_summaries\n\n           sigma_e   sigma_u  \nmean       2.124416  0.6509163\nsd         0.1980493 0.4174327\nquant0.025 1.764134  0.1815358\nquant0.25  1.985395  0.3685645\nquant0.5   2.113864  0.54116  \nquant0.75  2.252029  0.8073493\nquant0.975 2.541426  1.758747",
    "crumbs": [
      "Home",
      "Practical 2"
    ]
  },
  {
    "objectID": "day1_practical_2.html#sec-linmodel",
    "href": "day1_practical_2.html#sec-linmodel",
    "title": "inlabru workshop",
    "section": "GLM model checking",
    "text": "GLM model checking\nIn this exercise we will:\n\nLearn about some model assessments techniques available in INLA\nConduct posterior predictive model checking\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nIn this exercise, we will use data on horseshoe crabs (Limulus polyphemus) where the number of satellites males surrounding a breeding female are counted along with the female‚Äôs color and carapace width.\n Download data set \nA possible model to study the factors that affect the number of satellites for female crabs is\n\\[\n\\begin{aligned}\ny_i&\\sim\\mathrm{Poisson}(\\mu_i), \\qquad i = 1,\\dots,N \\\\\n\\eta_i &= \\mu_i = \\beta_0 + \\beta_1 x_i + \\ldots\n\\end{aligned}\n\\]\nWe can explore the conditional means and variances given the female‚Äôs color:\n\ncrabs &lt;- read.csv(\"datasets/crabs.csv\")\n\n# conditional means and variances\ncrabs %&gt;%\n  summarise( Mean = mean(satell ),\n             Variance = var(satell),\n                     .by = color)\n\n   color     Mean  Variance\n1 medium 3.294737 10.273908\n2   dark 2.227273  6.737844\n3  light 4.083333  9.719697\n4 darker 2.045455 13.093074\n\n\nThe mean of the number of satellites vary by color which gives a good indication that color might be useful for predicting satellites numbers. However, notice that the mean is lower than its variance suggesting that overdispersion might be present and that a negative binomial model would be more appropriate for the data (we will cover this later).\nFitting the model\nFirst, lets begin fitting the Poisson model above using the carapace‚Äôs color and width as predictors. Since, color is a categorical variable in our model we need to create a dummy variable for it. We can use the model.matrix function to help us constructing the design matrix and then append this to our data:\n\ncrabs_df = model.matrix( ~  color , crabs) %&gt;%\n  as.data.frame() %&gt;%\n  select(-1) %&gt;%        # drop intercept\n  bind_cols(crabs) %&gt;%  # append to original data\n  select(-color)        # remove original color categorical variable\n\nThe new data set crabs_df contains a dummy variable for the different color categories (dark being the reference category). Then we can fit the model in inlabru as follows:\n\ncmp =  ~ -1 + beta0(1) +  colordarker +\n       colorlight + colormedium +\n       w(weight, model = \"linear\")\n\nlik =  bru_obs(formula = satell ~.,\n            family = \"poisson\",\n            data = crabs_df)\n\nfit_pois = bru(cmp, lik)\n\nsummary(fit_pois)\n\ninlabru version: 2.13.0\nINLA version: 25.08.21-1\nComponents:\nbeta0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\ncolordarker: main = linear(colordarker), group = exchangeable(1L), replicate = iid(1L), NULL\ncolorlight: main = linear(colorlight), group = exchangeable(1L), replicate = iid(1L), NULL\ncolormedium: main = linear(colormedium), group = exchangeable(1L), replicate = iid(1L), NULL\nw: main = linear(weight), group = exchangeable(1L), replicate = iid(1L), NULL\nObservation models:\n  Family: 'poisson'\n    Tag: &lt;No tag&gt;\n    Data class: 'data.frame'\n    Response class: 'integer'\n    Predictor: satell ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[beta0, colordarker, colorlight, colormedium, w], latent[]\nTime used:\n    Pre = 0.338, Running = 0.285, Post = 0.0635, Total = 0.686 \nFixed effects:\n              mean    sd 0.025quant 0.5quant 0.975quant   mode kld\nbeta0       -0.501 0.196     -0.885   -0.501     -0.117 -0.501   0\ncolordarker -0.008 0.180     -0.362   -0.008      0.345 -0.008   0\ncolorlight   0.445 0.176      0.101    0.445      0.790  0.445   0\ncolormedium  0.248 0.118      0.017    0.248      0.479  0.248   0\nw            0.001 0.000      0.000    0.001      0.001  0.001   0\n\nMarginal log-Likelihood:  -489.43 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\n\nModel assessment and model choice\nNow that we have fitted the model we would like to carry some model assessments. In a Bayesian setting, this is often based on posterior predictive checks. To do so, we will use the CPO and PIT - two commonly used Bayesian model assessment criteria based on the posterior predictive distribution.\n\n\n\n\n\n\nPosterior predictive model checking\n\n\n\nThe posterior predictive distribution for a predicted value \\(\\hat{y}\\) is\n\\[\n\\pi(\\hat{y}|\\mathbf{y}) = \\int_\\theta \\pi(\\hat{y}|\\theta)\\pi(\\theta|\\mathbf{y})d\\theta.\n\\]\nThe probability integral transform (PIT) introduced by Dawid (1984) is defined for each observation as:\n\\[\n\\mathrm{PIT}_i = \\pi(\\hat{y}_i \\leq y_i |\\mathbf{y}{-i})\n\\]\nThe PIT evaluates how well a model‚Äôs predicted values match the observed data distribution. It is computed as the cumulative distribution function (CDF) of the observed data evaluated at each predicted value. If the model is well-calibrated, the PIT values should be approximately uniformly distributed. Deviations from this uniform distribution may indicate issues with model calibration or overfitting.\nAnother metric we could used to asses the model fit is the conditional predictive ordinate (CPO) introduced by Pettit (1990), and deÔ¨Åned as:\n\\[\n\\text{CPO}_i = \\pi(y_i| \\mathbf{y}{-i})\n\\]\nThe CPO measures the probability of the observed value of \\(y_i\\) when model is fit using all data but \\(y_i\\). CPO provides a measure of how well the model predicts each individual observation while taking into account the rest of the data and the model. Large values indicate a better fit of the model to the data, while small values indicate a bad fitting of the model\n\n\nTo compute PIT and CPO we can either:\n\nask inlabru to compute them by set options = list(control.compute = list(cpo = TRUE)) in the bru() function arguments.\nset this as default in inlabru global option using the bru_options_set function.\n\nHere we will do the later and re-run the model\n\nbru_options_set(control.compute = list(cpo = TRUE))\n\nfit_pois = bru(cmp, lik)\n\nNow we can produce histograms and QQ plots to assess for uniformity in the PIT values which can be accessed through inlabru_model$cpo$pit :\n\nPlotR Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfit_pois$cpo$pit %&gt;%\n  hist(main = \"Histogram of PIT values\")\n\nqqplot(qunif(ppoints(length(fit_pois$cpo$pit))),\n       fit_pois$cpo$pit,\n       main = \"Q-Q plot for Unif(0,1)\",\n       xlab = \"Theoretical Quantiles\",\n       ylab = \"Sample Quantiles\")\n\nqqline(fit_pois$cpo$pit,\n       distribution = function(p) qunif(p),\n       prob = c(0.1, 0.9))\n\n\n\n\nBoth Q-Q plots and histogram of the PIT values suggest a not so great model fit. For the CPO values, usually the following summary of the CPO is often used:\n\\[\n-\\sum_{i=1}^n \\log (\\text{CPO}\\_i)\n\\]\nThis quantities is useful when comparing different models - a smaller values indicate a better model fit. CPO values can be accessed by typing inlabru_model$cpo$cpo.\n\n\n\n\n\n\n Task\n\n\n\nThe model assessment above suggests that a Poisson model might not be the most appropriate model, likely due to the overdispersion we detected previously. Fit a Negative binomial to relax the Poisson model assumption that the conditional mean and variance are equal. Then, compute the CPO summary statistic and PIT QQ plot to decide which model gives the better fit.\n\n\nTake hint\n\nTo specify a negative binomial model you only need to change the family distribution to family =  \"nbinomial\".\n\n\n\n\nClick here to see the solution\n\n\nCode\npar(mfrow=c(1,2))\n\n# Fit the negative binomial model\n\nlik_nbinom =  bru_obs(formula = satell ~.,\n            family = \"nbinomial\",\n            data = crabs_df)\n\nfit_nbinom = bru(cmp, lik_nbinom)\n\n# PIT checks\n\nfit_nbinom$cpo$pit %&gt;%\n  hist(main = \"Histogram of PIT values\")\n\nqqplot(qunif(ppoints(length(fit_nbinom$cpo$pit))),\n       fit_nbinom$cpo$pit,\n       main = \"Q-Q plot for Unif(0,1)\",\n       xlab = \"Theoretical Quantiles\",\n       ylab = \"Sample Quantiles\")\n\nqqline(fit_nbinom$cpo$pit,\n       distribution = function(p) qunif(p),\n       prob = c(0.1, 0.9))\n\n\n\n\n\n\n\n\n\nCode\n# CPO comparison\n\ndata.frame( CPO = c(-sum(log(fit_pois$cpo$cpo)),\n                    -sum(log(fit_nbinom$cpo$cpo))),\n          Model = c(\"Poisson\",\"Negative Binomial\"))\n\n\n       CPO             Model\n1 465.4061           Poisson\n2 379.3340 Negative Binomial\n\n\nCode\n# Overall, we can see that the negative binomial model provides a better fit to the data.",
    "crumbs": [
      "Home",
      "Practical 2"
    ]
  },
  {
    "objectID": "day1_practical_2.html#hierarchical-generalised-additive-mixed-models-with-inlabru",
    "href": "day1_practical_2.html#hierarchical-generalised-additive-mixed-models-with-inlabru",
    "title": "inlabru workshop",
    "section": "Hierarchical generalised additive mixed models with inlabru",
    "text": "Hierarchical generalised additive mixed models with inlabru\nIn this excercise we will:\n\nFit an hierarchical generalised additive mixed models\nFit a model with a global smooth term\nFit a model with global and group-level smooth terms\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nThe oceans represent Earth‚Äôs largest habitat, with life distributed unevenly across depths primarily due to variations in light, temperature, and pressure. Biomass generally decreases with depth, though complex factors like water density layers create non-linear patterns. A significant portion of deep-sea organisms exhibit bioluminescence, which scientists measure using specialized equipment like free-fall camera systems to profile vertical distribution.\nIn this exercise, we analyze the ISIT dataset, which contains bioluminescence measurements from the northeast Atlantic Ocean. This dataset was previously examined in Zuur et al.¬†(2009) and Gillibrand et al.¬†(2007) and consists of observations collected across a depth gradient (0‚Äì4,800 m) during spring and summer cruises in 2001‚Äì2002 using an ISIT free-fall profiler.\n Download data set \nThe focus of this excersice will be on characterizing seasonal variation in the relationship between bioluminescent source density (sources m\\(^{2}\\)) and depth. We begin by exploring distribution patterns of pelagic bioluminescence through source-depth profiles, with each profile representing measurements from an individual sampling station. These profiles will be grouped by month to examine temporal patterns in the water column‚Äôs bioluminescent structure.\n\nPlotR-Code\n\n\n\n\n\n\n\nSource‚Äìdepth proÔ¨Åles per month. Each line represents a station.\n\n\n\n\n\n\n\nicit &lt;- read.csv(\"datasets/ISIT.csv\")\n\nicit$Month &lt;- as.factor(icit$Month)\nlevels(icit$Month) &lt;- month.abb[unique(icit$Month)]\n\nggplot(icit,aes(x=SampleDepth,y= Sources,\n                group=as.factor(Station),\n                colour=as.factor(Station)))+\n  geom_line()+\n  facet_wrap(~Month)+\n  theme(legend.position = \"none\")\n\n\n\n\nAs expected, there seems to be a non-linear depth effect with some important variability across months.\n\nFitting a global smoother\nWe could begin analysing these data with a global smoother and a random intercept for each month. Thus, a possible model is of the form:\n\\[\nS_{is} = \\beta_0 + f(\\text{Depth})_s + \\text{Month}_i +  \\epsilon_{is} ~~\\text{such that}~ \\epsilon \\sim \\mathcal{N}(0,\\sigma^2_e);~ \\text{Month} \\sim \\mathrm{N}(0,\\sigma^2_m).\n\\]\nwhere the source during month \\(i\\) at depth \\(s\\), \\(S_{is}\\), are modelled as smoothing function of depth and a month effect. The model has one smoothing curve for all months and can be fitted in inlabru as follows:\n\nicit$Month_id &lt;- as.numeric(icit$Month) # numeric index for the i-th month\n\ncmp_g =  ~ -1+ beta_0(1) + \n  smooth_g(SampleDepth, model = \"rw1\") + \n  month_reff(Month_id, model = \"iid\") \n\nlik =  bru_obs(formula = Sources ~.,\n               family = \"gaussian\",\n               data = icit)\n\nfit_g = bru(cmp_g, lik)\n\nsummary(fit_g)\n\ninlabru version: 2.13.0\nINLA version: 25.08.21-1\nComponents:\nbeta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\nsmooth_g: main = rw1(SampleDepth), group = exchangeable(1L), replicate = iid(1L), NULL\nmonth_reff: main = iid(Month_id), group = exchangeable(1L), replicate = iid(1L), NULL\nObservation models:\n  Family: 'gaussian'\n    Tag: &lt;No tag&gt;\n    Data class: 'data.frame'\n    Response class: 'numeric'\n    Predictor: Sources ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[beta_0, smooth_g, month_reff], latent[]\nTime used:\n    Pre = 0.392, Running = 0.669, Post = 0.291, Total = 1.35 \nFixed effects:\n         mean    sd 0.025quant 0.5quant 0.975quant   mode kld\nbeta_0 10.014 1.678       6.55   10.023     13.423 10.022   0\n\nRandom effects:\n  Name    Model\n    smooth_g RW1 model\n   month_reff IID model\n\nModel hyperparameters:\n                                          mean    sd 0.025quant 0.5quant\nPrecision for the Gaussian observations  0.024 0.001      0.021    0.024\nPrecision for smooth_g                  21.132 5.450     12.184   20.537\nPrecision for month_reff                 0.143 0.113      0.026    0.113\n                                        0.975quant   mode\nPrecision for the Gaussian observations      0.026  0.024\nPrecision for smooth_g                      33.484 19.476\nPrecision for month_reff                     0.441  0.067\n\nMarginal log-Likelihood:  -2217.14 \nCPO, PIT is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\nWe can plot the smoother marginal effect as follows:\n\n\nGlobal smoother marginal effect\ndata.frame(fit_g$summary.random$smooth_g) %&gt;% \n  ggplot() + \n  geom_ribbon(aes(ID,ymin = X0.025quant, ymax= X0.975quant), alpha = 0.5) + \n  geom_line(aes(ID,mean)) + \n  xlab(\"covariate\") + ylab(\"smooth effect\")\n\n\n\n\n\n\n\n\n\nYou might want to have a smoother function by placing a RW2 prior. Unfortunately, this assumes that all the knots are regularly spaced and some depth values are too close to be used for building the RW2 priors. For the case, it is possible to use function¬†inla.group()¬†to bin data into groups according to the values of the covariate:\n\nicit$depth_grouped &lt;- inla.group(icit$SampleDepth,n=50)\n\n\n\n\n\n\n\n Task\n\n\n\nRe-run the global smoother model using a RW2 prior for the depth smoother and compare your results with the RW1 model.\n\n\nTake hint\n\nUse the depth_grouped covariate to define the smoother.\n\n\n\n\nClick here to see the solution\n\ncmp_rw2 =  ~ -1+ beta_0(1) + \n  smooth_g(depth_grouped, model = \"rw2\") + \n  month_reff(Month_id, model = \"iid\") \n\nlik =  bru_obs(formula = Sources ~.,\n               family = \"gaussian\",\n               data = icit)\n\nfit_rw2 = bru(cmp_rw2, lik)\n\ndata.frame(fit_rw2$summary.random$smooth_g) %&gt;% \n  ggplot() + \n  geom_ribbon(aes(ID,ymin = X0.025quant, ymax= X0.975quant), alpha = 0.5) + \n  geom_line(aes(ID,mean)) + \n  xlab(\"covariate\") + ylab(\"Global smooth effect\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting group-level smoothers\nHere we fit a model where each month is allowed to have its own smoother for depth, i.e., \\(f_i(\\text{Depth})_s\\). The model structure is given by:\n\\[\nS_{is} = \\beta_0 + f_i(\\text{Depth})_s + \\text{Month}_i +  \\epsilon_{is}.\n\\]\nNotice the only different between the global smoother model (Model G) and the group level model (Model GS) is the indexing of the smooth function for depth. We can fit a group-level smoother using the group argument within the model component as follows:\n\ncmp_gs =  ~ -1+ beta_0(1) +\n  smooth_g(SampleDepth, model = \"rw1\") + \n  month_reff(Month_id, model = \"iid\")+\n  smooth_loc(SampleDepth, model = \"rw1\", group = Month_id)\n\nThen, we simply run the model (since the observational model has not changed -only the model components have):\n\nfit_gs = bru(cmp_gs, lik) \n\nLastly, we can generate model predictions using the predict function.\n\npred_gs = predict(fit_gs, icit, ~ (beta_0 + smooth_g+month_reff+smooth_loc))\n\nThen, we plot the predicted mean values with their corresponding 95% CrIs.\n\n\nGlobal + group smoother predictions\nggplot(pred_gs,aes(y=mean,x=SampleDepth))+\n  geom_ribbon(aes(SampleDepth,ymin = q0.025, ymax= q0.975), alpha = 0.5,fill=\"tomato\") +\n  geom_line()+\n  geom_point(aes(x=SampleDepth,y=Sources ),alpha=0.25,col=\"grey40\")+\n  facet_wrap(~Month)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nRe-fit the model GS without the global smoother. By omitting the global smoother, we do not longer force group-level smooths to follow a shared pattern, which is useful when groups may differ substantially from a common trend.\n\n\nTake hint\n\nYou only need to modify the model components in cmp_gs\nAdd hint details here‚Ä¶\n\n\n\n\nClick here to see the solution\n\n\nCode\ncmp_s =  ~ -1+ beta_0(1) +\n  month_reff(Month_id, model = \"iid\")+\n  smooth_loc(SampleDepth, model = \"rw1\", group = Month_id)\n\nfit_s = bru(cmp_s, lik) \n\npred_s = predict(fit_s, icit, ~ (beta_0 +month_reff+smooth_loc))\n\nggplot(pred_s,aes(y=mean,x=SampleDepth))+\n  geom_ribbon(aes(SampleDepth,ymin = q0.025, ymax= q0.975), alpha = 0.5,fill=\"tomato\") +\n  geom_line()+\n  geom_point(aes(x=SampleDepth,y=Sources ),alpha=0.25,col=\"grey40\")+\n  facet_wrap(~Month)",
    "crumbs": [
      "Home",
      "Practical 2"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "inlabru Workshop",
    "section": "",
    "text": "Welcome to the course!\n\nWelcome to the inlabu workshop!\nThe goal for the workshop is to ‚Ä¶\nThe workshop is intended for ‚Ä¶ No knowledge of R-INLA is required.\nWorkshop materials in the github repository inlabru-workshop\n\n\n\nLearning Objectives for the workshop\nAt the end of the workshop, participants will be able to:\n\nILO1\nILO2\nILO3, etc\n\nIntended audience and level: The tutorial is intended for ‚Ä¶ No knowledge of R-INLA is required.\n\n\nSchedule Program\n\nDay 1Day 2Day 3Day 4Day 5\n\n\n\n\n\nTime\nTopic\n\n\n\n\n10:00 - 10:30\nICES informative session\n\n\n10:30 - 11:30\nSession 1: Introduction to inlabru\n\n\n11:30 - 13:00\nPractical Session 1\n\n\n13:00 - 14:30\nLunch break\n\n\n14:30 - 15:30\nSession 2: Latent Gaussian Models and INLA\n\n\n15:30 - 15:45\nCoffee Break\n\n\n15:45 - 16:45\nPractical Session 2\n\n\n16:45 - 17:00\nwrap-up and outlook\n\n\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 10:00\nSession 3: Time models on a discrete domains\n\n\n10:00 - 10:30\nSnack\n\n\n10:30 - 11:30\nSession 4: Time models on a continuous domains\n\n\n11:30 - 13:00\nPractical Session 3\n\n\n13:00 - 14:30\nLunch break\n\n\n14:30 - 15:30\nSession 5: Introduction to Spatial Statistics\n\n\n15:35 - 15:45\nCoffee Break\n\n\n15:45 - 16:45\nPractical Session 4\n\n\n16:45 - 17:00\nwrap-up and outlook\n\n\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 10:00\nSession 6: Areal Processes\n\n\n10:00 - 10:30\nSnack\n\n\n10:30 - 11:30\nSession 7: Geostatistics\n\n\n11:30 - 13:00\nPractical Session 5\n\n\n13:00 - 14:30\nLunch break\n\n\n14:30 - 15:30\nSession 8: Spatial Point processes\n\n\n15:35 - 15:45\nCoffee Break\n\n\n15:45 - 16:45\nPractical Session 6\n\n\n16:45 - 17:00\nwrap-up and outlook\n\n\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 10:00\nSession 9: Spatiotemporal models\n\n\n10:00 - 10:30\nSnack\n\n\n10:30 - 11:30\nSession 10: Multilikelihoods/joint likelihood\n\n\n11:30 - 13:00\nPractical Session 7\n\n\n13:00 - 14:30\nLunch break\n\n\n14:30 - 15:30\nSession 11: Non-linear models\n\n\n15:35 - 15:45\nCoffee Break\n\n\n15:45 - 16:45\nPractical Session 8\n\n\n16:45 - 17:00\nwrap-up and outlook\n\n\n\n\n\n\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 10:00\nSession 12: Complex observational processes: Zero inflated models\n\n\n10:00 - 10:30\nSnack\n\n\n10:30 - 11:30\nSession 13: Complex observational processes: Thinned point processes\n\n\n11:30 - 13:00\nPractical Session 9\n\n\n13:00 - 13:15\nCoffee Break\n\n\n13:15 - 14:00\nClosing session\n\n\n\n\n\n\n\n\nIn preparation for the workshop\nParticipants are required to follow the next steps before the day of the workshop:\n\nInstall R-INLA\nInstall inlabru (available from CRAN)\nMake sure you have the latest R-INLA, inlabru and R versions installed.",
    "crumbs": [
      "Home",
      "`inlabru` Workshop"
    ]
  },
  {
    "objectID": "day1_practical1.html",
    "href": "day1_practical1.html",
    "title": "Practical 1",
    "section": "",
    "text": "Aim of this practical:  In this first practical we are going to look at some simple models\nwe are going to learn:\nDownload Practical 1 R script",
    "crumbs": [
      "Home",
      "Practical 1"
    ]
  },
  {
    "objectID": "day1_practical1.html#sec-linmodel",
    "href": "day1_practical1.html#sec-linmodel",
    "title": "Practical 1",
    "section": "Linear Model",
    "text": "Linear Model\nIn this practical we will:\n\nSimulate Gaussian data\nLearn how to fit a linear model with inlabru\nGenerate predictions from the model\n\nStart by loading useful libraries:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n# load some libraries to generate nice plots\nlibrary(scico)\n\nAs our first example we consider a simple linear regression model with Gaussian observations \\[\ny_i\\sim\\mathcal{N}(\\mu_i, \\sigma^2), \\qquad i = 1,\\dots,N\n\\]\nwhere \\(\\sigma^2\\) is the observation error, and the mean parameter \\(\\mu_i\\) is linked to the linear predictor (\\(\\eta_i\\)) through an identity function: \\[\n\\eta_i = \\mu_i = \\beta_0 + \\beta_1 x_i\n\\] where \\(x_i\\) is a covariate and \\(\\beta_0, \\beta_1\\) are parameters to be estimated. We assign \\(\\beta_0\\) and \\(\\beta_1\\) a vague Gaussian prior.\nTo finalize the Bayesian model we assign a \\(\\text{Gamma}(a,b)\\) prior to the precision parameter \\(\\tau = 1/\\sigma^2\\) and two independent Gaussian priors with mean \\(0\\) and precision \\(\\tau_{\\beta}\\) to the regression parameters \\(\\beta_0\\) and \\(\\beta_1\\) (we will use the default prior settings in INLA for now).\n\n\n\n\n\n\n Question\n\n\n\nWhat is the dimension of the hyperparameter vector and latent Gaussian field?\n\n\nAnswer\n\nThe hyperparameter vector has dimension 1, \\(\\pmb{\\theta} = (\\tau)\\) while the latent Gaussian field \\(\\pmb{u} = (\\beta_0, \\beta_1)\\) has dimension 2, \\(0\\) mean, and sparse precision matrix:\n\\[\n\\pmb{Q} = \\begin{bmatrix}\n\\tau_{\\beta_0} & 0\\\\\n0 & \\tau_{\\beta_1}\n\\end{bmatrix}\n\\] Note that, since \\(\\beta_0\\) and \\(\\beta_1\\) are fixed effects, the precision parameters \\(\\tau_{\\beta_0}\\) and \\(\\tau_{\\beta_1}\\) are fixed.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can write the linear predictor vector \\(\\pmb{\\eta} = (\\eta_1,\\dots,\\eta_N)\\) as\n\\[\n\\pmb{\\eta} = \\pmb{A}\\pmb{u} = \\pmb{A}_1\\pmb{u}_1 + \\pmb{A}_2\\pmb{u}_2 = \\begin{bmatrix}\n1 \\\\\n1\\\\\n\\vdots\\\\\n1\n\\end{bmatrix} \\beta_0 + \\begin{bmatrix}\nx_1 \\\\\nx_2\\\\\n\\vdots\\\\\nx_N\n\\end{bmatrix} \\beta_1\n\\]\nOur linear predictor consists then of two components: an intercept and a slope.\n\n\n\nSimulate example data\nFirst, we simulate data from the model\n\\[\ny_i\\sim\\mathcal{N}(\\eta_i,0.1^2), \\ i = 1,\\dots,100\n\\]\nwith\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_i\n\\]\nwhere \\(\\beta_0 = 2\\), \\(\\beta_1 = 0.5\\) and the values of the covariate \\(x\\) are generated from an Uniform(0,1) distribution. The simulated response and covariate data are then saved in a data.frame object.\n\n\nSimulate Data from a LM\nbeta = c(2,0.5)\nsd_error = 0.1\n\nn = 100\nx = rnorm(n)\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error)\n\ndf = data.frame(y = y, x = x)  \n\n\n\n\nFitting a linear regression model with inlabru\n\nDefining model components\nThe model has two parameters to be estimated \\(\\beta_1\\) and \\(\\beta_2\\). We need to define the two corresponding model components:\n\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n\nThe cmp object is here used to define model components. We can give them any useful names we like, in this case, beta_0 and beta_1.\n\n\n\n\n\n\nNote\n\n\n\nNote that we have excluded the default Intercept term in the model by typing -1 in the model components. However, inlabru has automatic intercept that can be called by typing Intercept() , which is one of inlabru special names and it is used to define a global intercept, e.g.\n\ncmp =  ~  Intercept(1) + beta_1(x, model = \"linear\")\n\n\n\nObservation model construction\nThe next step is to construct the observation model by defining the model likelihood. The most important inputs here are the formula, the family and the data.\nThe formula defines how the components should be combined in order to define the model predictor.\n\nformula = y ~ beta_0 + beta_1\n\n\n\n\n\n\n\nNote\n\n\n\nIn this case we can also use the shortcut formula = y ~ .. This will tell inlabru that the model is linear and that it is not necessary to linearize the model and assess convergence.\n\n\nThe likelihood is defined using the bru_obs() function as follows:\n\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n\nFit the model\nWe fit the model using the bru() functions which takes as input the components and the observation model:\n\nfit.lm = bru(cmp, lik)\n\nExtract results\nThe summary() function will give access to some basic information about model fit and estimates\n\nsummary(fit.lm)\n## inlabru version: 2.13.0\n## INLA version: 25.08.21-1\n## Components:\n## beta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\n## beta_1: main = linear(x), group = exchangeable(1L), replicate = iid(1L), NULL\n## Observation models:\n##   Family: 'gaussian'\n##     Tag: &lt;No tag&gt;\n##     Data class: 'data.frame'\n##     Response class: 'numeric'\n##     Predictor: y ~ .\n##     Additive/Linear: TRUE/TRUE\n##     Used components: effects[beta_0, beta_1], latent[]\n## Time used:\n##     Pre = 0.46, Running = 0.273, Post = 0.119, Total = 0.852 \n## Fixed effects:\n##         mean    sd 0.025quant 0.5quant 0.975quant  mode kld\n## beta_0 1.998 0.010      1.979    1.998      2.018 1.998   0\n## beta_1 0.495 0.011      0.473    0.495      0.518 0.495   0\n## \n## Model hyperparameters:\n##                                           mean    sd 0.025quant 0.5quant\n## Precision for the Gaussian observations 105.53 14.92      78.36   104.83\n##                                         0.975quant   mode\n## Precision for the Gaussian observations     136.75 103.42\n## \n## Marginal log-Likelihood:  68.75 \n##  is computed \n## Posterior summaries for the linear predictor and the fitted values are computed\n## (Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\nWe can see that both the intercept and slope and the error precision are correctly estimated.\n\n\nGenerate model predictions\n\nNow we can take the fitted bru object and use the predict function to produce predictions for \\(\\mu\\) given a new set of values for the model covariates or the original values used for the model fit\n\nnew_data = data.frame(x = c(df$x, runif(10)),\n                      y = c(df$y, rep(NA,10)))\npred = predict(fit.lm, new_data, ~ beta_0 + beta_1,\n               n.samples = 1000)\n\nThe predict function generate samples from the fitted model. In this case we set the number of samples to 1000.\n\nPlotR Code\n\n\n\n\n\n\n\nData and 95% credible intervals\n\n\n\n\n\n\n\n\nCode\npred %&gt;% ggplot() + \n  geom_point(aes(x,y), alpha = 0.3) +\n  geom_line(aes(x,mean)) +\n  geom_line(aes(x, q0.025), linetype = \"dashed\")+\n  geom_line(aes(x, q0.975), linetype = \"dashed\")+\n  xlab(\"Covariate\") + ylab(\"Observations\")\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nGenerate predictions for a new observation with \\(x_0 = 0.45\\)\n\n\nTake hint\n\nYou can create a new data frame containing the new observation \\(x_0\\) and then use the predict function.\n\n\n\n\nClick here to see the solution\n\n\nCode\nnew_data = data.frame(x = 0.45)\npred = predict(fit.lm, new_data, ~ beta_0 + beta_1,\n               n.samples = 1000)",
    "crumbs": [
      "Home",
      "Practical 1"
    ]
  },
  {
    "objectID": "day1_practical1.html#linear-mixed-model",
    "href": "day1_practical1.html#linear-mixed-model",
    "title": "Practical 1",
    "section": "Linear Mixed Model",
    "text": "Linear Mixed Model\nIn this practical we will:\n\nUnderstand the basic structure of a Linear Mixed Model (LLM)\nSimulate data from a LMM\nLearn how to fit a LMM with inlabru and predict from the model.\n\nConsider the a simple linear regression model except with the addition that the data that comes in groups. Suppose that we want to include a random effect for each group \\(j\\) (equivalent to adding a group random intercept). The model is then: \\[\ny_{ij}  = \\beta_0 + \\beta_1 x_i + u_j + \\epsilon_{ij} ~~~  \\text{for}~i = 1,\\ldots,N~ \\text{and}~ j = 1,\\ldots,m.\n\\]\nHere the random group effect is given by the variable \\(u_j \\sim \\mathcal{N}(0, \\tau^{-1}_u)\\) with \\(\\tau_u = 1/\\sigma^2_u\\) describing the variability between groups (i.e., how much the group means differ from the overall mean). Then, \\(\\epsilon_j \\sim \\mathcal{N}(0, \\tau^{-1}_\\epsilon)\\) denotes the residuals of the model and \\(\\tau_\\epsilon = 1/\\sigma^2_\\epsilon\\) captures how much individual observations deviate from their group mean (i.e., variability within group).\nThe model design matrix for the random effect has one row for each observation (this is equivalent to a random intercept model). The row of the design matrix associated with the \\(ij\\)-th observation consists of zeros except for the element associated with \\(u_j\\), which has a one.\n\\[\n\\pmb{\\eta} = \\pmb{A}\\pmb{u} = \\pmb{A}_1\\pmb{u}_1 + \\pmb{A}_2\\pmb{u}_2 + \\pmb{A}_3\\pmb{u}_3\n\\]\n\n\n\n\n\n\nSupplementary material: LMM as a LGM\n\n\n\nIn matrix form, the linear mixed model for the j-th group can be written as:\n\\[ \\overbrace{\\mathbf{y}_j}^{ N \\times 1} = \\overbrace{X_j}^{ N \\times 2} \\underbrace{\\beta}_{1\\times 1} + \\overbrace{Z_j}^{n_j \\times 1} \\underbrace{u_j}_{1\\times1} + \\overbrace{\\epsilon_j}^{n_j \\times 1}, \\]\nIn a latent Gaussian model (LGM) formulation the mixed model predictor for the i-th observation can be written as :\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_i + \\sum_k^K f_k(u_j)\n\\]\nwhere \\(f_k(u_j) = u_j\\) since there‚Äôs only one random effect per group (i.e., a random intercept for group \\(j\\)). The fixed effects \\((\\beta_0,\\beta_1)\\) are assigned Gaussian priors (e.g., \\(\\beta \\sim \\mathcal{N}(0,\\tau_\\beta^{-1})\\)). The random effects \\(\\mathbf{u} = (u_1,\\ldots,u_m)^T\\) follow a Gaussian density \\(\\mathcal{N}(0,\\mathbf{Q}_u^{-1})\\) where \\(\\mathbf{Q}_u = \\tau_u\\mathbf{I}_m\\) is the precision matrix for the random intercepts. Then, the components for the LGM are the following:\n\nLatent field given by\n\\[\n\\begin{bmatrix} \\beta \\\\\\mathbf{u}\n\\end{bmatrix} \\sim \\mathcal{N}\\left(\\mathbf{0},\\begin{bmatrix}\\tau_\\beta^{-1}\\mathbf{I}_2&\\mathbf{0}\\\\\\mathbf{0} &\\tau_u^{-1}\\mathbf{I}_m\\end{bmatrix}\\right)\n\\]\nLikelihood:\n\\[\ny_i \\sim \\mathcal{N}(\\eta_i,\\tau_{\\epsilon}^{-1})\n\\]\nHyperparameters:\n\n\\(\\tau_u\\sim\\mathrm{Gamma}(a,b)\\)\n\\(\\tau_\\epsilon \\sim \\mathrm{Gamma}(c,d)\\)\n\n\n\n\n\nSimulate example data\n\nset.seed(12)\nbeta = c(1.5,1)\nsd_error = 1\ntau_group = 1\n\nn = 100\nn.groups = 5\nx = rnorm(n)\nv = rnorm(n.groups, sd = tau_group^{-1/2})\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error) +\n  rep(v, each = 20)\n\ndf = data.frame(y = y, x = x, j = rep(1:5, each = 20))  \n\nNote that inlabru expects an integer indexing variable to label the groups.\n\n\nCode\nggplot(df) +\n  geom_point(aes(x = x, colour = factor(j), y = y)) +\n  theme_classic() +\n  scale_colour_discrete(\"Group\")\n\n\n\n\n\nData for the linear mixed model example with 5 groups\n\n\n\n\n\n\nFitting a LMM in inlabru\n\nDefining model components and observational model\nIn order to specify this model we must use the group argument to tell inlabru which variable indexes the groups. The model = \"iid\" tells INLA that the groups are independent from one another.\n\n# Define model components\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\") +\n  u(j, model = \"iid\")\n\nThe group variable is indexed by column j in the dataset. We have chosen to name this component v() to connect with the mathematical notation that we used above.\n\n# Construct likelihood\nlik =  like(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n\nWarning: `like()` was deprecated in inlabru 2.12.0.\n‚Ñπ Please use `bru_obs()` instead.\n\n\nFitting the model\nThe model can be fitted exactly as in the previous examples by using the bru function with the components and likelihood objects.\n\nfit = bru(cmp, lik)\nsummary(fit)\n## inlabru version: 2.13.0\n## INLA version: 25.08.21-1\n## Components:\n## beta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\n## beta_1: main = linear(x), group = exchangeable(1L), replicate = iid(1L), NULL\n## u: main = iid(j), group = exchangeable(1L), replicate = iid(1L), NULL\n## Observation models:\n##   Family: 'gaussian'\n##     Tag: &lt;No tag&gt;\n##     Data class: 'data.frame'\n##     Response class: 'numeric'\n##     Predictor: y ~ .\n##     Additive/Linear: TRUE/TRUE\n##     Used components: effects[beta_0, beta_1, u], latent[]\n## Time used:\n##     Pre = 0.32, Running = 0.253, Post = 0.15, Total = 0.723 \n## Fixed effects:\n##         mean    sd 0.025quant 0.5quant 0.975quant  mode kld\n## beta_0 2.108 0.438      1.229    2.108      2.986 2.108   0\n## beta_1 1.172 0.120      0.936    1.172      1.407 1.172   0\n## \n## Random effects:\n##   Name     Model\n##     u IID model\n## \n## Model hyperparameters:\n##                                          mean    sd 0.025quant 0.5quant\n## Precision for the Gaussian observations 0.995 0.144      0.738    0.986\n## Precision for u                         1.613 1.060      0.369    1.356\n##                                         0.975quant  mode\n## Precision for the Gaussian observations       1.30 0.971\n## Precision for u                               4.35 0.918\n## \n## Marginal log-Likelihood:  -179.93 \n##  is computed \n## Posterior summaries for the linear predictor and the fitted values are computed\n## (Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\n\nModel predictions\nTo compute model predictions we can create a data.frame containing a range of values of covariate where we want the response to be predicted for each group. Then we simply call the predict function while spe\n\n\nLMM fitted values\n# New data\nxpred = seq(range(x)[1], range(x)[2], length.out = 100)\nj = 1:n.groups\npred_data = expand.grid(x = xpred, j = j)\npred = predict(fit, pred_data, formula = ~ beta_0 + beta_1 + u) \n\n\npred %&gt;%\n  ggplot(aes(x=x,y=mean,color=factor(j)))+\n  geom_line()+\n  geom_ribbon(aes(x,ymin = q0.025, ymax= q0.975,fill=factor(j)), alpha = 0.5) + \n  geom_point(data=df,aes(x=x,y=y,colour=factor(j)))+\n  facet_wrap(~j)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Question\n\n\n\nSuppose that we are also interested in including random slopes into our model. Assuming intercept and slopes are independent, can your write down the linear predictor and the components of this model as a LGM?\n\n\nGive me a hint\n\nIn general, the mixed model predictor can decomposed as:\n\\[ \\pmb{\\eta} = X\\beta + Z\\mathbf{u} \\]\nWhere \\(X\\) is a \\(n \\times p\\) design matrix and \\(\\beta\\) the corresponding p-dimensional vector of fixed effects. Then \\(Z\\) is a \\(n\\times q_J\\) design matrix for the \\(q_J\\) random effects and \\(J\\) groups; \\(\\mathbf{v}\\) is then a \\(q_J \\times 1\\) vector of \\(q\\) random effects for the \\(J\\) groups. In a latent Gaussian model (LGM) formulation this can be written as:\n\\[ \\eta_i = \\beta_0 + \\sum\\beta_j x_{ij} + \\sum_k f(k) (u_{ij}) \\]\n\n\n\nSee Solution\n\n\nThe linear predictor is given by\n\\[\n\\eta_i = \\beta_0 + \\beta_1x_i + u_{0j} + u_{1j}x_i\n\\]\nLatent field defined by:\n\n\\(\\beta \\sim \\mathcal{N}(0,\\tau_\\beta^{-1})\\)\n\\(\\mathbf{u}_j = \\begin{bmatrix}u_{0j} \\\\ u_{1j}\\end{bmatrix}, \\mathbf{u}_j \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{Q}_u^{-1})\\) where the precision matrix is a block-diagonal matrix with entries \\(\\mathbf{Q}_u= \\begin{bmatrix}\\tau_{u_0} & {0} \\\\{0} & \\tau_{u_1}\\end{bmatrix}\\)\n\nThe hyperparameters are then:\n\n\\(\\tau_{u_0},\\tau_{u_1} \\text{and}~\\tau_\\epsilon\\)\n\n\nTo fit this model in inlabru we can simply modify the model components as follows:\n\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\") +\n  u0(j, model = \"iid\") + u1(j,x, model = \"iid\")",
    "crumbs": [
      "Home",
      "Practical 1"
    ]
  },
  {
    "objectID": "day1_practical1.html#sec-genlinmodel",
    "href": "day1_practical1.html#sec-genlinmodel",
    "title": "Practical 1",
    "section": "Generalized Linear Model",
    "text": "Generalized Linear Model\nIn this practical we will:\n\nSimulate non-Gaussian data\nLearn how to fit a generalised linear model with inlabru\nGenerate predictions from the model\n\nA generalised linear model allows for the data likelihood to be non-Gaussian. In this example we have a discrete response variable which we model using a Poisson distribution. Thus, we assume that our data \\[\ny_i \\sim \\text{Poisson}(\\lambda_i)\n\\] with rate parameter \\(\\lambda_i\\) which, using a log link, has associated predictor \\[\n\\eta_i = \\log \\lambda_i = \\beta_0 + \\beta_1 x_i\n\\] with parameters \\(\\beta_0\\) and \\(\\beta_1\\), and covariate \\(x\\). This is identical in form to the predictor in Section¬†1. The only difference is now we must specify a different data likelihood.\n\nSimulate example data\nThis code generates 100 samples of covariate x and data y.\n\nset.seed(123)\nn = 100\nbeta = c(1,1)\nx = rnorm(n)\nlambda = exp(beta[1] + beta[2] * x)\ny = rpois(n, lambda  = lambda)\ndf = data.frame(y = y, x = x)  \n\n\n\nFitting a GLM in inlabru\n\nDefine model components and likelihood\nSince the predictor is the same as Section¬†1, we can use the same component definition:\n\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n\nHowever, when building the observation model likelihood we must now specify the Poisson likelihood using the family argument (the default link function for this family is the \\(\\log\\) link).\n\nlik =  bru_obs(formula = y ~.,\n            family = \"poisson\",\n            data = df)\n\nFit the model\nOnce the likelihood object is constructed, fitting the model is exactly the same process as in Section¬†1.\n\nfit_glm = bru(cmp, lik)\n\nAnd model summaries can be viewed using\n\nsummary(fit_glm)\n\ninlabru version: 2.13.0\nINLA version: 25.08.21-1\nComponents:\nbeta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\nbeta_1: main = linear(x), group = exchangeable(1L), replicate = iid(1L), NULL\nObservation models:\n  Family: 'poisson'\n    Tag: &lt;No tag&gt;\n    Data class: 'data.frame'\n    Response class: 'integer'\n    Predictor: y ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[beta_0, beta_1], latent[]\nTime used:\n    Pre = 0.288, Running = 0.216, Post = 0.0377, Total = 0.542 \nFixed effects:\n        mean    sd 0.025quant 0.5quant 0.975quant  mode kld\nbeta_0 0.915 0.071      0.775    0.915      1.054 0.915   0\nbeta_1 1.048 0.056      0.938    1.048      1.157 1.048   0\n\nMarginal log-Likelihood:  -204.02 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\n\n\nGenerate model predictions\n\nTo generate new predictions we must provide a data frame that contains the covariate values for \\(x\\) at which we want to predict.\nThis code block generates predictions for the data we used to fit the model (contained in df$x) as well as 10 new covariate values sampled from a uniform distribution runif(10).\n\n# Define new data, set to NA the values for prediction\n\nnew_data = data.frame(x = c(df$x, runif(10)),\n                      y = c(df$y, rep(NA,10)))\n\n# Define predictor formula\npred_fml &lt;- ~ exp(beta_0 + beta_1)\n\n# Generate predictions\npred_glm &lt;- predict(fit_glm, new_data, pred_fml)\n\nSince we used a log link (which is the default for family = \"poisson\"), we want to predict the exponential of the predictor. We specify this using a general R expression using the formula syntax.\n\n\n\n\n\n\nNote\n\n\n\nNote that the predict function will call the component names (i.e.¬†the ‚Äúlabels‚Äù) that were decided when defining the model.\n\n\nSince the component definition is looking for a covariate named \\(x\\), all we need to provide is a data frame that contains one, and the software does the rest.\n\nPlotR Code\n\n\n\n\n\n\n\nData and 95% credible intervals\n\n\n\n\n\n\n\npred_glm %&gt;% ggplot() + \n  geom_point(aes(x,y), alpha = 0.3) +\n  geom_line(aes(x,mean)) +\n    geom_ribbon(aes(x = x, ymax = q0.975, ymin = q0.025),fill = \"tomato\", alpha = 0.3)+\n  xlab(\"Covariate\") + ylab(\"Observations (counts)\")\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nSuppose a binary response such that\n\\[\n    \\begin{aligned}\ny_i &\\sim \\mathrm{Bernoulli}(\\psi_i)\\\\\n\\eta_i &= \\mathrm{logit}(\\psi_i) = \\alpha_0 +\\alpha_1 \\times w_i\n\\end{aligned}\n\\] Using the following simulated data, use inlabru to fit the logistic regression above. Then, plot the predictions for the data used to fit the model along with 10 new covariate values\n\nset.seed(123)\nn = 100\nalpha = c(0.5,1.5)\nw = rnorm(n)\npsi = plogis(alpha[1] + alpha[2] * w)\ny = rbinom(n = n, size = 1, prob =  psi) # set size = 1 to draw binary observations\ndf_logis = data.frame(y = y, w = w)  \n\nHere we use the logit link function \\(\\mathrm{logit}(x) = \\log\\left(\\frac{x}{1-x}\\right)\\) (plogis() function in R) to link the linear predictor to the probabilities \\(\\psi\\).\n\n\nTake hint\n\nYou can set family = \"binomial\" for binary responses and the plogis() function for computing the predicted values.\n\n\n\n\n\n\nNote\n\n\n\nThe Bernoulli distribution is equivalent to a \\(\\mathrm{Binomial}(1, \\psi)\\) pmf. If you have proportional data (e.g.¬†no. successes/no. trials) you can specify the number of events as your response and then the number of trials via the Ntrials = n argument of the bru_obs function (where n is the known vector of trials in your data set).\n\n\n\n\n\n\nClick here to see the solution\n\n\nCode\n# Model components\ncmp_logis =  ~ -1 + alpha_0(1) + alpha_1(w, model = \"linear\")\n# Model likelihood\nlik_logis =  bru_obs(formula = y ~.,\n            family = \"binomial\",\n            data = df_logis)\n# fit the model\nfit_logis &lt;- bru(cmp_logis,lik_logis)\n\n# Define data for prediction\nnew_data = data.frame(w = c(df_logis$w, runif(10)),\n                      y = c(df_logis$y, rep(NA,10)))\n# Define predictor formula\npred_fml &lt;- ~ plogis(alpha_0 + alpha_1)\n\n# Generate predictions\npred_logis &lt;- predict(fit_logis, new_data, pred_fml)\n\n# Plot predictions\npred_logis %&gt;% ggplot() + \n  geom_point(aes(w,y), alpha = 0.3) +\n  geom_line(aes(w,mean)) +\n    geom_ribbon(aes(x = w, ymax = q0.975, ymin = q0.025),fill = \"tomato\", alpha = 0.3)+\n  xlab(\"Covariate\") + ylab(\"Observations\")",
    "crumbs": [
      "Home",
      "Practical 1"
    ]
  },
  {
    "objectID": "day1_practical1.html#sec-gam_ex",
    "href": "day1_practical1.html#sec-gam_ex",
    "title": "Practical 1",
    "section": "Generalised Additive Model",
    "text": "Generalised Additive Model\nIn this practical we will:\n\nLearn how to fit a GAM with inlabru\nGenerate predictions from the model\n\nGeneralised Additive Models (GAMs) are very similar to linear models, but with an additional basis set that provides flexibility.\nAdditive models are a general form of statistical model which allows us to incorporate smooth functions alongside linear terms. A general expression for the linear predictor of a GAM is given by\n\\[\n\\eta_i = g(\\mu_i) = \\beta_0 + \\sum_{j=1}^L f_j(x_{ij})\n\\]\nwhere the mean \\(\\pmb{\\mu} = E(\\mathbf{y}|\\mathbf{x}_1,\\ldots,\\mathbf{x}_L)\\) and \\(g()\\) is a link function (notice that the distribution of the response and the link between the predictors and this distribution can be quite general). The term \\(f_j()\\) is a smooth function for the j-th explanatory variable that can be represented as\n\\[\nf(x_i) = \\sum_{k=0}^q\\beta_k b_k(x_i)\n\\]\nwhere \\(b_k\\) denote the basis functions and \\(\\beta_K\\) are their coefficients.\nIncreasing the number of basis functions leads to a more wiggly line. Too few basis functions might make the line too smooth, too many might lead to overfitting.To avoid this, we place further constraints on the spline coefficients which leads to constrained optimization problem where the objective function to be minimized is given by:\n\\[\n\\mathrm{min}\\sum_i(y_i-f(x_i))^2 + \\lambda(\\sum_kb^2_k)\n\\] The first term measures how close the function \\(f()\\) is to the data while the second term \\(\\lambda(\\sum_kb^2_k)\\), penalizes the roughness in the function. Here, \\(\\lambda &gt;0\\) is known as the smoothing parameter because it controls the degree of smoothing (i.e.¬†the trade-off between the two terms). In a Bayesian setting,including the penalty term is equivalent to setting a specific prior on the coefficients of the covariates.\nIn this exercise we will set a random walk prior of order 1 on \\(f\\), i.e.¬†\\(f(x_i)-f(x_i-1) \\sim \\mathcal{N}(0,\\sigma^2_f)\\) where \\(\\sigma_f^2\\) is the smoothing parameter such that small values give large smoothing. Notice that we will assume \\(x_i\\)‚Äôs are equally spaced for now (we will cover a stochastic differential equation approach that relaxes this assumption later on in the course).\n\nSimulate Data\nLets generate some data so evaluate how RW models perform when estimating a smooth curve. The data are simulated from the following model:\n\\[\ny_i = 1 + \\mathrm{cos}(x) + \\epsilon_i, ~ \\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2_\\epsilon)\n\\] where \\(\\sigma_\\epsilon^2 = 0.25\\)\n\nn = 100\nx = rnorm(n)\neta = (1 + cos(x))\ny = rnorm(n, mean =  eta, sd = 0.5)\n\ndf = data.frame(y = y, \n                x_smooth = inla.group(x)) # equidistant x's \n\n\n\nFitting a GAM in inlabru\nNow lets fit a flexible model by setting a random walk of order 1 prior on the coefficients. This can be done bye specifying model = \"rw1\" in the model component (similarly,a random walk of order 2 can be placed by setting model = \"rw2\" )\n\ncmp =  ~ Intercept(1) + \n  smooth(x_smooth, model = \"rw1\")\n\nNow we define the observational model:\n\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n\nWe then can fit the model:\n\nfit = bru(cmp, lik)\nfit$summary.fixed\n\n              mean         sd 0.025quant 0.5quant 0.975quant     mode\nIntercept 1.298799 0.06506719    1.17164 1.298565   1.427293 1.298569\n                   kld\nIntercept 9.965773e-09\n\n\nThe posterior summary regarding the estimated function using RW1 can be accessed through fit$summary.random$smooth, the output includes the value of \\(x_i\\) (ID) as well as the posterior mean, standard deviation, quantiles and mode of each \\(f(x_i)\\). We can use this information to plot the posterior mean and associated 95% credible intervals.\n\nR plotR Code\n\n\n\n\n\n\n\nSmooth effect of the covariate\n\n\n\n\n\n\n\ndata.frame(fit$summary.random$smooth) %&gt;% \n  ggplot() + \n  geom_ribbon(aes(ID,ymin = X0.025quant, ymax= X0.975quant), alpha = 0.5) + \n  geom_line(aes(ID,mean)) + \n  xlab(\"covariate\") + ylab(\"\")\n\n\n\n\n\n\nModel Predictions\nWe can obtain the model predictions using the predict function.\n\npred = predict(fit, df, ~ (Intercept + smooth))\n\nThe we can plot them together with the true curve and data points:\n\n\nCode\npred %&gt;% ggplot() + \n  geom_point(aes(x_smooth,y), alpha = 0.3) +\n  geom_line(aes(x_smooth,1+cos(x_smooth)),col=2)+\n  geom_line(aes(x_smooth,mean)) +\n  geom_line(aes(x_smooth, q0.025), linetype = \"dashed\")+\n  geom_line(aes(x_smooth, q0.975), linetype = \"dashed\")+\n  xlab(\"Covariate\") + ylab(\"Observations\")\n\n\n\n\n\nData and 95% credible intervals\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nFit a flexible model using a random walk of order 2 (RW2) and compare the results with the ones above.\n\n\nTake hint\n\nYou can set model = \"rw2\" for assigning a random walk 2 prior.\n\n\n\n\nClick here to see the solution\n\n\nCode\ncmp_rw2 =  ~ Intercept(1) + \n  smooth(x_smooth, model = \"rw2\")\nlik_rw2 =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\nfit_rw2 = bru(cmp_rw2, lik_rw2)\n\n# Plot the fitted functions\nggplot() + \n  geom_line(data= fit$summary.random$smooth,aes(ID,mean,colour=\"RW1\"),lty=2) + \n  geom_line(data= fit_rw2$summary.random$smooth,aes(ID,mean,colour=\"RW2\")) + \n  xlab(\"covariate\") + ylab(\"\") + scale_color_discrete(name=\"Model\")\n\n\n\n\n\n\n\n\n\n\nWe see that the RW1 fit is too wiggly while the RW2 is smoother and seems to have better fit.",
    "crumbs": [
      "Home",
      "Practical 1"
    ]
  },
  {
    "objectID": "day2_practical_3.html",
    "href": "day2_practical_3.html",
    "title": "inlabru workshop",
    "section": "",
    "text": "Aim of this practical:",
    "crumbs": [
      "Home",
      "Practical 3"
    ]
  },
  {
    "objectID": "day2_practical_3.html#ar1-models-in-inlabru",
    "href": "day2_practical_3.html#ar1-models-in-inlabru",
    "title": "inlabru workshop",
    "section": "AR(1) models in inlabru",
    "text": "AR(1) models in inlabru\nIn this exercise we will:\n\nSimulate a time series with autocorrelated errors.\nFit an AR(1) process with inlabru\nVisualize model predictions.\nForecasting for future observations\n\nStart by loading useful libraries:\n\nlibrary(tidyverse)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nTime series analysis is particularly valuable for modelling data with temporal dependence or autocorrelation, where observations taken at nearby time points tend to be more similar than those further apart.\nA time series process is a stochastic process \\(\\{X_t~|~t \\in T\\}\\), which is a collection of random variables that are ordered in time where \\(T\\) is the index set that determines the set of discrete and equally spaced time points at which the process is defined and observations are made.\nAutoregressive processes allow us to account for the time dependence by regressing \\(X_t\\) on past values \\(X_{t-1},\\ldots,X_{t-p}\\) with associated coefficients \\(\\phi_k\\) for each lag \\(k = 1,\\ldots,p\\). Thus an autoregressive process of order \\(p\\), denoted AR(\\(p\\)) , is given by:\n\\[\nX_t = \\phi_1 X_{t-1} + \\ldots + \\phi_p X_{t-p} + \\varepsilon_t; ~~ \\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2_e)\n\\]\nConsider now an univariate time series \\(y_t\\) which evolves over time according to some autoregressive stochastic process. For example, a time series where the system follows an AR(1) process can be defined as:\n\\[\n\\begin{aligned}\ny_t &\\sim \\mathcal{N}(\\mu_t,\\tau_y^{-1})\\\\\n\\eta_t &= g^{-1}(\\mu_t) = \\alpha + u_t \\\\\nu_t &= \\phi u_{t-1} + \\delta_t ; ~~ \\delta_t \\sim \\mathcal{N}(0,\\tau_u^{-1}); ~~ t &gt; 1 \\\\\nu_1 &= \\mathcal{N}(0,\\kappa^{-1})\\\\\n\\kappa &= \\tau_u (1-\\phi^2)\n\\end{aligned}\n\\]\nThe response \\(y_t\\) is assumed to be normal distributed with mean \\(\\alpha + u_t\\) and precision error \\(\\tau_y\\) ( here \\(g(\\cdot)\\) is just the identity link function that maps the linear predictor to the mean of the process). Then, the process \\(u_t\\) follows an AR(1) process where \\(u_1\\) is drawn from a stationary normal distribution such that \\(\\kappa\\) denotes the marginal precision for state \\(u_t\\)\nThe covariance matrix is then given by:\n\\[\n\\Sigma = \\frac{\\tau^{-1}_u}{1-\\phi^2}\n\\begin{bmatrix}\n1 & \\phi & \\phi^2 & \\ldots & \\phi^{n-1}\\\\\n\\phi & 1 & \\phi & \\ldots & \\phi^{n-2} \\\\\n\\phi^2 & \\phi & 1 & \\ldots & \\phi^{n-3} \\\\\n\\phi^{n-1} & \\phi^{n-2} & \\phi^{n-3} & \\ldots & 1\n\\end{bmatrix}\n\\]\nNotice that conditionally on \\(u_t\\), the observed data \\(y_y\\) are independent from\\(y_{t-1},y_{t-2}.y_{t-3},\\ldots\\), also the conditional distribution of \\(u_t\\) is a markov chain such that \\(\\pi(u_t|u_{t-1},u_{t-2},u_{t-3}) = \\pi(u_t|u_{t-1})\\). Thus, each time point is only conditionally dependent on the two closest time points:\n\\[\nu_t|\\mathbf{u}_{-t} \\sim \\mathcal{N}\\left(\\frac{\\phi}{1-\\phi^2}(u_{t-1}+u_{t+1}),\\frac{\\tau_u^{-1}}{1-\\phi^2}\\right)\n\\]\n\nSimulate example data\nFirst, we simulate data from the model:\n\\[\n\\begin{aligned}\ny_t &= \\alpha + u_t + \\varepsilon_t;~ \\varepsilon_t \\sim \\mathcal{N}(0,\\tau_y^{-1})\\\\\nu_t &= \\phi y_{t-1} + \\delta_t; ~ \\delta_t \\sim \\mathcal{N}(0,\\tau_u^{-1})\n\\end{aligned}\n\\]\n\nset.seed(123)\n\nphi = 0.8\ntau_u = 10\nmarg.prec = tau_u * (1-phi^2) # ar1 in INLA is parametrized as marginal variance\nu_t =  as.vector(arima.sim(list(order = c(1,0,0), ar = phi), \n                          n = 100,\n                          sd=sqrt(1/tau_u)))\na = 1\ntau_e = 5\nepsilon_t = rnorm(100, sd = sqrt(1/tau_e))\ny = a + u_t + epsilon_t\n\n\nts_dat &lt;- data.frame(y =y , x= 1:100)\n\n\n\n\n\n\n\n\n\n\n\n\nFitting an AR(1) model with inlabru\nModel components\nFirst, we define the model components, notice that the latent field is defined by two components: the intercept \\(\\alpha\\) and the autoregressive random effects \\(u_t\\):\n\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(x,model = \"ar1\")\n\nThe we can define the formula for the linear predictor and specify the observational model\n\n# Model formula\nformula = y ~ alpha + ut\n# Observational model\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = ts_dat)\n\nLastly, we fit the model using the bru function and compare the model estimates with the true mdeol parameters we simulate our data from.\n\n# fit the model\nfit.ar1 = bru(cmp, lik)\n\n# compare against the true values\n\ndata.frame(\n  true = c(a,tau_e,marg.prec,phi),\n  rbind(fit.ar1$summary.fixed[,c(1,3,5)],\n        fit.ar1$summary.hyperpar[,c(1,3,5)])\n        ) %&gt;% round(2)\n\n                                        true mean X0.025quant X0.975quant\nalpha                                    1.0 1.00        0.69        1.28\nPrecision for the Gaussian observations  5.0 4.91        3.11        7.29\nPrecision for ut                         3.6 7.96        2.91       18.10\nRho for ut                               0.8 0.80        0.54        0.94\n\n\nModel predictions\nHere, we will predict the mean of our time series along with 95% credible intervals. Note that this interval are for the mean and not for new observations, we will cover forecasting new observations next.\n\npred_ar1 = predict(fit.ar1, ts_dat, ~ alpha + ut)\n\nggplot(pred_ar1,aes(y=mean,x=x))+\n  geom_line()+\n    geom_ribbon(aes(x = x, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(aes(y=y,x=x))\n\n\n\n\n\n\n\n\n\n\nForecasting\nA common goal in time series modelling is forecasting into the future. Forecasting can be treated as a missing data problem where future values of the response variable are missing. Let \\(y_m\\) be the missing response, then, by fitting a statistical model to the observed data \\(\\mathbf{y}_{obs}\\), we condition on its parameters to obtain the posterior predictive distribution:\n\\[\n\\pi(y_{m} \\mid \\mathbf{y}_{obs}) = \\int \\pi(y_{m}, \\theta \\mid \\mathbf{y}_{obs})  d\\theta = \\int \\pi(y_{m} \\mid \\mathbf{y}_{obs}, \\theta) \\pi(\\theta \\mid \\mathbf{y}_{obs})  d\\theta\n\\]\nThis distribution, which integrates over all parameter uncertainty, provides the complete probabilistic forecast for the missing values. INLA will automatically compute the predictive distributions for all missing values in the response. To do so, we can augment our data set by including the new time points at which the prediction will be made and setting the response value to¬†NA¬†for these new time points:\n\nts.forecast &lt;- rbind(ts_dat, \n  data.frame(y = rep(NA, 50), x = 101:150))\n\nNext, we fit the¬†ar1¬†model to the new dataset so that the predictive distributions are computed:\n\ncmp =  ~ -1 + alpha(1) + ut(x,model = \"ar1\")\n\npred_lik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = ts.forecast)\n\nfit.forecast = bru(cmp, pred_lik)\n\nLastly, we can draw samples from the posterior predictive distribution using the predict function and visualize our forecast as follows:\n\npred_forecast = predict(fit.forecast, ts.forecast, ~ alpha + ut)\n\np1= ggplot(pred_forecast,aes(y=mean,x=x))+\n  geom_line()+\n    geom_ribbon(aes(x = x, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(data=ts_dat, aes(y=y,x=x))\n\nUsing generate to sample from the PPD\n\nff = generate(fit.ar1, \n        data.frame(x = seq(1,150)), \n        ~ data.frame(mean = alpha + ut,\n                     sd = sqrt(1/Precision_for_the_Gaussian_observations)),\n        n.samples = 5000)\n\nff1 = sapply(ff, function(xx) rnorm(150, mean = xx[,1], sd = xx[,2]))\n\n\np_pred = ggplot(pred_ar1,aes(y=mean,x=x))+\n  geom_line()+\n    geom_ribbon(aes(x = x, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(aes(y=y,x=x))\n\n\npred_int = data.frame(x = seq(1,150),\n                      mean = apply(ff1,1,mean),\n                      q1 = apply(ff1, 1, quantile,probs =  0.025),\n                      q2 = apply(ff1, 1, quantile, 0.975)\n                      )\n p1 + geom_line(data = pred_int ,aes(x=x,y=mean))+\n   geom_ribbon(data = pred_int, aes(x, ymin = q1, ymax = q2), alpha = 0.5, fill = \"red\")",
    "crumbs": [
      "Home",
      "Practical 3"
    ]
  },
  {
    "objectID": "day2_practical_3.html#great-lakes-water-level",
    "href": "day2_practical_3.html#great-lakes-water-level",
    "title": "inlabru workshop",
    "section": "Great Lakes water level",
    "text": "Great Lakes water level\nIn this exercise we will:\n\nFit an AR(1) process with inlabru to model lakes water levels\nChange the default priors for the observational error\nSet penalized complexity priors for the correlation and precision parameters of the latent effects.\nFit a RW(1) model\nFit a an AR(1) model with gorup-level correlation\nFit an RW(2) model to non-Gaussian data\n\nStart by loading useful libraries:\n\nlibrary(tidyverse) \nlibrary(INLA) \nlibrary(ggplot2)\nlibrary(patchwork) \nlibrary(inlabru)\nlibrary(DAAG)\n\nIn this exercise we will look at greatLakes dataset from the DAAG package. The data set contains the water level heights for the lakes Erie, Michigan/Huron, Ontario and St Clair from 1918 to 2009.\nLets begin by loading and formatting the data into a tidy format.\n\ndata(\"greatLakes\")\n\ngreatLakes.df = data.frame(as.matrix(greatLakes),\n                           year = time(greatLakes)) %&gt;%\n  pivot_longer(cols = c(\"Erie\",\"michHuron\",\"Ontario\",\"StClair\"),\n               names_to = \"Lakes\",\n               values_to = \"height\" )",
    "crumbs": [
      "Home",
      "Practical 3"
    ]
  },
  {
    "objectID": "day2_practical_3.html#fitting-an-ar1-model-in-inlabru",
    "href": "day2_practical_3.html#fitting-an-ar1-model-in-inlabru",
    "title": "inlabru workshop",
    "section": "Fitting an AR(1) model in inlabru",
    "text": "Fitting an AR(1) model in inlabru\nWe will focus on the Erin lake for now. Lets begin by fitting an AR(1) model of the form:\n\\[\n\\begin{aligned}\n\\text{height}_t &= \\alpha + u_t +\\varepsilon_t~; ~~ \\varepsilon_t\\sim \\mathcal{N}(0,\\tau_e^{-1}) \\\\\nu_t &= \\phi u_{t-1} + \\delta_t~~~ ; ~~ \\delta_t \\sim \\mathcal{N}(0,\\tau_u^{-1}); ~~ t &gt; 1 \\\\\nx_1 &= \\mathcal{N}(0,\\kappa^{-1})\\\\\n\\kappa &= \\tau_u (1-\\phi^2)\n\\end{aligned}\n\\]\nWhere \\(\\alpha\\) is the intercept, \\(\\phi\\) is the correlation term, \\(\\varepsilon\\) is the observational Gaussian error with mean zero and precision \\(\\tau_e\\) and \\(\\kappa\\) is the marginal precision for the state \\(u_t\\) for \\(t= 1,\\ldots,92\\).\nFirst we make a subset of the dataset and create a time index \\(T\\):\n\ngreatLakes.df$t.idx &lt;- greatLakes.df$year-1917\n\nErie.df = greatLakes.df %&gt;% filter(Lakes == \"Erie\")\n\n\n\n\n\n\n\n Task\n\n\n\nFit an AR(1) model to the Erie lake data using inlabru, then plot the model fitted values showing 95% credible intervals.\n\n\nTake hint\n\nRemember this is done by (1) defining the model components, (2) the formula and (3) the observational model. Then you can use the predict function to compute the predicted values for the mean along with 95% credible intervals.\n\n\n\n\nClick here to see the solution\n\n\nCode\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(t.idx,model = \"ar1\")\n# Model formula\nformula = height ~ alpha + ut\n\n\n# Observational model\nlik =  bru_obs(formula = height   ~.,\n            family = \"gaussian\",\n            data = Erie.df )\n\n# fit the model\nfit.Erie_ar1 = bru(cmp, lik)\n\n# Model predictions \n\npred_ar1.Erie = predict(fit.Erie_ar1, Erie.df, ~ alpha + ut)\n\n# plot model fitted values\nggplot(pred_ar1.Erie,aes(y=mean,x=year))+\n  geom_line()+\n    geom_ribbon(aes(x = year, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(aes(y=height,x=year))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Question\n\n\n\nAre there any issues with the fitted model, and if so, how do you think we should address them?\n\n\nAnswer\n\nIt is clear that the model overfits the data, leading to poor predictive performance. Thus, we need to introduce some prior information on the what we expect the variation of the process to be.\n\n\n\nPriors\nLet review INLA‚Äô prior parametrization for autoregressive models:\nLet \\(\\pmb{\\theta} = \\{\\theta_y,\\theta_u,\\theta_\\phi\\}\\) be INLA‚Äôs internal representation of the hyperparameters such that:\n\\(\\theta_y = \\log(\\tau^2_y)\\)\n\\(\\theta_u = \\log(\\kappa) = \\log\\left(\\tau_u[1-\\phi^2]\\right)\\)\n\\(\\theta_\\phi = \\log \\left(\\frac{1+\\phi}{1-\\phi}\\right)\\)\nThe default priors for \\(\\{\\theta_y,\\theta_u\\}\\) are \\(\\text{log-gamma} (1, 5\\times 10^{-5} )\\) priors with default initial values set to 4 in each case. Then, Gaussian priors \\(\\alpha \\sim \\mathcal{N}(0,\\tau_y = 0.001)\\) and \\(\\theta_\\phi \\sim \\mathcal{N}(0, \\tau_y= 0.15)\\) are used for the intercept and correlation parameter respectively.\n\n\n\n\n\n\nNote\n\n\n\nSpecifically for AR(1) correlation parameter \\(\\phi\\), INLA uses the following logit transformation on \\(\\theta_\\phi\\):\n\\[ \\phi = \\frac{2\\exp(\\theta_\\phi)}{1+ \\exp(\\theta_\\phi)} -1. \\]\n\n\nSetting priors and PC-priors\nLets now set a Gamma prior with parameters 1 and 1, so that the precision of the Gaussian osbervational error is centered at 1 with a variance of 1. Additionally we will set Penalized Complexity (PC) priors according to the following probability statements:\n\n\\(P(\\sigma &gt; 1) = 0.01\\)\n\\(P(\\phi &gt; 0.5) = 0.3\\)\n\nNotice that the PC prior for the precision \\(\\tau_u\\) is defined on the standard deviation \\(\\sigma_u = \\tau_u^{-1/2}\\)\n\npc_prior &lt;- list(theta = list(prior = \"pc.prec\", param = c(1, 0.01)),\n                 rho = list(prior = \"pc.cor0\", param = c(0.5, 0.3))) \n\nprec.tau_e &lt;- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(1, 1))) # prior values\n\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(t.idx, model = \"ar1\",  hyper = pc_prior)\n# Model formula\nformula = height ~ alpha + ut\n\n\n# Observational model\nlik =  bru_obs(formula = height  ~.,\n            family = \"gaussian\",\n            data = Erie.df,\n            control.family = list(hyper = prec.tau_e))\n\n# fit the model\nfit.Erie_ar1 = bru(cmp, lik)\n\n\n\n\n\n\n\n Question\n\n\n\nWhat is the posterior mean for the correlation parameter \\(\\rho\\)? \n\n\n\n\n\n\n\n\n Task\n\n\n\n\n\nTake hint\n\nPlot the fitted values of the model, has the overfitting problem being alleviated?\n\n\n\n\nClick here to see the solution",
    "crumbs": [
      "Home",
      "Practical 3"
    ]
  },
  {
    "objectID": "day2_practical_3.html#fitting-a-rw1-model",
    "href": "day2_practical_3.html#fitting-a-rw1-model",
    "title": "inlabru workshop",
    "section": "Fitting a RW(1) model",
    "text": "Fitting a RW(1) model\nNow we fit a random walk of order 1 to the Erie lake data:\n\\[\n\\begin{aligned}\ny_t &= \\alpha + u_t + \\varepsilon_t, ~ t = 1,\\ldots,92 \\\\\n\\varepsilon_t & \\sim \\mathcal{N}(0,\\tau_e) \\\\\nu_t - u_{t-1} &\\sim \\mathcal{N}(0,\\tau_u),~ t = 2,\\ldots,92 \\\\\n\\end{aligned}\n\\]\nFirs we define model priors:\n\npc_prior &lt;- list(theta = list(prior = \"pc.prec\", param = c(1, 0.01))) \n\nprec.tau_e &lt;- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(1, 1))) # prior values\n\nNow we define model components:\n\ncmp_rw =  ~ -1 + alpha(1) + \n  ut(t.idx ,\n     model = \"rw1\",\n     constr = FALSE,\n     hyper=pc_prior,\n     scale.model = TRUE)\n\nNotice that we have set scale.model = TRUE to scale the latent effects. This is particularly important when Intrinsic Gaussian Markov random fields (IGMRFs) are used as priors (e.g., random walk models or some spatial models) for the latent effects. By defining scale.model = TRUE, the rw1-model is scaled to have a generalized variance equal to one. By scaling scaling the models we ensure that a fixed hyperprior for the precision parameter has a similar interpretation for different types of IGMRFs, making precision estimates comparable between different models. Scaling also allows estimates to be less sensitive to re-scaling covariates in the linear predictor and makes the precision invariant to changes in the shape and size of the latent effect (see (s√∏rbye2014?) for further details) .\nWe can now fit the model with the updated components and plot the predicted values\n\n# fit the model\nfit.Erie_rw1 = bru(cmp_rw, lik)\n# Model predictions\npred_rw1.Erie = predict(fit.Erie_rw1, Erie.df, ~ alpha + ut)",
    "crumbs": [
      "Home",
      "Practical 3"
    ]
  },
  {
    "objectID": "day2_practical_3.html#group-level-effects",
    "href": "day2_practical_3.html#group-level-effects",
    "title": "inlabru workshop",
    "section": "Group-level effects",
    "text": "Group-level effects\nNow we will model the height water levels for all four lakes by grouping the random effects. This will allow a within-lakes correlation to be included. In the next example, we allow for correlated effects using an ar1 model for the years and iid random effects on the lakes. First we create a lakes id and set the priors for our model:\n\ngreatLakes.df$lake_id &lt;- as.numeric(as.factor(greatLakes.df$Lakes))\n\npc_prior &lt;- list(theta = list(prior = \"pc.prec\", param = c(1, 0.01)),\n                 rho = list(prior = \"pc.cor0\", param = c(0.5, 0.3))) \n\nprec.tau_e &lt;- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(1, 10))) # prior values\n\nNow we define the model components. The lakes IDs that define the group are passed with parameter group argument and the iid model and other parameters are passed through the control.group parameter.\n\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(year,model = \"ar1\",\n                            hyper = pc_prior,\n                            group =lake_id,\n                            control.group = \n                            list(model = \"iid\", \n                                 scale.model = TRUE))\n\nWe fit the model in a similar fashion as we did before:\n\n# Model formula\nformula = height ~ alpha + ut\n\n\n# Observational model\nlik =  bru_obs(formula = height  ~.,\n            family = \"gaussian\",\n            data = greatLakes.df,\n            control.family = list(hyper = prec.tau_e))\n\n# fit the model\nfit.all_lakes_ar1 = bru(cmp, lik)\n\n# Model predictions\npred_ar1.all = predict(fit.all_lakes_ar1, greatLakes.df, ~ alpha + ut)\n\nLastly we can visualize group-level model predictions as follows:\n\nggplot(pred_ar1.all,aes(y=mean,x=year))+\n  geom_line()+\n    geom_ribbon(aes(x = year, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(aes(y=height,x=year)) + facet_wrap(~Lakes,scales = \"free\")",
    "crumbs": [
      "Home",
      "Practical 3"
    ]
  },
  {
    "objectID": "day2_practical_3.html#non-gaussian-data",
    "href": "day2_practical_3.html#non-gaussian-data",
    "title": "inlabru workshop",
    "section": "Non-Gaussian data",
    "text": "Non-Gaussian data\nIn the next example we will use the Toyo data set to illustrate how temporal models can be fit to non-Gaussian data. The Tokyo data set available in INLA contains the recorded days of rain above 1 mm in Tokyo for 2 years, 1983:84. The data set contains the following variables:\n\ny : number of days with rain\nn : total number of days\ntime : day of the year\n\n\ndata(\"Tokyo\")\n\nA possible observational model for these data is\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim\\text{Bin}(n_t, p_t) \\\\\n\\eta_t &= \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\end{aligned}\n\\] \\[\nn_t = \\left\\{\n\\begin{array}{lr}\n1, & \\text{for}\\; 29\\; \\text{February}\\\\\n2, & \\text{other days}\n\\end{array}\\right.\n\\] \\[\ny_t =\n\\begin{cases}\n\\{0,1\\}, & \\text{for}\\; 29\\; \\text{February}\\\\\n\\{0,1,2\\}, & \\text{other days}\n\\end{cases}\n\\]\nThen, the latent field is given by\n\\[\n\\eta_t = \\beta_0 + f(\\text{time}_t)\n\\]\n\nWhere the probability of rain depends on on the day of the year \\(t\\)\n\\(\\beta_0\\) is an intercept\n\\(f(\\text{time}_t)\\) is a temporal model, e.g., a RW2 model (this is just a smoother).\n\nThe smoothness is controlled by a hyperparameter \\(\\tau_f\\) . Thus, we assign a prior to \\(\\tau_f\\) to finalize the model.\nWe can fit the model as follows:\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)\n\nNotice that we have set cyclic = TRUE as this is a cyclic effect. Finally, we can produce model predictions in a similar fashion as we did before:\n\npTokyo = predict(fit, Tokyo, ~ plogis(beta0 + time_effect))\n\nggplot(data=pTokyo , aes(x= time, y= y) ) +\n  geom_point() + \n  ylab(\"\") + xlab(\"\") +\n  # Custom the Y scales:\n  scale_y_continuous(\n    # Features of the first axis\n    name = \"\",\n    # Add a second axis and specify its features\n    sec.axis = sec_axis( transform=~./2, name=\"Probability\")\n  )  + geom_line(aes(y=mean*2,x=time)) +\n  geom_ribbon(aes( ymin = q0.025*2, \n                             ymax = q0.975*2), alpha = 0.5)",
    "crumbs": [
      "Home",
      "Practical 3"
    ]
  },
  {
    "objectID": "slides/slides_2.html#outline",
    "href": "slides/slides_2.html#outline",
    "title": "Lecture 1",
    "section": "Outline",
    "text": "Outline\n\nWhat are INLA and inlabru?\nWhy the Bayesian framework?\nWhich model are inlabru-friendly?\nWhat are Latent Gaussian Models?\nHow are they implemented in inlabru?"
  },
  {
    "objectID": "slides/slides_2.html#what-is-inla-what-is-inlabru",
    "href": "slides/slides_2.html#what-is-inla-what-is-inlabru",
    "title": "Lecture 1",
    "section": "What is INLA? What is inlabru?",
    "text": "What is INLA? What is inlabru?\nThe short answer:\n\nINLA is a fast method to do Bayesian inference with latent Gaussian models and inlabru is an R-package that implements this method with a flexible and simple interface.\n\nThe (much) longer answer:\n\nRue, H., Martino, S. and Chopin, N. (2009), Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71: 319-392.\nVan Niekerk, J., Krainski, E., Rustand, D., & Rue, H. (2023). A new avenue for Bayesian inference with INLA. Computational Statistics & Data Analysis, 181, 107692.\nLindgren, F., Bachl, F., Illian, J., Suen, M. H., Rue, H., & Seaton, A. E. (2024). inlabru: software for fitting latent Gaussian models with non-linear predictors. arXiv preprint arXiv:2407.00791.\nLindgren, F., Bolin, D., & Rue, H. (2022). The SPDE approach for Gaussian and non-Gaussian fields: 10 years and still running. Spatial Statistics, 50, 100599."
  },
  {
    "objectID": "slides/slides_2.html#where",
    "href": "slides/slides_2.html#where",
    "title": "Lecture 1",
    "section": "Where?",
    "text": "Where?\n\n\n\n Website-tutorials\n\n\ninlabru https://inlabru-org.github.io/inlabru/\nR-INLA https://www.r-inla.org/home\n\n\n\n\n\n\n Discussion forums\n\n\ninlabru https://github.com/inlabru-org/inlabru/discussions\nR-INLA https://groups.google.com/g/r-inla-discussion-group\n\n\n\n\n\n\n Books\n\n\n\nBlangiardo, M., & Cameletti, M. (2015). Spatial and spatio-temporal Bayesian models with R-INLA. John Wiley & Sons.\nG√≥mez-Rubio, V. (2020). Bayesian inference with INLA. Chapman and Hall/CRC.\nKrainski, E., G√≥mez-Rubio, V., Bakka, H., Lenzi, A., Castro-Camilo, D., Simpson, D., ‚Ä¶ & Rue, H. (2018). Advanced spatial modeling with stochastic partial differential equations using R and INLA. Chapman and Hall/CRC.\nWang, X., Yue, Y. R., & Faraway, J. J. (2018). Bayesian regression modeling with INLA. Chapman and Hall/CRC."
  },
  {
    "objectID": "slides/slides_2.html#so-why-should-you-use-inlabru",
    "href": "slides/slides_2.html#so-why-should-you-use-inlabru",
    "title": "Lecture 1",
    "section": "So‚Ä¶ Why should you use inlabru?",
    "text": "So‚Ä¶ Why should you use inlabru?\n\n\nWhat type of problems can we solve?\nWhat type of models can we use?\nWhen can we use it?"
  },
  {
    "objectID": "slides/slides_2.html#so-why-should-you-use-inlabru-1",
    "href": "slides/slides_2.html#so-why-should-you-use-inlabru-1",
    "title": "Lecture 1",
    "section": "So‚Ä¶ Why should you use inlabru?",
    "text": "So‚Ä¶ Why should you use inlabru?\n\nWhat type of problems can we solve?\nWhat type of models can we use?\nWhen can we use it?\n\n\nTo give proper answers to these questions, we need to start at the very beginning .."
  },
  {
    "objectID": "slides/slides_2.html#the-core",
    "href": "slides/slides_2.html#the-core",
    "title": "Lecture 1",
    "section": "The core",
    "text": "The core\n\nWe have observed something."
  },
  {
    "objectID": "slides/slides_2.html#the-core-1",
    "href": "slides/slides_2.html#the-core-1",
    "title": "Lecture 1",
    "section": "The core",
    "text": "The core\n\nWe have observed something.\nWe have questions."
  },
  {
    "objectID": "slides/slides_2.html#the-core-2",
    "href": "slides/slides_2.html#the-core-2",
    "title": "Lecture 1",
    "section": "The core",
    "text": "The core\n\nWe have observed something.\nWe have questions.\nWe want answers!"
  },
  {
    "objectID": "slides/slides_2.html#how-do-we-find-answers",
    "href": "slides/slides_2.html#how-do-we-find-answers",
    "title": "Lecture 1",
    "section": "How do we find answers?",
    "text": "How do we find answers?\nWe need to make choices:\n\n\n\nBayesian or frequentist?\nHow do we model the data?\nHow do we compute the answer?"
  },
  {
    "objectID": "slides/slides_2.html#how-do-we-find-answers-1",
    "href": "slides/slides_2.html#how-do-we-find-answers-1",
    "title": "Lecture 1",
    "section": "How do we find answers?",
    "text": "How do we find answers?\nWe need to make choices:\n\nBayesian or frequentist?\nHow do we model the data?\nHow do we compute the answer?\n\nThese questions are not independent."
  },
  {
    "objectID": "slides/slides_2.html#bayesian-or-frequentist",
    "href": "slides/slides_2.html#bayesian-or-frequentist",
    "title": "Lecture 1",
    "section": "Bayesian or frequentist?",
    "text": "Bayesian or frequentist?\nIn this course we embrace the Bayesian perspective\n\nThere are no ‚Äútrue but unknown‚Äù parameters !"
  },
  {
    "objectID": "slides/slides_2.html#bayesian-or-frequentist-1",
    "href": "slides/slides_2.html#bayesian-or-frequentist-1",
    "title": "Lecture 1",
    "section": "Bayesian or frequentist?",
    "text": "Bayesian or frequentist?\nIn this course we embrace the Bayesian perspective\n\nThere are no ‚Äútrue but unknown‚Äù parameters !\nEvery parameter is described by a probability distribution!"
  },
  {
    "objectID": "slides/slides_2.html#bayesian-or-frequentist-2",
    "href": "slides/slides_2.html#bayesian-or-frequentist-2",
    "title": "Lecture 1",
    "section": "Bayesian or frequentist?",
    "text": "Bayesian or frequentist?\nIn this course we embrace the Bayesian perspective\n\nThere are no ‚Äútrue but unknown‚Äù parameters !\nEvery parameter is described by a probability distribution!\nEvidence from the data is used to update the belief we had before observing the data!"
  },
  {
    "objectID": "slides/slides_2.html#some-more-details-i",
    "href": "slides/slides_2.html#some-more-details-i",
    "title": "Lecture 1",
    "section": "Some more details I",
    "text": "Some more details I\nWe define linear predictor the mean (or a function of the mean) of our observations given the model components.\n\nIn this case \\(E(y_i|\\beta_0, \\beta_i) = \\eta_i = \\beta_0 + \\beta_1 x_i\\)"
  },
  {
    "objectID": "slides/slides_2.html#some-more-details-i-1",
    "href": "slides/slides_2.html#some-more-details-i-1",
    "title": "Lecture 1",
    "section": "Some more details I",
    "text": "Some more details I\nWe define linear predictor the mean (or a function of the mean) of our observations given the model components.\n\nIn this case \\(E(y_i|\\beta_0, \\beta_i) =\\eta_i =  \\color{red}{\\boxed{\\beta_0}} +  \\color{red}{\\boxed{\\beta_1 x_i}}\\)\nThis model as two components !"
  },
  {
    "objectID": "slides/slides_2.html#some-more-details-ii",
    "href": "slides/slides_2.html#some-more-details-ii",
    "title": "Lecture 1",
    "section": "Some more details II",
    "text": "Some more details II\nGiven the linear predictor \\(\\eta\\) the observations a independent on each other!\n\nThis means that all dependencies in the observations are accounted for by the components!"
  },
  {
    "objectID": "slides/slides_2.html#some-more-details-ii-1",
    "href": "slides/slides_2.html#some-more-details-ii-1",
    "title": "Lecture 1",
    "section": "Some more details II",
    "text": "Some more details II\nGiven the linear predictor \\(\\eta\\) the observations a independent on each other!\n\nThe observation model (likelihood) can be written as: \\[\n\\pi(\\mathbf{y}|\\eta,\\sigma^2) = \\prod_{i = 1}^n\\pi(y_i|\\eta_i,\\sigma^2)\n\\]"
  },
  {
    "objectID": "slides/slides_2.html#lets-formalize-this-a-bit",
    "href": "slides/slides_2.html#lets-formalize-this-a-bit",
    "title": "Lecture 1",
    "section": "Let‚Äôs formalize this a bit‚Ä¶",
    "text": "Let‚Äôs formalize this a bit‚Ä¶\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\\\\nE(y_i|\\eta_i, \\sigma^2) & = \\eta_i\n\\end{aligned}\n\\] Note: We assume that, given the linear predictor \\(\\eta\\), the data are independent on each other! Data dependence is expressed through the components if the linear predictor."
  },
  {
    "objectID": "slides/slides_2.html#lets-formalize-this-a-bit-1",
    "href": "slides/slides_2.html#lets-formalize-this-a-bit-1",
    "title": "Lecture 1",
    "section": "Let‚Äôs formalize this a bit‚Ä¶",
    "text": "Let‚Äôs formalize this a bit‚Ä¶\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\(y_i|\\eta_i,\\sigma^2\\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\)\nA model for the linear predictor \\[\nE(y_i|\\eta_i,\\sigma^2) = \\eta_i = \\beta_0 + \\beta_1x_i\n\\]"
  },
  {
    "objectID": "slides/slides_2.html#lets-formalize-this-a-bit-2",
    "href": "slides/slides_2.html#lets-formalize-this-a-bit-2",
    "title": "Lecture 1",
    "section": "Let‚Äôs formalize this a bit‚Ä¶",
    "text": "Let‚Äôs formalize this a bit‚Ä¶\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\(y_i|\\eta_i,\\sigma^2\\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\)\nA model for the linear predictor\n\n\\[\nE(y_i|\\eta_i,\\sigma^2) = \\eta_i = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_1x_i} }\n\\]\nNote 1: These are the components of our model! These explain the dependence structure of the data."
  },
  {
    "objectID": "slides/slides_2.html#lets-formalize-this-a-bit-3",
    "href": "slides/slides_2.html#lets-formalize-this-a-bit-3",
    "title": "Lecture 1",
    "section": "Let‚Äôs formalize this a bit‚Ä¶",
    "text": "Let‚Äôs formalize this a bit‚Ä¶\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\(y_i|\\eta_i,\\sigma^2\\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\)\nA model for the linear predictor \\(\\eta_i = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_1x_i} }\\)\nA prior for the model components \\(\\textbf{u}\\) \\[\n\\mathbf{u} = \\{\\beta_0, \\beta_1\\}\\sim\\mathcal{N}(0,\\mathbf{Q}^{-1})\n\\] Note: These have always a Gaussian prior and are use to explain the dependencies among data!"
  },
  {
    "objectID": "slides/slides_2.html#lets-formalize-this-a-bit-4",
    "href": "slides/slides_2.html#lets-formalize-this-a-bit-4",
    "title": "Lecture 1",
    "section": "Let‚Äôs formalize this a bit‚Ä¶",
    "text": "Let‚Äôs formalize this a bit‚Ä¶\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\(y_i|\\eta_i,\\sigma^2\\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\)\nA model for the linear predictor \\(\\eta_i = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_1x_i} }\\)\nA prior for the model components \\(\\mathbf{u} = \\{\\beta_0, \\beta_1\\}\\sim\\mathcal{N}(0,\\mathbf{Q}^{-1})\\)\nA prior for the non-gaussian parameters \\(\\theta\\) \\[\n\\theta = \\sigma^2\n\\]"
  },
  {
    "objectID": "slides/slides_2.html#latent-gaussian-models-lgm",
    "href": "slides/slides_2.html#latent-gaussian-models-lgm",
    "title": "Lecture 1",
    "section": "Latent Gaussian Models (LGM)",
    "text": "Latent Gaussian Models (LGM)\n\n\n\nThe observation model: \\[\n\\pi(\\mathbf{y}|\\eta,\\theta) = \\prod_{i=1}^{n}\\pi(y_i|\\eta_i,\\theta)\n\\]\nLinear predictor \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\)\nLatent Gaussian field \\(\\pi(\\mathbf{u}|\\theta)\\)\nThe hyperparameters: \\(\\pi(\\theta)\\)\n\n\n\nStage 1 The data generating process"
  },
  {
    "objectID": "slides/slides_2.html#latent-gaussian-models-lgm-1",
    "href": "slides/slides_2.html#latent-gaussian-models-lgm-1",
    "title": "Lecture 1",
    "section": "Latent Gaussian Models (LGM)",
    "text": "Latent Gaussian Models (LGM)\n\n\n\nThe observation model: \\[\n\\pi(\\mathbf{y}|\\eta,\\theta) = \\prod_{i=1}^{n}\\pi(y_i|\\eta_i,\\theta)\n\\]\nLinear predictor \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\)\nLatent Gaussian field \\(\\pi(\\mathbf{u}|\\theta)\\)\nThe hyperparameters: \\(\\pi(\\theta)\\)\n\n\n\nStage 1 The data generating process\nStage 2 The dependence structure"
  },
  {
    "objectID": "slides/slides_2.html#latent-gaussian-models-lgm-2",
    "href": "slides/slides_2.html#latent-gaussian-models-lgm-2",
    "title": "Lecture 1",
    "section": "Latent Gaussian Models (LGM)",
    "text": "Latent Gaussian Models (LGM)\n\n\n\nThe observation model: \\[\n\\pi(\\mathbf{y}|\\eta,\\theta) = \\prod_{i=1}^{n}\\pi(y_i|\\eta_i,\\theta)\n\\]\nLinear predictor \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\)\nLatent Gaussian field \\(\\pi(\\mathbf{u}|\\theta)\\)\nThe hyperparameters: \\(\\pi(\\theta)\\)\n\n\n\nStage 1 The data generating process\nStage 2 The dependence structure\nStage 3 The hyperparameters"
  },
  {
    "objectID": "slides/slides_2.html#latent-gaussian-models-lgm-3",
    "href": "slides/slides_2.html#latent-gaussian-models-lgm-3",
    "title": "Lecture 1",
    "section": "Latent Gaussian Models (LGM)",
    "text": "Latent Gaussian Models (LGM)\n\n\n\n\nThe observation model: \\[\n\\pi(\\mathbf{y}|\\eta,\\theta) = \\prod_{i=1}^{n}\\pi(y_i|\\eta_i,\\theta)\n\\]\nLinear predictor \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\)\nLatent Gaussian field \\(\\pi(\\mathbf{u}|\\theta)\\)\nThe hyperparameters: \\(\\pi(\\theta)\\)\n\n\n\n\n\nStage 1 The data generating process\nStage 2 The dependence structure\nStage 3 The hyperparameters\n\n\n\n\nQ: What are we interested in?"
  },
  {
    "objectID": "slides/slides_2.html#the-posterior-distribution",
    "href": "slides/slides_2.html#the-posterior-distribution",
    "title": "Lecture 1",
    "section": "The posterior distribution",
    "text": "The posterior distribution\n\n\n\n\n\n\n\nposterior\n\n\n\nA\n\nPrior\n belief\n\n\n\nC\n\nBayes Theorem\n &\n Bayesian Computations\n\n\n\nA-&gt;C\n\n\n\n\n\nB\n\nObservation\n model\n\n\n\nB-&gt;C\n\n\n\n\n\nD\n\nPosterior\n distribution\n\n\n\nC-&gt;D\n\n\n\n\n\n\n\n\n\n\n\\[\n\\color{red}{\\pi(\\mathbf{u},\\theta|\\mathbf{y})}\\propto \\color{blue}{\\pi(\\mathbf{y}|\\mathbf{u},\\theta)}\\color{green}{\\pi(\\mathbf{u}|\\theta)\\pi(\\theta)}\n\\]"
  },
  {
    "objectID": "slides/slides_2.html#the-posterior-distribution-1",
    "href": "slides/slides_2.html#the-posterior-distribution-1",
    "title": "Lecture 1",
    "section": "The posterior distribution",
    "text": "The posterior distribution\n\n\n\n\n\n\n\nposterior\n\n\n\nC\n\nBayes Theorem\n &\n Bayesian Computations\n\n\n\nD\n\nPosterior\n distribution\n\n\n\nC-&gt;D\n\n\n\n\n\nE\n\nBayesian Computation are hard!!\n Here is where\n INLA\n comes in!!!\n\n\n\nE-&gt;C\n\n\n\n\n\nA\n\nPrior\n belief\n\n\n\nA-&gt;C\n\n\n\n\n\nB\n\nObservation\n model\n\n\n\nB-&gt;C"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression",
    "href": "slides/slides_2.html#inlabru-for-linear-regression",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\beta_0 + \\beta_i x_i\n\\end{aligned}\n\\]\n\n\nlibrary(inlabru)\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression-1",
    "href": "slides/slides_2.html#inlabru-for-linear-regression-1",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_i x_i}}\n\\end{aligned}\n\\]\n\n\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression-2",
    "href": "slides/slides_2.html#inlabru-for-linear-regression-2",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\color{red}{\\boxed{\\beta_0 + \\beta_i x_i}}\n\\end{aligned}\n\\]\n\n\nlibrary(inlabru)\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression-3",
    "href": "slides/slides_2.html#inlabru-for-linear-regression-3",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\n\\color{red}{\\boxed{y_i|\\eta_i, \\sigma^2}} & \\color{red}{\\boxed{\\sim \\mathcal{N}(\\eta_i,\\sigma^2)}}\\\\\n\\eta_i & = \\beta_0 + \\beta_i x_i\n\\end{aligned}\n\\]\n\n\nlibrary(inlabru)\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression-4",
    "href": "slides/slides_2.html#inlabru-for-linear-regression-4",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\beta_0 + \\beta_i x_i\n\\end{aligned}\n\\]\n\n\nlibrary(inlabru)\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression-5",
    "href": "slides/slides_2.html#inlabru-for-linear-regression-5",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression"
  },
  {
    "objectID": "slides/slides_2.html#real-datasets-are-more-complicated",
    "href": "slides/slides_2.html#real-datasets-are-more-complicated",
    "title": "Lecture 1",
    "section": "Real datasets are more complicated!",
    "text": "Real datasets are more complicated!\nData can have several dependence structures: temporal, spatial,‚Ä¶\nUsing a Bayesian framework:\n\nBuild (hierarchical) models to account for potentially complicated dependency structures in the data.\nAttribute uncertainty to model parameters and latent variables using priors.\n\nTwo main challenges:\n\nNeed computationally efficient methods to calculate posteriors (this is where INLA helps!).\nSelect priors in a sensible way (we‚Äôll talk about this)"
  },
  {
    "objectID": "slides/slides_2.html#the-good-news",
    "href": "slides/slides_2.html#the-good-news",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! üòÉ\n\nStage 1: What is the distribution of the responses?\nStage 2: What are the model components? and what is their distribution?\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?"
  },
  {
    "objectID": "slides/slides_2.html#the-good-news-1",
    "href": "slides/slides_2.html#the-good-news-1",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! üòÉ\n\nStage 1: What is the distribution of the responses?\n\nGaussian response? (temperature, rainfall, fish weight ‚Ä¶)\nCount data? (people infected with a disease in each area)\nPoint pattern? (locations of trees in a forest)\nBinary data? (yes/no response, binary image)\nSurvival data? (recovery time, time to death)\n‚Ä¶ (many more examples!!)\n\nStage 2: What are the model components? and what is their distribution?\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?"
  },
  {
    "objectID": "slides/slides_2.html#the-good-news-2",
    "href": "slides/slides_2.html#the-good-news-2",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! üòÉ\n\nStage 1: What is the distribution of the responses?\n\nWe assume data to be conditionally independent given the model components and some hyperparameters\nThis means that all dependencies in data are explained in Stage\n\n\n\n\nStage 2: What are the model components? and what is their distribution?\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?"
  },
  {
    "objectID": "slides/slides_2.html#the-good-news-3",
    "href": "slides/slides_2.html#the-good-news-3",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! üòÉ\n\nStage 1: What is the distribution of the responses?\nStage 2: What are the model components? and what is their distribution?\n\nHere we can have:\n\nFixed effects for covariates\nUnstructured random effects (individual effects, group effects)\nStructured random effects (AR(1), regional effects, )\n‚Ä¶\n\nThese are linked to the responses in the likelihood through linear predictors.\n\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?"
  },
  {
    "objectID": "slides/slides_2.html#the-good-news-4",
    "href": "slides/slides_2.html#the-good-news-4",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! üòÉ\n\nStage 1: What is the distribution of the responses?\nStage 2: What are the model components? and what is their distribution?\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?\nThe likelihood and the latent model typically have hyperparameters that control their behavior.\nThey can include:\n\nVariance of observation noise\nDispersion parameter in the negative binomial model\nVariance of unstructured effects\n‚Ä¶"
  },
  {
    "objectID": "slides/slides_2.html#the-second-good-news",
    "href": "slides/slides_2.html#the-second-good-news",
    "title": "Lecture 1",
    "section": "The second good news!",
    "text": "The second good news!\nNo matter how complicated is your model, the inlabru workflow is always the same üòÉ\n\n# Define model components\ncomps &lt;- component_1(...) + \n  component_2(...) + ...\n\n# Define the model predictor\npred &lt;- linear_function(component_1, \n                            component_2, ...)\n\n# Build the observation model\nlik &lt;- bru_obs(formula = pred,\n               family = ... ,\n               data = ... ,\n                ...)\n\n# Fit the model\nfit &lt;- bru(comps, lik, ...)"
  },
  {
    "objectID": "slides/slides_2.html#the-second-good-news-1",
    "href": "slides/slides_2.html#the-second-good-news-1",
    "title": "Lecture 1",
    "section": "The second good news!",
    "text": "The second good news!\nNo matter how complicated is your model, the inlabru workflow is always the same üòÉ\n\n# Define model components\ncomps &lt;- component_1(...) + \n  component_2(...) + ...\n\n# Define the model predictor\npred &lt;- linear_function(component_1, \n                            component_2, ...)\n\n# Build the observation model\nlik &lt;- bru_obs(formula = pred,\n               family = ... ,\n               data = ... ,\n                ...)\n\n# Fit the model\nfit &lt;- bru(comps, lik, ...)\n\nNOTE we will see later that this function can also be non-linear‚Ä¶.üòÅ"
  },
  {
    "objectID": "slides/slides_2.html#the-tokyo-rainfall-data",
    "href": "slides/slides_2.html#the-tokyo-rainfall-data",
    "title": "Lecture 1",
    "section": "The Tokyo rainfall data",
    "text": "The Tokyo rainfall data\nOne example with time series: Rainfall over 1 mm in the Tokyo area for each calendar day during two years (1983-84) are registered."
  },
  {
    "objectID": "slides/slides_2.html#the-model",
    "href": "slides/slides_2.html#the-model",
    "title": "Lecture 1",
    "section": "The model",
    "text": "The model\nStage 1 The observation model\n\\[\ny_t|\\eta_t\\sim\\text{Bin}(n_t, p_t),\\qquad \\eta_t = \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\] \\[\nn_t = \\left\\{\n\\begin{array}{lr}\n1, & \\text{for}\\; 29\\; \\text{February}\\\\\n2, & \\text{other days}\n\\end{array}\\right.\n\\] \\[\ny_t =\n\\begin{cases}\n\\{0,1\\}, & \\text{for}\\; 29\\; \\text{February}\\\\\n\\{0,1,2\\}, & \\text{other days}\n\\end{cases}\n\\]\n\nthe likelihood has no hyperparameters"
  },
  {
    "objectID": "slides/slides_2.html#the-model-1",
    "href": "slides/slides_2.html#the-model-1",
    "title": "Lecture 1",
    "section": "The model",
    "text": "The model\nStage 1 The observation model\n\\[\ny_t|\\eta_t\\sim\\text{Bin}(n_t, p_t),\\qquad \\eta_t = \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\]\nStage 2 The latent field \\[\n\\eta_t = \\beta_0 + f(\\text{time}_t)\n\\]\n\nprobability of rain depends on on the day of the year \\(t\\)\n\\(\\beta_0\\) is an intercept\n\\(f(\\text{time}_t)\\) is a RW2 model (this is just a smoother). The smoothness is controlled by a hyperparameter \\(\\tau_f\\)"
  },
  {
    "objectID": "slides/slides_2.html#the-model-2",
    "href": "slides/slides_2.html#the-model-2",
    "title": "Lecture 1",
    "section": "The model",
    "text": "The model\nStage 1 The observation model\n\\[\ny_t|\\eta_t\\sim\\text{Bin}(n_t, p_t),\\qquad \\eta_t = \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\]\nStage 2 The latent field \\[\n\\eta_t = \\beta_0 + f(\\text{time}_t)\n\\]\nStage 3 The hyperparameters\n\nThe structured time effect is controlled by one parameter \\(\\tau_f\\).\nWe assign a prior to \\(\\tau_f\\) to finalize the model."
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-time-series",
    "href": "slides/slides_2.html#inlabru-for-time-series",
    "title": "Lecture 1",
    "section": "inlabru for time series",
    "text": "inlabru for time series\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim \\text{Binomial}(n_t,p_t)\\\\\n\\text{logit}(p_t) = \\eta_i & = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{f(\\text{time}_t)}}\n\\end{aligned}\n\\]\n\n\nTokyo[1:3,]\n\n  y n time\n1 0 2    1\n2 0 2    2\n3 1 2    3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-time-series-1",
    "href": "slides/slides_2.html#inlabru-for-time-series-1",
    "title": "Lecture 1",
    "section": "inlabru for time series",
    "text": "inlabru for time series\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim \\text{Binomial}(n_t,p_t)\\\\\n\\text{logit}(p_t) = \\color{red}{\\boxed{\\eta_i}} & = \\color{red}{\\boxed{\\beta_0 + f(\\text{time}_t)}}\n\\end{aligned}\n\\]\n\n\nTokyo[1:3,]\n\n  y n time\n1 0 2    1\n2 0 2    2\n3 1 2    3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-time-series-2",
    "href": "slides/slides_2.html#inlabru-for-time-series-2",
    "title": "Lecture 1",
    "section": "inlabru for time series",
    "text": "inlabru for time series\n\n\nThe Model\n\\[\n\\begin{aligned}\n\\color{red}{\\boxed{y_t|\\eta_t}} & \\color{red}{\\boxed{\\sim \\text{Binomial}(n_t,p_t)}}\\\\\n\\text{logit}(p_t) = \\eta_i & = \\beta_0 + f(\\text{time}_t)\n\\end{aligned}\n\\]\n\n\nTokyo[1:3,]\n\n  y n time\n1 0 2    1\n2 0 2    2\n3 1 2    3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-time-series-3",
    "href": "slides/slides_2.html#inlabru-for-time-series-3",
    "title": "Lecture 1",
    "section": "inlabru for time series",
    "text": "inlabru for time series\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim \\text{Binomial}(n_t,p_t)\\\\\n\\text{logit}(p_t) = \\eta_i & = \\beta_0 + f(\\text{time}_t)\n\\end{aligned}\n\\]\n\n\nTokyo[1:3,]\n\n  y n time\n1 0 2    1\n2 0 2    2\n3 1 2    3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#example-disease-mapping",
    "href": "slides/slides_2.html#example-disease-mapping",
    "title": "Lecture 1",
    "section": "Example: disease mapping",
    "text": "Example: disease mapping\nWe observed larynx cancer mortality counts for males in 544 district of Germany from 1986 to 1990 and want to make a model.\n\n\n\n\\(y_i\\): The count at location \\(i\\).\n\\(E_i\\): An offset; expected number of cases in district \\(i\\).\n\\(c_i\\): A covariate (level of smoking consumption) at \\(i\\)\n\\(\\boldsymbol{s}_i\\): spatial location \\(i\\) ."
  },
  {
    "objectID": "slides/slides_2.html#bayesian-disease-mapping",
    "href": "slides/slides_2.html#bayesian-disease-mapping",
    "title": "Lecture 1",
    "section": "Bayesian disease mapping",
    "text": "Bayesian disease mapping\n\n\n\nStage 1: We assume the responses are Poisson distributed: \\[          \ny_i \\mid \\eta_i \\sim \\text{Poisson}(E_i\\exp(\\eta_i)))\n\\]\n\n\n\n\n\n\nStage 2: \\(\\eta_i\\) is a linear function of three components: an intercept, a covariate \\(c_i\\), a spatially structured effect \\(\\omega\\) likelihood by \\[\n\\eta_i = \\beta_0 + \\beta_1\\ c_i + \\omega_i\n\\]\n\n\n\n\n\n\nStage 3:\n\n\\(\\tau_{\\omega}\\): Precisions parameter for the random effects\n\n\n\n\n\nThe latent field is \\(\\boldsymbol{u} = (\\beta_0, \\beta_1, \\omega_1, \\omega_2,\\ldots, \\omega_n)\\), the hyperparameters are \\(\\boldsymbol{\\theta} = (\\tau_{\\omega})\\), and must be given a prior."
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-disease-mapping",
    "href": "slides/slides_2.html#inlabru-for-disease-mapping",
    "title": "Lecture 1",
    "section": "inlabru for disease mapping",
    "text": "inlabru for disease mapping\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_i|\\eta_t & \\sim \\text{Poisson}(E_i\\lambda_i)\\\\\n\\text{log}(\\lambda_i) = \\eta_i & = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_1\\ c_i}} + \\color{red}{\\boxed{\\omega_i}}\n\\end{aligned}\n\\]\n\n\ng = system.file(\"demodata/germany.graph\",\n                package=\"INLA\")\nGermany[1:3,]\n\n  region         E  Y  x region.struct\n1      1  7.965008  8 56             1\n2      2 22.836219 22 65             2\n3      3 22.094716 19 50             3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + beta1(x, model = \"linear\") +\n  space(region, model = \"besag\", graph = g)\n\n# define model predictor\neta = Y ~ beta0 + beta1 + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"poisson\",\n              E = E,\n              data = Germany)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-disease-mapping-1",
    "href": "slides/slides_2.html#inlabru-for-disease-mapping-1",
    "title": "Lecture 1",
    "section": "inlabru for disease mapping",
    "text": "inlabru for disease mapping\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_i|\\eta_t & \\sim \\text{Poisson}(E_i\\lambda_i)\\\\\n\\text{log}(\\lambda_i) = \\color{red}{\\boxed{\\eta_i}} & = \\color{red}{\\boxed{\\beta_0 + \\beta_1\\ c_i + \\omega_i}}\n\\end{aligned}\n\\]\n\n\ng = system.file(\"demodata/germany.graph\",\n                package=\"INLA\")\nGermany[1:3,]\n\n  region         E  Y  x region.struct\n1      1  7.965008  8 56             1\n2      2 22.836219 22 65             2\n3      3 22.094716 19 50             3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + beta1(x, model = \"linear\") +\n  space(region, model = \"bym2\", graph = g)\n\n# define model predictor\neta = Y ~ beta0 + beta1 + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"poisson\",\n              E = E,\n              data = Germany)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-disease-mapping-2",
    "href": "slides/slides_2.html#inlabru-for-disease-mapping-2",
    "title": "Lecture 1",
    "section": "inlabru for disease mapping",
    "text": "inlabru for disease mapping\n\n\nThe Model\n\\[\n\\begin{aligned}\n\\color{red}{\\boxed{y_i|\\eta_t}} & \\color{red}{\\boxed{\\sim \\text{Poisson}(E_i\\lambda_i)}}\\\\\n\\text{log}(\\lambda_i) = \\eta_i & = \\beta_0 + \\beta_1\\ c_i + \\omega_i\n\\end{aligned}\n\\]\n\n\ng = system.file(\"demodata/germany.graph\",\n                package=\"INLA\")\nGermany[1:3,]\n\n  region         E  Y  x region.struct\n1      1  7.965008  8 56             1\n2      2 22.836219 22 65             2\n3      3 22.094716 19 50             3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + beta1(x, model = \"linear\") +\n  space(region, model = \"bym2\", graph = g)\n\n# define model predictor\neta = Y ~ beta0 + beta1 + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"poisson\",\n              E = E,\n              data = Germany)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-disease-mapping-3",
    "href": "slides/slides_2.html#inlabru-for-disease-mapping-3",
    "title": "Lecture 1",
    "section": "inlabru for disease mapping",
    "text": "inlabru for disease mapping"
  },
  {
    "objectID": "slides/slides_2.html#bayesian-geostatistics",
    "href": "slides/slides_2.html#bayesian-geostatistics",
    "title": "Lecture 1",
    "section": "Bayesian Geostatistics",
    "text": "Bayesian Geostatistics\nEncounter probability of Pacific Cod (Gadus macrocephalus) from a trawl survey.\n\n\n\n\n\n\n\n\\(y(s)\\) Presence of absence in location \\(s\\)"
  },
  {
    "objectID": "slides/slides_2.html#bayesian-geostatistics-1",
    "href": "slides/slides_2.html#bayesian-geostatistics-1",
    "title": "Lecture 1",
    "section": "Bayesian Geostatistics",
    "text": "Bayesian Geostatistics\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim\\text{Binom}(1, p(s))\n\\]\nStage 2 Latent field model \\[\n\\eta(s) = \\text{logit}(p(s)) = \\beta_0 + f( x(s)) + \\omega(s)\n\\]\nStage 3 Hyperparameters"
  },
  {
    "objectID": "slides/slides_2.html#bayesian-geostatistics-2",
    "href": "slides/slides_2.html#bayesian-geostatistics-2",
    "title": "Lecture 1",
    "section": "Bayesian Geostatistics",
    "text": "Bayesian Geostatistics\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim\\text{Binom}(1, p(s))\n\\]\nStage 2 Latent field model \\[\n\\eta(s) = \\text{logit}(p(s)) = \\beta_0 + f( x(s)) + \\omega(s)\n\\]\n\nA global intercept \\(\\beta_0\\)\nA smooth effect of covariate \\(x(s)\\) (depth)\nA Gaussian field \\(\\omega(s)\\) (will discuss this later..)\n\nStage 3 Hyperparameters"
  },
  {
    "objectID": "slides/slides_2.html#bayesian-geostatistics-3",
    "href": "slides/slides_2.html#bayesian-geostatistics-3",
    "title": "Lecture 1",
    "section": "Bayesian Geostatistics",
    "text": "Bayesian Geostatistics\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim\\text{Binom}(1, p(s))\n\\]\nStage 2 Latent field model \\[\n\\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\beta_1 x(s) + \\omega(s)\n\\]\nStage 3 Hyperparameters\n\nPrecision for the smooth function \\(f(\\cdot)\\)\nRange and sd in the Gaussian field \\(\\sigma_{\\omega}, \\tau_{\\omega}\\)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-geostatistics",
    "href": "slides/slides_2.html#inlabru-for-geostatistics",
    "title": "Lecture 1",
    "section": "inlabru for geostatistics",
    "text": "inlabru for geostatistics\n\n\nThe Model\n$$\n\\[\\begin{aligned}\ny(s)|\\eta(s) & \\sim\\text{Binom}(1, p(s))\\\\\n\\eta(s) &  = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{ f(x(s))}} + \\color{red}{\\boxed{ \\omega(s)}}\\\\\n\n\\end{aligned}\\]\n$$\n\n\ndf %&gt;% select(depth, present) %&gt;% print(n = 3)\n\nSimple feature collection with 2143 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 343.0617 ymin: 5635.893 xmax: 579.3681 ymax: 5839.019\nProjected CRS: +proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\n# A tibble: 2,143 √ó 3\n  depth present            geometry\n  &lt;dbl&gt;   &lt;dbl&gt;        &lt;POINT [km]&gt;\n1   201       1 (446.4752 5793.426)\n2   212       1 (446.4594 5800.136)\n3   220       0 (448.5987 5801.687)\n# ‚Ñπ 2,140 more rows\n\n\n\nThe code\n\n# define model component\ncmp = ~ -1 + Intercept(1) +  depth_smooth(log(depth), model='rw2') + \n  space(geometry, model = spde_model)\n\n# define model predictor\neta = present ~ Intercept + depth_smooth + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              data = df,\n              family = \"binomial\")\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-geostatistics-1",
    "href": "slides/slides_2.html#inlabru-for-geostatistics-1",
    "title": "Lecture 1",
    "section": "inlabru for geostatistics",
    "text": "inlabru for geostatistics\n\n\nThe Model\n$$\n\\[\\begin{aligned}\ny(s)|\\eta(s) & \\sim\\text{Binom}(1, p(s))\\\\\n\\color{red}{\\boxed{\\eta(s)}} &  = \\color{red}{\\boxed{\\beta_0 +  f(x(s)) +  \\omega(s)}}\\\\\n\n\\end{aligned}\\]\n$$\n\n\ndf %&gt;% select(depth, present) %&gt;% print(n = 3)\n\nSimple feature collection with 2143 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 343.0617 ymin: 5635.893 xmax: 579.3681 ymax: 5839.019\nProjected CRS: +proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\n# A tibble: 2,143 √ó 3\n  depth present            geometry\n  &lt;dbl&gt;   &lt;dbl&gt;        &lt;POINT [km]&gt;\n1   201       1 (446.4752 5793.426)\n2   212       1 (446.4594 5800.136)\n3   220       0 (448.5987 5801.687)\n# ‚Ñπ 2,140 more rows\n\n\n\nThe code\n\n# define model component\ncmp = ~ -1 + Intercept(1) +  depth_smooth(log(depth), model='rw2') + \n  space(geometry, model = spde_model)\n\n# define model predictor\neta = present ~ Intercept + depth_smooth + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              data = df,\n              family = \"binomial\")\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-geostatistics-2",
    "href": "slides/slides_2.html#inlabru-for-geostatistics-2",
    "title": "Lecture 1",
    "section": "inlabru for geostatistics",
    "text": "inlabru for geostatistics\n\n\nThe Model\n$$\n\\[\\begin{aligned}\n\\color{red}{\\boxed{y(s)|\\eta(s)}} & \\sim \\color{red}{\\boxed{\\text{Binom}(1, p(s))}}\\\\\n\\eta(s) &  = \\beta_0 +  f(x(s)) +  \\omega(s)\\\\\n\n\\end{aligned}\\]\n$$\n\n\ndf %&gt;% select(depth, present) %&gt;% print(n = 3)\n\nSimple feature collection with 2143 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 343.0617 ymin: 5635.893 xmax: 579.3681 ymax: 5839.019\nProjected CRS: +proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\n# A tibble: 2,143 √ó 3\n  depth present            geometry\n  &lt;dbl&gt;   &lt;dbl&gt;        &lt;POINT [km]&gt;\n1   201       1 (446.4752 5793.426)\n2   212       1 (446.4594 5800.136)\n3   220       0 (448.5987 5801.687)\n# ‚Ñπ 2,140 more rows\n\n\n\nThe code\n\n# define model component\ncmp = ~ -1 + Intercept(1) +  depth_smooth(log(depth), model='rw2') + \n  space(geometry, model = spde_model)\n\n# define model predictor\neta = present ~ Intercept + depth_smooth + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              data = df,\n              family = \"binomial\")\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-geostatistics-3",
    "href": "slides/slides_2.html#inlabru-for-geostatistics-3",
    "title": "Lecture 1",
    "section": "inlabru for geostatistics",
    "text": "inlabru for geostatistics"
  },
  {
    "objectID": "slides/slides_2.html#take-home-message",
    "href": "slides/slides_2.html#take-home-message",
    "title": "Lecture 1",
    "section": "Take home message!",
    "text": "Take home message!\n\nMany of the model you have used (and some you have never used but will learn about) are just special cases of the large class of Latent Gaussian models\ninlabru provides an efficient and unified way to fit all these models!"
  }
]