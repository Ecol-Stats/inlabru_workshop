{
  "hash": "34959be5158d2189bd944da19d86b7e9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Practical 8\"\nexecute: \n  freeze: true\nformat: \n  html:\n    mainfont: \"12\"\n  PrettyPDF-pdf:\n    keep-tex: true\n    number-sections: true\nembed-resources: true\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n<font size=\"5\"> **Aim of this practical:** </font>\n\nIn this practical we are going to look at some model comparison and validation techniques.\n\n\n\n\n\n\n{{< downloadthis day5_practical_8.R dname=\"day5_practical_8.R\" label=\"Download Practical 8 R script\" icon=\"database-fill-down\" type=\"success\" >}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n# Distance Sampling\n\nIn this practical we will:\n\n-   Fit a spatial distance sampling model\n-   Estimate animal abundance\n-   Compare models that use different detection functions\n\nLibraries to load:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \nlibrary(sf)\n# load some libraries to generate nice map plots\nlibrary(scico)\nlibrary(mapview)\n```\n:::\n\n## The data\n\nIn the next exercise, we will explore data from a combination of several NOAA shipboard surveys conducted on pan-tropical spotted dolphins in the Gulf of Mexico. The data set is available in `inlabru` (originally obtianed from the `dsm` R package) and contains the following information:\n\n-   A total of 47 observations of groups of dolphins were detected. The group size was recorded, as well as the Beaufort sea state at the time of the observation.\n\n-   Transect width is 16 km, i.e. maximal detection distance 8 km (transect half-width 8 km).\n\nWe can load and visualize the data as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\nmexdolphin <- mexdolphin_sf\nmexdolphin$depth <- mexdolphin$depth %>% mutate(depth=scale(depth)%>%c())\nmapviewOptions(basemaps = c( \"OpenStreetMap.DE\"))\n\nmapview(mexdolphin$points,zcol=\"size\")+\n  mapview(mexdolphin$samplers)+\n mapview(mexdolphin$ppoly )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nfile:///C:/Users/admin/AppData/Local/Temp/Rtmp0u11j8/file6f88745b6a95/widget6f886684850.html screenshot completed\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](day5_practical_8_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n:::\n\n## The workflow\n\nTo model the density of spotted dolphins we take a thinned point process model of the form:\n\n$$\np(\\mathbf{y} | \\lambda)  \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) p(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) p(\\mathbf{s}_i)) \n$$ {#eq-thinned_pp}\n\nWhen fitting a distance sampling model we need to fulfill the following tasks:\n\n1.  Build the mesh\n\n2.  Define the SPDE representation of the spatial GF. This includes defining the priors for the range and sd of the spatial GF\n\n3.  Define the *components* of the linear predictor. This includes the spatial GF and all eventual covariates\n\n4.  Define the observation model using the `bru_obs()` function\n\n5.  Run the model using the `bru()` function\n\n###  Building the mesh\n\n\n\nThe first task is to build the mesh that covers the area of interest. For this purpose we use the function `fm_mesh_2d`. To do so, we need to define the area of interest. We can either use a predefined boundary or create a non convex hull surrounding the location of the specie sightseeings\n\n::: panel-tabset\n## non-covex hull\n\n::: {.cell}\n\n```{.r .cell-code}\nboundary0 = fm_nonconvex_hull(mexdolphin$points,convex = -0.1)\n\nmesh_0 = fm_mesh_2d(boundary = boundary0,\n                          max.edge = c(30, 150), # The largest allowed triangle edge length.\n                          cutoff = 15,\n                          crs = fm_crs(mexdolphin$points))\nggplot() + gg(mesh_0)\n```\n\n::: {.cell-output-display}\n![](day5_practical_8_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n## domain boundary\n\nThe `mexdolphin` object contains a predefined region of interest which can be accessed through `mexdolphin$ppoly`\n\n::: {.cell}\n\n```{.r .cell-code}\nmesh_1 = fm_mesh_2d(boundary = mexdolphin$ppoly,\n                    max.edge = c(30, 150),\n                    cutoff = 15,\n                    crs = fm_crs(mexdolphin$points))\nggplot() + gg(mesh_1)\n```\n\n::: {.cell-output-display}\n![](day5_practical_8_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n:::\n\nKey parameters in mesh construction include: `max.edge` for maximum triangle edge lengths, `offset` for inner and outer extensions (to prevent edge effects), and cutoff to avoid overly small triangles in clustered areas.\n\n::: callout-note\n**General guidelines for creating the mesh**\n\n1.  Create triangulation meshes with `fm_mesh_2d()`\n2.  Move undesired boundary effects away from the domain of interest by extending to a smooth external boundary\n3.  Use a coarser resolution in the extension to reduce computational cost (`max.edge=c(inner, outer)`)\n4.  Use a fine resolution (subject to available computational resources) for the domain of interest (inner correlation range) and filter out small input point clusters (0 \\< `cutoff` \\< inner)\n5.  Coastlines and similar can be added to the domain specification in `fm_mesh_2d()` through the `boundary` argument.\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nLook at the documentation for the `fm_mesh_2d` function typing\n\n::: {.cell}\n\n```{.r .cell-code}\n?fm_mesh_2d\n```\n:::\n\nplay around with the different options and create different meshes. You can compare these against a pre-computed mesh available by typing `plot(mexdolphin$mesh)`\n\nThe *rule of thumb* is that your mesh should be:\n\n-   fine enough to well represent the spatial variability of your process, but not too fine in order to avoid computation burden\n-   the triangles should be regular, avoid long and thin triangles.\n-   The mesh should contain a buffer around your area of interest (this is what is defined in the `offset` option) in order to avoid boundary artefact in the estimated variance.\n:::\n\n###  Define the SPDE representation of the spatial GF\n\nTo define the SPDE representation of the spatial GF we use the function `inla.spde2.pcmatern`. This takes as input the mesh we have defined and the PC-priors definition for $\\rho$ and $\\sigma$ (the range and the marginal standard deviation of the field).\n\nPC priors Gaussian Random field are defined in (Fuglstad et al. 2018). From a practical perspective for the range $\\rho$ you need to define two paramters $\\rho_0$ and $p_{\\rho}$ such that you believe it is reasonable that\n\n$$\nP(\\rho<\\rho_0)=p_{\\rho}\n$$\n\nwhile for the marginal variance $\\sigma$ you need to define two parameters $\\sigma_0$ and $p_{\\sigma}$ such that you believe it is reasonable that\n\n$$\nP(\\sigma>\\sigma_0)=p_{\\sigma}\n$$\n\n::: {.callout-tip icon=\"false\"}\n## {{< bi question-octagon color=#6dc83c >}} Question\n\nTake a look at the code below and select which of the following statements about the specified Matern PC priors are true.\n\n::: {.cell}\n\n```{.r .cell-code}\nspde_model <- inla.spde2.pcmatern(mexdolphin$mesh,\n  prior.sigma = c(2, 0.01),\n  prior.range = c(50, 0.01)\n)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n* (A) there is probability of 0.01 that the spatial range is greater or equal than 50  \n* (B) the probability that the spatial range is smaller than 50 is very small  \n* (C) the probability that the marginal standard deviation is smaller than 2 is very small  \n* (D) there is probability of 0.99 that the marginal standard deviation is less or equal than 2  \n\n\n:::\n\n###  Define the components of the linear predictor\n\nWe have now defined a mesh and a SPDE representation of the spatial GF. We now need to define the model components.\n\nFirst, we need to define the detection function. Here, we will define a half-normal detection probability function. This must take distance as its first argument and the linear predictor of the sigma parameter as its second:\n\n::: {.cell}\n\n```{.r .cell-code}\nhn <- function(distance, sigma) {\n  exp(-0.5 * (distance / sigma)^2)\n}\n```\n:::\n\nWe need to now separately define the components of the model including the SPDE model, the Intercept, the effect of depth and the detection function parameter `sigma`.\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp <- ~ space(main = geometry, model = spde_model) +\n  sigma(1,\n    prec.linear = 1,\n    marginal = bm_marginal(qexp, pexp, dexp, rate = 1 / 8)\n  ) +\n  Intercept(1)\n```\n:::\n\n::: callout-note\nTo control the prior distribution for the `sigma` parameter, we use a transformation mapper that converts a latent variable into an exponentially distributed variable with expectation 8 (this is a somewhat arbitrary value, but motivated by the maximum observation distance W)\n\nThe `marginal` argument in the `sigma` component specifies the transformation function taking N(0,1) to Exponential(1/8).\n:::\n\nThe formula, which describes how these components are combined to form the linear predictor \n\n$$\\log \\color{red}{\\tilde{\\lambda}(s)} = \\overbrace{\\log \\lambda (s)}^{\\beta_0 + \\xi(s)} + \\overbrace{\\log \\color{red}{g(d(s))}}^{-0.5~d(\\mathbf{s})^2\\sigma^{-2}}$$\n\n::: {.cell}\n\n```{.r .cell-code}\neta <- geometry + distance ~ space +\n  log(hn(distance, sigma)) +\n  Intercept + log(2) \n```\n:::\n\nHere, the `log(2)` offset in the predictor takes care of the two-sided detections\n\n\n###  Define the observation model\n\n`inlabru` has support for latent Gaussian Cox processes through the `cp` likelihood family. To fit a point process model recall that we need to approximate the integral in using a numerical integration scheme as:\n\n$$\n\\approx\\exp\\left(-\\sum_{k=1}^{N_k}w_k\\lambda(s_k)\\right)\\prod_{i=1}^n \\lambda(\\mathbf{s}_i)\n$$ \n\nThus, we first create our integration scheme using the `fm_int` function by specifying integration domains for the spatial and distance dimensions.\n\nHere we use the same points to define the SPDE approximation and to approximate the integral in @eq-thinned_pp, so that the integration weight and SPDE weights are consistent with each other. We also need to explicitly integrate over the distance dimension so we use the `fm_mesh_1d()` to create mesh over the samplers (which are the transect lines in this dataset, so we need to tell `inlabru` about the strip half-width).\n\n::: {.cell}\n\n```{.r .cell-code}\n# build integration scheme\ndistance_domain <-  fm_mesh_1d(seq(0, 8,\n                              length.out = 30))\nips = fm_int(list(geometry = mexdolphin$mesh,\n                  distance = distance_domain),\n             samplers = mexdolphin$samplers)\n```\n:::\n\n\nNow, we just need to supply the `sf` object as our data and the integration scheme `ips`:\n\n::: {.cell}\n\n```{.r .cell-code}\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = mexdolphin$points,\n              ips = ips)\n```\n:::\n\nThen we fit the model, passing both the components and the observaional model\n\n::: {.cell}\n\n```{.r .cell-code}\nfit = bru(cmp, lik)\n```\n:::\n\n\n::: callout-note\n`inlabru` supports a shortcut for defining the integration points using the `domain` and `samplers` argument of `bru_obs()`. This `domain` argument expects a list of named domains with inputs that are then internally passed to `fm_int()` to build the integration scheme. The `samplers` argument is used to define subsets of the domain over which the integral should be computed. An equivalent way to define the same model as above is:\n\n::: {.cell}\n\n```{.r .cell-code}\nlik = bru_obs(formula = eta, \n              data = mexdolphin$points, \n              family = \"cp\",\n              domain = list(\n                geometry = mesh,\n                distance = fm_mesh_1d(seq(0, 8, length.out = 30))),\n              samplers = mexdolphin$samplers)\n```\n:::\n:::\n\n## Visualize model Results\n\n\n### Posterior summaries\n\nWe can use the `fit$summary.fixed` and `summary.hyperpar` to obtain posterior summaries of the model parameters.\n\n::: {.cell}\n::: {.cell-output-display}\n\\begin{table}\n\\fontsize{12.0pt}{14.0pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}l|rrr}\n\\toprule\n & mean & 0.025quant & 0.975quant \\\\ \n\\midrule\\addlinespace[2.5pt]\nsigma & -0.05 & -0.46 & 0.36 \\\\ \nIntercept & -8.16 & -9.29 & -7.34 \\\\ \nRange for space & 295.48 & 110.54 & 673.68 \\\\ \nStdev for space & 0.81 & 0.42 & 1.39 \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n\nLook at the SPDE parameter posteriors as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\nplot( spde.posterior(fit, \"space\", what = \"range\")) +\nplot( spde.posterior(fit, \"space\", what = \"log.variance\"))  \n```\n\n::: {.cell-output-display}\n![](day5_practical_8_files/figure-pdf/unnamed-chunk-18-1.pdf){fig-pos='H'}\n:::\n:::\n\n###  Model predictions\n\nWe now want to extract the estimated posterior mean and sd of spatial GF. To do this we first need to define a grid of points where we want to predict. We do this using the function `fm_pixel()` which creates a regular grid of points covering the mesh\n\n::: {.cell}\n\n```{.r .cell-code}\npxl <- fm_pixels(mexdolphin$mesh, dims = c(200, 100), mask = mexdolphin$ppoly)\n```\n:::\n\nthen compute the prediction for both the spatial GF and the linear predictor (spatial GF + intercept)\n\n::: {.cell}\n\n```{.r .cell-code}\npr.int = predict(fit, pxl, ~data.frame(spatial = space,\n                                      lambda = exp(Intercept + space)))\n```\n:::\n\nFinally, we can plot the maps of the spatial effect\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + geom_sf(data = pr.int$spatial,aes(color = mean)) + scale_color_scico() + ggtitle(\"Posterior mean\")\n```\n\n::: {.cell-output-display}\n![](day5_practical_8_files/figure-pdf/unnamed-chunk-21-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nggplot() + geom_sf(data = pr.int$spatial,aes(color = sd)) + scale_color_scico() + ggtitle(\"Posterior sd\")\n```\n\n::: {.cell-output-display}\n![](day5_practical_8_files/figure-pdf/unnamed-chunk-21-2.pdf){fig-pos='H'}\n:::\n:::\n\n**Note** The posterior sd is lowest at the observation points. Note how the posterior sd is inflated around the border, this is the \"border effect\" due to the SPDE representation.\n\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nUsing the predictions stored in `pr.int`, produce a map of the posterior mean intensity.\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nRecall that the predicted intensity is given by $\\lambda(s) = \\exp(\\beta_0+\\xi(s))$\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nggplot() + \n  geom_sf(data = pr.int$lambda,aes(color = mean)) +\n  scale_color_scico(palette = \"imola\") +\n  ggtitle(\"Posterior mean\")\n```\n\n::: {.cell-output-display}\n![](day5_practical_8_files/figure-pdf/unnamed-chunk-22-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n\n</div>\n:::\n:::\n\n\nWe can predict the detection function in a similar fashion.Here, we should make sure that it doesn’t try to evaluate the effects of components that can’t be evaluated using the given input data. \n\n::: {.cell}\n\n```{.r .cell-code}\ndistdf <- data.frame(distance = seq(0, 8, length.out = 100))\ndfun <- predict(fit, distdf, ~ hn(distance, sigma))\nplot(dfun)\n```\n\n::: {.cell-output-display}\n![](day5_practical_8_files/figure-pdf/unnamed-chunk-23-1.pdf){fig-pos='H'}\n:::\n:::\n\n## Abundance estimates\n\nThe mean expected number of animals can be computed  by integrating the intensity over the region of interest as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\npredpts <- fm_int(mexdolphin$mesh, mexdolphin$ppoly)\nLambda <- predict(fit, predpts, ~ sum(weight * exp(space + Intercept)))\nLambda\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      mean       sd   q0.025     q0.5   q0.975   median sd.mc_std_err\n1 244.2944 59.85261 157.8649 237.0273 400.0186 237.0273      4.908211\n  mean.mc_std_err\n1        6.966903\n```\n\n\n:::\n:::\n\n\n\nTo fully propagate the uncertainty on the expected number animals we can draw Monte Carlo samples from the fitted model as follows (this could take a couple of minutes):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNs <- seq(50, 450, by = 1)\nNest <- predict(fit, predpts,\n  ~ data.frame(\n    N = Ns,\n    density = dpois(\n      Ns,\n      lambda = sum(weight * exp(space + Intercept))\n    )\n  ),\n  n.samples = 2000\n)\n```\n:::\n\n\nWe can compare this with a simpler \"plug-in\" approximation:\n\n::: {.cell}\n\n```{.r .cell-code}\nNest <- dplyr::bind_rows(\n  cbind(Nest, Method = \"Posterior\"),\n  data.frame(\n    N = Nest$N,\n    mean = dpois(Nest$N, lambda = Lambda$mean),\n    mean.mc_std_err = 0,\n    Method = \"Plugin\"\n  )\n)\n```\n:::\n\nThen, we can visualize the result as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = Nest) +\n  geom_line(aes(x = N, y = mean, colour = Method)) +\n  geom_ribbon(\n    aes(\n      x = N,\n      ymin = mean - 2 * mean.mc_std_err,\n      ymax = mean + 2 * mean.mc_std_err,\n      fill = Method,\n    ),\n    alpha = 0.2\n  ) +\n  geom_line(aes(x = N, y = mean, colour = Method)) +\n  ylab(\"Probability mass function\")\n```\n\n::: {.cell-output-display}\n![](day5_practical_8_files/figure-pdf/unnamed-chunk-27-1.pdf){fig-pos='H'}\n:::\n:::\n\n## Model checks \n\nLastly, we can assess the goodness-of-fit of the models by comparing the observed counts across different distance bins and the expected counts and their associated uncertainty:\n\n::: {.cell}\n\n```{.r .cell-code}\nbc <- bincount(\n  result = fit,\n  observations = mexdolphin$points$distance,\n  breaks = seq(0, max(mexdolphin$points$distance), length.out = 9),\n  predictor = distance ~ hn(distance, sigma)\n)\nattributes(bc)$ggp\n```\n\n::: {.cell-output-display}\n![](day5_practical_8_files/figure-pdf/unnamed-chunk-28-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nFit a model using a hazard detection function instead and compare the GoF of this model with that from the half-normal detection model. Recall that the hazard detection function is given by:\n\n$$\ng(\\mathbf{s}|\\sigma) = 1 - \\exp(-(d(\\mathbf{s})/\\sigma)^{-1})\n$$\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nThe hazard function can be codes as:\n\n::: {.cell}\n\n```{.r .cell-code}\nhr <- function(distance, sigma) {\n  1 - exp(-(distance / sigma)^-1)\n}\n```\n:::\n\nYou can use the same prior for the `sigma` parameter as for the half-Normal model (such parameters aren’t always comparable, but in this example it’s a reasonable choice). You can also use the `lgcp` function as a shortcut to fit the model (type `?lgcp` for further details).\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nformula1 <- geometry + distance ~ space +\n  log(hr(distance, sigma)) +\n  Intercept + log(2)\n\n# here we use the shorcut to specify the model\nfit1 <- lgcp(\n  components = cmp,\n  mexdolphin$points,\n  samplers = mexdolphin$samplers,\n  domain = list(\n    geometry = mexdolphin$mesh,\n    distance = fm_mesh_1d(seq(0, 8, length.out = 30))\n  ),\n  formula = formula1\n)\n\nbc1 <- bincount(\n  result = fit1,\n  observations = mexdolphin$points$distance,\n  breaks = seq(0, max(mexdolphin$points$distance), length.out = 9),\n  predictor = distance ~ hn(distance, sigma)\n)\nattributes(bc1)$ggp\n```\n\n::: {.cell-output-display}\n![](day5_practical_8_files/figure-pdf/unnamed-chunk-30-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n\n</div>\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# ZIP/ZAP and Hurdle Models\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(inlabru)\nlibrary(INLA)\nlibrary(terra)\nlibrary(sf)\nlibrary(scico)\nlibrary(magrittr)\nlibrary(patchwork)\nlibrary(tidyterra)\n\n\n# We want to obtain CPO data from the estimations\nbru_options_set(control.compute = list(dic = TRUE,\n                                       waic = TRUE,\n                                       mlik = TRUE,\n                                       cpo = TRUE))\n```\n:::\n\nIn this practical we are going to work with data with excess zeros. We will\n\n- [Create  count data from a `gorillas` dataset](#sec-prep) \n\n- Fit a [zero inflated model](#sec-zip)\n\n- Fit a [hurdle model](#sec-zap)\n\n- Fit a [hurdle model using two likelihoods](#sec-two-lik)\n\n- Fit a [hurdle model using two likelihoods and a shared component](#sec-two-lik-share)\n\n\n\n## Data Preparation {#sec-prep}\n\n\nThe following example use  the `gorillas` dataset available in the \n`inlabru` library.  \n\nThe data give the locations of Gorilla's nests in an area:\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngorillas_sf <- inlabru::gorillas_sf\nnests <- gorillas_sf$nests\nboundary <- gorillas_sf$boundary\n\nggplot() + geom_sf(data = nests) +\n  geom_sf(data = boundary, alpha = 0)\n```\n\n::: {.cell-output-display}\n![Location of gorilla nests](day5_practical_8_files/figure-pdf/nests_loc-1.png){fig-align='center' fig-pos='H' width=80%}\n:::\n:::\n\nThe dataset also contains covariates in the form or raster data. We consider two of them here:\n::: {.cell}\n\n```{.r .cell-code}\ngcov = gorillas_sf_gcov()\nelev_cov <- gcov$elevation\ndist_cov <-  gcov$waterdist\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Covariates](day5_practical_8_files/figure-pdf/unnamed-chunk-64-1.png){fig-align='center' width=80%}\n:::\n:::\n\n**Note:** the covariates have been expanded to cover all the nodes in the mesh.\n\n---\n\nTo obtain the count data, we rasterize the species counts to match the spatial resolution of the covariates available. Then we aggregate the pixels to a rougher resolution (5x5 pixels in the original covariate raster dimensions). Finally, we mask regions outside the study area.\n\nIn addition we compute the area of each grid cell.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Rasterize data\ncounts_rstr <-\n  terra::rasterize(vect(nests), gcov, fun = sum, background = 0) %>%\n  terra::aggregate(fact = 5, fun = sum) %>%\n  mask(vect(sf::st_geometry(boundary)))\nplot(counts_rstr)\n```\n\n::: {.cell-output-display}\n![Counts of gorilla nests](day5_practical_8_files/figure-pdf/unnamed-chunk-65-1.png){fig-align='center' fig-pos='H' width=80%}\n:::\n\n```{.r .cell-code}\n# compute cell area\ncounts_rstr <- counts_rstr %>%\n  cellSize(unit = \"km\") %>%\n  c(counts_rstr)\n```\n:::\n\nTo create our dataset of counts, we extract also the coordinate of center point of each raster pixel. In addition we create a column with presences and one with the pixel area\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncounts_df <- crds(counts_rstr, df = TRUE, na.rm = TRUE) %>%\n  bind_cols(values(counts_rstr, mat = TRUE, na.rm = TRUE)) %>%\n  rename(count = sum) %>%\n  mutate(present = (count > 0) * 1L) %>%\n  st_as_sf(coords = c(\"x\", \"y\"), crs = st_crs(nests))\n```\n:::\n\nWe then aggregate the covariates to the same resolution  as the nest counts and scale them. \n\n::: {.cell}\n\n```{.r .cell-code}\nelev_cov1 <- elev_cov %>% \n  terra::aggregate(fact = 5, fun = mean) %>% scale()\ndist_cov1 <- dist_cov %>% \n  terra::aggregate(fact = 5, fun = mean) %>% scale()\n```\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Covariates ](day5_practical_8_files/figure-pdf/fig-covariate-raster-1.png){#fig-covariate-raster fig-align='center' width=80%}\n:::\n:::\n\n### Mesh building\n\nWe now define the mesh and the spde object.\n\n::: {.cell}\n\n```{.r .cell-code}\n\nmesh <- fm_mesh_2d(\n  loc = st_as_sfc(counts_df),\n  max.edge = c(0.5, 1),\n  crs = st_crs(counts_df)\n)\n\nmatern <- inla.spde2.pcmatern(mesh,\n  prior.sigma = c(1, 0.01),\n  prior.range = c(5, 0.01)\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Mesh over the count locations](day5_practical_8_files/figure-pdf/unnamed-chunk-69-1.png){fig-align='center' width=80%}\n:::\n:::\nIn our dataset, the number of zeros is quite substantial, and our model may\nstruggle to account for them adequately. To address this, we should select a\nmodel capable of handling an \"inflated\" number of zeros, exceeding what a\nstandard Poisson model would imply. For this purpose, we opt for a\n\"zero-inflated Poisson model,\" commonly abbreviated as ZIP.\n\n\n\n\n## Zero-Inflated model (Type1) {#sec-zip}\n\nWe fit now a Zero-Inflated model to our data.  \n\n\nThe [Type 1 Zero-inflated Poisson model](https://inla.r-inla-download.org/r-inla.org/doc/likelihood/zeroinflated.pdf) is defined as follows:\n\n$$\n\\text{Prob}(y\\vert\\dots)=\\pi\\times 1_{y=0}+(1-\\pi)\\times \\text{Poisson}(y)\n$$\n\nHere, $\\pi=\\text{logit}^{-1}(\\theta)$\n\nThe expected value and variance for the counts are calculated as:\n\n$$\n\\begin{gathered}\nE(count)=(1-\\pi)\\lambda \\\\\nVar(count)= (1-\\pi)(\\lambda+\\pi \\lambda^2)\n\\end{gathered}\n$${#eq-mean-zip}\n\nThis model has two parameters:\n\n- The probability of excess zero $\\pi$ - This is a _hyperparameter_ and therefore it is constant \n- The mean of the Poisson distribution $\\lambda$. This is linked to the linear predictor as:\n$$\n\\eta = E\\log(\\lambda) = \\log(E) + \\beta_0 + \\beta_1\\text{Elevation} + \\beta_2\\text{Distance } + u\n$$\nwhere $\\log(E)$ is an offset (the area of the pixel) that accounts for the size of the cell.\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nFit a zero-inflated model to the data (`zeroinflatedpoisson1`) by completing the following code:\n::: {.cell}\n\n```{.r .cell-code}\ncmp = ~ Intercept(1) + elevation(...) + distance(...) + space(...)\n\nlik = bru_obs(...,\n    E = area)\n\nfit_zip <- bru(cmp, lik)\n```\n:::\n\n\n<div class='webex-solution'><button>Take hint</button>\n\nThe `E = area` is an offset that adjusts for the size of each cell.\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\n\ncmp = ~ Intercept(1) + elevation(elev_cov1, model = \"linear\") + distance(dist_cov1, model = \"linear\") + space(geometry, model = matern)\n\n\n\nlik = bru_obs(formula = count ~ .,\n    family = \"zeroinflatedpoisson1\", \n    data = counts_df,\n    E = area)\n\nfit_zip <- bru(cmp, lik)\n```\n\n\n</div>\n:::\n:::\n\nOnce the model is fitted we can look at the results\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nCheck what the estimated excess zero probaility is. \n\nUse the `predict()` function to look at the estimated  $\\lambda(s)$ and mean count in  @eq-mean-zip\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nTo get the right name for the hyperparameters to use in the `predict()` function, you can use the function `bru_names()`.\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\n# to check the estimated excess zero probability:\n# fit_zip$summary.hyperpar\n\npred_zip <- predict(\n  fit_zip, \n  counts_df,\n  ~ {\n    pi <- zero_probability_parameter_for_zero_inflated_poisson_1\n    lambda <- area * exp( distance + elevation + space + Intercept)\n    expect <- (1-pi) * lambda\n    variance <- (1-pi) * (lambda + pi * lambda^2)\n    list(\n      lambda = lambda,\n      expect = expect,\n      variance = variance\n    )\n  },\n  n.samples = 2500\n)\n```\n\n\n</div>\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Estimated $\\lambda$ (left) and expected counts (right) with zero inflated model](day5_practical_8_files/figure-pdf/unnamed-chunk-73-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Hurdle model (Type0) {#sec-zap}\n\nWe now fit a hurdle model to the same data.\n\nIn the `zeroinflatedpoisson0` model is defined by the [following\nobservation probability\nmodel](https://inla.r-inla-download.org/r-inla.org/doc/likelihood/zeroinflated.pdf)\n\n$$\n\\text{Prob}(y\\vert\\dots)=\\pi\\times 1_{y=0}+(1-\\pi)\\times \\text{Poisson}(y\\vert y>0)\n$$\n\nwhere $\\pi$ is the probability of zero.  \n\nThe expectation and variance of the counts are  as follows:\n\n$$\n\\begin{aligned}\nE(\\text{count})&=\\frac{1}{1-\\exp(-\\lambda)}\\pi\\lambda \\\\\nVar(\\text{count})&=  E(\\text{count}) \\left(1-\\exp(-\\lambda) E(\\text{count})\\right)\n\\end{aligned}\n$${#eq-mean-zap}\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nFit  a hurdle model to the data using the `zeroinflatedpoisson0` likelihood\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nYou do not need to redefine the components as the linear predictor is not changing.\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nlik = bru_obs(formula = count ~ .,\n    family = \"zeroinflatedpoisson0\", \n    data = counts_df,\n    E = area)\n\nfit_zap <- bru(cmp, lik)\n\n```\n\n\n</div>\n:::\n:::\n\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nAs before, check what the estimated probability of zero is and use `predict()` to obtain a map of the estimated mean counts in @eq-mean-zap over the domain.\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\n\npred_zap <- predict( fit_zap, counts_df,\n  ~ {\n    pi <- zero_probability_parameter_for_zero_inflated_poisson_0\n    lambda <- area * exp( distance + elevation + space + Intercept)\n    expect <- ((1-exp(-lambda))^(-1) * pi * lambda)\n    list(\n      lambda = lambda,\n      expect = expect\n    )\n  },\n  n.samples = 2500\n)\n\n```\n\n\n</div>\n:::\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Estimated $\\lambda$ (left) and expected counts (right) with hurdle model](day5_practical_8_files/figure-pdf/unnamed-chunk-76-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Hurdle model using two likelihoods {#sec-two-lik}\n\nHere the model is the same as in @sec-zap, but this time we also want to model $\\pi$ using covariates and random effects.\nTherefore we define a second linear predictor\n$$\n\\eta^2 =\\beta_0^2 + \\beta_1^2\\text{Elevation} +  \\beta_2^2\\text{Distance} + u^2 \n$$\n**Note** here we have defined the two linear predictor to use the same covariates, but this is not necessary, they can be totally independent.\n\nTo fit this model we have to define two likelihoods:\n  - One will account for the presence-absence process and has a Binomial model\n  - One will account for the counts and has a truncated Poisson model \n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nComplete the following code to fit a hurdle model based on two likelihoods:\n\n::: {.cell}\n\n```{.r .cell-code}\n# define components\ncmp <- ~\n  Intercept_count(1) +\n    elev_count(elev_cov1, model = \"linear\") +\n    dist_count(dist_cov1, model = \"linear\") +\n    space_count(geometry, model = matern) +\n    Intercept_presence(1) +\n    elev_presence(elev_cov1, model = \"linear\") +\n    dist_presence(dist_cov1, model = \"linear\") +\n    space_presence(geometry, model = matern) \n\n# positive count model\npos_count_obs <- bru_obs(formula = ...,\n      family = ...,\n      data = counts_df[counts_df$present > 0, ],\n      E = area)\n  \n# presence model\npresence_obs <- bru_obs(formula ...,\n  family = ...,\n  data = counts_df,\n)\n\n# fit the model\nfit_zap2 <- bru(...)\n```\n:::\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nAdd hint details here...\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\ncmp <- ~\n  Intercept_count(1) +\n    elev_count(elev_cov1, model = \"linear\") +\n    dist_count(dist_cov1, model = \"linear\") +\n    space_count(geometry, model = matern) +\n    Intercept_presence(1) +\n    elev_presence(elev_cov1, model = \"linear\") +\n    dist_presence(dist_cov1, model = \"linear\") +\n    space_presence(geometry, model = matern) \n\n\npos_count_obs <- bru_obs(formula = count ~ Intercept_count + elev_count + \n                                   dist_count + space_count,\n      family = \"nzpoisson\",\n      data = counts_df[counts_df$present > 0, ],\n      E = area)\n  \n\npresence_obs <- bru_obs(formula = present ~ Intercept_presence + elev_presence + dist_presence +\n                          space_presence,\n  family = \"binomial\",\n  data = counts_df,\n)\n\nfit_zap2 <- bru(\n  cmp,\n  presence_obs,\n  pos_count_obs\n)\n\n```\n\n\n</div>\n:::\n:::\n\n## Hurdle model using two likelihoods and a shared component {#sec-two-lik-share}\n\nNote that in the model above, there is no direct link between the parameters of the\ntwo observation parts, and we could estimate them separately.\nHowever, the two likelihoods could share some of the components; for example the  `space_count` component could be used for both\npredictors. This would be possible using the  `copy` argument.\n\nWe would then need to define one component as `space(geometry, model = matern)` and then a copy of it as `space_copy(geometry, copy = \"space\", fixed = FALSE)`.\n\nThe results from the model in @sec-sec-two-lik show that the  estimated\ncovariance parameters for the two fields are very different, so it is probably  not\nsensible to share the same component between the two parts. We do it anyway to show an example:\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp <- ~\n  Intercept_count(1) +\n    elev_count(elev_cov1, model = \"linear\") +\n    dist_count(dist_cov1, model = \"linear\") +\n    Intercept_presence(1) +\n    elev_presence(elev_cov1, model = \"linear\") +\n    dist_presence(dist_cov1, model = \"linear\") +\n    space(geometry, model = matern) +\n  space_copy(geometry, copy = \"space\", fixed = FALSE)\n\n\npos_count_obs <- bru_obs(formula = count ~ Intercept_count + elev_count + dist_count + space,\n      family = \"nzpoisson\",\n      data = counts_df[counts_df$present > 0, ],\n      E = area)\n\npresence_obs <- bru_obs(formula = present ~ Intercept_presence + elev_presence + dist_presence + space_copy,\n  family = \"binomial\",\n  data = counts_df)\n\nfit_zap3 <- bru(\n  cmp,\n  presence_obs,\n  pos_count_obs)\n```\n:::\n\n\n\n## Comparing models\n\nWe have fitted four different models. Now we want to compare them and see how they fit the data.\n\n### Comparing model predictions\n\nWe first want to compare the estimated surfaces of expected counts.\nTo do this we want to produce the estimated expected counts, similar to what we did in  @sec-zip and @sec-zap for all four models and plot them together:\n\n::: {.cell}\n\n```{.r .cell-code}\npred_zip <- predict(\n  fit_zip, \n  counts_df,\n  ~ {\n    pi <- zero_probability_parameter_for_zero_inflated_poisson_1\n    lambda <- area * exp( distance + elevation + space + Intercept)\n    expect <- (1-pi) * lambda\n    variance <- (1-pi) * (lambda + pi * lambda^2)\n    list(\n      expect = expect\n    )\n  },n.samples = 2500)\n\npred_zap <- predict( fit_zap, counts_df,\n  ~ {\n    pi <- zero_probability_parameter_for_zero_inflated_poisson_0\n    lambda <- area * exp( distance + elevation + space + Intercept)\n    expect <- ((1-exp(-lambda))^(-1) * pi * lambda)\n    list(\n      expect = expect)\n  },n.samples = 2500)\n\ninv.logit = function(x) (exp(x)/(1+exp(x)))\n\npred_zap2 <- predict( fit_zap2, counts_df,\n  ~ {\n    pi <- inv.logit(Intercept_presence + elev_presence + dist_presence + space_presence)\n    lambda <- area * exp( dist_count + elev_count + space_count + Intercept_count)\n    expect <- ((1-exp(-lambda))^(-1) * pi * lambda)\n    list(\n      expect = expect)\n  },n.samples = 2500)\n\npred_zap3 <- predict( fit_zap3, counts_df,\n  ~ {\n    pi <- inv.logit(Intercept_presence + elev_presence + dist_presence + space_copy)\n    lambda <- area * exp( dist_count + elev_count + space + Intercept_count)\n    expect <- ((1-exp(-lambda))^(-1) * pi * lambda)\n    list(\n      expect = expect)\n  },n.samples = 2500)\n\n\n\n\n  data.frame(x = st_coordinates(counts_df)[,1],\n             y = st_coordinates(counts_df)[,2],\n    zip = pred_zip$expect$mean,\n         hurdle = pred_zap$expect$mean,\n         hurdle2 = pred_zap2$expect$mean,\n         hurdle3 = pred_zap3$expect$mean)  %>%\n  pivot_longer(-c(x,y)) %>%\n  ggplot() + geom_tile(aes(x,y, fill = value)) + facet_wrap(.~name) +\n    theme_map + scale_fill_scico(direction = -1)\n```\n\n::: {.cell-output-display}\n![Estimated expected counts for all four models](day5_practical_8_files/figure-pdf/unnamed-chunk-79-1.png){fig-pos='H'}\n:::\n:::\n\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nCreate plots of the estimated variance of the counts. \n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nThe fomulas for the variances are in @eq-mean-zip and @eq-mean-zap.\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\npred_zip <- predict(\n  fit_zip, \n  counts_df,\n  ~ {\n    pi <- zero_probability_parameter_for_zero_inflated_poisson_1\n    lambda <- area * exp( distance + elevation + space + Intercept)\n    variance <- (1-pi) * (lambda + pi * lambda^2)\n    list( variance = variance)\n  },n.samples = 2500)\n\npred_zap <- predict( fit_zap, counts_df,\n  ~ {\n    pi <- zero_probability_parameter_for_zero_inflated_poisson_0\n    lambda <- area * exp( distance + elevation + space + Intercept)\n    expect <- ((1-exp(-lambda))^(-1) * pi * lambda)\n    variance = expect *(1-exp(-lambda) * expect)\n    list(variance = variance)\n  },\n  n.samples = 2500)\n\ninv.logit = function(x) (exp(x)/(1+exp(x)))\n\npred_zap2 <- predict( fit_zap2, counts_df,\n  ~ {\n    pi <- inv.logit(Intercept_presence + elev_presence + dist_presence + space_presence)\n    lambda <- area * exp( dist_count + elev_count + space_count + Intercept_count)\n    expect <- ((1-exp(-lambda))^(-1) * pi * lambda)\n    variance = expect *(1-exp(-lambda) * expect)\n    list(variance = variance)\n  },\n  n.samples = 2500)\n\npred_zap3 <- predict( fit_zap3, counts_df,\n  ~ {\n    pi <- inv.logit(Intercept_presence + elev_presence + dist_presence + space_copy)\n    lambda <- area * exp( dist_count + elev_count + space + Intercept_count)\n    expect <- ((1-exp(-lambda))^(-1) * pi * lambda)\n    variance = expect *(1-exp(-lambda) * expect)\n    list(variance = variance)\n  },\n  n.samples = 2500)\n\n\n\n\n  data.frame(x = st_coordinates(counts_df)[,1],\n             y = st_coordinates(counts_df)[,2],\n    zip = pred_zip$variance$mean,\n         hurdle = pred_zap$variance$mean,\n         hurdle2 = pred_zap2$variance$mean,\n         hurdle3 = pred_zap3$variance$mean)  %>%\n  pivot_longer(-c(x,y)) %>%\n  ggplot() + geom_tile(aes(x,y, fill = value)) + facet_wrap(.~name) +\n    theme_map + scale_fill_scico(direction = -1)\n```\n\n::: {.cell-output-display}\n![](day5_practical_8_files/figure-pdf/unnamed-chunk-80-1.png){fig-align='center' fig-pos='H'}\n:::\n\n\n</div>\n:::\n:::\n\n\n### Using scores\n\nWe can compare model using the scores that the _bru()_ function computes since we have set, at the beginning. the options to \n::: {.cell}\n\n```{.r .cell-code}\nbru_options_set(control.compute = list(dic = TRUE,\n                                       waic = TRUE,\n                                       mlik = TRUE,\n                                       cpo = TRUE))\n\n```\n:::\n\n\nLets use these scores to compare the models.\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nExtract the DIC, WAIC and MLIK values for the four models and compare them\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\ndata.frame( Model = c(\"ZIP\", \"HURDLE\", \"HURDLE_2\",\"HURDLE_3\" ),\n  DIC = c(fit_zip$dic$dic, fit_zap$dic$dic, WAIC = fit_zap2$dic$dic, fit_zap3$dic$dic),\n            WAIC = c(fit_zip$waic$waic, fit_zap$waic$waic, fit_zap2$waic$waic, fit_zap3$waic$waic),\n            MLIK = c(fit_zip$mlik[1], fit_zap$mlik[1], fit_zap2$mlik[1], fit_zap3$mlik[1]))\n#>      Model      DIC     WAIC       MLIK\n#> 1      ZIP 1214.138 1223.718  -686.9498\n#> 2   HURDLE 1886.066 1909.722  -994.3807\n#> 3 HURDLE_2 1268.322 1285.524  -734.3697\n#> 4 HURDLE_3 1518.373 1526.178 -2026.4552\n```\n\n\n</div>\n:::\n:::\n\n\n\nFrom the table above we can see that the model that best balances complexity and fit is the zero inflated one (ZIP).\n\n",
    "supporting": [
      "day5_practical_8_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "\\usepackage{booktabs}\n\\usepackage{caption}\n\\usepackage{longtable}\n\\usepackage{colortbl}\n\\usepackage{array}\n\\usepackage{anyfontsize}\n\\usepackage{multirow}\n"
      ]
    },
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}