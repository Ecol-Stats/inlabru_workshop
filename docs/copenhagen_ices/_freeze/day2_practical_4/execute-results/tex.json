{
  "hash": "f2e5a504456b40074b3bd0a49ee9d7a3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Practical 4\"\nexecute: \n  freeze: true\nformat: \n  html:\n    mainfont: \"12\"\n  PrettyPDF-pdf:\n    keep-tex: true\n    number-sections: true\nembed-resources: true\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n<font size=\"5\"> **Aim of this practical:** </font>\n\n1.  Work with different types of spatial data types including: Areal, Geostatistical and Spatial Point process data.\n2.  Read and visualize shapefiles into R\n3.  Manipulate and visualize `sf` spatial object in R\n4.  Manipulate and visualize raster data in R.\n\nwe are going to learn:\n\n-   Explore tools for spatial data wrangling and visualization.\n-   Use some commonly used metrics to assess spatial autocorrelation in our data.\n\n\n\n\n\n\n{{< downloadthis day2_practical_4.R dname=\"practical_4.R\" label=\"Download Practical 3 R script\" icon=\"database-fill-down\" type=\"success\" >}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\nIn this practical we will:\n\n-   Explore tools for areal spatial data wrangling and visualization.\n-   Compute Morans'I to identify spatial autocorrelation in our data.\n\n## Areal (lattice) data {#sec-areal_data}\n\nAreal data our measurements are summarised across a set of discrete, non-overlapping spatial units such as postcode areas, health board or pixels on a satellite image. In consequence, the spatial domain is a countable collection of (regular or irregular) areal units at which variables are observed. Many public health studies use data aggregated over groups rather than data on individuals - often this is for privacy reasons, but it may also be for convenience.\n\nIn the next example we are going to explore data on respiratory hospitalisations for Greater Glasgow and Clyde between 2007 and 2011. The data are available from the `CARBayesdata` R Package:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(CARBayesdata)\n\ndata(pollutionhealthdata)\ndata(GGHB.IZ)\n```\n:::\n\nThe `pollutionhealthdata` contains the spatiotemporal data on respiratory hospitalisations, air pollution concentrations and socio-economic deprivation covariates for the 271 Intermediate Zones (IZ) that make up the Greater Glasgow and Clyde health board in Scotland. Data are provided by the [Scottish Government](http://statistics.gov.scot.) and the available variables are:\n\n-   `IZ`: unique identifier for each IZ.\n-   `year`: the year were the measruments were taken\n-   `observed`: observed numbers of hospitalisations due to respiratory disease.\n-   `expected`: expected numbers of hospitalisations due to respiratory disease computed using indirect standardisation from Scotland-wide respiratory hospitalisation rates.\n-   `pm10`: Average particulate matter (less than 10 microns) concentrations.\n-   `jsa`: The percentage of working age people who are in receipt of Job Seekers Allowance\n-   `price`: Average property price (divided by 100,000).\n\nThe `GGHB.IZ` data is a Simple Features (`sf`) object containing the spatial polygon information for the set of 271 Intermediate Zones (IZ), that make up of the Greater Glasgow and Clyde health board in Scotland ( @fig-GGC ).\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Greater Glasgow and Clyde health board represented by 271 Intermediate Zones](day2_practical_4_files/figure-pdf/fig-GGC-1.pdf){#fig-GGC fig-align='center'}\n:::\n:::\n\nLet's start by loading useful libraries:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(scico)\n```\n:::\n\nThe `sf` package allows us to work with vector data which is used to represent points, lines, and polygons. It can also be used to read vector data stored as a shapefiles.\n\nFirst, lets combine both data sets based on the Intermediate Zones (IZ) variable using the `merge` function from `base` R:\n\n::: {.cell}\n\n```{.r .cell-code}\nresp_cases <- merge(GGHB.IZ, pollutionhealthdata, by = \"IZ\")\n```\n:::\n\nIn epidemiology, disease risk is usually estimated using Standardized Mortality Ratios (SMR). The SMR for a given spatial areal unit $i$ is defined as the ratio between the observed ( $Y_i$ ) and expected ( $E_i$ ) number of cases:\n\n$$\nSMR_i = \\dfrac{Y_i}{E_i}\n$$\n\nA value $SMR > 1$ indicates that there are more observed cases than expected which corresponds to a high risk area. On the other hand, if $SMR<1$ then there are fewer observed cases than expected, suggesting a low risk area.\n\nWe can manipulate `sf` objects the same way we manipulate standard data frame objects via the `dplyr` package. Lets use the pipeline command `%>%` and the `mutate` function to calculate the yearly SMR values for each IZ:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nresp_cases <- resp_cases %>% \n  mutate(SMR = observed/expected, .by = year )\n```\n:::\n\nNow we use `ggplot` to visualize our data by adding a `geom_sf` layer and coloring it according to our variable of interest (i.e., SMR). We can further use `facet_wrap` to create a layer per year and chose an appropriate color palette using the `scale_fill_scico` from the `scico` package:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot()+\n  geom_sf(data=resp_cases,aes(fill=SMR))+\n  facet_wrap(~year)+scale_fill_scico(palette = \"roma\")\n```\n\n::: {.cell-output-display}\n![](day2_practical_4_files/figure-pdf/unnamed-chunk-7-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nProduce a map that shows the spatial distribution of each of the following variables for the year 2011:\n\n-   Average particulate matter `pm10`\n\n-   Average property price `price`\n\n-   Percentage of working age people who are in receipt of Job Seekers Allowance `jsa`\n\n\n<div class='webex-solution'><button>hint</button>\n\n\nYou can use the `filter` function from `dplyr` to subset the data according to the year of interest.\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code}\n# Library for plotting multiple maps together\n\nlibrary(patchwork)\n\n# subset data set for 2011\n\nresp_cases_2011 <- resp_cases %>% filter(year ==2011)\n\n# pm10 plot\n\npm10_plot <- ggplot()+\n  geom_sf(data=resp_cases_2011,aes(fill=pm10))+\n  scale_fill_scico(palette = \"navia\")\n\n# property price\n\nprice_plot <- ggplot()+\n  geom_sf(data=resp_cases_2011,aes(fill=price))+\n  facet_wrap(~year)+scale_fill_scico(palette = \"bilbao\")\n\n#  percentage jsa\n\njsa_plot <- ggplot()+\n  geom_sf(data=resp_cases_2011,aes(fill=jsa))+\n  facet_wrap(~year)+scale_fill_scico(palette = \"lapaz\") \n\n# plot maps together\n\npm10_plot + price_plot + jsa_plot + plot_layout(ncol=3)\n```\n\n::: {.cell-output-display}\n![](day2_practical_4_files/figure-pdf/unnamed-chunk-8-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n\n</div>\n:::\n:::\n\nAs with the other types of spatial modelling, our goal is to observe and explain spatial variation in our data. Generally, we are aiming to produce a smoothed map which summarises the spatial patterns we observe in our data.\n\nA key aspect of any spatial analysis is that observations closer together in space are likely to have more in common than those further apart. This can lead us towards approaches similar to those used in time series, where we consider the spatial *closeness* of our regions in terms of a *neighbourhood structure*.\n\nThe function [`poly2nb()`](https://r-spatial.github.io/spdep/reference/poly2nb.html) of the `spdep` package can be used to construct a list of neighbors based on areas with contiguous boundaries (e.g., using Queen contiguity).\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(spdep)\n\nW.nb <- poly2nb(GGHB.IZ,queen = TRUE)\nW.nb\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNeighbour list object:\nNumber of regions: 271 \nNumber of nonzero links: 1424 \nPercentage nonzero weights: 1.938971 \nAverage number of links: 5.254613 \n2 disjoint connected subgraphs\n```\n\n\n:::\n:::\n\nThe warning tell us that the neighbourhood is comprised of two interconnected regions. By looking at the neighbourhood graph below, we can see that these are the North and South Glasgow regions which are separated by the River Clyde.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(st_geometry(GGHB.IZ), border = \"lightgray\")\nplot.nb(W.nb, st_geometry(GGHB.IZ), add = TRUE)\n```\n\n::: {.cell-output-display}\n![](day2_practical_4_files/figure-pdf/unnamed-chunk-10-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\nYou could use the `snap` argument within `poly2nb` to set a distance at which the different regions centroids are consider neighbours. To do so we first need to be aware about the spatial units of the spatial coordinate reference system (CRS). We can check this as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\nst_crs(GGHB.IZ)$units\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"m\"\n```\n\n\n:::\n:::\n\nThen, we could set a distance of 250m to join the IZ centroids that are are less than 250m apart.\n\n::: {.cell}\n\n```{.r .cell-code}\nW.nb250 <- poly2nb(GGHB.IZ,snap=250)\nW.nb250\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNeighbour list object:\nNumber of regions: 271 \nNumber of nonzero links: 1758 \nPercentage nonzero weights: 2.393758 \nAverage number of links: 6.487085 \n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(st_geometry(GGHB.IZ), border = \"lightgray\")\nplot.nb(W.nb250, st_geometry(GGHB.IZ), add = TRUE)\n```\n\n::: {.cell-output-display}\n![](day2_practical_4_files/figure-pdf/unnamed-chunk-13-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\nOnce we have identified a set of neighbours using our chosen method, we can use this to account for correlation.\n\nMoran's $I$ is a measure of global spatial autocorrelation, and can be considered an extension of the Pearson correlation coefficient. For a set of data $Z_1, \\ldots, Z_m$ measured on regions $B_1, \\ldots B_m$, with neighbourhood matrix $W$, we can compute Moran's I as:\n\n$$\nI = \\frac{m}{\\sum_{i=1}^m \\sum_{j=1}^m w_{ij}}\\frac{\\sum_{i=1}^m \\sum_{j=1}^m w_{ij} (Z_i - \\bar{Z})(Z_j - \\bar{Z})}{\\sum_{i=1}^m (Z_i - \\bar{Z})^2}\n$$\n\nThis is basically a function of differences in values between neighbouring areas. By far the most common approach is to use a binary neighbourhood matrix, $W$, denoted by\n\n$$ w_{ij} = \\begin{cases} 1 & \\text{if areas } (B_i, B_j) \\text{ are neighbours.}\\\\ 0 & \\text{otherwise.} \\end{cases} $$\n\nBinary matrices are used for their simplicity. Fitting spatial models often requires us to invert $W$, and this is less computationally intensive for sparse matrices.\n\nMoran's $I$ ranges between -1 and 1, and can be interpreted in a similar way to a standard correlation coefficient.\n\n-   $I=1$ implies that we have **perfect spatial correlation**.\n\n-   $I=0$ implies that we have **complete spatial randomness**.\n\n-   $I=-1$ implies that we have **perfect dispersion** (negative correlation).\n\nOur observed $I$ is a point estimate, and we may also wish to assess whether it is significantly different from zero. We can test for a statistically significant spatial correlation using a permutation test, with hypotheses:\n\n$$\n\\begin{aligned}\nH_0&: \\text{ negative or no spatial association } (I \\leq 0)\\\\\nH_1&: \\text{ positive spatial association } (I > 0)\n\\end{aligned}\n$$\n\nWe can use `moran.test()` to test this hypothesis by setting `alternative = \"greater\"`. To do so, we need to supply list containing the neighbors via the `nb2listw()` function from the `spdep` package. Lets assess now the spatial autocorrelation of the SMR in 2011:\n\n::: {.cell}\n\n```{.r .cell-code}\n# subset the data\nresp_cases_2011 <- resp_cases %>% filter(year ==2011)\n\n# neighbors list \nnbw <- nb2listw(W.nb, style = \"W\")\n\n# Global Moran's I\ngmoran <- moran.test(resp_cases_2011$SMR, nbw,\n                     alternative = \"greater\")\ngmoran\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMoran I test under randomisation\n\ndata:  resp_cases_2011$SMR  \nweights: nbw    \n\nMoran I statistic standard deviate = 11.42, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.439899780      -0.003703704       0.001508809 \n```\n\n\n:::\n:::\n\n::: {.callout-tip icon=\"false\"}\n## {{< bi question-octagon color=#6dc83c >}} Question\n\nWhat do we conclude from the Moran's I test?\n\n\n<div class='webex-solution'><button>Answer</button>\n\n\nSince have set the alternative hypothesis to be \\$ I \\> 0 \\$ and have a *p*-value $<0.05$, we then reject the null hypothesis and conclude there is evidence for positive spatial autocorrelation.\n\n\n</div>\n\n:::\n\nA local version of Moran’s I can also be used to measure the similarity between each IZ via the `localmoran` function:\n\n::: {.cell}\n\n```{.r .cell-code}\nlmoran <- localmoran(resp_cases_2011$SMR, nbw, alternative = \"two.sided\")\n```\n:::\n\nIn this case we set `alternative = \"two.sided\"` to test whether there is any evidence of spatial autocorrelation in our data:\n\n$$\n\\begin{aligned}\nH_0&: \\text{ no spatial association } (I=0)\\\\\nH_1&: \\text{ some spatial association } (I \\neq 0)\n\\end{aligned}\n$$\n\nWe can obtain the $Z$-scores from the test (`Z.Ii`) where any values smaller than –1.96 indicate negative spatial autocorrelation, and z-score values greater than 1.96 indicate positive spatial autocorrelation:\n\n::: {.cell}\n\n```{.r .cell-code}\nresp_cases_2011_m <- resp_cases_2011 %>% mutate(Zscores = lmoran[,\"Z.Ii\"])\n\nresp_cases_2011_m <- resp_cases_2011_m %>% \n  mutate (SAC = case_when(\n  Zscores > qnorm(0.975) ~ \" M > 1\" ,\n  Zscores < -1*qnorm(0.975) ~ \" M < 1\",\n  .default = \"M = 0\"),\n  SAC=as.factor(SAC)\n)\n```\n:::\n\nWe can visualize these results using either `ggplot` as we did before, or via the `mapview` library which contains interactive features:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mapview)\nmapview(resp_cases_2011_m, \n        zcol = \"SAC\",\n        layer.name = \"SAC\",\n        col.regions=c(\"turquoise\",\"orange\",\"grey40\"))\n```\n\n::: {.cell-output-display}\n![](day2_practical_4_files/figure-pdf/unnamed-chunk-17-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\nIn this practical we will:\n\n-   Explore tools for geostatistical spatial data wrangling and visualization.\n-   Compute a variogram to assess for spatial autocorrelation in our data.\n\nFirst, lets load some useful libraries for data wrangling and visualization\n\n::: {.cell}\n\n```{.r .cell-code}\n# For plotting\nlibrary(mapview)\nlibrary(ggplot2)\nlibrary(scico) # for colouring palettes\n\n# Data manipulation\nlibrary(dplyr)\n```\n:::\n\n## Georeferenced data\n\nTobler's first law of geography states that:\n\n\"*Everything is related to everything else, but near things are more related than distant things*\"\n\nSpatial patterns are fundamental in environmental and ecological data. In many ecological and environmental settings, measurements from fixed sampling units, aiming to quantify spatial variation and interpolate values at unobserved sites.\n\n**Georefernced** data are the most common form of spatial data found in environmental setting. In these data we regularly take measurements of a spatial ecological or environmental process at a set of fixed locations. This could be data from transects (e.g, where the height of trees is recorded), samples taken across a region (e.g., water depth in a lake) or from monitoring stations as part of a network (e.g., air pollution). In each of these cases, our goal is to estimate the value of our variable across the entire space.\n\nLet $D$ be our two-dimensional region of interest. In principle, there is aninfinite number of locations within $D$, each of which can be represented by mathematical coordinates (e.g., latitude and longitude). We then can identify any individual location as $s_i = (x_i, y_i)$, where $x_i$ and $y_i$ are their coordinates.\n\nWe can treat our variable of interest as a random variable, $Z$ which can be observed at any location as $Z(\\mathbf{s}_i)$.\n\nOur geostatistical process can therefore be written as: $$\\{Z(\\mathbf{s}); \\mathbf{s} \\in D\\}$$\n\nIn practice, our data are observed in a finite number of locations, $m$, and can be denoted as:\n\n$$z = \\{z(\\mathbf{s}_1), \\ldots z(\\mathbf{s}_m) \\}$$\n\nIn the next example, we will explore data on the Pacific Cod (*Gadus macrocephalus*) from a trawl survey in Queen Charlotte Sound. The `pcod` dataset is available from the `sdmTMB` package and contains the presence/absence records of the Pacific Cod during each surveys along with the biomass density of Pacific cod in the area swept (kg/Km$^2$). The `qcs_grid` data contain the depth values stored as $2\\times 2$ km grid for Queen Charlotte Sound.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sdmTMB)\n\npcod_df = sdmTMB::pcod \nqcs_grid = sdmTMB::qcs_grid\n```\n:::\n\n### Georeferrenced data\n\nLet's create an initial `sf` spatial object using the standard geographic coordinate system (`EPSG:4326`). This correctly defines the point locations based on latitude and longitude.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sf)\npcod_sf =   st_as_sf(pcod_df, coords = c(\"lon\",\"lat\"), crs = 4326)\n```\n:::\n\nNow we can transform to the standard UTM Zone 9N projection (EPSG:32609) which uses meters:\n\n::: {.cell}\n\n```{.r .cell-code}\npcod_sf_proj <- st_transform(pcod_sf, crs = 32609)\nst_crs(pcod_sf_proj)$units\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"m\"\n```\n\n\n:::\n:::\n\nWe can change the spatial units to *km* to better reflect the scale of our ecological study and to make resulting distance/area values more intuitive to interpret:\n\n::: {.cell}\n\n```{.r .cell-code}\npcod_sf_proj = st_transform(pcod_sf_proj,\n                            gsub(\"units=m\",\"units=km\",\n                                 st_crs(pcod_sf_proj)$proj4string)) \nst_crs(pcod_sf_proj)$units\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"km\"\n```\n\n\n:::\n:::\n\nInstead of first setting an EPSG code and then transforming, we can define the target Coordinate Reference System (CRS) directly using a `proj4string`. This allows us to customize non-standard parameters in a single step, in this case, explicitly setting the projection units to kilometers (`+units=km`).\n\n::: {.cell}\n\n```{.r .cell-code}\npcod_sf = st_transform(pcod_sf,\n                       crs = \"+proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\" )\nst_crs(pcod_sf)$units\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"km\"\n```\n\n\n:::\n:::\n\nSpatial `sf` objects can be manipulated the same way we manipulate standard data frame objects via the `dplyr` package. For example, you can select a specific year using the `filter` function from `dplyr`. Let's map the present/absence of the Pacific Cod in 2017 using the `mapview` function:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npcod_sf %>% \n  filter(year== 2017) %>%\n  mutate(present = as.factor(present)) %>%\nmapview(zcol = \"present\",\n        layer.name = \"Occupancy status of Pacific Cod in 2017\")\n```\n\n::: {.cell-output-display}\n![](day2_practical_4_files/figure-pdf/unnamed-chunk-43-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nUse `ggplot` and the `sf` library to map the biomass density of the pacific cod across years.\n\n\n<div class='webex-solution'><button>hint</button>\n\n\nYou can plot an`sf` object by adding a `geom_sf` layer to a ggplot object. You can also use the `facet_wrap` argument to plot an arrange of plots according to a grouping variable.\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code}\nggplot()+ \n  geom_sf(data=pcod_sf,aes(color=density))+ \n  facet_wrap(~year)+\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](day2_practical_4_files/figure-pdf/unnamed-chunk-44-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n\n</div>\n:::\n:::\n\n### Raster Data\n\nEnvironmental data are typically stored in raster format, which represents spatially continuous phenomena by dividing a region into a grid of equally-sized cells, each storing a value for the variable of interest. In R, the `terra` package is a modern and powerful tool for efficiently working with raster data. The function `rast()`, can be used both to read raster files from standard formats (e.g., `.tif` or `.tiff`) and to create a new raster object from a data frame. For instance, the following code creates a raster from the `qcs_grid` grid data for Queen Charlotte Sound.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(terra)\ndepth_r <- rast(qcs_grid, type = \"xyz\")\ndepth_r\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclass       : SpatRaster \nsize        : 102, 121, 3  (nrow, ncol, nlyr)\nresolution  : 2, 2  (x, y)\nextent      : 341, 583, 5635, 5839  (xmin, xmax, ymin, ymax)\ncoord. ref. :  \nsource(s)   : memory\nnames       :    depth, depth_scaled, depth_scaled2 \nmin values  :  12.0120,    -6.000040,  4.892624e-08 \nmax values  : 805.7514,     3.453937,  3.600048e+01 \n```\n\n\n:::\n:::\n\nThe raster object contains three layers corresponding to the (i) depth values, (ii) the scaled depth values and (iii) the squared depth values.\n\nNotice that there are no CRS associated with the raster. Thus, we can assign appropriate CRS using the `crs` function. Additionally, we also want the raster CRS to match the CRS in the survey data (recall that we have previously reprojected our data to utm coordinates). We can assign an appropiate CRS that matches the CRS of the `sf` object as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\ncrs(depth_r) <- crs(pcod_sf)\n```\n:::\n\nWe can use the `tidyterra` package to plot raster data using `ggplot` by adding a `geom_spatraster` function and then select an appropriate `fill` and `color` palettes:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyterra)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'tidyterra'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:stats':\n\n    filter\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot()+ \n  geom_spatraster(data=depth_r$depth)+\n  geom_sf(data=pcod_sf,aes(color=factor(present))) +\n  facet_wrap(~year)+\n    scale_color_manual(name=\"Occupancy status for the Pacific Cod\",\n                     values = c(\"black\",\"orange\"),\n                     labels= c(\"Absence\",\"Presence\"))+\n  scale_fill_scico(name = \"Depth\",\n                   palette = \"nuuk\",\n                   na.value = \"transparent\" )\n```\n\n::: {.cell-output-display}\n![](day2_practical_4_files/figure-pdf/unnamed-chunk-47-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nMap the scaled depth and the presence/absence records of the Pacific cod for 2003 to 2005 only.\n\n\n<div class='webex-solution'><button>hint</button>\n\n\nThe different layers of a raster can be accessed using the `$` symbol.\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code}\nggplot()+ \n  geom_spatraster(data=depth_r$depth_scaled)+\n  geom_sf(data=pcod_sf %>% filter(year %in% 2003:2005),\n          aes(color=factor(present)))+ \n  facet_wrap(~year)+\n  scale_color_manual(name=\"Occupancy status for the Pacific Cod\",\n                     values = c(\"black\",\"orange\"),\n                     labels= c(\"Absence\",\"Presence\"))+\n    scale_fill_scico(name = \"Scaled Depth\",\n                     palette = \"davos\",\n                     na.value = \"transparent\" )\n```\n\n::: {.cell-output-display}\n![](day2_practical_4_files/figure-pdf/unnamed-chunk-48-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n\n</div>\n:::\n:::\n\n### Autocorrelation and Variograms\n\nSpatial statistics quantifies the fundamental principle that nearby things are closely related. This spatial dependence means that observation are not independent, as assumed by most classical statistical models, but are instead correlated relative to their proximity. While this correlation can be a valuable source of information, it must be explicitly accounted for to avoid wrong inference and incorrect conclusions.\n\nThe first step is to assess whether there is any evidence of spatial dependency in our data. Spatial dependence in georeferenced data can be explored by a function known as a variogram $2\\gamma(\\cdot)$ (or semivariogram $\\gamma(\\cdot)$). The variogram is similar in many ways to the autocorrelation function used in time series modelling. In simple terms, it is a function which measures the difference in the spatial process between a pair of locations a fixed distance apart.\n\nThe variogram measures the variance of the difference in the process $Z(\\cdot)$ at two spatial locations $\\mathbf{s}$ and $\\mathbf{s+h}$ and is defined as :\n\n$$\\mathrm{Var}[Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h})] = E[(Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h}))^2] = 2\\gamma_z(\\mathbf{h}).$$\n\nHere, $2\\gamma_z(\\mathbf{h})$ is the variogram, but in practice we use the semi-variogram, $\\gamma_z(\\mathbf{h})$. We use the semi-variogram because our points come in pairs, and the semi-variance is equivalent to the variance per point at a given lag.\n\n-   When the variance of the difference $Z(\\mathbf{s}) - Z(\\mathbf{t})$ is relatively small, then $Z(\\mathbf{s})$ and $Z(\\mathbf{t})$ are similar (spatially correlated).\n\n-   When the variance of the difference $Z(\\mathbf{s}) - Z(\\mathbf{t})$ is relatively large, then $Z(\\mathbf{s})$ and $Z(\\mathbf{t})$ are less similar (closer to independence).\n\nThe variogram is a function of the underlying geostatistical process $Z$. In practice, we only have access to $m$ realisations of this process, and therefore we have to estimate the variogram. This is known as the empirical variogram.\n\nWe obtain this by computing the semi-variance for all possible pairs of observations: $\\gamma(\\mathbf{s}, \\mathbf{t}) = 0.5(Z(\\mathbf{s}) - Z(\\mathbf{t}))^2$.\n\n::: {.callout-note icon=\"false\"}\n## {{< bi box color=#22697a >}} Example\n\nTo illustrate how an empirical variogram is computed, consider the biomass density of the Pacific Cod in 2017 for the two highlighted locations below.\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2_practical_4_files/figure-pdf/unnamed-chunk-49-1.pdf){fig-align='center'}\n:::\n:::\n\n1.  We can first compute the distance between the two locations using the standard Euclidean distance formula as\n\n$$h = \\sqrt{(441.6 -481)^2 + (5743.4-5748.2)^2} \\approx  39 ~\\text{Km}$$\n\n2.  Next, we compute the semi-variance between the points using their observed values as $$\\gamma(\\mathbf{s}, \\mathbf{t}) = 0.5(Z(\\mathbf{s}) - Z(\\mathbf{t}))^2 = 0.5(149.5 - 40.64)^2 = 5925.25$$\n\n3.  We repeat this process for every possible pair of points, and plot $h$ against $\\gamma(\\mathbf{s}, \\mathbf{t})$ for each.\n:::\n\nTo make the variogram easier to use and interpret, we divide the distances into a set of discrete bins, and compute the average semi-variance in each. We compute this binned empirical variogram as:\n\n$$\\gamma(\\mathbf{h}) = \\frac{1}{2N(h_k)}\\sum_{(\\mathbf{s},\\mathbf{t}) \\in N(h_k)}[z(\\mathbf{s}) - z(\\mathbf{t})]^2$$\n\nWe can calculate the binned- empirical variogram for the data using `variogram` function from the `gstat` library. This plot shows the semi-variances for each pair of points. Lets compute a variogram for the biomass density of the Pacific Cod in 2017:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(gstat)\n\npcod_sf_subset <- pcod_sf %>% filter(year ==2017)\n\nvgm1 <- variogram(density~1, pcod_sf_subset)\nplot(vgm1)\n```\n\n::: {.cell-output-display}\n![](day2_practical_4_files/figure-pdf/unnamed-chunk-50-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n**Assessing spatial dependence**\n\nWe can construct null envelope based on permutations of the data values across the locations, i.e. envelopes built under the assumption of no spatial correlation. By overlapping these envelopes with the empirical variograms we can determine whether there is some spatial dependence in our data ,e.g. if our observed variograms falls outside of the envelopes constructed under spatial randomness.\n\nWe can construct permutation envelopes on the gstat empirical variogram using the `envelope` function from the `variosig` R package. Then we can visualize the results using the `envplot` function:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(variosig)\n\nvarioEnv <- envelope(vgm1,\n                     data = pcod_sf_subset,\n                     locations = st_coordinates(pcod_sf_subset),\n                     formula = density ~ 1,\n                     nsim = 499)\n\nenvplot(varioEnv)\n```\n\n::: {.cell-output-display}\n![](day2_practical_4_files/figure-pdf/unnamed-chunk-51-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"There are 1 out of 15 variogram estimates outside the 95% envelope.\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\nIn this practical we will:\n\n-   Explore tools for spatial point pattern data wrangling and visualization.\n\nFirst, lets load some useful libraries:\n\n::: {.cell}\n\n```{.r .cell-code}\n# For plotting\nlibrary(ggplot2)\nlibrary(scico) # for colouring palettes\n\n# Data manipulation\nlibrary(dplyr)\n```\n:::\n\n## Spatial Point processes data\n\nIn point processes we measure the locations where events occur and the coordinates of such occurrences are our data.\n\nPoint process models are probabilistic models that describe the likelihood of *patterns* of points that represent the random location of some event. A spatial point process is a set of locations that have been generated by some form of stochastic (random) mechanism. In other words, the point process is a random variable operating in continuous space, and we observe realisations of this variable as point patterns across space (and/or time).\n\nConsider a fixed geographical region $A$. The set of locations at which events occur are denoted $\\mathbf{s} = s_1,\\ldots,s_n$. We let $N(A)$ be the random variable which represents the number of events in region $A$.\n\nOur primary interest is in measuring where events occur, so the locations are our data. We typically assume that a spatial point pattern is generated by an unique point process over the whole study area. This means that the delimitation of the study area will affect the observed point patters.\n\nThe observed distribution of points can be described based on the intensity of points within a delimited region. We can define the (first order) intensity of a point process as the expected number of events per unit area. This can also be thought of as a measure of the density of our points. In some cases, the intensity will be constant over space (homogeneous), while in other cases it can vary by location (inhomogeneous or heterogenous).\n\nIn the next example, we will explore tools for visualizing spatial point patterns. Specifically, we will map the spatial distribution of the Ringlet butterfly in Scotland's Cairngorms National Park (CNP).\n\n### BNM citizen science program\n\nCitizen science initiatives have become an important source of information in ecological research, offering large volumes of species distribution data collected by volunters for multiple taxonomic groups across wide spatial and temporal scales\n\nButterflies for the New Millennium (BNM) is a large-scale monitoring scheme launched in the earlies 70's to keep track of butterflies' populations in the UK. With over 12 million butterflies sighting and more than 10,000 volunteers, this recording scheme has proven to be a successful program that has been used to assess long-term changes in the distributions of UK butterfly species.\n\nHere we will focus on the distribution of the Ringlet butterfly species, which holds particular significance in environmental studies as one of the *Habitat specialists* species (UK Government, 2024). The data set consists of Ringlet butterfly presence-only records collected by volunteers in Scotland's Cairngorms National Park (CNP).\n\n**Reading shapefiles into R**\n\nFirst, we load the geographical region of interest which can be downloaded [here](https://maps.gov.scot/ATOM/shapefiles/SG_CairngormsNationalPark_2010.zip) (i.e., CNP boundaries). We can use thre `st_read` function from the `sf` library to load the `.shp` file by specifying the directory where you downloaded the files:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sf)\nshp_SGC <-  st_read(\"datasets/SG_CairngormsNationalPark/SG_CairngormsNationalPark_2010.shp\",quiet =T)\n```\n:::\n\nThen, we can use appropriate CRS for the UK (i.e., EPSG code: 27700) :\n\n::: {.cell}\n\n```{.r .cell-code}\nshp_SGC <- shp_SGC %>% st_transform(crs = 27700)\nst_crs(shp_SGC)$units\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"m\"\n```\n\n\n:::\n:::\n\nNotice that the spatial resolution is in meters. Let's change the spatial units to *km* to make resulting distance/area values more intuitive to interpret:\n\n::: {.cell}\n\n```{.r .cell-code}\nshp_SGC <- st_transform(shp_SGC,gsub(\"units=m\",\"units=km\",st_crs(shp_SGC)$proj4string)) \nst_crs(shp_SGC)$units\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"km\"\n```\n\n\n:::\n:::\n\nWe can then plot the CNP boundary as follows:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot()+\n  geom_sf(data=shp_SGC)\n```\n\n::: {.cell-output-display}\n![](day2_practical_4_files/figure-pdf/unnamed-chunk-74-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n**Creating sf spatial objects in R**\n\nNow we will read the Ringlet butterfly records which can be downloaded below:\n\n {{< downloadthis datasets/bnm_ringlett.csv dname=\"bnm_ringlett\" label=\"Download data set\" icon=\"database-fill-down\" type=\"info\" >}}\n\n::: {.cell}\n\n```{.r .cell-code}\nringlett <- read.csv(\"datasets/bnm_ringlett.csv\")\nhead(ringlett)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         y         x\n1 57.58752 -2.712498\n2 54.97742 -3.274879\n3 54.89929 -3.771451\n4 55.40323 -5.737059\n5 54.91438 -3.959336\n6 55.87255 -4.167174\n```\n\n\n:::\n:::\n\nThe data set contains the longitude latitude where an observation was made. We can convert this into a spatial `sf` object using the `st_as_sf` function by declaring the columns in our data that contain the spatial coordinates:\n\n::: {.cell}\n\n```{.r .cell-code}\nringlett_sf <- ringlett %>% st_as_sf(coords = c(\"x\",\"y\"),crs = \"+proj=longlat +datum=WGS84\") \n```\n:::\n\n\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nWe have set standard WGS84 coordinates for the  Ringlet butterfly occurrence records. Set the CRS to match the CRS used in shapefile. Then, produce a map of the CNP region with the projected observations overlayed.\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nYou can use the `st_transform()` function to change the coordinates of an `sf` object (type `?st_transform` for more details)/\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nringlett_sf <- ringlett_sf %>%\n  st_transform(st_crs(shp_SGC))\n\nggplot()+\n  geom_sf(data=shp_SGC)+\n  geom_sf(data=ringlett_sf)\n```\n\n::: {.cell-output-display}\n![](day2_practical_4_files/figure-pdf/unnamed-chunk-77-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n\n</div>\n:::\n:::\n\n\nWe can subset two `sf` objects with the **same** CRS in the same way as we subset a data frame in R. For example, if we want to subset the Ringlet butterfly occurrence records to those contained only within the CNP, we can type the following:\n\n::: {.cell}\n\n```{.r .cell-code}\nringlett_CNP <- ringlett_sf[shp_SGC,] # crop to mainland\n```\n:::\n\nIf we  plot the `ringlett_CNP` object along with the CNP boundary, we should then obtain a map of the occurrence records within the park:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot()+\n  geom_sf(data=shp_SGC)+\n  geom_sf(data=ringlett_CNP)\n```\n\n::: {.cell-output-display}\n![](day2_practical_4_files/figure-pdf/unnamed-chunk-79-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n**Reading Raster Data**\n\nWe can use the `terra` R package to read raster files. The `Scotland_elev.tiff` raster contains the output of a digital elevation model for Scotland:\n\n{{< downloadthis datasets/Scotland_elev.tiff dname=\"Scotland_elev.tiff\" label=\"Download raster data\" icon=\"database-fill-down\" type=\"info\" >}}\n\nOnce you download the raster you can read it using the `rast` function after specifying the path where the file has been stored. Then, we assign the same CRS as our data.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(terra)\nelevation_r <- rast(\"datasets/Scotland_elev.tiff\")\ncrs(elevation_r) = crs(shp_SGC)\nplot(elevation_r)\n```\n\n::: {.cell-output-display}\n![](day2_practical_4_files/figure-pdf/unnamed-chunk-80-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\nWe can apply different R functions to our rasters. For example, we can scale the elevation values as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\nelevation_r <- elevation_r %>% scale()\n```\n:::\n\n\nLastly, we can crop the raster to the boundaries of our region of interest. Let's crop the elevation raster to the CNP area using the `crop` function:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nelev_CNP <- terra::crop(elevation_r,shp_SGC,mask=T)\nplot(elev_CNP)\n```\n\n::: {.cell-output-display}\n![](day2_practical_4_files/figure-pdf/unnamed-chunk-82-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nUsing `tidyterra` and `ggplot`, produce a map of the elevation profile in the CNP and overlay the spatial point pattern of the Ringlet butterfly occurrence records. Use an appropriate colouring scheme for the elevation values. Do you see any pattern?\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nYou can use the `geom_spatraster()` to add a raster layer to a ggplot object. Furthermore the `scico` library contains a nice range of coloring palettes you can choose, type `scico_palette_show()` to see the color palettes that are available.\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(scico)\n\nggplot()+ \n  tidyterra::geom_spatraster(data=elev_CNP)+\n  geom_sf(data=ringlett_CNP)+\n  scale_fill_scico(name = \"Elevation scaled\",\n                   palette = \"devon\",\n                   na.value = \"transparent\" )\n```\n\n::: {.cell-output-display}\n![](day2_practical_4_files/figure-pdf/unnamed-chunk-83-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n\n</div>\n:::\n:::\n\n",
    "supporting": [
      "day2_practical_4_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}