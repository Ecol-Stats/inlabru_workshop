{
  "hash": "c59ad35b81a0d2020cd39903676f9488",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Practical 1\"\nexecute: \n  freeze: true\nformat: \n  html:\n    mainfont: \"12\"\n  PrettyPDF-pdf:\n    keep-tex: true\n    number-sections: true\nembed-resources: true\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n<font size=\"5\"> **Aim of this practical:** </font>\n\nIn this first practical we are going to look at some simple models\n\n1.  A Gaussian model with simulated data\n2.  A Linear mixed model\n3.  A GLM model with random effects\n\nwe are going to learn:\n\n-   How to fit some commonly used models with `inlabru`\n-   How to explore the results\n-   How to get predictions for missing data points\n\n\n\n\n\n\n{{< downloadthis day1_practical.R dname=\"practical_1.R\" label=\"Download Practical 1 R script\" icon=\"database-fill-down\" type=\"success\" >}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Linear Model {#sec-linmodel}\n\nIn this practical we will:\n\n-   Simulate Gaussian data\n-   Learn how to fit a linear model with `inlabru`\n-   Generate predictions from the model\n\nStart by loading useful libraries:\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Load libraries\"}\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n# load some libraries to generate nice plots\nlibrary(scico)\n```\n:::\n\nAs our first example we consider a simple linear regression model with Gaussian observations $$\ny_i\\sim\\mathcal{N}(\\mu_i, \\sigma^2), \\qquad i = 1,\\dots,N\n$$\n\nwhere $\\sigma^2$ is the observation error, and the mean parameter $\\mu_i$ is linked to the **linear predictor** ($\\eta_i$) through an identity function: $$\n\\eta_i = \\mu_i = \\beta_0 + \\beta_1 x_i\n$$ where $x_i$ is a covariate and $\\beta_0, \\beta_1$ are parameters to be estimated. We assign $\\beta_0$ and $\\beta_1$ a vague Gaussian prior.\n\nTo finalize the Bayesian model we assign a $\\text{Gamma}(a,b)$ prior to the precision parameter $\\tau = 1/\\sigma^2$ and two independent Gaussian priors with mean $0$ and precision $\\tau_{\\beta}$ to the regression parameters $\\beta_0$ and $\\beta_1$ (we will use the default prior settings in INLA for now).\n\n::: {.callout-tip icon=\"false\"}\n## {{< bi question-octagon color=#6dc83c >}} Question\n\nWhat is the dimension of the hyperparameter vector and latent Gaussian field?\n\n\n<div class='webex-solution'><button>Answer</button>\n\n\nThe hyperparameter vector has dimension 1, $\\pmb{\\theta} = (\\tau)$ while the latent Gaussian field $\\pmb{u} = (\\beta_0, \\beta_1)$ has dimension 2, $0$ mean, and sparse precision matrix:\n\n$$\n\\pmb{Q} = \\begin{bmatrix}\n\\tau_{\\beta_0} & 0\\\\\n0 & \\tau_{\\beta_1}\n\\end{bmatrix}\n$$ Note that, since $\\beta_0$ and $\\beta_1$ are fixed effects, the precision parameters $\\tau_{\\beta_0}$ and $\\tau_{\\beta_1}$ are fixed.\n\n\n</div>\n\n:::\n\n::: callout-note\nWe can write the linear predictor vector $\\pmb{\\eta} = (\\eta_1,\\dots,\\eta_N)$ as\n\n$$\n\\pmb{\\eta} = \\pmb{A}\\pmb{u} = \\pmb{A}_1\\pmb{u}_1 + \\pmb{A}_2\\pmb{u}_2 = \\begin{bmatrix}\n1 \\\\\n1\\\\\n\\vdots\\\\\n1\n\\end{bmatrix} \\beta_0 + \\begin{bmatrix}\nx_1 \\\\\nx_2\\\\\n\\vdots\\\\\nx_N\n\\end{bmatrix} \\beta_1\n$$\n\nOur linear predictor consists then of two components: an intercept and a slope.\n:::\n\n### Simulate example data\n\nFirst, we simulate data from the model\n\n$$\ny_i\\sim\\mathcal{N}(\\eta_i,0.1^2), \\ i = 1,\\dots,100\n$$\n\nwith\n\n$$\n\\eta_i = \\beta_0 + \\beta_1 x_i\n$$\n\nwhere $\\beta_0 = 2$, $\\beta_1 = 0.5$ and the values of the covariate $x$ are generated from an Uniform(0,1) distribution. The simulated response and covariate data are then saved in a `data.frame` object.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\" code-summary=\"Simulate Data from a LM\"}\nbeta = c(2,0.5)\nsd_error = 0.1\n\nn = 100\nx = rnorm(n)\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error)\n\ndf = data.frame(y = y, x = x)  \n```\n:::\n\n### Fitting a linear regression model with `inlabru`\n\n------------------------------------------------------------------------\n\n**Defining model components**\n\nThe model has two parameters to be estimated $\\beta_1$ and $\\beta_2$. We need to define the two corresponding model components:\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Define LM components\"}\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n```\n:::\n\nThe `cmp` object is here used to define model components. We can give them any useful names we like, in this case, `beta_0` and `beta_1`.\n\n::: callout-note\nNote that we have excluded the default Intercept term in the model by typing `-1` in the model components. However, `inlabru` has automatic intercept that can be called by typing `Intercept()` , which is one of `inlabru` special names and it is used to define a global intercept, e.g.\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp =  ~  Intercept(1) + beta_1(x, model = \"linear\")\n```\n:::\n:::\n\n**Observation model construction**\n\nThe next step is to construct the observation model by defining the model likelihood. The most important inputs here are the `formula`, the `family` and the `data`.\n\nThe `formula` defines how the components should be combined in order to define the model predictor.\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Define LM formula\"}\nformula = y ~ beta_0 + beta_1\n```\n:::\n\n::: callout-note\nIn this case we can also use the shortcut `formula = y ~ .`. This will tell `inlabru` that the model is linear and that it is not necessary to linearize the model and assess convergence.\n:::\n\nThe likelihood is defined using the `bru_obs()` function as follows:\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Define Observational model\"}\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n```\n:::\n\n**Fit the model**\n\nWe fit the model using the `bru()` functions which takes as input the components and the observation model:\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Fit LM in `inlabru`\"}\nfit.lm = bru(cmp, lik)\n```\n:::\n\n**Extract results**\n\nThe `summary()` function will give access to some basic information about model fit and estimates\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Model summaries\"}\nsummary(fit.lm)\n## inlabru version: 2.13.0\n## INLA version: 25.08.21-1\n## Components:\n## beta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\n## beta_1: main = linear(x), group = exchangeable(1L), replicate = iid(1L), NULL\n## Observation models:\n##   Family: 'gaussian'\n##     Tag: <No tag>\n##     Data class: 'data.frame'\n##     Response class: 'numeric'\n##     Predictor: y ~ .\n##     Additive/Linear: TRUE/TRUE\n##     Used components: effects[beta_0, beta_1], latent[]\n## Time used:\n##     Pre = 0.558, Running = 0.286, Post = 0.174, Total = 1.02 \n## Fixed effects:\n##         mean   sd 0.025quant 0.5quant 0.975quant  mode kld\n## beta_0 1.998 0.01      1.978    1.998      2.019 1.998   0\n## beta_1 0.511 0.01      0.491    0.511      0.531 0.511   0\n## \n## Model hyperparameters:\n##                                          mean    sd 0.025quant 0.5quant\n## Precision for the Gaussian observations 96.88 13.70      71.93    96.24\n##                                         0.975quant  mode\n## Precision for the Gaussian observations     125.54 94.95\n## \n## Marginal log-Likelihood:  64.33 \n##  is computed \n## Posterior summaries for the linear predictor and the fitted values are computed\n## (Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n```\n:::\n\nWe can see that both the intercept and slope and the error precision are correctly estimated.\n\n### Generate model predictions\n\n------------------------------------------------------------------------\n\nNow we can take the fitted `bru` object and use the `predict` function to produce predictions for $\\mu$ given a new set of values for the model covariates or the original values used for the model fit\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_data = data.frame(x = c(df$x, runif(10)),\n                      y = c(df$y, rep(NA,10)))\npred = predict(fit.lm, new_data, ~ beta_0 + beta_1,\n               n.samples = 1000)\n```\n:::\n\nThe `predict` function generate samples from the fitted model. In this case we set the number of samples to 1000.\n\n::: panel-tabset\n## Plot\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Data and 95% credible intervals](day1_practical_files/figure-pdf/unnamed-chunk-11-1.pdf){fig-align='center'}\n:::\n:::\n\n## R Code\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\npred %>% ggplot() + \n  geom_point(aes(x,y), alpha = 0.3) +\n  geom_line(aes(x,mean)) +\n  geom_line(aes(x, q0.025), linetype = \"dashed\")+\n  geom_line(aes(x, q0.975), linetype = \"dashed\")+\n  xlab(\"Covariate\") + ylab(\"Observations\")\n```\n:::\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nGenerate predictions for a new observation with $x_0 = 0.45$\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nYou can create a new data frame containing the new observation $x_0$ and then use the `predict` function.\n\n\n</div>\n\n\n::: {.cell webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nnew_data = data.frame(x = 0.45)\npred = predict(fit.lm, new_data, ~ beta_0 + beta_1,\n               n.samples = 1000)\n```\n\n\n</div>\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Linear Mixed Model\n\nIn this practical we will:\n\n-   Understand the basic structure of a Linear Mixed Model (LLM)\n-   Simulate data from a LMM\n-   Learn how to fit a LMM with `inlabru` and predict from the model.\n\nConsider the a simple linear regression model except with the addition that the data that comes in groups. Suppose that we want to include a random effect for each group $j$ (equivalent to adding a group random intercept). The model is then: $$\n y_{ij}  = \\beta_0 + \\beta_1 x_i + u_j + \\epsilon_{ij} ~~~  \\text{for}~i = 1,\\ldots,N~ \\text{and}~ j = 1,\\ldots,m.\n$$\n\nHere the random group effect is given by the variable $u_j \\sim \\mathcal{N}(0, \\tau^{-1}_u)$ with $\\tau_u = 1/\\sigma^2_u$ describing the variability between groups (i.e., how much the group means differ from the overall mean). Then, $\\epsilon_j \\sim \\mathcal{N}(0, \\tau^{-1}_\\epsilon)$ denotes the residuals of the model and $\\tau_\\epsilon = 1/\\sigma^2_\\epsilon$ captures how much individual observations deviate from their group mean (i.e., variability within group).\n\nThe model design matrix for the random effect has one row for each observation (this is equivalent to a random intercept model). The row of the design matrix associated with the $ij$-th observation consists of zeros except for the element associated with $u_j$, which has a one.\n\n$$\n\\pmb{\\eta} = \\pmb{A}\\pmb{u} = \\pmb{A}_1\\pmb{u}_1 + \\pmb{A}_2\\pmb{u}_2 + \\pmb{A}_3\\pmb{u}_3\n$$\n\n::: {.callout-note icon=\"false\"}\n## Supplementary material: LMM as a LGM\n\nIn matrix form, the linear mixed model for the *j*-th group can be written as:\n\n$$ \\overbrace{\\mathbf{y}_j}^{ N \\times 1} = \\overbrace{X_j}^{ N \\times 2} \\underbrace{\\beta}_{1\\times 1} + \\overbrace{Z_j}^{n_j \\times 1} \\underbrace{u_j}_{1\\times1} + \\overbrace{\\epsilon_j}^{n_j \\times 1}, $$\n\nIn a latent Gaussian model (LGM) formulation the mixed model predictor for the *i*-th observation can be written as :\n\n$$\n\\eta_i = \\beta_0 + \\beta_1 x_i + \\sum_k^K f_k(u_j)\n$$\n\nwhere $f_k(u_j) = u_j$ since thereâ€™s only one random effect per group (i.e., a random intercept for group $j$). The fixed effects $(\\beta_0,\\beta_1)$ are assigned Gaussian priors (e.g., $\\beta \\sim \\mathcal{N}(0,\\tau_\\beta^{-1})$). The random effects $\\mathbf{u} = (u_1,\\ldots,u_m)^T$ follow a Gaussian density $\\mathcal{N}(0,\\mathbf{Q}_u^{-1})$ where $\\mathbf{Q}_u = \\tau_u\\mathbf{I}_m$ is the precision matrix for the random intercepts. Then, the components for the LGM are the following:\n\n-   Latent field given by\n\n    $$\n    \\begin{bmatrix} \\beta \\\\\\mathbf{u} \n    \\end{bmatrix} \\sim \\mathcal{N}\\left(\\mathbf{0},\\begin{bmatrix}\\tau_\\beta^{-1}\\mathbf{I}_2&\\mathbf{0}\\\\\\mathbf{0} &\\tau_u^{-1}\\mathbf{I}_m\\end{bmatrix}\\right)\n    $$\n\n-   Likelihood:\n\n    $$\n    y_i \\sim \\mathcal{N}(\\eta_i,\\tau_{\\epsilon}^{-1})\n    $$\n\n-   Hyperparameters:\n\n    -   $\\tau_u\\sim\\mathrm{Gamma}(a,b)$\n    -   $\\tau_\\epsilon \\sim \\mathrm{Gamma}(c,d)$\n:::\n\n### **Simulate example data**\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Simulate data from a LMM\"}\nset.seed(12)\nbeta = c(1.5,1)\nsd_error = 1\ntau_group = 1\n\nn = 100\nn.groups = 5\nx = rnorm(n)\nv = rnorm(n.groups, sd = tau_group^{-1/2})\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error) +\n  rep(v, each = 20)\n\ndf = data.frame(y = y, x = x, j = rep(1:5, each = 20))  \n```\n:::\n\nNote that `inlabru` expects an integer indexing variable to label the groups.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(df) +\n  geom_point(aes(x = x, colour = factor(j), y = y)) +\n  theme_classic() +\n  scale_colour_discrete(\"Group\")\n```\n\n::: {.cell-output-display}\n![Data for the linear mixed model example with 5 groups](day1_practical_files/figure-pdf/plot_data_lmm-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n### Fitting a LMM in `inlabru`\n\n------------------------------------------------------------------------\n\n**Defining model components and observational model**\n\nIn order to specify this model we must use the `group` argument to tell `inlabru` which variable indexes the groups. The `model = \"iid\"` tells INLA that the groups are independent from one another.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define model components\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\") +\n  u(j, model = \"iid\")\n```\n:::\n\nThe group variable is indexed by column `j` in the dataset. We have chosen to name this component `v()` to connect with the mathematical notation that we used above.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Construct likelihood\nlik =  like(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `like()` was deprecated in inlabru 2.12.0.\ni Please use `bru_obs()` instead.\n```\n\n\n:::\n:::\n\n**Fitting the model**\n\nThe model can be fitted exactly as in the previous examples by using the `bru` function with the components and likelihood objects.\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Fit a LMM in inlabru\"}\nfit = bru(cmp, lik)\nsummary(fit)\n## inlabru version: 2.13.0\n## INLA version: 25.08.21-1\n## Components:\n## beta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\n## beta_1: main = linear(x), group = exchangeable(1L), replicate = iid(1L), NULL\n## u: main = iid(j), group = exchangeable(1L), replicate = iid(1L), NULL\n## Observation models:\n##   Family: 'gaussian'\n##     Tag: <No tag>\n##     Data class: 'data.frame'\n##     Response class: 'numeric'\n##     Predictor: y ~ .\n##     Additive/Linear: TRUE/TRUE\n##     Used components: effects[beta_0, beta_1, u], latent[]\n## Time used:\n##     Pre = 0.409, Running = 0.403, Post = 0.252, Total = 1.06 \n## Fixed effects:\n##         mean    sd 0.025quant 0.5quant 0.975quant  mode kld\n## beta_0 2.108 0.438      1.229    2.108      2.986 2.108   0\n## beta_1 1.172 0.120      0.936    1.172      1.407 1.172   0\n## \n## Random effects:\n##   Name\t  Model\n##     u IID model\n## \n## Model hyperparameters:\n##                                          mean    sd 0.025quant 0.5quant\n## Precision for the Gaussian observations 0.995 0.144      0.738    0.986\n## Precision for u                         1.613 1.060      0.369    1.356\n##                                         0.975quant  mode\n## Precision for the Gaussian observations       1.30 0.971\n## Precision for u                               4.35 0.918\n## \n## Marginal log-Likelihood:  -179.93 \n##  is computed \n## Posterior summaries for the linear predictor and the fitted values are computed\n## (Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n```\n:::\n\n### Model predictions\n\nTo compute model predictions we can create a `data.frame` containing a range of values of covariate where we want the response to be predicted for each group. Then we simply call the predict function while spe\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"LMM fitted values\"}\n# New data\nxpred = seq(range(x)[1], range(x)[2], length.out = 100)\nj = 1:n.groups\npred_data = expand.grid(x = xpred, j = j)\npred = predict(fit, pred_data, formula = ~ beta_0 + beta_1 + u) \n\n\npred %>%\n  ggplot(aes(x=x,y=mean,color=factor(j)))+\n  geom_line()+\n  geom_ribbon(aes(x,ymin = q0.025, ymax= q0.975,fill=factor(j)), alpha = 0.5) + \n  geom_point(data=df,aes(x=x,y=y,colour=factor(j)))+\n  facet_wrap(~j)\n```\n\n::: {.cell-output-display}\n![](day1_practical_files/figure-pdf/unnamed-chunk-31-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n::: {.callout-tip icon=\"false\"}\n## {{< bi question-octagon color=#6dc83c >}} Question\n\nSuppose that we are also interested in including random slopes into our model. Assuming intercept and slopes are independent, can your write down the linear predictor and the components of this model as a LGM?\n\n\n<div class='webex-solution'><button>Give me a hint</button>\n\n\nIn general, the mixed model predictor can decomposed as:\n\n$$ \\pmb{\\eta} = X\\beta + Z\\mathbf{u} $$\n\nWhere $X$ is a $n \\times p$ design matrix and $\\beta$ the corresponding *p*-dimensional vector of fixed effects. Then $Z$ is a $n\\times q_J$ design matrix for the $q_J$ random effects and $J$ groups; $\\mathbf{v}$ is then a $q_J \\times 1$ vector of $q$ random effects for the $J$ groups. In a latent Gaussian model (LGM) formulation this can be written as:\n\n$$ \\eta_i = \\beta_0 + \\sum\\beta_j x_{ij} + \\sum_k f(k) (u_{ij}) $$\n\n\n</div>\n\n\n\n<div class='webex-solution'><button>See Solution</button>\n\n\n-   The linear predictor is given by\n\n    $$\n    \\eta_i = \\beta_0 + \\beta_1x_i + u_{0j} + u_{1j}x_i\n    $$\n\n-   Latent field defined by:\n\n    -   $\\beta \\sim \\mathcal{N}(0,\\tau_\\beta^{-1})$\n\n    -   $\\mathbf{u}_j = \\begin{bmatrix}u_{0j} \\\\ u_{1j}\\end{bmatrix}, \\mathbf{u}_j \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{Q}_u^{-1})$ where the precision matrix is a block-diagonal matrix with entries $\\mathbf{Q}_u= \\begin{bmatrix}\\tau_{u_0} & {0} \\\\{0} & \\tau_{u_1}\\end{bmatrix}$\n\n-   The hyperparameters are then:\n\n    -   $\\tau_{u_0},\\tau_{u_1} \\text{and}~\\tau_\\epsilon$\n\nTo fit this model in `inlabru` we can simply modify the model components as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\") +\n  u0(j, model = \"iid\") + u1(j,x, model = \"iid\")\n```\n:::\n\n\n</div>\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Generalized Linear Model {#sec-genlinmodel}\n\nIn this practical we will:\n\n-   Simulate non-Gaussian data\n-   Learn how to fit a generalised linear model with `inlabru`\n-   Generate predictions from the model\n\n::: {.cell}\n\n:::\n\nA generalised linear model allows for the data likelihood to be non-Gaussian. In this example we have a discrete response variable which we model using a Poisson distribution. Thus, we assume that our data $$\ny_i \\sim \\text{Poisson}(\\lambda_i)\n$$ with rate parameter $\\lambda_i$ which, using a log link, has associated predictor $$\n\\eta_i = \\log \\lambda_i = \\beta_0 + \\beta_1 x_i \n$$ with parameters $\\beta_0$ and $\\beta_1$, and covariate $x$. This is identical in form to the predictor in @sec-linmodel. The only difference is now we must specify a different data likelihood.\n\n### **Simulate example data**\n\nThis code generates 100 samples of covariate `x` and data `y`.\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Simulate Data from a GLM\"}\nset.seed(123)\nn = 100\nbeta = c(1,1)\nx = rnorm(n)\nlambda = exp(beta[1] + beta[2] * x)\ny = rpois(n, lambda  = lambda)\ndf = data.frame(y = y, x = x)  \n```\n:::\n\n### Fitting a GLM in `inlabru`\n\n------------------------------------------------------------------------\n\n**Define model components and likelihood**\n\nSince the predictor is the same as @sec-linmodel, we can use the same component definition:\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"GLM components\"}\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n```\n:::\n\nHowever, when building the observation model likelihood we must now specify the Poisson likelihood using the `family` argument (the default link function for this family is the $\\log$ link).\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"GLM likelihood\"}\nlik =  bru_obs(formula = y ~.,\n            family = \"poisson\",\n            data = df)\n```\n:::\n\n**Fit the model**\n\nOnce the likelihood object is constructed, fitting the model is exactly the same process as in @sec-linmodel.\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Fit a GLM\"}\nfit_glm = bru(cmp, lik)\n```\n:::\n\nAnd model summaries can be viewed using\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"GLM summaries\"}\nsummary(fit_glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ninlabru version: 2.13.0\nINLA version: 25.08.21-1\nComponents:\nbeta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\nbeta_1: main = linear(x), group = exchangeable(1L), replicate = iid(1L), NULL\nObservation models:\n  Family: 'poisson'\n    Tag: <No tag>\n    Data class: 'data.frame'\n    Response class: 'integer'\n    Predictor: y ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[beta_0, beta_1], latent[]\nTime used:\n    Pre = 0.419, Running = 0.268, Post = 0.0457, Total = 0.733 \nFixed effects:\n        mean    sd 0.025quant 0.5quant 0.975quant  mode kld\nbeta_0 0.915 0.071      0.775    0.915      1.054 0.915   0\nbeta_1 1.048 0.056      0.938    1.048      1.157 1.048   0\n\nMarginal log-Likelihood:  -204.02 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n```\n\n\n:::\n:::\n\n### Generate model predictions\n\n------------------------------------------------------------------------\n\nTo generate new predictions we must provide a data frame that contains the covariate values for $x$ at which we want to predict.\n\nThis code block generates predictions for the data we used to fit the model (contained in `df$x`) as well as 10 new covariate values sampled from a uniform distribution `runif(10)`.\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Predcited values for Poisson GLM\"}\n# Define new data, set to NA the values for prediction\n\nnew_data = data.frame(x = c(df$x, runif(10)),\n                      y = c(df$y, rep(NA,10)))\n\n# Define predictor formula\npred_fml <- ~ exp(beta_0 + beta_1)\n\n# Generate predictions\npred_glm <- predict(fit_glm, new_data, pred_fml)\n```\n:::\n\nSince we used a log link (which is the default for `family = \"poisson\"`), we want to predict the exponential of the predictor. We specify this using a general `R` expression using the formula syntax.\n\n::: callout-note\nNote that the `predict` function will call the component names (i.e. the \"labels\") that were decided when defining the model.\n:::\n\nSince the component definition is looking for a covariate named $x$, all we need to provide is a data frame that contains one, and the software does the rest.\n\n::: panel-tabset\n## Plot\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Data and 95% credible intervals](day1_practical_files/figure-pdf/unnamed-chunk-49-1.pdf){fig-align='center'}\n:::\n:::\n\n## R Code\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Plot GLM predicted values\"}\npred_glm %>% ggplot() + \n  geom_point(aes(x,y), alpha = 0.3) +\n  geom_line(aes(x,mean)) +\n    geom_ribbon(aes(x = x, ymax = q0.975, ymin = q0.025),fill = \"tomato\", alpha = 0.3)+\n  xlab(\"Covariate\") + ylab(\"Observations (counts)\")\n```\n:::\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}}Task\n\nSuppose a binary response such that\n\n$$\n    \\begin{aligned}\ny_i &\\sim \\mathrm{Bernoulli}(\\psi_i)\\\\\n\\eta_i &= \\mathrm{logit}(\\psi_i) = \\alpha_0 +\\alpha_1 \\times w_i \n\\end{aligned}\n$$ Using the following simulated data, use `inlabru` to fit the logistic regression above. Then, plot the predictions for the data used to fit the model along with 10 new covariate values\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"GLM Task\"}\nset.seed(123)\nn = 100\nalpha = c(0.5,1.5)\nw = rnorm(n)\npsi = plogis(alpha[1] + alpha[2] * w)\ny = rbinom(n = n, size = 1, prob =  psi) # set size = 1 to draw binary observations\ndf_logis = data.frame(y = y, w = w)  \n```\n:::\n\nHere we use the logit link function $\\mathrm{logit}(x) = \\log\\left(\\frac{x}{1-x}\\right)$ (`plogis()` function in R) to link the linear predictor to the probabilities $\\psi$.\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nYou can set `family = \"binomial\"` for binary responses and the `plogis()` function for computing the predicted values.\n\n::: callout-note\nThe Bernoulli distribution is equivalent to a $\\mathrm{Binomial}(1, \\psi)$ pmf. If you have proportional data (e.g. no. successes/no. trials) you can specify the number of events as your response and then the number of trials via the `Ntrials = n` argument of the `bru_obs` function (where `n` is the known vector of trials in your data set).\n:::\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\n# Model components\ncmp_logis =  ~ -1 + alpha_0(1) + alpha_1(w, model = \"linear\")\n# Model likelihood\nlik_logis =  bru_obs(formula = y ~.,\n            family = \"binomial\",\n            data = df_logis)\n# fit the model\nfit_logis <- bru(cmp_logis,lik_logis)\n\n# Define data for prediction\nnew_data = data.frame(w = c(df_logis$w, runif(10)),\n                      y = c(df_logis$y, rep(NA,10)))\n# Define predictor formula\npred_fml <- ~ plogis(alpha_0 + alpha_1)\n\n# Generate predictions\npred_logis <- predict(fit_logis, new_data, pred_fml)\n\n# Plot predictions\npred_logis %>% ggplot() + \n  geom_point(aes(w,y), alpha = 0.3) +\n  geom_line(aes(w,mean)) +\n    geom_ribbon(aes(x = w, ymax = q0.975, ymin = q0.025),fill = \"tomato\", alpha = 0.3)+\n  xlab(\"Covariate\") + ylab(\"Observations\")\n```\n\n::: {.cell-output-display}\n![](day1_practical_files/figure-pdf/unnamed-chunk-52-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n\n</div>\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Generalised Additive Model {#sec-gam_ex}\n\n::: {.cell}\n\n:::\n\nIn this practical we will:\n\n-   Learn how to fit a GAM with `inlabru`\n-   Generate predictions from the model\n\nGeneralised Additive Models (GAMs) are very similar to linear models, but with an additional basis set that provides flexibility.\n\nAdditive models are a general form of statistical model which allows us to incorporate smooth functions alongside linear terms. A general expression for the linear predictor of a GAM is given by\n\n$$\n\\eta_i = g(\\mu_i) = \\beta_0 + \\sum_{j=1}^L f_j(x_{ij}) \n$$\n\nwhere the mean $\\pmb{\\mu} = E(\\mathbf{y}|\\mathbf{x}_1,\\ldots,\\mathbf{x}_L)$ and $g()$ is a link function (notice that the distribution of the response and the link between the predictors and this distribution can be quite general). The term $f_j()$ is a smooth function for the *j*-th explanatory variable that can be represented as\n\n$$\nf(x_i) = \\sum_{k=0}^q\\beta_k b_k(x_i)\n$$\n\nwhere $b_k$ denote the basis functions and $\\beta_K$ are their coefficients.\n\nIncreasing the number of basis functions leads to a more *wiggly* line. Too few basis functions might make the line too smooth, too many might lead to overfitting.To avoid this, we place further constraints on the spline coefficients which leads to constrained optimization problem where the objective function to be minimized is given by:\n\n$$\n\\mathrm{min}\\sum_i(y_i-f(x_i))^2 + \\lambda(\\sum_kb^2_k)\n$$ The first term measures how close the function $f()$ is to the data while the second term $\\lambda(\\sum_kb^2_k)$, penalizes the roughness in the function. Here, $\\lambda >0$ is known as the smoothing parameter because it controls the degree of smoothing (i.e. the trade-off between the two terms). In a Bayesian setting,including the penalty term is equivalent to setting a specific prior on the coefficients of the covariates.\n\nIn this exercise we will set a random walk prior of order 1 on $f$, i.e. $f(x_i)-f(x_i-1) \\sim \\mathcal{N}(0,\\sigma^2_f)$ where $\\sigma_f^2$ is the smoothing parameter such that small values give large smoothing. Notice that we will assume $x_i$'s are equally spaced for now (we will cover a stochastic differential equation approach that relaxes this assumption later on in the course).\n\n### Simulate Data\n\nLets generate some data so evaluate how RW models perform when estimating a smooth curve. The data are simulated from the following model:\n\n$$\ny_i = 1 + \\mathrm{cos}(x) + \\epsilon_i, ~ \\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2_\\epsilon)\n$$ where $\\sigma_\\epsilon^2 = 0.25$\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Simulate GAM Data\"}\nn = 100\nx = rnorm(n)\neta = (1 + cos(x))\ny = rnorm(n, mean =  eta, sd = 0.5)\n\ndf = data.frame(y = y, \n                x_smooth = inla.group(x)) # equidistant x's \n```\n:::\n\n### Fitting a GAM in `inlabru`\n\nNow lets fit a flexible model by setting a random walk of order 1 prior on the coefficients. This can be done bye specifying `model = \"rw1\"` in the model component (similarly,a random walk of order 2 can be placed by setting `model = \"rw2\"` )\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp =  ~ Intercept(1) + \n  smooth(x_smooth, model = \"rw1\")\n```\n:::\n\nNow we define the observational model:\n\n::: {.cell}\n\n```{.r .cell-code}\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n```\n:::\n\nWe then can fit the model:\n\n::: {.cell}\n\n```{.r .cell-code}\nfit = bru(cmp, lik)\nfit$summary.fixed\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            mean         sd 0.025quant 0.5quant 0.975quant    mode          kld\nIntercept 1.2988 0.06506729   1.171641 1.298566   1.427295 1.29857 9.965649e-09\n```\n\n\n:::\n:::\n\nThe posterior summary regarding the estimated function using RW1 can be accessed through `fit$summary.random$smooth`, the output includes the value of $x_i$ (`ID`) as well as the posterior mean, standard deviation, quantiles and mode of each $f(x_i)$. We can use this information to plot the posterior mean and associated 95% credible intervals.\n\n::: panel-tabset\n# R plot\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Smooth effect of the covariate](day1_practical_files/figure-pdf/unnamed-chunk-68-1.pdf){fig-align='center'}\n:::\n:::\n\n# R Code\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(fit$summary.random$smooth) %>% \n  ggplot() + \n  geom_ribbon(aes(ID,ymin = X0.025quant, ymax= X0.975quant), alpha = 0.5) + \n  geom_line(aes(ID,mean)) + \n  xlab(\"covariate\") + ylab(\"\")\n```\n:::\n:::\n\n### Model Predictions\n\nWe can obtain the model predictions using the `predict` function.\n\n::: {.cell}\n\n```{.r .cell-code}\npred = predict(fit, df, ~ (Intercept + smooth))\n```\n:::\n\nThe we can plot them together with the true curve and data points:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npred %>% ggplot() + \n  geom_point(aes(x_smooth,y), alpha = 0.3) +\n  geom_line(aes(x_smooth,1+cos(x_smooth)),col=2)+\n  geom_line(aes(x_smooth,mean)) +\n  geom_line(aes(x_smooth, q0.025), linetype = \"dashed\")+\n  geom_line(aes(x_smooth, q0.975), linetype = \"dashed\")+\n  xlab(\"Covariate\") + ylab(\"Observations\")\n```\n\n::: {.cell-output-display}\n![Data and 95% credible intervals](day1_practical_files/figure-pdf/plot_gam-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}}Task\n\nFit a flexible model using a random walk of order 2 (RW2) and compare the results with the ones above.\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nYou can set `model = \"rw2\"` for assigning a random walk 2 prior.\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\ncmp_rw2 =  ~ Intercept(1) + \n  smooth(x_smooth, model = \"rw2\")\nlik_rw2 =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\nfit_rw2 = bru(cmp_rw2, lik_rw2)\n\n# Plot the fitted functions\nggplot() + \n  geom_line(data= fit$summary.random$smooth,aes(ID,mean,colour=\"RW1\"),lty=2) + \n  geom_line(data= fit_rw2$summary.random$smooth,aes(ID,mean,colour=\"RW2\")) + \n  xlab(\"covariate\") + ylab(\"\") + scale_color_discrete(name=\"Model\")\n```\n\n::: {.cell-output-display}\n![](day1_practical_files/figure-pdf/unnamed-chunk-70-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n\n</div>\n:::\n\nWe see that the RW1 fit is too wiggly while the RW2 is smoother and seems to have better fit.\n:::\n",
    "supporting": [
      "day1_practical_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}