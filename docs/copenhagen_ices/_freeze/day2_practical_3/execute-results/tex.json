{
  "hash": "73208fedeb29fea15ab919a3c0eb3c98",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Practical 3\"\nexecute: \n  freeze: true\nformat: \n  html: \n    mainfont: \"12\"\n  PrettyPDF-pdf:\n    keep-tex: true\n    number-sections: true\nembed-resources: true\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n<font size=\"5\"> **Aim of this practical:** </font>\n\n1.  Fit temporal models for discrete time points using `inlabru`\n2.  Forecasting for future observations\n3.  Set penalized complexity (PC) priors\n4.  Fit temporal models to non-Gaussian data\n\nwe are going to learn:\n\n-   How to fit autoregressive and random walk models for time series data\n-   How to set PC priors in `inlabru`\n-   Forecast future observations using the posterior predictive density\n\n\n\n\n\n\n{{< downloadthis day2_practical_3.R dname=\"practical_3.R\" label=\"Download Practical 3 R script\" icon=\"database-fill-down\" type=\"success\" >}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## AR(1) models in `inlabru`\n\nIn this exercise we will:\n\n-   Simulate a time series with autocorrelated errors.\n\n-   Fit an AR(1) process with `inlabru`\n\n-   Visualize model predictions.\n\n-   Forecasting for future observations\n\nStart by loading useful libraries:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n```\n:::\n\nTime series analysis is particularly valuable for modelling data with temporal dependence or autocorrelation, where observations taken at nearby time points tend to be more similar than those further apart.\n\nA time series process is a stochastic process $\\{X_t~|~t \\in T\\}$, which is a collection of random variables that are ordered in time where $T$ is the *index set* that determines the set of discrete and equally spaced time points at which the process is defined and observations are made.\n\nAutoregressive processes allow us to account for the time dependence by regressing $X_t$ on past values $X_{t-1},\\ldots,X_{t-p}$ with associated coefficients $\\phi_k$ for each lag $k = 1,\\ldots,p$. Thus an **autoregressive process of order** $p$, denoted AR($p$) , is given by:\n\n$$\nX_t = \\phi_1 X_{t-1} + \\ldots + \\phi_p X_{t-p} + \\varepsilon_t; ~~ \\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2_e)\n$$\n\nConsider now an univariate time series $y_t$ which evolves over time according to some autoregressive stochastic process. For example, a time series where the system follows an AR(1) process can be defined as:\n\n$$\n\\begin{aligned}\ny_t &\\sim \\mathcal{N}(\\mu_t,\\tau_y^{-1})\\\\\n\\eta_t &= g^{-1}(\\mu_t) = \\alpha + u_t \\\\\nu_t &= \\phi u_{t-1} + \\delta_t ; ~~ \\delta_t \\sim \\mathcal{N}(0,\\tau_u^{-1}); ~~ t > 1 \\\\\nu_1 &= \\mathcal{N}(0,\\kappa^{-1})\\\\\n\\kappa &= \\tau_u (1-\\phi^2)\n\\end{aligned}\n$$\n\nThe response $y_t$ is assumed to be normal distributed with mean $\\alpha + u_t$ and precision error $\\tau_y$ ( here $g(\\cdot)$ is just the identity link function that maps the linear predictor to the mean of the process). Then, the process $u_t$ follows an AR(1) process where $u_1$ is drawn from a stationary normal distribution such that $\\kappa$ denotes the marginal precision for state $u_t$\n\nThe covariance matrix is then given by:\n\n$$\n\\Sigma = \\frac{\\tau^{-1}_u}{1-\\phi^2}\n\\begin{bmatrix}\n1 & \\phi & \\phi^2 & \\ldots & \\phi^{n-1}\\\\\n\\phi & 1 & \\phi & \\ldots & \\phi^{n-2} \\\\\n\\phi^2 & \\phi & 1 & \\ldots & \\phi^{n-3} \\\\\n\\phi^{n-1} & \\phi^{n-2} & \\phi^{n-3} & \\ldots & 1\n\\end{bmatrix}\n$$\n\nNotice that conditionally on $u_t$, the observed data $y_y$ are independent from$y_{t-1},y_{t-2}.y_{t-3},\\ldots$, also the conditional distribution of $u_t$ is a markov chain such that $\\pi(u_t|u_{t-1},u_{t-2},u_{t-3}) = \\pi(u_t|u_{t-1})$. Thus, each time point is only conditionally dependent on the two closest time points:\n\n$$\nu_t|\\mathbf{u}_{-t} \\sim \\mathcal{N}\\left(\\frac{\\phi}{1-\\phi^2}(u_{t-1}+u_{t+1}),\\frac{\\tau_u^{-1}}{1-\\phi^2}\\right)\n$$\n\n### Simulate example data\n\nFirst, we simulate data from the model:\n\n$$\n\\begin{aligned}\ny_t &= \\alpha + u_t + \\varepsilon_t;~ \\varepsilon_t \\sim \\mathcal{N}(0,\\tau_y^{-1})\\\\\nu_t &= \\phi y_{t-1} + \\delta_t; ~ \\delta_t \\sim \\mathcal{N}(0,\\tau_u^{-1})\n\\end{aligned}\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\nphi = 0.8\ntau_u = 10\nmarg.prec = tau_u * (1-phi^2) # ar1 in INLA is parametrized as marginal variance\nu_t =  as.vector(arima.sim(list(order = c(1,0,0), ar = phi), \n                          n = 100,\n                          sd=sqrt(1/tau_u)))\na = 1\ntau_e = 5\nepsilon_t = rnorm(100, sd = sqrt(1/tau_e))\ny = a + u_t + epsilon_t\n\n\nts_dat <- data.frame(y =y , x= 1:100)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2_practical_3_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-align='center'}\n:::\n:::\n\n### Fitting an AR(1) model with `inlabru`\n\n**Model components**\n\nFirst, we define the model components, notice that the latent field is defined by two components: the intercept $\\alpha$ and the autoregressive random effects $u_t$:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(x,model = \"ar1\")\n```\n:::\n\nThe we can define the formula for the linear predictor and specify the observational model\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model formula\nformula = y ~ alpha + ut\n# Observational model\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = ts_dat)\n```\n:::\n\nLastly, we fit the model using the `bru` function and compare the model estimates with the true mdeol parameters we simulate our data from.\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit the model\nfit.ar1 = bru(cmp, lik)\n\n# compare against the true values\n\ndata.frame(\n  true = c(a,tau_e,marg.prec,phi),\n  rbind(fit.ar1$summary.fixed[,c(1,3,5)],\n        fit.ar1$summary.hyperpar[,c(1,3,5)])\n        ) %>% round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                        true mean X0.025quant X0.975quant\nalpha                                    1.0 1.00        0.69        1.28\nPrecision for the Gaussian observations  5.0 4.91        3.11        7.29\nPrecision for ut                         3.6 7.96        2.91       18.10\nRho for ut                               0.8 0.80        0.54        0.94\n```\n\n\n:::\n:::\n\n**Model predictions**\n\nHere, we will predict the mean of our time series along with 95% credible intervals. Note that this interval are for the mean and not for new observations, we will cover forecasting new observations next.\n\n::: {.cell}\n\n```{.r .cell-code}\npred_ar1 = predict(fit.ar1, ts_dat, ~ alpha + ut)\n\nggplot(pred_ar1,aes(y=mean,x=x))+\n  geom_line()+\n    geom_ribbon(aes(x = x, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(aes(y=y,x=x))\n```\n\n::: {.cell-output-display}\n![](day2_practical_3_files/figure-pdf/unnamed-chunk-8-1.pdf){fig-pos='H'}\n:::\n:::\n\n### **Forecasting**\n\nA common goal in time series modelling is forecasting into the future. Forecasting can be treated as a missing data problem where future values of the response variable are missing. Let $y_m$ be the missing response, then, by fitting a statistical model to the observed data $\\mathbf{y}_{obs}$, we condition on its parameters to obtain the posterior predictive distribution:\n\n$$\n\\pi(y_{m} \\mid \\mathbf{y}_{obs}) = \\int \\pi(y_{m}, \\theta \\mid \\mathbf{y}_{obs})  d\\theta = \\int \\pi(y_{m} \\mid \\mathbf{y}_{obs}, \\theta) \\pi(\\theta \\mid \\mathbf{y}_{obs})  d\\theta\n$$\n\nThis distribution, which integrates over all parameter uncertainty, provides the complete probabilistic forecast for the missing values. `INLA` will automatically compute the predictive distributions for all missing values in the response. To do so, we can augment our data set by including the new time points at which the prediction will be made and setting the response value to `NA` for these new time points:\n\n::: {.cell}\n\n```{.r .cell-code}\nts.forecast <- rbind(ts_dat, \n  data.frame(y = rep(NA, 50), x = 101:150))\n```\n:::\n\nNext, we fit the `ar1` model to the new dataset so that the predictive distributions are computed:\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp =  ~ -1 + alpha(1) + ut(x,model = \"ar1\")\n\npred_lik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = ts.forecast)\n\nfit.forecast = bru(cmp, pred_lik)\n```\n:::\n\nLastly, we can draw samples from the posterior predictive distribution using the `predict` function and visualize our forecast as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\npred_forecast = predict(fit.forecast, ts.forecast, ~ alpha + ut)\n\np1= ggplot(pred_forecast,aes(y=mean,x=x))+\n  geom_line()+\n    geom_ribbon(aes(x = x, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(data=ts_dat, aes(y=y,x=x))\n```\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Modelling Great Lakes water level\n\nIn this exercise we will:\n\n-   Fit an AR(1) process with `inlabru` to model lakes water levels\n\n-   Change the default priors for the observational error\n\n-   Set penalized complexity priors for the correlation and precision parameters of the latent effects.\n\n-   Fit a RW(1) model\n\n-   Fit a an AR(1) model with group-level correlation\n\n\nStart by loading useful libraries:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) \nlibrary(INLA) \nlibrary(ggplot2)\nlibrary(patchwork) \nlibrary(inlabru)\nlibrary(DAAG)\n```\n:::\n\nIn this exercise we will look at [greatLakes](https://rdrr.io/cran/DAAG/man/greatLakes.html) dataset from the `DAAG` package. The data set contains the water level heights for the lakes Erie, Michigan/Huron, Ontario and St Clair from 1918 to 2009.\n\nLets begin by loading and formatting the data into a tidy format.\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"greatLakes\")\n\ngreatLakes.df = data.frame(as.matrix(greatLakes),\n                           year = time(greatLakes)) %>%\n  pivot_longer(cols = c(\"Erie\",\"michHuron\",\"Ontario\",\"StClair\"),\n               names_to = \"Lakes\",\n               values_to = \"height\" ) \n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2_practical_3_files/figure-pdf/unnamed-chunk-25-1.pdf){fig-align='center'}\n:::\n:::\n\n### Fitting an AR(1) model in `inlabru`\n\nWe will focus on the Erin lake for now. Lets begin by fitting an AR(1) model of the form:\n\n$$\n\\begin{aligned}\n\\text{height}_t &= \\alpha + u_t +\\varepsilon_t~; ~~ \\varepsilon_t\\sim \\mathcal{N}(0,\\tau_e^{-1}) \\\\\nu_t &= \\phi u_{t-1} + \\delta_t~~~ ; ~~ \\delta_t \\sim \\mathcal{N}(0,\\tau_u^{-1}); ~~ t > 1 \\\\\nx_1 &= \\mathcal{N}(0,\\kappa^{-1})\\\\\n\\kappa &= \\tau_u (1-\\phi^2)\n\\end{aligned}\n$$\n\nWhere $\\alpha$ is the intercept, $\\phi$ is the correlation term, $\\varepsilon$ is the observational Gaussian error with mean zero and precision $\\tau_e$ and $\\kappa$ is the marginal precision for the state $u_t$ for $t= 1,\\ldots,92$.\n\nFirst we make a subset of the dataset and create a time index $T$:\n\n::: {.cell}\n\n```{.r .cell-code}\ngreatLakes.df$t.idx <- greatLakes.df$year-1917\n\nErie.df = greatLakes.df %>% filter(Lakes == \"Erie\")\n```\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nFit an AR(1) model to the Erie lake data using `inlabru`, then plot the model fitted values showing 95% credible intervals.\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nRemember this is done by (1) defining the model components, (2) the formula and (3) the observational model. Then you can use the `predict` function to compute the predicted values for the mean along with 95% credible intervals.\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(t.idx,model = \"ar1\")\n# Model formula\nformula = height ~ alpha + ut\n\n\n# Observational model\nlik =  bru_obs(formula = height   ~.,\n            family = \"gaussian\",\n            data = Erie.df )\n\n# fit the model\nfit.Erie_ar1 = bru(cmp, lik)\n\n# Model predictions \n\npred_ar1.Erie = predict(fit.Erie_ar1, Erie.df, ~ alpha + ut)\n\n# plot model fitted values\nggplot(pred_ar1.Erie,aes(y=mean,x=year))+\n  geom_line()+\n    geom_ribbon(aes(x = year, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(aes(y=height,x=year))\n```\n\n::: {.cell-output-display}\n![](day2_practical_3_files/figure-pdf/unnamed-chunk-27-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n\n</div>\n:::\n:::\n\n::: {.callout-tip icon=\"false\"}\n## {{< bi question-octagon color=#6dc83c >}} Question\n\nAre there any issues with the fitted model, and if so, how do you think we should address them?\n\n\n<div class='webex-solution'><button>Answer</button>\n\n\nIt is clear that the model overfits the data, leading to poor predictive performance. Thus, we need to introduce some prior information on the what we expect the variation of the process to be.\n\n\n</div>\n\n:::\n\n**Priors**\n\nLet review INLA' prior parametrization for autoregressive models:\n\nLet $\\pmb{\\theta} = \\{\\theta_y,\\theta_u,\\theta_\\phi\\}$ be INLA's internal representation of the hyperparameters such that:\n\n$\\theta_y = \\log(\\tau^2_y)$\n\n$\\theta_u = \\log(\\kappa) = \\log\\left(\\tau_u[1-\\phi^2]\\right)$\n\n$\\theta_\\phi = \\log \\left(\\frac{1+\\phi}{1-\\phi}\\right)$\n\nThe default priors for $\\{\\theta_y,\\theta_u\\}$ are $\\text{log-gamma} (1, 5\\times 10^{-5} )$ priors with default initial values set to 4 in each case. Then, Gaussian priors $\\alpha \\sim \\mathcal{N}(0,\\tau_y = 0.001)$ and $\\theta_\\phi \\sim \\mathcal{N}(0, \\tau_y= 0.15)$ are used for the intercept and correlation parameter respectively.\n\n::: callout-note\nSpecifically for AR(1) correlation parameter $\\phi$, INLA uses the following logit transformation on $\\theta_\\phi$:\n\n$$ \\phi = \\frac{2\\exp(\\theta_\\phi)}{1+ \\exp(\\theta_\\phi)} -1. $$\n:::\n\n**Setting priors and PC-priors**\n\nLets now set a Gamma prior with parameters 1 and 1, so that the precision of the Gaussian osbervational error is centered at 1 with a variance of 1. Additionally we will set Penalized Complexity (PC) priors according to the following probability statements:\n\n-   $P(\\sigma > 1) = 0.01$\n\n-   $P(\\phi > 0.5) = 0.3$\n\nNotice that the PC prior for the precision $\\tau_u$ is defined on the standard deviation $\\sigma_u = \\tau_u^{-1/2}$\n\n::: {.cell}\n\n```{.r .cell-code}\npc_prior <- list(theta = list(prior = \"pc.prec\", param = c(1, 0.01)),\n                 rho = list(prior = \"pc.cor0\", param = c(0.5, 0.3))) \n\nprec.tau_e <- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(1, 1))) # prior values\n\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(t.idx, model = \"ar1\",  hyper = pc_prior)\n# Model formula\nformula = height ~ alpha + ut\n\n\n# Observational model\nlik =  bru_obs(formula = height  ~.,\n            family = \"gaussian\",\n            data = Erie.df,\n            control.family = list(hyper = prec.tau_e))\n\n# fit the model\nfit.Erie_ar1 = bru(cmp, lik)\n```\n:::\n\n::: {.callout-tip icon=\"false\"}\n## {{< bi question-octagon color=#6dc83c >}} Question\n\nWhat is the posterior mean for the correlation parameter $\\rho$? ________________\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nPlot the fitted values of the model, has the overfitting problem being alleviated?\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n::: {.cell-output-display}\n![](day2_practical_3_files/figure-pdf/unnamed-chunk-29-1.pdf){fig-align='center'}\n:::\n\n\n</div>\n:::\n:::\n\n### Fitting a RW(1) model\n\nNow we fit a random walk of order 1 to the Erie lake data:\n\n$$\n\\begin{aligned}\ny_t &= \\alpha + u_t + \\varepsilon_t, ~ t = 1,\\ldots,92 \\\\\n \\varepsilon_t & \\sim \\mathcal{N}(0,\\tau_e) \\\\\nu_t - u_{t-1} &\\sim \\mathcal{N}(0,\\tau_u),~ t = 2,\\ldots,92 \\\\\n\\end{aligned}\n$$\n\nFirs we define model priors:\n\n::: {.cell}\n\n```{.r .cell-code}\npc_prior <- list(theta = list(prior = \"pc.prec\", param = c(1, 0.01))) \n\nprec.tau_e <- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(1, 1))) # prior values\n```\n:::\n\nNow we define model components:\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp_rw =  ~ -1 + alpha(1) + \n  ut(t.idx ,\n     constr=FALSE,\n     model = \"rw1\",\n     hyper=pc_prior,\n     scale.model = TRUE)\n```\n:::\n\nNotice that we have set `scale.model = TRUE` to scale the latent effects. This is particularly important when Intrinsic Gaussian Markov random fields (IGMRFs) are used as priors (e.g., random walk models or some spatial models) for the latent effects. By defining `scale.model = TRUE`, the `rw1`-model is scaled to have a generalized variance equal to one. By scaling scaling the models we ensure that a fixed hyperprior for the precision parameter has a similar interpretation for different types of IGMRFs, making precision estimates comparable between different models. Scaling also allows estimates to be less sensitive to re-scaling covariates in the linear predictor and makes the precision invariant to changes in the shape and size of the latent effect (see Sørbye ([2014](https://www.sciencedirect.com/science/article/pii/S2211675313000407)) for further details) .\n\nWe can now fit the model with the updated components and plot the predicted values\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model formula\nformula = height ~ alpha + ut\n# Observational model\nlik =  bru_obs(formula = height  ~.,\n            family = \"gaussian\",\n            data = Erie.df,\n            control.family = list(hyper = prec.tau_e))\n# fit the model\nfit.Erie_rw1 = bru(cmp_rw, lik)\n# Model predictions\npred_rw1.Erie = predict(fit.Erie_rw1, Erie.df, ~ alpha + ut)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day2_practical_3_files/figure-pdf/unnamed-chunk-33-1.pdf){fig-align='center'}\n:::\n:::\n\n::: {.callout-tip icon=\"false\"}\n## {{< bi question-octagon color=#6dc83c >}} Question\n\nTake a look at the model summaries using the `summary` function, do you see anything odd? \n<div class='webex-solution'><button>Answer</button>\n\n\nThe intercept has zero mean and a very large variance. This is because we have not imposed a sum-to-zero constraint on the model random effects (`constr=FALSE`). Without this constraint, intrinsic models are non-identifiable. The intercept and the random effects are confounded, For example, you could add a constant value to every random effect and subtract it from the intercept without changing the model's predictions. Take for instance for any constant $c$, the following models are identical:\n\n$y_t = \\alpha + u_{t} + \\varepsilon_t$ $y_t = (\\alpha - c) + (u_{t} + c) + \\varepsilon_t$\n\nThus, you need to set `constr=FALSE` so that $\\sum_t u_t=0$ to ensure identifiability of $\\alpha$\n\n\n</div>\n\n:::\n\n\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nFit an RW(1) model to the  Erie data but now set  `constr=TRUE` to impose a sum-to-zero constraint on the random effect. Then compare your results with the unconstrained model.\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\n# Model components\ncmp_rw =  ~ -1 + alpha(1) + \n  ut(t.idx ,\n     constr=TRUE,\n     model = \"rw1\",\n     hyper=pc_prior,\n     scale.model = TRUE)\n\nfit.Erie_rw1_constr = bru(cmp_rw, lik)\n\nfit.Erie_rw1_constr$summary.fixed\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          mean        sd 0.025quant 0.5quant 0.975quant     mode          kld\nalpha 174.1381 0.0237476   174.0914 174.1381   174.1848 174.1381 1.935519e-08\n```\n\n\n:::\n\n\n</div>\n:::\n:::\n\n\n\n\n### Group-level effects\n\nNow we will model the height water levels for all four lakes by grouping the random effects. This will allow a within-lakes correlation to be included. In the next example, we allow for correlated effects using an `ar1` model for the years and `iid` random effects on the lakes. First we create a lakes id and set the priors for our model:\n\n::: {.cell}\n\n```{.r .cell-code}\ngreatLakes.df$lake_id <- as.numeric(as.factor(greatLakes.df$Lakes))\n\npc_prior <- list(theta = list(prior = \"pc.prec\", param = c(1, 0.01)),\n                 rho = list(prior = \"pc.cor0\", param = c(0.5, 0.3))) \n\nprec.tau_e <- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(1, 10))) # prior values\n```\n:::\n\nNow we define the model components. The lakes IDs that define the group are passed with parameter `group` argument and the `iid` model and other parameters are passed through the `control.group` parameter.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(year,model = \"ar1\",\n                            hyper = pc_prior,\n                            group =lake_id,\n                            control.group = \n                            list(model = \"iid\", \n                                 scale.model = TRUE))\n```\n:::\n\nWe fit the model in a similar fashion as we did before:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model formula\nformula = height ~ alpha + ut\n\n\n# Observational model\nlik =  bru_obs(formula = height  ~.,\n            family = \"gaussian\",\n            data = greatLakes.df,\n            control.family = list(hyper = prec.tau_e))\n\n# fit the model\nfit.all_lakes_ar1 = bru(cmp, lik)\n\n# Model predictions\npred_ar1.all = predict(fit.all_lakes_ar1, greatLakes.df, ~ alpha + ut)\n```\n:::\n\nLastly we can visualize group-level model predictions as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(pred_ar1.all,aes(y=mean,x=year))+\n  geom_line()+\n    geom_ribbon(aes(x = year, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(aes(y=height,x=year)) + facet_wrap(~Lakes,scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](day2_practical_3_files/figure-pdf/unnamed-chunk-38-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Non-Gaussian data\n\nIn the next example we will use the `Toyo` data set to illustrate how temporal models can be fit to non-Gaussian data. \n\nThe `Tokyo` data set available in `INLA` contains the recorded days of rain above 1 mm in Tokyo for 2 years, 1983:84. The data set contains the following variables:\n\n-   `y` : number of days with rain\n\n-   `n` : total number of days\n\n-   `time` : day of the year\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(INLA)\nlibrary(inlabru)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n\ndata(\"Tokyo\")\n```\n:::\n\nA possible observational model for these data is\n\n$$\n\\begin{aligned}\ny_t|\\eta_t & \\sim\\text{Bin}(n_t, p_t) \\\\\n\\eta_t &= \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\end{aligned}\n$$ \n\n$$\nn_t = \\left\\{\n \\begin{array}{lr}\n1, & \\text{for}\\; 29\\; \\text{February}\\\\\n2, & \\text{other days}\n\\end{array}\\right.\n$$ \n\n$$\ny_t =\n\\begin{cases}\n\\{0,1\\}, & \\text{for}\\; 29\\; \\text{February}\\\\\n\\{0,1,2\\}, & \\text{other days}\n \\end{cases}\n$$\n\nThen, the latent field is given by\n\n$$\n\\eta_t = \\beta_0 + f(\\text{time}_t)\n$$\n\n-   Where the probability of rain depends on on the day of the year $t$\n\n-   $\\beta_0$ is an intercept\n\n-   $f(\\text{time}_t)$ is a temporal model, e.g., a RW2 model (this is just a smoother).\n\nThe smoothness is controlled by a hyperparameter $\\tau_f$ . Thus, we assign a prior to $\\tau_f$ to finalize the model.\n\nWe can fit the model as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)\n```\n:::\n\n\nNotice that we have set `cyclic = TRUE` as this is a cyclic effect. Finally, we can produce model predictions in a similar fashion as we did before:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npTokyo = predict(fit, Tokyo, ~ plogis(beta0 + time_effect))\n\nggplot(data=pTokyo , aes(x= time, y= y) ) +\n  geom_point() + \n  ylab(\"\") + xlab(\"\") +\n  # Custom the Y scales:\n  scale_y_continuous(\n    # Features of the first axis\n    name = \"\",\n    # Add a second axis and specify its features\n    sec.axis = sec_axis( transform=~./2, name=\"Probability\")\n  )  + geom_line(aes(y=mean*2,x=time)) +\n  geom_ribbon(aes( ymin = q0.025*2, \n                             ymax = q0.975*2), alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](day2_practical_3_files/figure-pdf/unnamed-chunk-59-1.pdf){fig-pos='H'}\n:::\n:::\n",
    "supporting": [
      "day2_practical_3_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}