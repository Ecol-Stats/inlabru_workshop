{
  "hash": "3d418c465caa27427310050cde90134a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Practical 5\"\nformat: \n  html:\n    mainfont: \"12\"\n  PrettyPDF-pdf:\n    keep-tex: true\n    number-sections: true\nembed-resources: true\nexecute: \n  warning: false\n  message: false\n  freeze: true\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n<font size=\"5\"> **Aim of this practical:** </font>\n\nIn this first practical we are going to fit spatial modes for Areal, Geostatistical and Point-Process Data.\n\n\n\n\n\n\n{{< downloadthis day3_practical_5.R dname=\"day3_practical_5.R\" label=\"Download Practical 5 R script\" icon=\"database-fill-down\" type=\"success\" >}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n# Models for areal data \n\n\nIn this practical we are going to fit an areal model. We will: \n\n- Learn how to fit an areal model in `inlabru`\n\n- Learn how to add spatial covariates to the model\n\n- Learn how to do predictions\n\n- Learn how to simulate from the fitted model\n\n\n\n---\n\nLibraries to load:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)   \nlibrary(mapview)\n\n# load some libraries to generate nice map plots\nlibrary(scico)\n```\n:::\n\n\n\n## The data\n\n\nWe consider data on respiratory hospitalizations for Greater Glasgow and Clyde in 2007. The data are available from the `CARBayesdata` R Package:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(CARBayesdata)\n\ndata(pollutionhealthdata)\ndata(GGHB.IZ)\n```\n:::\n\nThe `pollutionhealthdata` contains the spatiotemporal data on respiratory hospitalizations, air pollution concentrations and socio-economic deprivation covariates for the 271 Intermediate Zones (IZ) that make up the Greater Glasgow and Clyde health board in Scotland. Data are provided by the [Scottish Government](http://statistics.gov.scot.) and the available variables are:\n\n-   `IZ`: unique identifier for each IZ.\n-   `year`: the year when the measurements were taken\n-   `observed`: observed numbers of hospitalizations due to respiratory disease.\n-   `expected`: expected numbers of hospitalizations due to respiratory disease computed using indirect standardisation from Scotland-wide respiratory hospitalization rates.\n-   `pm10`: Average particulate matter (less than 10 microns) concentrations.\n-   `jsa`: The percentage of working age people who are in receipt of Job Seekers Allowance\n-   `price`: Average property price (divided by 100,000).\n\nThe `GGHB.IZ` data is a Simple Features (`sf`) object containing the spatial polygon information for the set of 271 Intermediate Zones (IZ), that make up of the Greater Glasgow and Clyde health board in Scotland ( @fig-GGC ).\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Greater Glasgow and Clyde health board represented by 271 Intermediate Zones](day3_practical_5_files/figure-pdf/fig-GGC-1.pdf){#fig-GGC fig-align='center'}\n:::\n:::\n\n\nWe first merge the two dataset and select only one year of data, compute the SME  and plot the observed \n\n::: {.cell}\n\n```{.r .cell-code}\nresp_cases <- merge(GGHB.IZ %>%\n                      mutate(space = 1:dim(GGHB.IZ)[1]),\n                             pollutionhealthdata, by = \"IZ\") %>%\n  filter(year == 2007) %>%\n    mutate(SMR = observed/expected)\n\nggplot() + geom_sf(data = resp_cases, aes(fill = SMR)) + scale_fill_scico(direction = -1)\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nThen we compute the adjacency matrix using the functions `poly2nb()` and  `nb2mat()` in the `spdep` library. We then convert the adjacency matrix into the precision matrix $\\mathbf{Q}$ of the CAR model. Remember this matrix has, on the diagonal the number of e \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(spdep)\n\nW.nb <- poly2nb(GGHB.IZ,queen = TRUE)\nR <- nb2mat(W.nb, style = \"B\", zero.policy = TRUE)\n\ndiag = apply(R,1,sum)\nQ = -R\ndiag(Q) = diag\n```\n:::\n\n\n\n## The model\n\nWe fit a first model to the data where we consider a Poisson model for the  observed cases.\n\n\n**Stage 1** Model for the response\n$$\ny_i|\\eta_i\\sim\\text{Poisson}(E_i\\lambda_i)\n$$\nwhere $E_i$ are the expected cases for area $i$.\n\n**Stage 2** Latent field model\n$$\n\\eta_i = \\text{log}(\\lambda_i) = \\beta_0 + \\omega_i + z_i\n$$\nwhere\n \n - $\\beta_0$ is a common intercept \n - $\\mathbf{\\omega} = (\\omega_1, \\dots, \\omega_k)$ is a conditional Autorgressive model (CAR) with precision matrix $\\tau_1\\mathbf{Q}$\n - $\\mathbf{z} = (z_1, \\dots, z_k)$ is an unstrictured random effect with precision $\\tau_2$\n\n \n**Stage 3** Hyperparameters\n\nThe hyperparameters of the model are $\\tau_1$ and $\\tau_2$\n\n\n**NOTE** In this case the linear predictor $\\eta$ consists of three components!!\n\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nFit the above model in using `inlabru` by completing the following code:\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp = ~ Intercept(1) + space(...) + iid(...)\n\nformula = ...\n\n\nlik = bru_obs(formula = formula, \n              family = ...,\n              E = ...,\n              data = ...)\n\nfit = bru(cmp, lik)\n```\n:::\n\n\n\n<div class='webex-solution'><button>Answer</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp = ~ Intercept(1) + space(space, model = \"besag\", graph = Q) + iid(space, model = \"iid\")\n\nformula = observed ~ Intercept + space + iid\n\nlik = bru_obs(formula = formula, \n              family = \"poisson\",\n              E = expected,\n              data = resp_cases)\n\nfit = bru(cmp, lik)\n```\n:::\n\n\n\n\n</div>\n\n:::\n\nAfter fitting the model we want to extract results.\n\n::: {.callout-tip icon=\"false\"}\n## {{< bi question-octagon color=#6dc83c >}} Question\n\n1. What is the estimated value for $\\beta_0$? __________________\n\n2. Look at the estimated values of the hyperparameters using `fit$summary.hyperpar` , which of the two spatial components (structured or unstructured) explains more of the variability in the counts? \n\n* (A) structured  \n* (B) unstructured  \n\n\n\n:::\n\n\nWe now look at the predictions over space.\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nComplete the code below to produce prediction of the linear predictor $\\eta_i$ and of the risk $\\lambda_i$ and of the expected cases $E_i\\exp(\\lambda_i)$ over the whole space of interest. Then plot the mean and sd of the resulting surfaces.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred = predict(fit, resp_cases, ~data.frame(log_risk = ...,\n                                             risk = exp(...),\n                                             cases = ...\n                                             ),\n               n.samples = 1000)\n```\n:::\n\n\n\n\n<div class='webex-solution'><button>Show Answer</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# produce predictions\npred = predict(fit, resp_cases, ~data.frame(log_risk = Intercept + space,\n                                             risk = exp(Intercept + space),\n                                             cases = expected * exp(Intercept + space)\n                                             ),\n               n.samples = 1000)\n# plot the predictions\n\np1 = ggplot() + geom_sf(data = pred$log_risk, aes(fill = mean)) + scale_fill_scico(direction = -1) + ggtitle(\"mean log risk\")\np2 = ggplot() + geom_sf(data = pred$log_risk, aes(fill = sd)) + scale_fill_scico(direction = -1) + ggtitle(\"sd log risk\")\np1 + p2\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-10-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\np1 = ggplot() + geom_sf(data = pred$risk, aes(fill = mean)) + scale_fill_scico(direction = -1) + ggtitle(\"mean  risk\")\np2 = ggplot() + geom_sf(data = pred$risk, aes(fill = sd)) + scale_fill_scico(direction = -1) + ggtitle(\"sd  risk\")\np1 + p2\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-10-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\np1 = ggplot() + geom_sf(data = pred$cases, aes(fill = mean)) + scale_fill_scico(direction = -1)+ ggtitle(\"mean  expected counts\")\np2 = ggplot() + geom_sf(data = pred$cases, aes(fill = sd)) + scale_fill_scico(direction = -1)+ ggtitle(\"sd  expected counts\")\np1 + p2\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-10-3.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n</div>\n\n\n:::\n\n\nFinally we want to compare our observations $y_i$ with the predicted means of the Poisson distribution $E_i\\exp(\\lambda_i)$\n\n::: {.cell}\n\n```{.r .cell-code}\npred$cases %>% ggplot() + geom_point(aes(observed, mean)) + \n  geom_errorbar(aes(observed, ymin = q0.025, ymax = q0.975)) +\n  geom_abline(intercept = 0, slope = 1)\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-11-1.pdf){fig-pos='H'}\n:::\n:::\n\n**Note:** Here we are predicting the _mean_ of counts, not the counts!!! Predicting counts is the theme of the next task!\n\n## Getting prediction densities\n\nPosterior predictive distributions, that is $\\pi(y_i^{\\text{new}}|\\mathbf{y})$ are of interest in many applied problems. The `bru()` function\ndoes not return predictive densities. In the previous step we have computed predictions for the `expected counts` $\\pi(E_i\\lambda_i|\\mathbf{y})$. The predictive distribution is then:\n$$\n\\pi(y_i^{\\text{new}}|\\mathbf{y}) = \\int \\pi(y_i|E_i\\lambda_i)\\pi(E_i\\lambda_i|\\mathbf{y})\\ dE_i\\lambda_i\n$$\nwhere, in our case, $\\pi(y_i|E_i\\lambda_i)$ is Poisson with mean $E_i\\lambda_i$. We can achieve this using the following algorith:\n\n  1. Simulate $n$ replicates of $g^k = E_i\\lambda_i$ for $k = 1,\\dots,n$ using the function _generate()_ which takes the same input as _predict()_\n  2. For each of the $k$ replicates simulate a new value $y_i^{new}$ using the function _rpois()_\n  3. Summarise the $n$ samples of $y_i^{new}$ using, for example the mean and the 0.025 and 0.975 quantiles.\n\nHere is the code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# simulate 1000 realizations of E_i\\lambda_i\nexpected_counts = generate(fit, resp_cases, \n                           ~ expected * exp(Intercept + space),\n                           n.samples = 1000)\n\n\n# simulate poisson data\naa = rpois(271*1000, lambda = as.vector(expected_counts))\nsim_counts = matrix(aa, 271, 1000)\n\n# summarise the samples with posterior means and quantiles\npred_counts = data.frame(observed = resp_cases$observed,\n                         m = apply(sim_counts,1,mean),\n                         q1 = apply(sim_counts,1,quantile, 0.025),\n                         q2 = apply(sim_counts,1,quantile, 0.975),\n                         vv = apply(sim_counts,1,var)\n                         )\n```\n:::\n\n\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nPlot the observations against the predicted new counts and the predicted expected counts. Include the uncertainty and compare the two.\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nggplot() + \n  geom_point(data = pred_counts, aes(observed, m, color = \"Pred_obs\")) + \n  geom_errorbar(data = pred_counts, aes(observed, ymin = q1, ymax = q2, color = \"Pred_obs\")) +\n  geom_point(data = pred$cases, aes(observed, mean, color = \"Pred_means\")) + \n  geom_errorbar(data = pred$cases, aes(observed, ymin = q0.025, ymax = q0.975, color = \"Pred_means\")) +\n  \n  geom_abline(intercept = 0, slope =1)\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-13-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n\n</div>\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n# Geostatistics \n\nIn this practical we are going to fit a geostatistical model. We will:\n\n-   Learn how to fit a geostatistical model in `inlabru`\n\n-   Learn how to build a mesh for the SPDE representation\n\n-   Learn how to add spatial covariates to the model\n\n-   Learn how to do predictions\n\n-   Learn how to simulate from the fitted model\n\n------------------------------------------------------------------------\n\nLibraries to load:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(inlabru) \nlibrary(sf)\nlibrary(terra)\n\n\n# load some libraries to generate nice map plots\nlibrary(scico)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(mapview)\nlibrary(tidyterra)\n```\n:::\n\n## The data\n\nIn this practical, we will explore data on the Pacific Cod (*Gadus macrocephalus*) from a trawl survey in Queen Charlotte Sound. The `pcod` dataset is available from the `sdmTMB` package and contains the presence/absence records of the Pacific Cod during each surveys along with the biomass density of Pacific cod in the area swept (kg/Km$^2$). The `qcs_grid` data contain the depth values stored as $2\\times 2$ km grid for Queen Charlotte Sound.\n\nThe dataset contains presence/absence data from 2003 to 2017. In this practical we only consider year 2003.\n\nWe first load the dataset and select the year of interest\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sdmTMB)\n\npcod_df = sdmTMB::pcod %>% filter(year==2003)\nqcs_grid = sdmTMB::qcs_grid\n```\n:::\n\nThen, we create ab `sf` object and assign the rough coordinate reference to it:\n\n::: {.cell}\n\n```{.r .cell-code}\npcod_sf =   st_as_sf(pcod_df, coords = c(\"lon\",\"lat\"), crs = 4326)\npcod_sf = st_transform(pcod_sf,\ncrs = \"+proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\" )\n```\n:::\n\nWe convert the covariate into a raster and assign the same coordinate reference:\n\n::: {.cell}\n\n```{.r .cell-code}\ndepth_r <- rast(qcs_grid, type = \"xyz\")\ncrs(depth_r) <- crs(pcod_sf)\n```\n:::\n\nFinally we can plot our dataset. Note that to plot the raster we need to upload also the `tidyterra` library.\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot()+ \n  geom_spatraster(data=depth_r$depth)+\n  geom_sf(data=pcod_sf,aes(color=factor(present))) +\n    scale_color_manual(name=\"Occupancy status for the Pacific Cod\",\n                     values = c(\"black\",\"orange\"),\n                     labels= c(\"Absence\",\"Presence\"))+\n  scale_fill_scico(name = \"Depth\",\n                   palette = \"nuuk\",\n                   na.value = \"transparent\" ) + xlab(\"\") + ylab(\"\")\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-34-1.pdf){fig-pos='H'}\n:::\n:::\n\n## The model\n\nWe first fit a simple model where we consider the observation as Bernoulli and where the linear predictor contains only one intercept and the GR field defined through the SPDE approach. The model is defined as:\n\n**Stage 1** Model for the response \n\n$$\ny(s)|\\eta(s)\\sim\\text{Binom}(1, p(s))\n$$ **Stage 2** Latent field model \n\n$$\n\\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s)\n$$ \n\nwith \n\n$$\n\\omega(s)\\sim \\text{  GF with range } \\rho\\  \\text{ and maginal variance }\\ \\sigma^2\n$$\n\n**Stage 3** Hyperparameters\n\nThe hyperparameters of the model are $\\rho$ and $\\sigma$\n\n**NOTE** In this case the linear predictor $\\eta$ consists of two components!!\n\n### The workflow\n\nWhen fitting a geostatistical model we need to fulfill the following tasks:\n\n1.  Build the mesh\n\n2.  Define the SPDE representation of the spatial GF. This includes defining the priors for the range and sd of the spatial GF\n\n3.  Define the *components* of the linear predictor. This includes the spatial GF and all eventual covariates\n\n4.  Define the observation model using the `bru_obs()` function\n\n5.  Run the model using the `bru()` function\n\n### 1. Building the mesh\n\nThe first task, when dealing with geostatistical models in `inlabru` is to build the mesh that covers the area of interest. For this purpose we use the function `fm_messh_2d`.\n\nOne way to build the mesh is to start from the locations where we have observations, these are contained in the dataset `pcod_sf`.\n\n::: {.cell}\n\n```{.r .cell-code}\nmesh = fm_mesh_2d(loc = pcod_sf,           # Build the mesh\n                  max.edge = c(10,20),     # The largest allowed triangle edge length.\n                  offset = c(5,50))       # The automatic extension distance\nggplot() + gg(mesh) + geom_sf(data= pcod_sf, aes(color = factor(present)), size = 0.1) + xlab(\"\") + ylab(\"\")\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-35-1.pdf){fig-pos='H'}\n:::\n:::\n\nAs you can see from the plot above, some of the locations are very close to each other, this causes some very small triangles. This can be avoided using the option `cutoff =` which collapses the locations that are closer than a cutoff (those points are collapsed in the mesh construction but, of course, not when it come to estimaation.)\n\n::: {.cell}\n\n```{.r .cell-code}\nmesh = fm_mesh_2d(loc = pcod_sf,           # Build the mesh\n                  cutoff = 2,\n                  max.edge = c(10,20),     # The largest allowed triangle edge length.\n                  offset = c(5,50))       # The automatic extension distance\nggplot() + gg(mesh) + geom_sf(data= pcod_sf, aes(color = factor(present)), size = 0.1) + xlab(\"\") + ylab(\"\")\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-36-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nLook at the documentation for the `fm_mesh_2d` function typing\n\n::: {.cell}\n\n```{.r .cell-code}\n?fm_mesh_2d\n```\n:::\n\nplay around with the different options and create different meshes.\n\nThe *rule of thumb* is that your mesh should be:\n\n-   fine enough to well represent the spatial variability of your process, but not too fine in order to avoid computation burden\n-   the triangles should be regular, avoid long and thin triangles.\n-   The mesh should contain a buffer around your area of interest (this is what is defined in the `offset` option) in order to avoid boundary artefact in the estimated variance.\n:::\n\n### 2. Define the SPDE representation of the spatial GF\n\nTo define the SPDE representation of the spatial GF we use the function `inla.spde2.pcmatern`. This takes as input the mesh we have defined and the PC-priors definition for $\\rho$ and $\\sigma$ (the range and the marginal standard deviation of the field).\n\nPC priors Gaussian Random field are defined in (Fuglstad et al. 2018). From a practical perspective for the range $\\rho$ you need to define two paramters $\\rho_0$ and $p_{\\rho}$ such that you believe it is reasonable that \n\n$$\nP(\\rho<\\rho_0)=p_{\\rho}\n$$ \n\nwhile for the margianal variance $\\sigma$ you need to define two parameters $\\sigma_0$ and $p_{\\sigma}$ such that you believe it is reasonable that\n\n$$\nP(\\sigma<\\sigma_0)=p_{\\sigma}\n$$\n\nYou can use the following function to compute and plot the prior distributions for the range and sd of the Matern field.\n\n::: {.cell}\n\n```{.r .cell-code}\ndens_prior_range = function(rho_0, p_alpha)\n{\n  # compute the density of the PC prior for the\n  # range rho of the Matern field\n  # rho_0 and p_alpha are defined such that\n  # P(rho<rho_0) = p_alpha\n  rho = seq(0, rho_0*10, length.out =100)\n  alpha1_tilde = -log(p_alpha) * rho_0\n  dens_rho =  alpha1_tilde / rho^2 * exp(-alpha1_tilde / rho)\n  return(data.frame(x = rho, y = dens_rho))\n}\n\ndens_prior_sd = function(sigma_0, p_sigma)\n{\n  # compute the density of the PC prior for the\n  # sd sigma of the Matern field\n  # sigma_0 and p_sigma are defined such that\n  # P(sigma>sigma_0) = p_sigma\n  sigma = seq(0, sigma_0*10, length.out =100)\n  alpha2_tilde = -log(p_sigma)/sigma_0\n  dens_sigma = alpha2_tilde* exp(-alpha2_tilde * sigma) \n  return(data.frame(x = sigma, y = dens_sigma))\n}\n```\n:::\n\nHere are some alternatives for defining priors for our model\n\n::: {.cell}\n\n```{.r .cell-code}\nspde_model1 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(.1, 0.5),\n                                  prior.range = c(30, 0.5))\nspde_model2 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(10, 0.5),\n                                  prior.range = c(1000, 0.5))\nspde_model3 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n```\n:::\n\nAnd here we plot the different priors for the range:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + \n  geom_line(data = dens_prior_range(30,.5), aes(x,y, color = \"model1\")) +\n  geom_line(data = dens_prior_range(1000,.5), aes(x,y, color = \"model2\")) +\n  geom_line(data = dens_prior_range(100,.5), aes(x,y, color = \"model3\")) \n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-40-1.pdf){fig-pos='H'}\n:::\n:::\n\nand for the sd:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + \n  geom_line(data = dens_prior_sd(1,.5), aes(x,y, color = \"model1\")) +\n  geom_line(data = dens_prior_sd(10,.5), aes(x,y, color = \"model2\")) +\n  geom_line(data = dens_prior_sd(.1,.5), aes(x,y, color = \"model3\")) \n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-41-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.callout-tip icon=\"false\"}\n## {{< bi question-octagon color=#6dc83c >}} Question\n\nConsider the `pcod_sf`, the spatial extension and type of the data...is some of the previous choices more reasonable than other? \n\n* (A) spde_model1  \n* (B) spde_model2  \n* (C) spde_model3  \n\n\n\n**NOTE** Remember that a prior should be reasonable..but the model should not totally depend on it.\n:::\n\n### 3. Define the components of the linear predictor\n\nWe have now defined a mesh and a SPDE representation of the spatial GF. We now need to define the model components:\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp = ~ Intercept(1) + space(geometry, model = spde_model3)\n```\n:::\n\n**NOTE** since the dataframe we use (`pcod_sf`) is an `sf` object the input in the `space()` component is the geometry of the dataset.\n\n### 4. Define the observation model\n\nOur data are Bernoulli distributed so we can define the observation model as:\n\n::: {.cell}\n\n```{.r .cell-code}\nformula = present ~ Intercept  + space\n\nlik = bru_obs(formula = formula, \n              data = pcod_sf, \n              family = \"binomial\")\n```\n:::\n\n### 5. Run the model\n\nFinally we are ready to run the model\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1 = bru(cmp,lik)\n```\n:::\n\n## Extract results\n\n### Hyperparameters\n\n::: {.callout-warning icon=\"false\"}\n### {{< bi pencil-square color=#c8793c >}} Task\n\nPlot the posterior densities for the range $\\rho$ and the standard deviation $\\sigma$ alog with the prior for both parameters.\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nPosterior marginals for the hyperparameters can be extracted from the fitted model as:\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nfit1$marginals.hyperpar$'name of the parameter'\n```\n\n\n</div>\n:::\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\n# Extract marginal for the range\n\nggplot() + \n  geom_line(data = fit1$marginals.hyperpar$`Range for space`,\n            aes(x,y, color = \"Posterior\")) +\n  geom_line(data = dens_prior_range(100,.5),\n            aes(x,y, color = \"Prior\"))\n\n\nggplot() + \n  geom_line(data = fit1$marginals.hyperpar$`Stdev for space`,\n            aes(x,y, color = \"Posterior\")) +\n  geom_line(data = dens_prior_sd(1,.5), aes(x,y, color = \"Prior\"))\n```\n\n\n</div>\n:::\n:::\n\n## Spatial prediction\n\nWe now want to extract the estimated posterior mean and sd of spatial GF. To do this we first need to define a grid of points where we want to predict. We do this using the function `fm_pixel()` which creates a regular grid of points covering the mesh\n\n::: {.cell}\n\n```{.r .cell-code}\npxl = fm_pixels(mesh)\n```\n:::\n\nthen compute the prediction for both the spatial GF and the linear predictor (spatial GF + intercept)\n\n::: {.cell}\n\n```{.r .cell-code}\npreds = predict(fit1, pxl, ~data.frame(spatial = space,\n                                      total = Intercept + space))\n```\n:::\n\nFinally, we can plot the maps\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + geom_sf(data = preds$spatial,aes(color = mean)) + scale_color_scico() + ggtitle(\"Posterior mean\")\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-49-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nggplot() + geom_sf(data = preds$spatial,aes(color = sd)) + scale_color_scico() + ggtitle(\"Posterior sd\")\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-49-2.pdf){fig-pos='H'}\n:::\n:::\n\n**Note** The posterior sd is lowest at the observation points. Note how the posterior sd is inflated around the border, this is the \"border effect\" due to the SPDE representation.\n\nInstead of predicting over a grid covering the whole mesh, we can limit our predictions to the points where the covariate is defined. We can do this by defining a `sf` object using coordinates in the object `depth_r`.\n\n::: {.cell}\n\n```{.r .cell-code}\npxl1 = data.frame(crds(depth_r), \n                  as.data.frame(depth_r$depth)) %>% \n       filter(!is.na(depth)) %>%\nst_as_sf(coords = c(\"x\",\"y\"))\n```\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nProduce prediction over `pxl1` unsing the same techniques as before. Plot your results.\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nAdd hint details here...\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\npred_pxl1 = predict(fit1, pxl1, ~data.frame(spatial = space,\n                                      total = Intercept + space))\n\nggplot() + geom_sf(data = pred_pxl1$total,aes(color = mean)) + scale_color_scico() + ggtitle(\"Posterior mean\")\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-51-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\nggplot() + geom_sf(data = pred_pxl1$total,aes(color = sd)) + scale_color_scico() + ggtitle(\"Posterior sd\")\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-51-2.pdf){fig-align='center' fig-pos='H'}\n:::\n\n\n</div>\n:::\n:::\n\nInstead of computing the posterior mean and standard deviations of the estimated surface, one can also *simulate* possible realizations of such surface. This will give the user a better idea of the type of realized surfaces one can expect. We can do this using the function `generate()`.\n\n::: {.cell}\n\n```{.r .cell-code}\n# we simulate 4 samples from the \ngens = generate(fit1, pxl1, ~ (Intercept + space),\n                n.samples = 4)\n\npp = cbind(pxl1, gens)\n\npp %>% select(-depth) %>%\n  pivot_longer(-geometry) %>%\n    ggplot() + \n      geom_sf(aes(color = value)) +\n      facet_wrap(.~name) +\n        scale_color_scico(direction = -1) +\n        ggtitle(\"Sample from the fitted model\")\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-52-1.pdf){fig-pos='H'}\n:::\n:::\n\n## An alternative model\n\nWe now want to check if the `depth` covatiate has an influende on the probability of presence. We do this in two different models\n\n1.  **Model 1** The depth enters the model in a linear way. The linear predictor is then defined as:\n\n$$\n  \\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s) + \\beta_1\\ \\text{depth}(s)\n$$\n\n2.  **Model 1** The depth enters the model in a non linear way. The linear predictor is then defined as:\n\n$$\n  \\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s) +  f(\\text{depth}(s))\n$$ where $f(.)$ is a smooth function. We will use a RW2 model for this.\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nFit model 1. Define components, observation model and use the `bru()` function to estimate the parameters.\n\n**Note** Use the scaled version of the covariate stored in `depth_r$depth_scaled`.\n\nWhat is the liner effect of depth on the logit probability?\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nAdd hint details here...\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\ncmp = ~ Intercept(1) + space(geometry, model = spde_model3) +\n        covariate(depth_r$depth_scaled, model = \"linear\")\n\nformula = present ~ Intercept  + space + covariate\n\nlik = bru_obs(formula = formula, \n              data = pcod_sf, \n              family = \"binomial\")\n\n\nfit2 = bru(cmp, lik)\n```\n\n\n</div>\n:::\n:::\n\nWe now want to fit **Model 2** where we allow the effect of depth to be non-linear. To use the RW2 model we need to *group* the values of depth into distinct classe. To do this we use the function `inla.group()` which, by default, creates 20 groups. The we can fit the model as usual\n\n::: {.cell}\n\n```{.r .cell-code}\n# create the grouped variable\ndepth_r$depth_group = inla.group(values(depth_r$depth_scaled))\n\n# run the model\ncmp = ~ Intercept(1) + space(geometry, model = spde_model3) +\n        covariate(depth_r$depth_group, model = \"rw2\")\n\nformula = present ~ Intercept  + space + covariate\n\nlik = bru_obs(formula = formula, \n              data = pcod_sf, \n              family = \"binomial\")\n\n\nfit3 = bru(cmp, lik)\n\n# plot the estimated effect of depth\n\nfit3$summary.random$covariate %>% \n  ggplot() + geom_line(aes(ID,mean)) + \n                                  geom_ribbon(aes(ID, ymin = `0.025quant`, \n                                                      ymax = `0.975quant`), alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-54-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nCreate a map of predicted *probability* from Model 3. You can use a inverse logit function defined as\n\n::: {.cell}\n\n```{.r .cell-code}\ninv_logit = function(x) (1+exp(-x))^(-1)\n```\n:::\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nThe `predict()` function can take as input also functions of elements of the components you want to consider \n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\ninv_logit = function(x) (1+exp(-x))^(-1)\n\npred3  = predict(fit3, pxl1, ~inv_logit(Intercept + space + covariate) )\n\npred3 %>% ggplot() + \n      geom_sf(aes(color = mean)) +\n        scale_color_scico(direction = -1) +\n        ggtitle(\"Sample from the fitted model\")\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-56-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n\n</div>\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n# Log-Gaussian Cox Processes {#sec-linmodel}\n\nIn this practical we will:\n\n-   Fit a [homogeneous Point Process](#HPP)\n-   Fit a [non-homogeneous Point Process](#NHPP)\n-   Fit a [LGCP (log-Gaussian Cox Process)](#LGCP)\n\nLibraries to load:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \nlibrary(spatstat)\nlibrary(sf)\nlibrary(scico)\nlibrary(spatstat)\nlibrary(lubridate)\nlibrary(terra)\nlibrary(tidyterra)\n```\n:::\n\n------------------------------------------------------------------------\n\n## The data \n\nIn this practical we consider the data `clmfires` in the `spatstat`\nlibrary.\n\nThis dataset is a record of forest fires in the Castilla-La Mancha\nregion of Spain between 1998 and 2007. This region is approximately 400\nby 400 kilometres. The coordinates are recorded in kilometres. For more\ninfo about the data you can type:\n\n::: {.cell}\n\n```{.r .cell-code}\n?clmfires\n```\n:::\n\nWe first read the data and transform them into an `sf` object. We also\ncreate a polygon that represents the border of the Castilla-La Mancha\nregion. We select the data for year 2004 and only those fires caused by\nlightning.\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"clmfires\")\npp = st_as_sf(as.data.frame(clmfires) %>%\n                mutate(x = x, \n                       y = y),\n              coords = c(\"x\",\"y\"),\n              crs = NA) %>%\n  filter(cause == \"lightning\",\n         year(date) == 2004)\n\npoly = as.data.frame(clmfires$window$bdry[[1]]) %>%\n  mutate(ID = 1)\n\nregion = poly %>% \n  st_as_sf(coords = c(\"x\", \"y\"), crs = NA) %>% \n  dplyr::group_by(ID) %>% \n  summarise(geometry = st_combine(geometry)) %>%\n  st_cast(\"POLYGON\") \n  \nggplot() + geom_sf(data = region, alpha = 0) + geom_sf(data = pp)  \n```\n\n::: {.cell-output-display}\n![Distribution of the observed forest fires caused by lightning in Castilla-La Mancha in 2004](day3_practical_5_files/figure-pdf/fig-points-1.pdf){#fig-points fig-pos='H'}\n:::\n:::\n\n## Fit a homogeneous Poisson Process {#HPP}\n\nAs a first exercise we are going to fit a homogeneous Poisson process\n(HPP) to the data. This is a model that assume constant intensity over\nthe whole space so our linear predictor is then:\n\n$$\n\\eta(s) = \\log\\lambda(s) = \\beta_0 , \\ \\mathbf{s}\\in\\Omega\n$$\n\nso the likelihood can be written as:\n\n$$\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n& = \\exp\\left( -\\int_{\\Omega}\\exp(\\beta_0)ds\\right)\\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n\\end{aligned}\n$$\n\nwhere $|\\Omega|$ is the area of the domain of interest.\n\nWe need to approximate the integral using a numerical integration scheme\nas:\n\n$$\n\\approx\\exp\\left(-\\sum_{k=1}^{N_k}w_k\\lambda(s_k)\\right)\\prod_{i=1}^n \\lambda(\\mathbf{s}_i)\n$$\n\nWhere $N_k$ is the number of integration points $s_1,\\dots,s_{N_k}$ and\n$w_1,\\dots,w_{N_k}$ are the integration weights.\n\nIn this case, since the intensity is constant, the integration scheme is\nreally simple: it is enough to consider one random point inside the\ndomain with weight equal to the area of the domain.\n\n::: {.cell}\n\n```{.r .cell-code}\n# define integration scheme\n\nips = st_sf(\ngeometry = st_sample(region, 1)) # some random location inside the domain\nips$weight = st_area(region) # integration weight is the area of the domain\n\ncmp = ~ 0 + beta_0(1)\n\nformula = geometry ~ beta_0\n\nlik = bru_obs(data = pp,\n              family = \"cp\",\n              formula = formula,\n              ips = ips)\nfit1 = bru(cmp, lik)\n```\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\n1.  Plot the estimated posterior distribution of the intensity\n2.  Compare the estimated expected number of fires on the whole domain\n    with the observed ones.\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nRemember that in the *inlabru* framework we model the log intensity\n$\\eta = log(\\lambda)$\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\n# 1) The estimated posterior distribution of the  intensity is\n\npost_int = inla.tmarginal(function(x) exp(x), fit1$marginals.fixed$beta_0)\npost_int %>% ggplot() + geom_line(aes(x,y))\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-91-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\n# 2) To compute the expected number of points in the area we need to multiply the\n# estimated intensity by the area of the domain.\n# In the same plot we also show the number of observed fires as a vertical line.\n\npost_int = inla.tmarginal(function(x) st_area(region)* exp(x), fit1$marginals.fixed$beta_0)\npost_int %>% ggplot() + geom_line(aes(x,y)) +\n  geom_vline(xintercept = dim(pp)[1])\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-91-2.pdf){fig-align='center' fig-pos='H'}\n:::\n\n\n</div>\n:::\n:::\n\n## Fit an Inhomogeneous Poisson Process {#NHPP}\n\nThe model above has the clear disadvantages that assumes a constant\nintensity and from @fig-points we clearly see that this is not\nthe case.\n\nThe library `spatstat` contains also some covariates that can help\nexplain the fires distribution. Figure @fit-altitude shows the location\nof fires together with the (scaled) altitude.\n\n::: {.cell}\n\n```{.r .cell-code}\n#|label: fig-altitude\n#|fig-cap: \"Distribution of the observed forest fires and scaled altitude\"\n#| \nelev_raster = rast(clmfires.extra[[2]]$elevation)\nelev_raster = scale(elev_raster)\nggplot() + \n  geom_spatraster(data = elev_raster) + \n  geom_sf(data = pp) +\n  scale_fill_scico()\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-92-1.pdf){fig-pos='H'}\n:::\n:::\n\nWe are now going to use the altitude as a covariate to explain the\nvariability of the intensity $\\lambda(s)$ over the domain of interest.\n\nOur model is $$\n\\log\\lambda(s) = \\beta_0 + \\beta_1x(s)\n$$ where $x(s)$ is the altitude at location $s$.\n\nThe likelihood becomes: \n\n$$\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n& = \\exp \\left( -\\int_\\Omega \\exp(\\beta_0 + \\beta_1x(s)) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n\\end{aligned}\n$$ \n\nNow we need to choose an integration scheme to solve the integral.\n\nIn this case we will take a simple grid based approach where each\nquadrature location has an equal weight. Our grid consists of\n$N_k = 1000$ points and the weights are all equal to $|\\Omega|/N_k$.\n\n::: {.cell}\n\n```{.r .cell-code}\n#|label: fig-int2\n#|fig-cap: \"Integration scheme.\"\n\nn.int = 1000\nips = st_sf(geometry = st_sample(region,\n            size = n.int,\n            type = \"regular\"))\n\nips$weight = st_area(region) / n.int\nggplot() + geom_sf(data = ips, aes(color = weight)) + geom_sf(data= region, alpha = 0)\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-93-1.pdf){fig-pos='H'}\n:::\n:::\n\n**OBS**: The implicit assumption here is that the intensity is constant\ninside each grid box, *and so is the covariate*!!\n\nWe can now fit the model:\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp = ~ Intercept(1) + elev(elev_raster, model = \"linear\")\nformula = geometry ~ Intercept + elev\nlik = bru_obs(data = pp,\n              family = \"cp\",\n              formula = formula,\n              ips = ips)\nfit2 = bru(cmp, lik)\n```\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nWhat is the effect of the altitude on the (log) intensity of the\nprocess?\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nYou can look at the summary for the fixed effects\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nfit2$summary.fixed\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                mean         sd 0.025quant   0.5quant 0.975quant       mode kld\nIntercept -6.6100831 0.10212134 -6.8102373 -6.6100831 -6.4099290 -6.6100831   0\nelev       0.6736681 0.06970396  0.5370509  0.6736681  0.8102854  0.6736681   0\n```\n\n\n:::\n\n\n</div>\n:::\n:::\n\n\n::: {.callout-warning}\n⚠️ **WARNING!!**⚠️ When fitting a Point process, the integration scheme\nhas to be fine enough to capture the spatial variability of the\ncovariate!!\n:::\n\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nRerun the model with the altitude as covariate, but this time change the integration scheme as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\nn.int2 = 50\n\nips2 = st_sf(geometry = st_sample(region,\n            size = n.int2,\n            type = \"regular\"))\nips2$weight = st_area(region) / n.int2\n```\n:::\n\nWhat happens to the effect of the covariate?\n\n\n<div class='webex-solution'><button>Take hint</button>\n\nRe-run the model changing the integration scheme in the _ips_ input of the _bru_obs()_ function.\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nlik_bis = bru_obs(data = pp,\n              family = \"cp\",\n              formula = formula,\n              ips = ips2)\n\nfit2bis = bru(cmp, lik_bis)\n\n# you can the check the differences between the two models\nrbind(fit2$summary.fixed,\nfit2bis$summary.fixed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 mean         sd 0.025quant   0.5quant 0.975quant       mode\nIntercept  -6.6100831 0.10212134 -6.8102373 -6.6100831 -6.4099290 -6.6100831\nelev        0.6736681 0.06970396  0.5370509  0.6736681  0.8102854  0.6736681\nIntercept1 -6.7892565 0.11329868 -7.0113179 -6.7892565 -6.5671952 -6.7892565\nelev1       0.9998966 0.08969021  0.8241071  0.9998966  1.1756862  0.9998966\n           kld\nIntercept    0\nelev         0\nIntercept1   0\nelev1        0\n```\n\n\n:::\n\n\n</div>\n:::\n\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nNow we want to predict the log-intensity over the whole domain. Use the  grid from the elevation raster to predict the intensity over the domain. \n\n::: {.cell}\n\n```{.r .cell-code}\nest_grid = st_as_sf(data.frame(crds(elev_raster)), coords = c(\"x\",\"y\"))\nest_grid  = st_intersection(est_grid, region)\n```\n:::\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\npreds2 = predict(fit2, est_grid, ~ data.frame(log_scale = Intercept + elev,\n\n                                              lin_scale = exp(Intercept + elev)))\n# then visualize it like\npreds2$log_scale %>% \n  ggplot() +\n  geom_sf(aes(color = mean)) +\n  scale_color_scico()\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-99-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n\n</div>\n:::\n\n:::\n\nFinally, we want to use the fitted model to estimate the total  number of fires over the whole region. To do this we first have to fine the expected number of fires as:\n\n$$\nE(N_{\\Omega}) = \\int_{\\Omega}\\exp(\\lambda(s))\\ ds\n$$\n\nThen simulate possible realizations of $N_{\\Omega}$ to include also the likelihood variability in our estimate:\n\n::: {.cell}\n\n```{.r .cell-code}\nN_fires = generate(fit2, ips,\n                      formula = ~ {\n                        lambda = sum(weight * exp(elev + Intercept))\n                        rpois(1, lambda)},\n                    n.samples = 2000)\n\nggplot(data = data.frame(N = as.vector(N_fires))) +\n  geom_histogram(aes(x = N),\n                 colour = \"blue\",\n                 alpha = 0.5,\n                 bins = 20) +\n  geom_vline(xintercept = nrow(pp),\n             colour = \"red\") +\n  theme_minimal() +\n  xlab(expression(Lambda))\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-100-1.pdf){fig-pos='H'}\n:::\n:::\n\n## Fit a Log-Gaussian Cox Process{#LGCP}\n\nFinally we want to fit a LGCP with log intensity:\n\n$$\n\\log(s) = \\beta_0 + \\beta_1x + u(s)\n$$\n\nwhere $\\beta_0$ is the intercept, $\\beta_1$ the effect of (standarized) altitude $x(s)$ as before and $u(s)$ is a Gaussian Random field defined through the SPDE approach.\n\n### Define the mesh\n\nThe first step, as any time we use the SPDE approach is to defie the mesh and the priors for the marginal variance and range:\n\n::: {.cell}\n\n```{.r .cell-code}\nmesh = fm_mesh_2d(boundary = region,\n                  max.edge = c(5, 10),\n                  cutoff = 4, crs = NA)\n\nggplot() + gg(mesh) + geom_sf(data = pp)\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-101-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nspde_model =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n```\n:::\n\nWe can then define the integration weight. Here we use the same points to `define the SPDE approximation` and to `approximate the integral` in the likelihood. We will see later that this does not have to be like this, BUT integration weight and SPDE weights have to be consistent with each other!\n\n::: {.cell}\n\n```{.r .cell-code}\nips = fm_int(mesh, samplers = region)\n\nggplot() + geom_sf(data = ips, aes(color = weight)) +\n  gg(mesh) +\n   scale_color_scico()\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-102-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n### Run the model\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nSet up the components and  the formula for the model above by completing the code below and run the model.\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp = ~ ...\n\nformula = geometry ~ ...\n\nlik = bru_obs(\"cp\",\n              formula = formula,\n              data = pp,\n              ips = ...)\n\nfit3 = bru(cmp, lik)\n```\n:::\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nThe model has three components: intercept, linear effect of altitude and the spatial GRF\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\ncmp = ~ Intercept(1) + space(geometry, model = spde_model) + elev(elev_raster, model = \"linear\")\n\nformula = geometry ~ Intercept + space + elev\n\nlik = bru_obs(\"cp\",\n              formula = formula,\n              data = pp,\n              ips = ips)\n\nfit3 = bru(cmp, lik)\n```\n\n\n</div>\n:::\n:::\n\n\n**Note** when running the model above you will get a warning:\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in bru_log_warn(msg): Model input 'elev_raster' for 'elev' returned some NA values.\nAttempting to fill in spatially by nearest available value.\nTo avoid this basic covariate imputation, supply complete data.\n```\n\n\n:::\n:::\n\n\nIt means that the _bru()_ function cannot find the covariate values for some of the mesh nodes. This is  a common situation. As the warning says, the _bru()_ function automatically imputes the value of the covarite using the nearest nodes. This increases the running time of the _bru()_  function, so one solution is to impute the values of the covariate over the whole mesh 'before' running the _bru()_  function. \n\nHere, we notice that there is a single point for which elevation values are missing (see @fig-points2 the red point that lies outside the raster extension ).\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Integration scheme for numerical approximation of the stochastic integral in La Mancha Region](day3_practical_5_files/figure-pdf/fig-points2-1.pdf){#fig-points2 fig-align='center'}\n:::\n:::\n\nTo solve this, we can increase the raster extension so it covers all both data-points and quadrature locations as well. Then, we can use the `bru_fill_missing()` function to input the missing values with the nearest-available-value. We can achieve this using the following code:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Extend raster ext by 30 % of the original raster so it covers the whole mesh\nre <- extend(elev_raster, ext(elev_raster)*1.3)\n# Convert to an sf spatial object\nre_df <- re %>% stars::st_as_stars() %>%  st_as_sf(na.rm=F)\n# fill in missing values using the original raster \nre_df$lyr.1 <- bru_fill_missing(elev_raster,re_df,re_df$lyr.1)\n# rasterize\nelev_rast_p <- stars::st_rasterize(re_df) %>% rast()\nggplot() + geom_spatraster(data = elev_rast_p) \n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-106-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n::: {.callout-note}\nThe `bru_fill_missing()` function was added mainly to handle very local infilling on domain boundaries. For properly missing data, one should consider doing a proper model of the spatial field instead.\n:::\n\n## Results\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nPlot the estimated mean and standard deviation of the spatial GF and the log-intensity over the domain of interest\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nUse the _fm_pixels()_ and _predict()_ functions.\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\npxl = fm_pixels(mesh, mask= region, dims = c(200,200))\npreds = predict(fit3, pxl, ~data.frame(spde = space,\n                                       log_int = Intercept + space + elev))\n\n#and plot as \nlibrary(scico)\nlibrary(patchwork)\n\nggplot(data= preds$spde) + \n  geom_sf(aes(color = mean)) + \n  scale_color_scico() +\n ggtitle(\"spde mean\") +\nggplot(data=preds$spde ) +\n  geom_sf(aes(color = sd)) +\n  scale_color_scico() +\n ggtitle(\"spde sd\") +\n\nggplot(data=preds$log_int) + \n  geom_sf(aes(color = mean)) + \n  scale_color_scico() +\n ggtitle(\"log-int mean\")\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-107-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\nggplot(data=preds$log_int) + \n  geom_sf(aes(color = sd)) +\n  scale_color_scico() +\n ggtitle(\"log-int sd\") +\n  plot_layout(ncol=2)\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-107-2.pdf){fig-align='center' fig-pos='H'}\n:::\n\n\n</div>\n:::\n:::\n\n\nInstead of just looking at the posterior mean and standard deviation, it can be usefull to look at `simulated fields` from the posterior distribution. This is because the mean field is, by definition, smoother than any realization of the field. So looking at simulation can give us a better idea of how the field might look like. We can do this using the _generate()_ function: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_fields = generate(fit3, pxl, ~data.frame(spde = space,\n                                       log_int = Intercept + space + elev),\n                     n.samples = 4)\n\ncbind(pxl,sapply(sim_fields, function(x) x$spde)) %>%\n  pivot_longer(-geometry) %>%\n  ggplot() + geom_sf(aes(color = value)) + \n  facet_wrap(.~name) + scale_color_scico() +\n  ggtitle(\"simulated spatial fields\")\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-108-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ncbind(pxl,sapply(sim_fields, function(x) x$log_int)) %>%\n  pivot_longer(-geometry) %>%\n  ggplot() + geom_sf(aes(color = value)) + \n  facet_wrap(.~name) + scale_color_scico() + \n  ggtitle(\"simulated log intensity\")\n```\n\n::: {.cell-output-display}\n![](day3_practical_5_files/figure-pdf/unnamed-chunk-108-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n",
    "supporting": [
      "day3_practical_5_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}