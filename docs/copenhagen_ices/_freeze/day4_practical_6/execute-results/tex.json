{
  "hash": "a719f8bc54691bcfb574504e89127edf",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Practical 6\"\nexecute: \n  freeze: true\nformat: \n  html:\n    mainfont: \"12\"\n  PrettyPDF-pdf:\n    keep-tex: true\n    number-sections: true\nembed-resources: true\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n<font size=\"5\"> **Aim of this practical:** </font>\n\nIn this practical we are going to look at some model comparison and validation techniques.\n\n\n\n\n\n\n{{< downloadthis day4_practical_6.R dname=\"day4_practical_6.R\" label=\"Download Practical 6 R script\" icon=\"database-fill-down\" type=\"success\" >}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n## Model Checking for Linear Models\n\nIn this exercise we will:\n\n-   Learn about some model assessments techniques available in INLA\n-   Conduct posterior predictive model checking\n\nLibraries to load:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n```\n:::\n\nRecall a simple linear regression model with Gaussian observations\n\n$$\ny_i\\sim\\mathcal{N}(\\mu_i, \\sigma^2), \\qquad i = 1,\\dots,N\n$$\n\nwhere $\\sigma^2$ is the observation error, and the mean parameter $\\mu_i$ is linked to the linear predictor through an identity function:\n\n$$\n\\eta_i = \\mu_i = \\beta_0 + \\beta_1 x_i\n$$ where $x_i$ is a covariate and $\\beta_0, \\beta_1$ are parameters to be estimated.\n\n### Simulate example data\n\nWe simulate data from a simple linear regression model\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nbeta = c(2,0.5)\nsd_error = 0.1\n\nn = 100\nx = rnorm(n)\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error)\n\ndf = data.frame(y = y, x = x)  \n```\n:::\n\n### Fitting the linear regression model with `inlabru`\n\nNow we fit a simple linear regression model in `inalbru` by defining (1) the model components, (2) the linear predictor and (3) the likelihood.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model components\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n# Linear predictor\nformula = y ~ Intercept + beta_1\n# Observational model likelihood\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n# Fit the Model\nfit.lm = bru(cmp, lik)\n```\n:::\n\n### Residuals analysis\n\nA common way for model diagnostics in regression analysis is by checking residual plots. In a Bayesian setting residuals can be defined in multiple ways depending on how you account for posterior uncertainty. Here, we will adopt a Bayesian approach by generating samples from the posterior distribution of the model parameters and then draw samples from the residuals defined as:\n\n$$\nr_i = y_i - x_i^T\\beta\n$$\n\nWe can use the `predict` function to achieve this:\n\n::: {.cell}\n\n```{.r .cell-code}\nres_samples <- predict(\n  fit.lm,         # the fitted model\n  df,             # the original data set\n  ~ data.frame(   \n    res = y-(beta_0 + beta_1)  # compute the residuals\n  ),\n  n.samples = 1000   # draw 1000 samples\n)\n```\n:::\n\nThe resulting data frame contains the posterior draw of the residuals mean for which we can produce some diagnostics plots , e.g.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Residuals checks for Linear Model\"}\nggplot(res_samples,aes(y=mean,x=1:100))+geom_point() +\nggplot(res_samples,aes(y=mean,x=x))+geom_point()\n```\n\n::: {.cell-output-display}\n![Bayesian residual plots: the left panel is the residual index plot; the right panel is the plot of the residual versus the covariate x](day4_practical_6_files/figure-pdf/unnamed-chunk-7-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\nWe can also compare these against the theoretical quantiles of the Normal distribution as follows:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"QQPlot for Linear Model\"}\narrange(res_samples, mean) %>%\n  mutate(theortical_quantiles = qnorm(1:100 / (1+100))) %>%\n  ggplot(aes(x=theortical_quantiles,y= mean)) + \n  geom_ribbon(aes(ymin = q0.025, ymax = q0.975), fill = \"grey70\")+\n  geom_abline(intercept = mean(res_samples$mean),\n              slope = sd(res_samples$mean)) +\n  geom_point() +\n  labs(x = \"Theoretical Quantiles (Normal)\",\n       y= \"Sample Quantiles (Residuals)\") \n```\n\n::: {.cell-output-display}\n![](day4_practical_6_files/figure-pdf/unnamed-chunk-8-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n### Posterior Predictive Checks\n\nNow, instead of generating samples from the mean, we will account for the observational process uncertainty by:\n\n1.  Sampling $y^{1k}_i\\sim\\pi(y_i|\\mathbf{y})$ $k = 1,\\dots,M;~i = 1,\\ldots,100$ using `generate()` (here we will draw $M=500$ samples)\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples =  generate(fit.lm, df,\n  formula = ~ {\n    mu <- (beta_0 + beta_1)\n    sd <- sqrt(1 / Precision_for_the_Gaussian_observations)\n    rnorm(100, mean = mu, sd = sd)\n  },\n  n.samples = 500\n) \n```\n:::\n\n2.  Comparing some summaries of the simulated data with the one of the observed one\n\nHere we compare (i) the estimated posterior densities $\\hat{\\pi}^k(y|\\mathbf{y})$ with the estimated data density and (ii) the samples means and 95% credible intervals against the observations.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tidy format for plotting\nsamples_long = data.frame(samples) %>% \n  mutate(id = 1:100) %>% # i-th observation\n  pivot_longer(-id)\n\n# compute the mean and quantiles for the samples\ndraws_summaries = data.frame(mean_samples = apply(samples,1,mean),\nq25 = apply(samples,1,function(x)quantile(x,0.025)),  \nq975 = apply(samples,1,function(x)quantile(x,0.975)),\nobservations = df$y)  \n\np1 = ggplot() + geom_density(data = samples_long, \n                        aes(value, group = name),  color = \"#E69F00\") +\n  geom_density(data = df, aes(y))  +\n  xlab(\"\") + ylab(\"\") \n\np2 = ggplot(draws_summaries,aes(y=mean_samples,x=observations))+\n  geom_errorbar(aes(ymin = q25,\n                   ymax = q975), \n               alpha = 0.5, color = \"grey50\")+\ngeom_point()+geom_abline(slope = 1,intercept = 0,lty=2)+labs()\n\np1 +p2\n```\n\n::: {.cell-output-display}\n![](day4_practical_6_files/figure-pdf/unnamed-chunk-10-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n## GLM model checking {#sec-linmodel}\n\nIn this exercise we will:\n\n-   Learn about some model assessments techniques available in INLA\n-   Conduct posterior predictive model checking using CPO and PIT\n\nLibraries to load:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n```\n:::\n\nIn this exercise, we will use data on horseshoe crabs (*Limulus polyphemus*) where the number of satellites males surrounding a breeding female are counted along with the female's color and carapace width.\n\n{{< downloadthis datasets/crabs.csv dname=\"crabs\" label=\"Download data set\" icon=\"database-fill-down\" type=\"info\" >}}\n\nA possible model to study the factors that affect the number of satellites for female crabs is\n\n$$\n\\begin{aligned}\ny_i&\\sim\\mathrm{Poisson}(\\mu_i), \\qquad i = 1,\\dots,N \\\\\n\\eta_i &= \\mu_i = \\beta_0 + \\beta_1 x_i + \\ldots\n\\end{aligned}\n$$\n\nWe can explore the conditional means and variances given the female's color:\n\n::: {.cell}\n\n```{.r .cell-code}\ncrabs <- read.csv(\"datasets/crabs.csv\")\n\n# conditional means and variances\ncrabs %>%\n  summarise( Mean = mean(satell ),\n             Variance = var(satell),\n                     .by = color)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   color     Mean  Variance\n1 medium 3.294737 10.273908\n2   dark 2.227273  6.737844\n3  light 4.083333  9.719697\n4 darker 2.045455 13.093074\n```\n\n\n:::\n:::\n\nThe mean of the number of satellites vary by color which gives a good indication that color might be useful for predicting satellites numbers. However, notice that the mean is lower than its variance suggesting that overdispersion might be present and that a negative binomial model would be more appropriate for the data (we will cover this later).\n\n**Fitting the model**\n\nFirst, lets begin fitting the Poisson model above using the carapace's color and width as predictors. Since, color is a categorical variable in our model we need to create a dummy variable for it. We can use the `model.matrix` function to help us constructing the design matrix and then append this to our data:\n\n::: {.cell}\n\n```{.r .cell-code}\ncrabs_df = model.matrix( ~  color , crabs) %>%\n  as.data.frame() %>%\n  select(-1) %>%        # drop intercept\n  bind_cols(crabs) %>%  # append to original data\n  select(-color)        # remove original color categorical variable\n```\n:::\n\nThe new data set `crabs_df` contains a dummy variable for the different color categories (`dark` being the reference category). Then we can fit the model in `inlabru` as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp =  ~ -1 + beta0(1) +  colordarker +\n       colorlight + colormedium +\n       w(weight, model = \"linear\")\n\nlik =  bru_obs(formula = satell ~.,\n            family = \"poisson\",\n            data = crabs_df)\n\nfit_pois = bru(cmp, lik)\n\nsummary(fit_pois)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ninlabru version: 2.13.0\nINLA version: 25.08.21-1\nComponents:\nbeta0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\ncolordarker: main = linear(colordarker), group = exchangeable(1L), replicate = iid(1L), NULL\ncolorlight: main = linear(colorlight), group = exchangeable(1L), replicate = iid(1L), NULL\ncolormedium: main = linear(colormedium), group = exchangeable(1L), replicate = iid(1L), NULL\nw: main = linear(weight), group = exchangeable(1L), replicate = iid(1L), NULL\nObservation models:\n  Family: 'poisson'\n    Tag: <No tag>\n    Data class: 'data.frame'\n    Response class: 'integer'\n    Predictor: satell ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[beta0, colordarker, colorlight, colormedium, w], latent[]\nTime used:\n    Pre = 0.425, Running = 0.312, Post = 0.0672, Total = 0.804 \nFixed effects:\n              mean    sd 0.025quant 0.5quant 0.975quant   mode kld\nbeta0       -0.501 0.196     -0.885   -0.501     -0.117 -0.501   0\ncolordarker -0.008 0.180     -0.362   -0.008      0.345 -0.008   0\ncolorlight   0.445 0.176      0.101    0.445      0.790  0.445   0\ncolormedium  0.248 0.118      0.017    0.248      0.479  0.248   0\nw            0.001 0.000      0.000    0.001      0.001  0.001   0\n\nMarginal log-Likelihood:  -489.43 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n```\n\n\n:::\n:::\n\n### Model assessment and model choice\n\nNow that we have fitted the model we would like to carry some model assessments. In a Bayesian setting, this is often based on posterior predictive checks. To do so, we will use the CPO and PIT - two commonly used Bayesian model assessment criteria based on the **posterior predictive distribution**.\n\n::: callout-note\n## Posterior predictive model checking\n\nThe posterior predictive distribution for a predicted value $\\hat{y}$ is\n\n$$\n\\pi(\\hat{y}|\\mathbf{y}) = \\int_\\theta \\pi(\\hat{y}|\\theta)\\pi(\\theta|\\mathbf{y})d\\theta.\n$$\n\nThe probability integral transform (PIT) introduced by Dawid (1984) is defined for each observation as:\n\n$$\n\\mathrm{PIT}_i = \\pi(\\hat{y}_i \\leq y_i |\\mathbf{y}{-i})\n$$\n\nThe PIT evaluates how well a model's predicted values match the observed data distribution. It is computed as the cumulative distribution function (CDF) of the observed data evaluated at each predicted value. If the model is well-calibrated, the PIT values should be *approximately uniformly distributed*. Deviations from this uniform distribution may indicate issues with model calibration or overfitting.\n\nAnother metric we could used to asses the model fit is the conditional predictive ordinate (CPO) introduced by Pettit (1990), and deï¬ned as:\n\n$$\n\\text{CPO}_i = \\pi(y_i| \\mathbf{y}{-i})\n$$\n\nThe CPO measures the density of the observed value of $y_i$ when model is fit using all data but $y_i$. CPO provides a measure of how well the model predicts each individual observation while taking into account the rest of the data and the model. *Large values indicate a better fit* of the model to the data, while small values indicate a bad fitting of the model\n:::\n\nTo compute PIT and CPO we can either:\n\n1.  ask `inlabru` to compute them by set `options = list(control.compute = list(cpo = TRUE))` in the `bru()` function arguments.\n\n2.  set this as default in `inlabru` global option using the `bru_options_set` function.\n\nHere we will do the later and re-run the model\n\n::: {.cell}\n\n```{.r .cell-code}\nbru_options_set(control.compute = list(cpo = TRUE))\n\nfit_pois = bru(cmp, lik)\n```\n:::\n\nNow we can produce histograms and QQ plots to assess for uniformity in the PIT values which can be accessed through `inlabru_model$cpo$pit` :\n\n::: panel-tabset\n# Plot\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day4_practical_6_files/figure-pdf/unnamed-chunk-27-1.pdf){fig-align='center'}\n:::\n:::\n\n# R Code\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_pois$cpo$pit %>%\n  hist(main = \"Histogram of PIT values\")\n\nqqplot(qunif(ppoints(length(fit_pois$cpo$pit))),\n       fit_pois$cpo$pit,\n       main = \"Q-Q plot for Unif(0,1)\",\n       xlab = \"Theoretical Quantiles\",\n       ylab = \"Sample Quantiles\")\n\nqqline(fit_pois$cpo$pit,\n       distribution = function(p) qunif(p),\n       prob = c(0.1, 0.9))\n```\n:::\n:::\n\nBoth Q-Q plots and histogram of the PIT values suggest a not so great model fit. For the CPO values, usually the following summary of the CPO is often used:\n\n$$\n-\\sum_{i=1}^n \\log (\\text{CPO}\\_i)\n$$\n\nThis quantities is useful when comparing different models - a smaller values indicate a better model fit. CPO values can be accessed by typing `inlabru_model$cpo$cpo`.\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nThe model assessment above suggests that a Poisson model might not be the most appropriate model, likely due to the overdispersion we detected previously. Fit a Negative binomial to relax the Poisson model assumption that the conditional mean and variance are equal. Then, compute the CPO summary statistic and PIT QQ plot to decide which model gives the better fit.\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nTo specify a negative binomial model you only need to change the family distribution to `family =  \"nbinomial\"`.\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"false\"}\npar(mfrow=c(1,2))\n\n# Fit the negative binomial model\n\nlik_nbinom =  bru_obs(formula = satell ~.,\n            family = \"nbinomial\",\n            data = crabs_df)\n\nfit_nbinom = bru(cmp, lik_nbinom)\n\n# PIT checks\n\nfit_nbinom$cpo$pit %>%\n  hist(main = \"Histogram of PIT values\")\n\nqqplot(qunif(ppoints(length(fit_nbinom$cpo$pit))),\n       fit_nbinom$cpo$pit,\n       main = \"Q-Q plot for Unif(0,1)\",\n       xlab = \"Theoretical Quantiles\",\n       ylab = \"Sample Quantiles\")\n\nqqline(fit_nbinom$cpo$pit,\n       distribution = function(p) qunif(p),\n       prob = c(0.1, 0.9))\n```\n\n::: {.cell-output-display}\n![](day4_practical_6_files/figure-pdf/unnamed-chunk-29-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\n# CPO comparison\n\ndata.frame( CPO = c(-sum(log(fit_pois$cpo$cpo)),\n                    -sum(log(fit_nbinom$cpo$cpo))),\n          Model = c(\"Poisson\",\"Negative Binomial\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       CPO             Model\n1 465.4061           Poisson\n2 379.3340 Negative Binomial\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\n# Overall, we can see that the negative binomial model provides a better fit to the data.\n```\n\n\n</div>\n:::\n:::\n",
    "supporting": [
      "day4_practical_6_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}