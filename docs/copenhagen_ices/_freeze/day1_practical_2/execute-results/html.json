{
  "hash": "984e16e2e33e80847da534b997cf9796",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Practical 2\"\nexecute: \n  freeze: true\nformat: \n  html:\n    mainfont: \"12\"\n  PrettyPDF-pdf:\n    keep-tex: true\n    number-sections: true\nembed-resources: true\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n<font size=\"5\"> **Aim of this practical:** </font>\n\n1.  Set priors for different linear models\n2.  Compute and visualize posterior densities and summaries for marginal effects\n3.  Fit hierarchical flexible models\n\nwe are going to learn:\n\n-   How to change some of the R default priors in `inlabru`\n-   How to explore and visualize model parameters\n-   Fit different flexible models\n\n\n\n\n\n\n{{< downloadthis day1_practical_2.R dname=\"practical_2.R\" label=\"Download Practical 2 R script\" icon=\"database-fill-down\" type=\"success\" >}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n## Setting priors and model checking for Linear Models\n\nIn this exercise we will:\n\n-   Learn how to set priors for linear effects $\\beta_0$ and $\\beta_1$\n-   Learn how to set the priors for the hyperparameter $\\tau = 1/\\sigma^2$.\n-   Visualize marginal posterior distributions\n\nStart by loading useful libraries:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n```\n:::\n\nRecall a simple linear regression model with Gaussian observations $$\ny_i\\sim\\mathcal{N}(\\mu_i, \\sigma^2), \\qquad i = 1,\\dots,N\n$$\n\nwhere $\\sigma^2$ is the observation error, and the mean parameter $\\mu_i$ is linked to the linear predictor through an identity function: $$\n\\eta_i = \\mu_i = \\beta_0 + \\beta_1 x_i\n$$ where $x_i$ is a covariate and $\\beta_0, \\beta_1$ are parameters to be estimated. In INLA, we assume that the model is a latent Gaussian model, i.e., we have to assign $\\beta_0$ and $\\beta_1$ a Gaussian prior. For the precision hyperparameter $\\tau = 1/\\sigma^2$ a typical prior choice is a $\\text{Gamma}(a,b)$ prior.\n\nIn `R-INLA`, the default choice of priors for each $\\beta$ is\n\n$$\n\\beta \\sim \\mathcal{N}(0,10^3).\n$$\n\nand the prior for the variance parameter in terms of the log precision is\n\n$$ \\log(\\tau) \\sim \\mathrm{logGamma}(1,5 \\times 10^{-5}) $$\n\n::: callout-note\nIf your model uses the default intercept construction (i.e., `Intercept(1)` in the linear predictor) INLA will assign a default $\\mathcal{N} (0,0)$ prior to it.\n:::\n\nLets see how can we change the default priors using some simulated data\n\n#### **Simulate example data**\n\nWe simulate data from a simple linear regression model\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nbeta = c(2,0.5)\nsd_error = 0.1\n\nn = 100\nx = rnorm(n)\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error)\n\ndf = data.frame(y = y, x = x)  \n```\n:::\n\n#### **Fitting the linear regression model with `inlabru`**\n\nNow we fit a simple linear regression model in `inalbru` by defining (1) the model components, (2) the linear predictor and (3) the likelihood.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model components\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n# Linear predictor\nformula = y ~ Intercept + beta_1\n# Observational model likelihood\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n# Fit the Model\nfit.lm = bru(cmp, lik)\n```\n:::\n\n### Change the prior distributions\n\nUntil now, we have used the default priors for both the precision $\\tau$ and the fixed effects $\\beta_0$ and $\\beta_1$. Let's see how to customize these.\n\nTo check which priors are used in a fitted model one can use the function `inla.prior.used()`\n\n::: {.cell}\n\n```{.r .cell-code}\ninla.priors.used(fit.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nsection=[family]\n\ttag=[INLA.Data1] component=[gaussian]\n\t\ttheta1:\n\t\t\tparameter=[log precision]\n\t\t\tprior=[loggamma]\n\t\t\tparam=[1e+00, 5e-05]\nsection=[linear]\n\ttag=[beta_0] component=[beta_0]\n\t\tbeta:\n\t\t\tparameter=[beta_0]\n\t\t\tprior=[normal]\n\t\t\tparam=[0.000, 0.001]\n\ttag=[beta_1] component=[beta_1]\n\t\tbeta:\n\t\t\tparameter=[beta_1]\n\t\t\tprior=[normal]\n\t\t\tparam=[0.000, 0.001]\n```\n\n\n:::\n:::\n\nFrom the output we see that the precision for the observation $\\tau\\sim\\text{Gamma}(1e+00,5e-05)$ while $\\beta_0$ and $\\beta_1$ have precision 0.001, that is variance $1/0.001$.\n\n**Change the precision for the linear effects**\n\nThe precision for linear effects is set in the component definition. For example, if we want to increase the precision to 0.01 for $\\beta_0$ we define the relative components as:\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp1 =  ~-1 +  beta_0(1, prec.linear = 0.01) + beta_1(x, model = \"linear\")\n```\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nRun the model again using 0.1 as default precision for both the intercept and the slope parameter.\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"false\"}\ncmp2 =  ~ -1 + \n          beta_0(1, prec.linear = 0.1) + \n          beta_1(x, model = \"linear\", prec.linear = 0.1)\n\nlm.fit2 = bru(cmp2, lik) \n```\n\n\n</div>\n:::\n\nNote that we can use the same observation model as before since both the formula and the dataset are unchanged.\n:::\n\n**Change the prior for the precision of the observation error** $\\tau$\n\nPriors on the hyperparameters of the observation model must be passed by defining argument `hyper` within `control.family` in the call to the `bru_obs()` function.\n\n::: {.cell}\n\n```{.r .cell-code}\n# First we define the logGamma (0.01,0.01) prior \n\nprec.tau <- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(0.01, 0.01))) # prior values\n\nlik2 =  bru_obs(formula = y ~.,\n                family = \"gaussian\",\n                data = df,\n                control.family = list(hyper = prec.tau))\n\nfit.lm2 = bru(cmp2, lik2) \n```\n:::\n\nThe names of the priors available in **R-INLA** can be seen with `names(inla.models()$prior)`\n\n### Visualizing the posterior marginals\n\nPosterior marginal distributions of the ﬁxed effects parameters and the hyperparameters can be visualized using the `plot()` function by calling the name of the component. For example, if want to visualize the posterior density of the intercept $\\beta_0$ we can type:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nplot(fit.lm, \"beta_0\")\n```\n\n::: {.cell-output-display}\n![](day1_practical_2_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=384}\n:::\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nPlot the posterior marginals for $\\beta_1$ and for the precision of the observation error $\\pi(\\tau|y)$\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nSee the `summary()` output to check the names for the different model components.\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nplot(fit.lm, \"beta_1\") +\nplot(fit.lm, \"Precision for the Gaussian observations\")\n```\n\n::: {.cell-output-display}\n![](day1_practical_2_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=576}\n:::\n\n\n</div>\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Linear Mixed Model for fish weight-length relationship {#sec-llm_fish}\n\nIn this exercise we will:\n\n-   Plot random effects of a LMM\n-   Compute posterior densities and summaries for the variance components\n\nLibraries to load:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n```\n:::\n\n::: {.cell}\n\n:::\n\nIn this exercise, we will use a subset of the Pygmy Whitefish (*Prosopium coulterii*) dataset from the `FSAdata` R package, containing biological data collected in 2001 from Dina Lake, British Columbia.\n\n{{< downloadthis datasets/PygmyWFBC.csv dname=\"PygmyWFBC\" label=\"Download data set\" icon=\"database-fill-down\" type=\"info\" >}}\n\nThe data set contains the following information:\n\n-   `net_no`Unique net identification number\n-   `wt` Fish weight (g)\n-   `tl` Total fish length (cm)\n-   `sex` Sex code (`F`=Female, `M` = Male)\n\nWe can visualize the distribution of the response (weight) across the nets split by sex as follows:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nPygmyWFBC <- read.csv(\"datasets/PygmyWFBC.csv\")\n\nggplot(PygmyWFBC, aes(x = factor(net_no), y = wt,fill = sex)) + \n  geom_boxplot() + \n  labs(y=\"Weight (g)\",x = \"Net no.\")\n```\n\n::: {.cell-output-display}\n![](day1_practical_2_files/figure-html/unnamed-chunk-26-1.png){fig-align='center' width=384}\n:::\n:::\n\nSuppose we are interested in modelling the weight-length relationship for captured fish. The exploratory plot suggest some important variability in this relationship, potentially attributable to differences among sampling nets deployed across various sites in the Dina Lake.\n\nTo account for this between-net variability, we model net as a random effect using the following linear mixed model:\n\n$$\n\\begin{aligned}\ny_{ij} &\\sim\\mathcal{N}(\\mu_{ij}, \\sigma_e^2), \\qquad i = 1,\\dots,a \\qquad j = 1,\\ldots,n \\\\\n\\eta_{ij} &= \\mu_{ij} = \\beta_0 + \\beta_1 \\times \\text{length}_j + \\beta_2 \\times \\mathbb{I}(\\mathrm{Sex}_{ij}=\\mathrm{M}) +  u_i \\\\\nu_i &\\sim \\mathcal{N}(0,\\sigma^2_u)\n\\end{aligned}\n$$\n\nwhere:\n\n-   $y_{ij}$ is the weight of the $j$-th fish from net $i$\n\n-   $\\text{length}_{ij}$ is the corresponding fish length\n\n-   $\\mathbb{I}(\\text{Sex}_{ij} = \\text{M})$ is an indicator/dummy such that for the *i*th net $$\n    \\mathbb{I}(\\mathrm{Sex}_{ij}) \\begin{cases}1 & \\text{if the } j \\text{th fish is Male} \\\\0 & \\text{otherwise} \\end{cases}\n    $$\n\n-   $u_i$ represents the random intercept for net $i$\n\n-   $\\sigma_u^2$ and $\\sigma_\\epsilon^2$ are the between-net and residual variances, respectively\n\nTo run this model in`inlabru` we first need to create our sex dummy variable :\n\n::: {.cell}\n\n```{.r .cell-code}\nPygmyWFBC$sex_M <- ifelse(PygmyWFBC$sex==\"F\",0,1)\n```\n:::\n\n`inlabru` will treat 0 as the reference category (i.e., the intercept $\\beta_0$ will represent the baseline weight for females ). Now we can define the model component, the likelihood and fit the model.\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp =  ~ -1 + sex_M +  beta_0(1)  + beta_1(tl, model = \"linear\") +   net_eff(net_no, model = \"iid\")\n\nlik =  bru_obs(formula = wt ~ .,\n            family = \"gaussian\",\n            data = PygmyWFBC)\n\nfit = bru(cmp, lik)\n\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ninlabru version: 2.13.0\nINLA version: 25.08.21-1\nComponents:\nsex_M: main = linear(sex_M), group = exchangeable(1L), replicate = iid(1L), NULL\nbeta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\nbeta_1: main = linear(tl), group = exchangeable(1L), replicate = iid(1L), NULL\nnet_eff: main = iid(net_no), group = exchangeable(1L), replicate = iid(1L), NULL\nObservation models:\n  Family: 'gaussian'\n    Tag: <No tag>\n    Data class: 'data.frame'\n    Response class: 'numeric'\n    Predictor: wt ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[sex_M, beta_0, beta_1, net_eff], latent[]\nTime used:\n    Pre = 0.369, Running = 0.397, Post = 0.162, Total = 0.927 \nFixed effects:\n          mean    sd 0.025quant 0.5quant 0.975quant    mode kld\nsex_M   -1.106 0.218     -1.534   -1.106     -0.678  -1.106   0\nbeta_0 -15.816 0.870    -17.516  -15.819    -14.098 -15.819   0\nbeta_1   2.555 0.072      2.414    2.555      2.696   2.555   0\n\nRandom effects:\n  Name\t  Model\n    net_eff IID model\n\nModel hyperparameters:\n                                         mean    sd 0.025quant 0.5quant\nPrecision for the Gaussian observations 0.475 0.044      0.393    0.473\nPrecision for net_eff                   2.146 1.311      0.569    1.839\n                                        0.975quant mode\nPrecision for the Gaussian observations      0.567 0.47\nPrecision for net_eff                        5.521 1.32\n\nMarginal log-Likelihood:  -467.54 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n```\n\n\n:::\n:::\n\nFor interpretability, we could have centered the predictors, but our primary focus here is on estimating the variance components of the mixed model.\n\nWe can plot the posterior density of the nets random intercept as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit,\"net_eff\")\n```\n\n::: {.cell-output-display}\n![](day1_practical_2_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\nFor theoretical and computational purposes, INLA works with the precision which is the inverse of the variance. To obtain the posterior summaries on the SDs scale we can sample from the posterior distribution for the precision while back-transforming the samples and then computing the summary statistics. Transforming the samples is necessary because some quantities such as the mean and mode are not invariant to monotone transformation; alternatively we can use some of the in-built `R-INLA` functions to achieve this (see supplementary note).\n\nWe use the `inla.hyperpar.sample` function to draw samples from the approximated joint posterior for the hyperparameters, then invert them to get variances and lastly compute the mean, std. dev., quantiles, etc.\n\n::: {.cell}\n\n```{.r .cell-code}\nsampvars <- 1/inla.hyperpar.sample(1000,fit,improve.marginals = T)\n\ncolnames(sampvars) <- c(\"Error variance\",\"Between-net Variance\")\n\napply(sampvars,2,\n      function(x) c(\"mean\"=mean(x),\n                    \"std.dev\" = sd(x),\n                    quantile(x,c(0.025,0.5,0.975))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        Error variance Between-net Variance\nmean         2.1248605            0.6492665\nstd.dev      0.1981528            0.4112481\n2.5%         1.7665997            0.1837601\n50%          2.1144330            0.5431690\n97.5%        2.5407998            1.7322448\n```\n\n\n:::\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nAnother useful quantity we can compute is the intraclass correlation coefﬁcient (ICC) which help us determine how much the response varies within groups compared to between groups. The intraclass correlation coefﬁcient is defined as:\n\n$$\n\\text{ICC} = \\frac{\\sigma^2_u}{\\sigma^2_u + \\sigma^2_e}\n$$\n\nCompute the median, and quantiles for the ICC using the posterior samples we draw for $\\sigma^2_e$ and $\\sigma^2_u$.\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nThe `rowSums` function can be used to compute $\\sigma^2_{u,s} + \\sigma^2_{e,s}$ for the $s$th posterior draw.\n\n\n</div>\n\n\n::: {.cell webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nsampicc <- sampvars[,2]/(rowSums(sampvars))\nquantile(sampicc, c(0.025,0.5,0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     2.5%       50%     97.5% \n0.0798669 0.2044105 0.4486958 \n```\n\n\n:::\n\n\n</div>\n:::\n:::\n\n::: callout-note\n## Supplementary Material\n\nThe marginal densities for the hyper parameters can be also found by calling`inlabru_model$marginals.hyperpar`. We can then apply a transformation using the `inla.tmarginal` function to transform the precision posterior distributions.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nvar_e <- fit$marginals.hyperpar$`Precision for the Gaussian observations` %>%\n  inla.tmarginal(function(x) 1/x,.) \n\nvar_u <- fit$marginals.hyperpar$`Precision for net_eff` %>%\n  inla.tmarginal(function(x) 1/x,.) \n```\n:::\n\nThe marginal densities for the hyper parameters can be found with `inlabru_model$marginals.hyperpar`, then we can apply a transformation using the `inla.tmarginal` function to transform the precision posterior distributions. Then, we can compute posterior summaries using `inla.zmarginal` function as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\npost_var_summaries <- cbind( inla.zmarginal(var_e,silent = T),\n                             inla.zmarginal(var_u,silent = T))\ncolnames(post_var_summaries) <- c(\"sigma_e\",\"sigma_u\")\npost_var_summaries\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           sigma_e   sigma_u  \nmean       2.124441  0.6486006\nsd         0.1980652 0.4115367\nquant0.025 1.764596  0.1816268\nquant0.25  1.985364  0.3691971\nquant0.5   2.113691  0.5415111\nquant0.75  2.251954  0.8051964\nquant0.975 2.541942  1.738421 \n```\n\n\n:::\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n## Hierarchical generalised additive mixed models with `inlabru`\n\nIn this excercise we will:\n\n-   Fit an hierarchical generalised additive mixed models\n-   Fit a model with a global smooth term\n-   Fit a model with global and group-level smooth terms\n\nLibraries to load:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n```\n:::\n\nThe oceans represent Earth's largest habitat, with life distributed unevenly across depths primarily due to variations in light, temperature, and pressure. Biomass generally decreases with depth, though complex factors like water density layers create non-linear patterns. A significant portion of deep-sea organisms exhibit bioluminescence, which scientists measure using specialized equipment like free-fall camera systems to profile vertical distribution.\n\nIn this exercise, we analyze the `ISIT` dataset, which contains bioluminescence measurements from the northeast Atlantic Ocean. This dataset was previously examined in Zuur et al. ([2009](https://doi.org/10.1007/978-0-387-87458-6)) and Gillibrand et al. ([2007](https://doi.org/10.3354/meps341037)) and consists of observations collected across a depth gradient (0–4,800 m) during spring and summer cruises in 2001–2002 using an ISIT free-fall profiler.\n\n{{< downloadthis datasets/ISIT.csv dname=\"ISIT\" label=\"Download data set\" icon=\"database-fill-down\" type=\"info\" >}}\n\nThe focus of this excersice will be on characterizing seasonal variation in the relationship between bioluminescent source density (sources m$^{2}$) and depth. We begin by exploring distribution patterns of pelagic bioluminescence through source-depth profiles, with each profile representing measurements from an individual sampling station. These profiles will be grouped by month to examine temporal patterns in the water column's bioluminescent structure.\n\n::: panel-tabset\n# Plot\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source–depth proﬁles per month. Each line represents a station.](day1_practical_2_files/figure-html/unnamed-chunk-49-1.png){fig-align='center' width=576}\n:::\n:::\n\n# R-Code\n\n::: {.cell}\n\n```{.r .cell-code}\nicit <- read.csv(\"datasets/ISIT.csv\")\n\nicit$Month <- as.factor(icit$Month)\nlevels(icit$Month) <- month.abb[unique(icit$Month)]\n\nggplot(icit,aes(x=SampleDepth,y= Sources,\n                group=as.factor(Station),\n                colour=as.factor(Station)))+\n  geom_line()+\n  facet_wrap(~Month)+\n  theme(legend.position = \"none\")\n```\n:::\n:::\n\nAs expected, there seems to be a non-linear depth effect with some important variability across months.\n\n### Fitting a global smoother\n\nWe could begin analysing these data with a global smoother and a random intercept for each month. Thus, a possible model is of the form:\n\n$$\nS_{is} = \\beta_0 + f(\\text{Depth})_s + \\text{Month}_i +  \\epsilon_{is} ~~\\text{such that}~ \\epsilon \\sim \\mathcal{N}(0,\\sigma^2_e);~ \\text{Month} \\sim \\mathrm{N}(0,\\sigma^2_m).\n$$\n\nwhere the source during month $i$ at depth $s$, $S_{is}$, are modelled as smoothing function of depth and a month effect. The model has one smoothing curve for all months and can be fitted in `inlabru` as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\nicit$Month_id <- as.numeric(icit$Month) # numeric index for the i-th month\n\ncmp_g =  ~ -1+ beta_0(1) + \n  smooth_g(SampleDepth, model = \"rw1\") + \n  month_reff(Month_id, model = \"iid\") \n\nlik =  bru_obs(formula = Sources ~.,\n               family = \"gaussian\",\n               data = icit)\n\nfit_g = bru(cmp_g, lik)\n\nsummary(fit_g)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ninlabru version: 2.13.0\nINLA version: 25.08.21-1\nComponents:\nbeta_0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\nsmooth_g: main = rw1(SampleDepth), group = exchangeable(1L), replicate = iid(1L), NULL\nmonth_reff: main = iid(Month_id), group = exchangeable(1L), replicate = iid(1L), NULL\nObservation models:\n  Family: 'gaussian'\n    Tag: <No tag>\n    Data class: 'data.frame'\n    Response class: 'numeric'\n    Predictor: Sources ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[beta_0, smooth_g, month_reff], latent[]\nTime used:\n    Pre = 0.397, Running = 0.832, Post = 0.252, Total = 1.48 \nFixed effects:\n         mean    sd 0.025quant 0.5quant 0.975quant   mode kld\nbeta_0 10.017 1.614      6.692   10.025     13.291 10.023   0\n\nRandom effects:\n  Name\t  Model\n    smooth_g RW1 model\n   month_reff IID model\n\nModel hyperparameters:\n                                          mean    sd 0.025quant 0.5quant\nPrecision for the Gaussian observations  0.024 0.001      0.021    0.024\nPrecision for smooth_g                  21.173 5.428     12.387   20.541\nPrecision for month_reff                 0.138 0.089      0.030    0.117\n                                        0.975quant   mode\nPrecision for the Gaussian observations      0.026  0.024\nPrecision for smooth_g                      33.606 19.363\nPrecision for month_reff                     0.366  0.078\n\nMarginal log-Likelihood:  -2217.28 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n```\n\n\n:::\n:::\n\nWe can plot the smoother marginal effect as follows:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Global smoother marginal effect\"}\ndata.frame(fit_g$summary.random$smooth_g) %>% \n  ggplot() + \n  geom_ribbon(aes(ID,ymin = X0.025quant, ymax= X0.975quant), alpha = 0.5) + \n  geom_line(aes(ID,mean)) + \n  xlab(\"covariate\") + ylab(\"smooth effect\")\n```\n\n::: {.cell-output-display}\n![](day1_practical_2_files/figure-html/unnamed-chunk-52-1.png){fig-align='center' width=384}\n:::\n:::\n\nYou might want to have a smoother function by placing a RW2 prior. Unfortunately, this assumes that all the knots are regularly spaced and some depth values are too close to be used for building the RW2 priors. For the case, it is possible to use function `inla.group()` to bin data into groups according to the values of the covariate:\n\n::: {.cell}\n\n```{.r .cell-code}\nicit$depth_grouped <- inla.group(icit$SampleDepth,n=50)\n```\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nRe-run the global smoother model using a RW2 prior for the depth smoother and compare your results with the RW1 model.\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nUse the `depth_grouped` covariate to define the smoother.\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code}\ncmp_rw2 =  ~ -1+ beta_0(1) + \n  smooth_g(depth_grouped, model = \"rw2\") + \n  month_reff(Month_id, model = \"iid\") \n\nlik =  bru_obs(formula = Sources ~.,\n               family = \"gaussian\",\n               data = icit)\n\nfit_rw2 = bru(cmp_rw2, lik)\n\ndata.frame(fit_rw2$summary.random$smooth_g) %>% \n  ggplot() + \n  geom_ribbon(aes(ID,ymin = X0.025quant, ymax= X0.975quant), alpha = 0.5) + \n  geom_line(aes(ID,mean)) + \n  xlab(\"covariate\") + ylab(\"Global smooth effect\")\n```\n\n::: {.cell-output-display}\n![](day1_practical_2_files/figure-html/unnamed-chunk-54-1.png){fig-align='center' width=384}\n:::\n\n\n</div>\n:::\n:::\n\n### Fitting group-level smoothers\n\nHere we fit a model where each month is allowed to have its own smoother for depth, i.e., $f_i(\\text{Depth})_s$. The model structure is given by:\n\n$$\nS_{is} = \\beta_0 + f_i(\\text{Depth})_s + \\text{Month}_i +  \\epsilon_{is}.\n$$\n\nNotice the only different between the global smoother model (Model G) and the group level model (Model GS) is the indexing of the smooth function for depth. We can fit a group-level smoother using the `group` argument within the model component as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp_gs =  ~ -1+ beta_0(1) +\n  smooth_g(SampleDepth, model = \"rw1\") + \n  month_reff(Month_id, model = \"iid\")+\n  smooth_loc(SampleDepth, model = \"rw1\", group = Month_id)\n```\n:::\n\nThen, we simply run the model (since the observational model has not changed -only the model components have):\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_gs = bru(cmp_gs, lik) \n```\n:::\n\nLastly, we can generate model predictions using the `predict` function.\n\n::: {.cell}\n\n```{.r .cell-code}\npred_gs = predict(fit_gs, icit, ~ (beta_0 + smooth_g+month_reff+smooth_loc))\n```\n:::\n\nThen, we plot the predicted mean values with their corresponding 95% CrIs.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Global + group smoother predictions\"}\nggplot(pred_gs,aes(y=mean,x=SampleDepth))+\n  geom_ribbon(aes(SampleDepth,ymin = q0.025, ymax= q0.975), alpha = 0.5,fill=\"tomato\") +\n  geom_line()+\n  geom_point(aes(x=SampleDepth,y=Sources ),alpha=0.25,col=\"grey40\")+\n  facet_wrap(~Month)\n```\n\n::: {.cell-output-display}\n![](day1_practical_2_files/figure-html/unnamed-chunk-58-1.png){width=672}\n:::\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nRe-fit the model GS without the global smoother. By omitting the global smoother, we do not longer force group-level smooths to follow a shared pattern, which is useful when groups may differ substantially from a common trend.\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nYou only need to modify the model components in `cmp_gs`\n\nAdd hint details here...\n\n\n</div>\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\ncmp_s =  ~ -1+ beta_0(1) +\n  month_reff(Month_id, model = \"iid\")+\n  smooth_loc(SampleDepth, model = \"rw1\", group = Month_id)\n\nfit_s = bru(cmp_s, lik) \n\npred_s = predict(fit_s, icit, ~ (beta_0 +month_reff+smooth_loc))\n\nggplot(pred_s,aes(y=mean,x=SampleDepth))+\n  geom_ribbon(aes(SampleDepth,ymin = q0.025, ymax= q0.975), alpha = 0.5,fill=\"tomato\") +\n  geom_line()+\n  geom_point(aes(x=SampleDepth,y=Sources ),alpha=0.25,col=\"grey40\")+\n  facet_wrap(~Month)\n```\n\n::: {.cell-output-display}\n![](day1_practical_2_files/figure-html/unnamed-chunk-59-1.png){fig-align='center' width=576}\n:::\n\n\n</div>\n:::\n:::\n",
    "supporting": [
      "day1_practical_2_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}