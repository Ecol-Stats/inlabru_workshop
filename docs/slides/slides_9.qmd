---
title: "Lecture ??"
from: markdown+emoji
subtitle: "Model Comparison and Model evaluation" 
format:
  revealjs:
    margin: 0
    logo:  NTNU_UofG.png
    theme: uofg_theme.scss
    header-includes: |
      <script src="custom.js" type="application/javascript"></script>
slide-number: "c/t"
title-slide-attributes:
#    data-background-image: images/trondheim3.png
    data-background-size: cover
    data-background-opacity: "0.55"
author:
  - name: Sara Martino
    #orcid: 0000-0002-6879-4412
    email: sara.martino@ntnu.no
    affiliations: Dept. of Mathematical Science, NTNU
  - name: Janine Illian
    #orcid: 0000-0002-6879-4412
    email: Janine.Illian@glasgow.ac.uk
    affiliations: University of Glasgow     
# date: May 22, 2025
# bibliography: references.bib
embed-resources: true
execute:
  allow-html: true
---

```{r setup}
# #| include: false

knitr::opts_chunk$set(echo = FALSE,
                      message=FALSE,
                      warning=FALSE,
                      strip.white=TRUE,
                      prompt=FALSE,
                      fig.align="center",
                       out.width = "60%")

library(knitr)    # For knitting document and include_graphics function
library(ggplot2)  # For plotting
library(png)
library(tidyverse)
library(INLA)
library(BAS)
library(patchwork)
library(DAAG)
library(inlabru)
library(cowplot) # needs install.packages("magick") to draw images

```


## Motivation 


<p style="font-size: 1.5em; text-align: center; margin-top: 150px;">
  “All models are wrong, some models are useful” — George Box
</p>

. . . 

1. How do we check that our model fits our data?  - **Model Validation**

. . . 

2. How do we choose the best  model? - **Model Comparison**


. . . 

But..it is not always easy to distinguish between the two!

# Bayesian model comparison 


##  Bayesian Model Comparison

- There is *no golden standard*
- It really depends what you want to do!
- Basically two types
    - Ones that look at the posterior probability of the data under the
model
    - Ones that look at how model the data fits the data
- In general *it is not an easy task*
- `inlabru` provides some options available

. . . 

## Model Comparison/Validation in `inlabru`{.smaller}

- Criteria of fit

  - Marginal likelihood ⇒Bayes factors
  - Deviance information criterion (DIC)
  - Widely applicable information criterion (WAIC)
  
- There are also some predictive checks for the model:
  
  - Conditional predictive ordinate (CPO)
  - Probability integral transform (PIT)

- You can sample from the posterior using  `generate()`  and compute
  - log-scores
  - CRPS
  - ...
  
# Criteria of fit


## Marginal likelihood

```{r}
#| echo: false
#| eval: true

x = rnorm(100)
y = 1 + 0.6 * x+ rnorm(100)
df = data.frame(x,y)
cmp = ~ Intercept(1) + cov(x, model = "linear")
lik = bru_obs(formula = y~.,
              data = df)
```



```{r}
#| echo: true
#| eval: true

# tell inlabru you want to compute mlik
bru_options_set(control.compute = list(mlik = TRUE))
fit = bru(cmp, lik)
# see the results
fit$mlik
```

<hr>


 - Calculates $\log(\pi(\mathbf{y}))$
 - Can calculate Bayes factors through differences in value
 - **NB:** Problematic for intrinsic models


## Deviance Information Criteria (DIC)



```{r}
#| echo: true
#| eval: true

# tell inlabru you want to compute DIC
bru_options_set(control.compute = list(dic = TRUE))
fit = bru(cmp, lik)
# see the results
fit$dic$dic
```

<hr>

- Measure of complexity and fit. 
- Defined as: $\text{DIC} = \bar{D} + p_D$
    -  $\bar{D}$  is the posterior mean of the deviance 
    -  $p_D$ is the effective number of parameters. 
- Smaller values indicate better trade-off between complexity and fit.

## Widely applicable information criterion (WAIC)

also known as *Watanabe–Akaike information criterion*.

```{r}
#| echo: true
#| eval: true

# tell inlabru you want to compute WAIC
bru_options_set(control.compute = list(waic = TRUE))
fit = bru(cmp, lik)
# see the results
fit$waic$waic
```

<hr>


- similar to the DIC...but maybe better^[See "Understanding predictive information criteria for Bayesian models" (2013) by Andrew Gelman, Jessica Hwang, and Aki Vehtari]
- Linked to the leave-one-out crossvalidation
- Smaller values indicate better trade-off between complexity and fit 


# Posterior predictive checks

## Posterior predictive distribution

**Remember:** Our GLM is defined as:
$$
\begin{eqnarray}
\pi(\mathbf{y}|\mathbf{u},\theta) & =\prod_i \pi(y_i|\mathbf{u},\theta)& \ \text{  likelihood}\\
\pi(\mathbf{u}|\theta)& &\ \text{  LGM}\\
\pi(\theta )& &\ \text{  hyperprior}\\
\end{eqnarray}
$$
using `inlabru` we estimate the _posterior distribution_ $\pi(\mathbf{u},\theta|\mathbf{y})$.


The *posterior predictive distribution* for a new data $\hat{y}$ is then:
$$
\pi(\hat{y}|\mathbf{y}) = \int\pi(y_i|\mathbf{u},\theta)\pi(\mathbf{u},\theta|\mathbf{y})\ d\mathbf{u}\ d\theta
$$

This ditribution can be used to check the model fit!




## Posterior predictive distribution{.smaller} 

$$
\pi(\hat{y}|\mathbf{y}) = \int\pi(y_i|\mathbf{u},\theta)\pi(\mathbf{u},\theta|\mathbf{y})\ d\mathbf{u}\ d\theta
$$


**NOTE:** In general this is NOT computed by `inlabru` but needs to be approximated 
  
  - Use generate to sample from the posterior
  $(\mathbf{u}^*_i,\theta^*_i)\sim\pi(\mathbf{u},\theta|\mathbf{y})$
  
  - Simulate a new datapoint $y^*_i\sim(y_i|\mathbf{u}^*_i,\theta^*_i)$
  
  - Use $y^*_1,\dots, y^*_N$ to approximate the posterior predictive distribution.
  
. . .

**BUT** `inlabru` computes automatically two quantities that are useful for model check!
  
  - Conditional predictive ordinate (CPO) 
  - Probability integral transform (PIT) 
  

## Conditional predictive ordinate (CPO) 
  
Definition:
$$
cpo_i = \pi(y^{obs}_i|\mathbf{y}_{-i})
$$  

- Introduced in Pettit (1990)^[Pettit, L. I. 1990. “The Conditional Predictive Ordinate for the Normal Distribution.” Journal of the Royal Statistical Society. Series B (Methodological)]
- Measures fit through the predictive density
- Can be used to compute the log-score as
$$
\text{Score} = -\sum \log(cpo_i)
$$
lower score correspond to better models


## Probability integral transform (PIT) 

How to compute:

```{r}
#| echo: true
#| eval: true

# tell inlabru you want to compute DIC
bru_options_set(control.compute = list(cpo = TRUE))
fit = bru(cmp, lik)
# see the results
head(fit$cpo$cpo)

```

**Note** it is possible to check for possible fails in computed CPOs

```{r}
#| echo: true
#| eval: true

head(fit$cpo$failure)
```
  
##   Probability integral transform (PIT) 

Definition:
$$
pit_i = \text{Prob}(\hat{y}_i<y_i|\mathbf{y}_{-i})
$$

- Linked to leave-one-out cross-validation

- $pit_i$ shows how well the ith data point is predicted by the rest of
the data

- Very small values indicate "suprising" observation under the model 

- For well-calibrated, the PIT values should be approximately uniformly distributed. 

## Probability integral transform (PIT) 

How to compute:

```{r}
#| echo: true
#| eval: true

# tell inlabru you want to compute DIC
bru_options_set(control.compute = list(cpo = TRUE))
fit = bru(cmp, lik)
# see the results
head(fit$cpo$pit)

```

## Good and Bad PIT plots

```{r}
#| echo: false
#| eval: true

n = 1000

df = data.frame(id = 1:n,
           Good = runif(n),
           Underdispersed = rbeta(n,shape1 = 2,shape2 = 2),
           Overdispersed = rbeta(n,shape1 = 0.5,shape2 = 0.5),
           Biased = rbeta(n,shape1 = 1,shape2 = 2)) %>% 
  pivot_longer(-id) %>%
  mutate(name = factor(name, 
                     levels = c("Good", "Underdispersed", "Overdispersed", "Biased")) ) 
df %>%
  ggplot() + geom_histogram(aes(x = value, y = ..density..), bins = 20) +
  facet_wrap(~name) + xlab("") + ylab("")
```

## Other scores{.smaller}

In the literature there are many proposed scores for evaluate predictions. For example:

  - Dawid-Sebastian score
  - Log-score
  - Continuous rank probility score (CRPS)
  - Brier score
  - ...

They all have their strength and wakness and which one is better depends on the goals of the model.

`inlabru` does not provide such scores automatcally, but they can be computed using simulations from the posterior distribution.


## Example:  CRPS for Poisson data

Our model:
$$
\begin{eqnarray}
y_i|\lambda_i & \sim \text{Poisson}(\lambda_i),&\ i = 1,\dots,N_{\text{data}}\\
\log(\lambda_i)  = \eta_i &= \beta_0 + \beta_1 x_i
\end{eqnarray}
$$

Simulate data and fit the model:
```{r}
#| echo: true
#| eval: true

df_pois <- data.frame(
  x = rnorm(50),
  y = rpois(length(x), exp(2 + 1 * x))
)
cmp = ~ Intercept(1) + cov(x, model = "linear")
lik = bru_obs(formula = y~.,
              family = "poisson",
              data = df_pois)
fit_pois <- bru(cmp, lik)

```



## Example:  CRPS for Poisson data{.smaller}
The CRPS score is defined as:
$$
\text{S}_{\text{CRPS}}(F_i, y_i) = \sum_{k=0}^\infty\left[\text{Prob}(Y_i\leq k|\mathbf{y})-I(y_i\leq k)\right]^2
$$

Computational algorithm:

1. Simulate  $\lambda^{(j)}\sim p(\lambda|\text{data}), j = 1,\dots, N_{\text{samples}}$ using `generate()` (size $N\times N_\text{samples}$).

2. For each $i=1,\dots,N_{\text{data}}$, estimate 
$r_{ik}=\text{Prob}(Y\leq k|\text{data})-I(y_i\leq k)$ as
$$
  \hat{r}_{ik} = \frac{1}{N_\text{samples}} \sum_{j=1}^{N_\text{samples}}
  \{
  \text{Prob}(Y\leq k|\lambda^{(j)}_i)-I(y_i\leq k)
  \} .
$$
3. Compute
$$
  S_\text{CRPS}(F_i,y_i) = \sum_{k=0}^{K} \hat{r}_{ik}^2
$$

## Example:  CRPS for Poisson data{.smaller}

Implementation:
```{r}
#| echo: true
#| eval: true

# some large value, so that 1-F(K) is small
max_K <- ceiling(max(df_pois$y) + 4 * sqrt(max(df_pois$y)))
k <- seq(0, max_K)
kk <- rep(k, times = length(df_pois$y))
i <- seq_along(df_pois$y)
pred_pois <- generate(fit_pois, df_pois,
  formula = ~ {
    lambda <- exp(Intercept + x)
    ppois(kk, lambda = rep(lambda, each = length(k)))
  },
  n.samples = 2000
)
results <- data.frame(
  i = rep(i, each = length(k)),
  k = kk,
  Fpred = rowMeans(pred_pois),
  residuals =
    rowMeans(pred_pois) - (rep(df_pois$y, each = length(k)) <= kk)
)

crps_scores <-
  (results %>%
    group_by(i) %>%
    summarise(crps = sum(residuals^2), .groups = "drop") %>%
    pull(crps))
summary(crps_scores)
```

# ..but how about residuals??


## Residuals: Frequentist vs Bayesian{.smaller}

:::: {.columns}

::: {.column width="50%"}
### 🎯 Frequentist View

- Model parameters are **fixed but unknown**.
- Fitted values (predictions): $\hat{y}_i$ are point estimates
- **Residuals**:  
$$
  r_i = y_i - \hat{y}_i
$$
- Single number per data point.
- Used for:
  - Checking model fit  / outliers

:::

::: {.column width="50%"}
### 🔮 Bayesian View


:::
::::


## Residuals: Frequentist vs Bayesian{.smaller}

:::: {.columns}

::: {.column width="50%"}
### 🎯 Frequentist View

- Model parameters are **fixed but unknown**.
- Fitted values (predictions): $\hat{y}_i$ are point estimates
- **Residuals**:  
$$
  r_i = y_i - \hat{y}_i
$$
- Single number per data point.
- Used for:
  - Checking model fit  / outliers

:::

::: {.column width="50%"}
### 🔮 Bayesian View

- Parameters  are **random variables** with posterior $\pi(\theta \mid y)$.
- Predictions $\tilde{y}_i$ also have a **posterior distribution**.
- No single “true” fitted value → residuals are **not uniquely defined**.
  - $r_i^{(\text{mean})} = (y_i - E[\tilde{y}_i \mid y])$ (mean residual)  
  - $r_i^{(\text{sample})} = (y_i - \tilde{y}^s)$ for  posterior sample 
$s$ 
  - Distribution of $y_i - \tilde{y}_i^{(s)}$ (posterior residuals)

:::
::::

.  .  .

<hr>

A better option is to use posterior predictive checks^[See Gelman et. al (2020) "Bayesian Workflow"]



## One example of posterior predictive checks

:::: {.columns}



::: {.column width="50%"}


$$
y_i|\eta_i\sim\mathcal{N}(\eta_i, \sigma^2)
$$

- **Model 1** 
$$
\eta_i = \beta_0 + \beta_1 x_i
$$
- **Model 2** 
$$
\eta_i = \beta_0 +  f(x_i)
$$

:::

::: {.column width="50%"}
```{r}
#| echo: false
N = 100
df = data.frame(x = runif(100, 0 , 100)) %>%
  mutate(y = log(x) + rnorm(100, sd = 0.5))

# fit model 1
cmp = ~ Intercept(1) + cov(x, model = "linear")
lik = bru_obs(formula = y~.,
              data = df)
fit = bru(cmp, lik)

# fit model 2
df$x_group = inla.group(df$x, n = 30)
cmp1 = ~ Intercept(1) + cov(x_group, model = "rw2", scale.model = TRUE)
lik1 = bru_obs(formula = y~.,
              data = df)
fit1 = bru(cmp1, lik1)


p1 = predict(fit, df, ~ Intercept + cov)
p2 = predict(fit1, df, ~ Intercept + cov)

```

```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 4
#| out-width: "100%"
df %>% ggplot() + geom_point(aes(x,y)) +
  geom_line(data = p1, aes(x,mean, color = "Model 1")) +
  geom_ribbon(data = p1, aes(x, ymin = q0.025 , ymax = q0.975,
                             fill = "Model 1"), alpha = 0.5) +
  geom_line(data = p2, aes(x_group,mean, color = "Model 2")) +
  geom_ribbon(data = p2, aes(x_group, ymin = q0.025 , ymax = q0.975,
                             fill = "Model 2"), alpha = 0.5) +
  xlab("") + ylab("") +
  
  scale_color_manual(
    name = "Model",        # shared legend title
    values = c("Model 1" = "steelblue", "Model 2" = "tomato")
  ) +
  scale_fill_manual(
    name = "Model",        # must match the color scale name
    values = c("Model 1" = "steelblue", "Model 2" = "tomato")
  ) 
  


```

:::
::::








## One example of posterior predictive checks

1. Sample $y^{1k}_i\sim\pi(y_i|\mathbf{y})$ using `generate()`
2. Compare the distribution of the simulated data with the one of the observed one

```{r}
preds =  generate(fit, df,
  formula = ~ {
    mu <- (Intercept + cov)
    sd <- sqrt(1 / Precision_for_the_Gaussian_observations)
    rnorm(100, mean = mu, sd = sd)
  },
  n.samples = 500
)
preds = data.frame(preds) %>% 
  mutate(id = 1:100) %>%
  pivot_longer(-id)

preds1 =  generate(fit1, df,
  formula = ~ {
    mu <- (Intercept + cov)
    sd <- sqrt(1 / Precision_for_the_Gaussian_observations)
    rnorm(100, mean = mu, sd = sd)
  },
  n.samples = 500
)
preds1 = data.frame(preds1) %>% 
  mutate(id = 1:100) %>%
  pivot_longer(-id)

p1 = ggplot() + geom_density(data = preds, 
                        aes(value, group = name),  color = "#E69F00") +
  geom_density(data = df, aes(y))  +
  xlab("") + ylab("") + ggtitle("Model 1")

p2 = ggplot() + geom_density(data = preds1, 
                        aes(value, group = name),  color = "#E69F00") +
  geom_density(data = df, aes(y))  +
  xlab("") + ylab("") + ggtitle("Model 2")
p1 + p2

```

. . . 

This is just a simple example, but more complex checks can be computed with the same idea!


## Leave Group Our Cross-Validation

This is a new option for cross-validation in `inlabru`

[HOW MUCH SHOULD WE SAY ABOUT THIS??]

# Model validation for LGCP

# Conclusion

## Take home messages

- Model check and model comparison are complex topics
- There are no universal solutions, it all depends on which model characteristics you are interested in.
- `inlabru` provides some easy to compute alternatives
- LGCP require own tools to validate the model

