---
title: "Lecture 1"
from: markdown+emoji
subtitle: "Introduction" 
format:
   metropolis-beamer-revealjs
#     logo:  images/logo_white.png
#     theme: style.scss
# header-logo: images/logo_white.png
slide-number: "c/t"
title-slide-attributes:
#    data-background-image: images/trondheim3.png
    data-background-size: cover
    data-background-opacity: "0.55"
author:
  - name: Sara Martino
    #orcid: 0000-0002-6879-4412
    email: sara.martino@ntnu.no
    affiliations: Dept. of Mathematical Science, NTNU
# date: May 22, 2025
# bibliography: references.bib
embed-resources: true
editor: 
  markdown: 
    wrap: 72
#nocite: |
#  @INLA_paper
---


```{r setup}
#| include: false

knitr::opts_chunk$set(echo = FALSE,  
                      message=FALSE, 
                      warning=FALSE, 
                      strip.white=TRUE, 
                      prompt=FALSE,
                      fig.align="center",
                       out.width = "60%")

library(knitr)    # For knitting document and include_graphics function
library(ggplot2)  # For plotting
library(png) 
library(tidyverse)
library(INLA)
library(BAS)
library(patchwork)

```



# Introduction

## What is INLA? What is `inlabru`?

**The short answer:**

> INLA is a fast method to do Bayesian inference with
latent Gaussian models and `inlabru` is an `R`-package that
implements this method with a flexible and simple interface.

\pause

**The (much) longer answer:**

cite some papers here


## Where?

The software, information, examples and help can be found at 

cite github for inlabru
	


* paper
* tutorials
* discussion group

## So... Why should you use `inlabru`?{ auto-animate="true"}

::: incremental
-  What type of problems can we solve?
-  What type of models can we use?
-  When can we use it?
:::


## So... Why should you use `inlabru`?{ auto-animate="true"}

-  What type of problems can we solve?
-  What type of models can we use?
-  When can we use it?


>To give proper answers to these questions, we 
need to start at the very beginning ..


## The core { auto-animate="true"}
* We have observed something.
```{r}
data("bodyfat")
p1 = bodyfat %>% ggplot() + geom_point(aes(Abdomen,Bodyfat)) 
p1
```

## The core{ auto-animate="true"}
* We have observed something.
* We have questions. 

```{r}
my_image <- readPNG("figures_slides2/question.png", native = TRUE)
p1  +
  annotate("text", x=120, y=10, label="Does bodyfat vary\n with abdomen circunference??", size = 6,
              color="red")+ 
  inset_element(p = my_image,
                left = 0.5,
                bottom = 0.55,
                right = 0.95,
                top = 0.95)
```

## The core{ auto-animate="true"}

* We have observed something.
* We have questions. 
* We want answers!



## How do we find answers?{ auto-animate="true"}
We need to make choices:

>- Bayesian or frequentist?
>- How do we model the data?
>- How do we compute the answer?

## How do we find answers?{ auto-animate="true"}
We need to make choices:

- Bayesian or frequentist?
- How do we model the data?
- How do we compute the answer?


These questions are **not** independent.


## Bayesian or frequentist?{ auto-animate="true"}

In this course we embrace the Bayesian perspective

- There are no "_true but unknown_" parameters !



```{r}
temp <-expression(y == paste(alpha, " + ", beta, "x"))
p1 +  annotate(geom="text", x=85, y=45, parse = T, label = as.character(temp),
              size=12) 

```


## Bayesian or frequentist?{ auto-animate="true"}

In this course we embrace the Bayesian perspective

- There are no "_true but unknown_" parameters !
- Every parameter is described by a probability distribution!



```{r}
set.seed(100)
b0 = rnorm(100, -40, 0.35)
b1 = rnorm(100, 0.63, 0.05)


temp <-expression(y == paste(alpha, " + ", beta, "x"))
p2 = p1 +  annotate(geom="text", x=85, y=45, parse = T, label = as.character(temp),
              size=12) +  geom_abline( slope = b1, intercept = b0, color = "grey", alpha = 0.5) + geom_point(aes(Abdomen,Bodyfat)) 
p2

```


## Bayesian or frequentist?{ auto-animate="true"}

In this course we embrace the Bayesian perspective

- There are no "_true but unknown_" parameters !
- Every parameter is described by a probability distribution!
- Evidence from the data is used to update the belief we had before observing the data!


```{r}

temp <-expression(y == paste(alpha, " + ", beta, "x"))

p2 + geom_smooth(aes(Abdomen, Bodyfat), method = "lm", fill = "red")

```


## Let's formalize this a bit...

Assume a simple linear regression model with Gaussian observations $y = (y_1 , \ldots, y_n)$, where
$$
		\text{E}(y_i) = \alpha + \beta x_i,  \text{Var}(y_i) = \tau^{-1}, \quad i=1,\ldots, n
$$



## Topics of this lecture

  - Why Bayesian statistics
  - What Latent Gaussian Models (LGMs) are and why are they useful
  - What are model components
  - How to fit simple models with inlabru
  

# The `inlabru` workflow

```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false


# Define model components
comps <- component_1(...) + component_2(...) + ...

# Define the model predictor
pred <- non_linear_function(component_1, component_2, ...)

# Build the observation model
lik <- bru_obs(components = comps,
            formula = pred,
            ...)

# Fit the model
fit <- bru(comps, lik, ...)
```


# Bayesian Statistics

## 

# Latent Gaussian Models

## What is a LGM?

## A simple linear model

$$
\begin{aligned}
y_i|\eta_i & \sim\mathcal{N}(\mu_i, \sigma^2), \qquad i = 1\dots,N\\
\eta_i & = \mu_i = \beta_0 + \beta_1x_i\\
\beta_0,\beta_1&\sim\mathcal{N}(0,\tau)
\end{aligned}
$$
  Is this a LGM?
  