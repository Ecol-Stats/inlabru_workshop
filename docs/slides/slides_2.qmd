---
title: "Lecture 1"
from: markdown+emoji
subtitle: "Introduction" 
format:
   metropolis-beamer-revealjs
#     logo:  images/logo_white.png
#     theme: style.scss
# header-logo: images/logo_white.png
slide-number: "c/t"
title-slide-attributes:
#    data-background-image: images/trondheim3.png
    data-background-size: cover
    data-background-opacity: "0.55"
author:
  - name: Sara Martino
    #orcid: 0000-0002-6879-4412
    email: sara.martino@ntnu.no
    affiliations: Dept. of Mathematical Science, NTNU
# date: May 22, 2025
# bibliography: references.bib
embed-resources: true
editor: 
  markdown: 
    wrap: 72
execute:
  allow-html: true
#nocite: |
#  @INLA_paper
---


```{r setup}
#| include: false

knitr::opts_chunk$set(echo = FALSE,  
                      message=FALSE, 
                      warning=FALSE, 
                      strip.white=TRUE, 
                      prompt=FALSE,
                      fig.align="center",
                       out.width = "60%")

library(knitr)    # For knitting document and include_graphics function
library(ggplot2)  # For plotting
library(png) 
library(tidyverse)
library(INLA)
library(BAS)
library(patchwork)

```



# Introduction

## What is INLA? What is `inlabru`?

**The short answer:**

> INLA is a fast method to do Bayesian inference with
latent Gaussian models and `inlabru` is an `R`-package that
implements this method with a flexible and simple interface.

\pause

**The (much) longer answer:**

cite some papers here


## Where?

The software, information, examples and help can be found at 

cite github for inlabru
	


* paper
* tutorials
* discussion group

## So... Why should you use `inlabru`?{ auto-animate="true"}

::: incremental
-  What type of problems can we solve?
-  What type of models can we use?
-  When can we use it?
:::


## So... Why should you use `inlabru`?{ auto-animate="true"}

-  What type of problems can we solve?
-  What type of models can we use?
-  When can we use it?


>To give proper answers to these questions, we 
need to start at the very beginning ..


## The core { auto-animate="true"}
* We have observed something.
```{r}
data("bodyfat")
p1 = bodyfat %>% ggplot() + geom_point(aes(Abdomen,Bodyfat)) 
p1
```

## The core{ auto-animate="true"}
* We have observed something.
* We have questions. 

```{r}
my_image <- readPNG("figures_slides2/question.png", native = TRUE)
p1  +
  annotate("text", x=120, y=10, label="Does bodyfat vary\n with abdomen circunference??", size = 6,
              color="red")+ 
  inset_element(p = my_image,
                left = 0.5,
                bottom = 0.55,
                right = 0.95,
                top = 0.95)
```

## The core{ auto-animate="true"}

* We have observed something.
* We have questions. 
* We want answers!



## How do we find answers?{ auto-animate="true"}
We need to make choices:

>- Bayesian or frequentist?
>- How do we model the data?
>- How do we compute the answer?

## How do we find answers?{ auto-animate="true"}
We need to make choices:

- Bayesian or frequentist?
- How do we model the data?
- How do we compute the answer?


These questions are **not** independent.


## Bayesian or frequentist?{ auto-animate="true"}

In this course we embrace the Bayesian perspective

- There are no "_true but unknown_" parameters !



```{r}
temp <-expression(y == paste(beta[0], " + ", beta[1], "x"))
p2 = p1 +  annotate(geom="text", x=85, y=45, parse = T, label = as.character(temp),
              size=12) 

p2
```


## Bayesian or frequentist?{ auto-animate="true"}

In this course we embrace the Bayesian perspective

- There are no "_true but unknown_" parameters !
- Every parameter is described by a probability distribution!



```{r}
set.seed(100)
b0 = rnorm(100, -40, 0.35)
b1 = rnorm(100, 0.63, 0.05)


temp <-expression(y == paste(alpha, " + ", beta, "x"))
p3 = p2  +  geom_abline( slope = b1, intercept = b0, color = "grey", alpha = 0.5) + geom_point(aes(Abdomen,Bodyfat)) 
p3

```


## Bayesian or frequentist?{ auto-animate="true"}

In this course we embrace the Bayesian perspective

- There are no "_true but unknown_" parameters !
- Every parameter is described by a probability distribution!
- Evidence from the data is used to update the belief we had before observing the data!


```{r}

temp <-expression(y == paste(alpha, " + ", beta, "x"))

p3 + geom_smooth(aes(Abdomen, Bodyfat), method = "lm", fill = "red")

```


## Let's formalize this a bit...{ auto-animate="true"}

The elements of a `inlabru` friendly statistical model are:

1. The observational model
$$
\begin{aligned}
y_i|\eta_i, \sigma^2 & \sim\mathcal{N}(\eta_i,\sigma^2),\qquad i = 1,\dots,n\\
E(y_i|\eta_i, \sigma^2) & = \eta_i
\end{aligned}
$$
**Note**: We assume that, given the  _linear predictor_ $\eta$, the data are independent on each other! Data dependence is expressed through the _components_ if the linear predictor.



## Let's formalize this a bit...{ auto-animate="true"}

The elements of a `inlabru` friendly statistical model are:

1. The observational model $y_i|\eta_i,\sigma^2\sim\mathcal{N}(\eta_i,\sigma^2),\qquad i = 1,\dots,n$


2. A model for the _linear predictor_ 
$$
E(y_i|\eta_i,\sigma^2) = \eta_i = \beta_0 + \beta_1x_i 
$$







## Let's formalize this a bit...{ auto-animate="true"}

The elements of a `inlabru` friendly statistical model are:

1. The observational model $y_i|\eta_i,\sigma^2\sim\mathcal{N}(\eta_i,\sigma^2),\qquad i = 1,\dots,n$


2. A model for the _linear predictor_

$$
E(y_i|\eta_i,\sigma^2) = \eta_i = \color{red}{\boxed{\beta_0}} + \color{red}{\boxed{\beta_1x_i} }
$$

**Note 1:** These are the _components_ of our model! These explain the _dependence structure_ of the data.



## Let's formalize this a bit...{ auto-animate="true"}

The elements of a `inlabru` friendly statistical model are:

1. The observational model $y_i|\eta_i,\sigma^2\sim\mathcal{N}(\eta_i,\sigma^2),\qquad i = 1,\dots,n$


2. A model for the _linear predictor_ $\eta_i = \color{red}{\boxed{\beta_0}} + \color{red}{\boxed{\beta_1x_i} }$




3. A prior  for the model components $\textbf{u}$
$$
\mathbf{u} = \{\beta_0, \beta_1\}\sim\mathcal{N}(0,\mathbf{Q}^{-1})
$$
**Note:** These have always a Gaussian prior and are use to explain the dependencies among data!







## Let's formalize this a bit...{ auto-animate="true"}

The elements of a `inlabru` friendly statistical model are:

1. The observational model $y_i|\eta_i,\sigma^2\sim\mathcal{N}(\eta_i,\sigma^2),\qquad i = 1,\dots,n$


2. A model for the _linear predictor_ $\eta_i = \color{red}{\boxed{\beta_0}} + \color{red}{\boxed{\beta_1x_i} }$




3. A prior  for the model components $\mathbf{u} = \{\beta_0, \beta_1\}\sim\mathcal{N}(0,\mathbf{Q}^{-1})$

4. A prior for the non-gaussian parameters $\theta$
$$
\theta = \sigma^2
$$


## Latent Gaussian Models (LGM){ auto-animate="true"}

::: columns
::: {.column width="50%"}

1. [The observation model: 
$$
\pi(\mathbf{y}|\eta,\theta) = \prod_{i=1}^{n}\pi(y_i|\eta_i,\theta)
$$]{style="color: red;"}

2. Linear predictor $\eta_i = \beta_0 = \beta_1 x_i$
3. Latent Gaussian field $\pi(\mathbf{u}|\theta)$
4. The hyperparameters:  $\pi(\theta)$
:::

::: {.column width="50%"}

- [**Stage 1** The data generating process]{style="color: red;"}

:::
:::


## Latent Gaussian Models (LGM){ auto-animate="true"}

::: columns
::: {.column width="50%"}

1. The observation model: 
$$
\pi(\mathbf{y}|\eta,\theta) = \prod_{i=1}^{n}\pi(y_i|\eta_i,\theta)
$$

2. [Linear predictor $\eta_i = \beta_0 = \beta_1 x_i$]{style="color: red;"}
3. [Latent Gaussian field $\pi(\mathbf{u}|\theta)$]{style="color: red;"}
4. The hyperparameters:  $\pi(\theta)$
:::

::: {.column width="50%"}

- **Stage 1** The data generating process

- [**Stage 2** The dependence structure]{style="color: red;"}


:::
:::

## Latent Gaussian Models (LGM){ auto-animate="true"}


::: columns
::: {.column width="50%"}

1. The observation model: 
$$
\pi(\mathbf{y}|\eta,\theta) = \prod_{i=1}^{n}\pi(y_i|\eta_i,\theta)
$$

2. Linear predictor $\eta_i = \beta_0 = \beta_1 x_i$
3. Latent Gaussian field $\pi(\mathbf{u}|\theta)$
4. [The hyperparameters:  $\pi(\theta)$]{style="color: red;"}
:::

::: {.column width="50%"}

- **Stage 1** The data generating process

- **Stage 2** The dependence structure

- [**Stage 3** The hyperparameters]{style="color: red;"}

:::
:::

## Latent Gaussian Models (LGM){ auto-animate="true"}


::: columns
::: {.column  width="50%"}

::: {.r-stack .smaller style="font-size: 0.9em;"}

1. The observation model: 
$$
\pi(\mathbf{y}|\eta,\theta) = \prod_{i=1}^{n}\pi(y_i|\eta_i,\theta)
$$

2. Linear predictor $\eta_i = \beta_0 = \beta_1 x_i$
3. Latent Gaussian field $\pi(\mathbf{u}|\theta)$
4. The hyperparameters:  $\pi(\theta)$
:::
:::

::: {.column   width="50%"}


::: {.r-stack .smaller style="font-size: 0.9em;"}


- **Stage 1** The data generating process

- **Stage 2** The dependence structure

- **Stage 3** The hyperparameters

:::



:::
:::



::: {.r-stack .larger style="font-size: 1.5em;"}
**Q**: What are we interested in?
:::


$$
\pi(\mathbf{u},\theta|\mathbf{y})\propto \pi(\mathbf{y}|\mathbf{u},\theta)\pi(\mathbf{u}|\theta)\pi(\theta)
$$

. . . 

Here is where `inlabru` comes in :smiley:


## `inlabru` for linear regression{ auto-animate="true"}

::: columns
::: {.column width="50%"}
**The Model**
$$
\begin{aligned}
y_i|\eta_i, \sigma^2 & \sim \mathcal{N}(\eta_i,\sigma^2)\\
\eta_i & = \beta_0 + \beta_i x_i
\end{aligned}
$$
:::

::: {.column width="50%"}
```{r}
#| echo: true
library(inlabru)
bodyfat[1:3,c("Abdomen","Bodyfat")]
```
:::
:::

**The code**
```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false

# define model components
cmp =  ~ -1 + beta0(1) + beta1(Abdomen, model = "linear")

# define model predictor
eta = Bodyfat ~ beta0 + beta1

# build the observation model
lik = bru_obs(formula = eta,
              family = "gaussian",
              data = bodyfat)

# fit the model
fit = bru(cmp, lik)

```


## `inlabru` for linear regression{ auto-animate="true"}
::: columns
::: {.column width="50%"}
**The Model**
$$
\begin{aligned}
y_i|\eta_i, \sigma^2 & \sim \mathcal{N}(\eta_i,\sigma^2)\\
\eta_i & = \color{red}{\boxed{\beta_0}} + \color{red}{\boxed{\beta_i x_i}}
\end{aligned}
$$
:::

::: {.column width="50%"}
```{r}
#| echo: true
bodyfat[1:3,c("Abdomen","Bodyfat")]
```
:::
:::

**The code**
```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "1-3"
# define model components
cmp =  ~ -1 + beta0(1) + beta1(Abdomen, model = "linear")

# define model predictor
eta = Bodyfat ~ beta0 + beta1

# build the observation model
lik = bru_obs(formula = eta,
              family = "gaussian",
              data = bodyfat)

# fit the model
fit = bru(cmp, lik)
```



## `inlabru` for linear regression{ auto-animate="true"}
::: columns
::: {.column width="50%"}
**The Model**
$$
\begin{aligned}
y_i|\eta_i, \sigma^2 & \sim \mathcal{N}(\eta_i,\sigma^2)\\
\eta_i & = \color{red}{\boxed{\beta_0 + \beta_i x_i}}
\end{aligned}
$$
:::

::: {.column width="50%"}
```{r}
#| echo: true
library(inlabru)
bodyfat[1:3,c("Abdomen","Bodyfat")]
```
:::
:::

**The code**
```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "4-6"

# define model components
cmp =  ~ -1 + beta0(1) + beta1(Abdomen, model = "linear")

# define model predictor
eta = Bodyfat ~ beta0 + beta1

# build the observation model
lik = bru_obs(formula = eta,
              family = "gaussian",
              data = bodyfat)

# fit the model
fit = bru(cmp, lik)

```












## `inlabru` for linear regression{ auto-animate="true"}
::: columns
::: {.column width="50%"}
**The Model**
$$
\begin{aligned}
\color{red}{\boxed{y_i|\eta_i, \sigma^2}} & \color{red}{\boxed{\sim \mathcal{N}(\eta_i,\sigma^2)}}\\
\eta_i & = \beta_0 + \beta_i x_i
\end{aligned}
$$
:::

::: {.column width="50%"}
```{r}
#| echo: true
library(inlabru)
bodyfat[1:3,c("Abdomen","Bodyfat")]
```
:::
:::

**The code**
```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "7-10"

# define model components
cmp =  ~ -1 + beta0(1) + beta1(Abdomen, model = "linear")

# define model predictor
eta = Bodyfat ~ beta0 + beta1

# build the observation model
lik = bru_obs(formula = eta,
              family = "gaussian",
              data = bodyfat)

# fit the model
fit = bru(cmp, lik)

```













## `inlabru` for linear regression{ auto-animate="true"}
::: columns
::: {.column width="50%"}
**The Model**
$$
\begin{aligned}
y_i|\eta_i, \sigma^2 & \sim \mathcal{N}(\eta_i,\sigma^2)\\
\eta_i & = \beta_0 + \beta_i x_i
\end{aligned}
$$
:::

::: {.column width="50%"}
```{r}
#| echo: true
library(inlabru)
bodyfat[1:3,c("Abdomen","Bodyfat")]
```
:::
:::

**The code**
```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "12-13"

# define model components
cmp =  ~ -1 + beta0(1) + beta1(Abdomen, model = "linear")

# define model predictor
eta = Bodyfat ~ beta0 + beta1

# build the observation model
lik = bru_obs(formula = eta,
              family = "gaussian",
              data = bodyfat)

# fit the model
fit = bru(cmp, lik)

```





## `inlabru` for linear regression

```{r}
#| echo: false
#| eval: true
#| cache: true

# define model components
library(inlabru)
library(ggplot2)
library(tidyverse)
library(patchwork)

cmp =  ~ -1 + beta0(1) + beta1(Abdomen, model = "linear")

# define model predictor
eta = Bodyfat ~ beta0 + beta1

# build the observation model
lik = bru_obs(formula = eta,
              family = "gaussian",
              data = bodyfat)

# fit the model
fit = bru(cmp, lik)

prior = data.frame(x = seq(-50,50)) %>%
  mutate(y = dnorm(x,0,sqrt(1/0.001)))
p_int = fit$marginals.fixed$beta0 %>% ggplot() + geom_line(aes(x,y)) +
  geom_line(data = prior, aes(x,y), linetype= "dashed") + xlab("") + ylab("") +
  ggtitle("Intercept")


p_cov = fit$marginals.fixed$beta1 %>% ggplot() + geom_line(aes(x,y)) +
  geom_line(data = prior, aes(x,y), linetype= "dashed") + xlab("") + ylab("") +
  ggtitle("Covariate") + coord_cartesian(xlim = c(0.5,0.8))


preds = predict(fit, data.frame(Abdomen = seq(60,150)), ~ beta0 + beta1)


p_pred = preds %>% ggplot() + 
  geom_point(data = bodyfat, aes(Abdomen, Bodyfat)) + 
  geom_line(aes(Abdomen, mean), color = "blue") +
  geom_ribbon(aes(Abdomen, ymin = q0.025, ymax = q0.975), alpha = 0.5, fill = "blue")


ff = generate(fit, 
        data.frame(Abdomen = seq(60,150)), 
        ~ data.frame(mean = beta0 + beta1,
                     sd = sqrt(1/Precision_for_the_Gaussian_observations)),
        n.samples = 5000)

ff1 = sapply(ff, function(xx) rnorm(91, mean = xx[,1], sd = xx[,2]))


pred_int = data.frame(Abdomen = seq(60,150),
                      mean = apply(ff1,1,mean),
                      q1 = apply(ff1, 1, quantile,probs =  0.025),
                      q2 = apply(ff1, 1, quantile, 0.975)
                      )
p3 = p_pred + 
   geom_ribbon(data = pred_int, aes(Abdomen, ymin = q1, ymax = q2), alpha = 0.5, fill = "red") + ggtitle("Credible and Prediction Interval")
```

```{r}
#| echo: false
#| eval: true
 
(p_int | p_cov) / p3

```









## Real-world datasets are  more complicated!

Data can have several dependence structures: temporal, spatial,...

Using a Bayesian framework:

- Build (hierarchical) models to account for potentially complicated dependency structures in the data.

- Attribute uncertainty to model parameters and latent variables using priors.

Two main challenges:

- Need computationally efficient methods to calculate posteriors (this is where INLA helps!). 
- Select priors in a sensible way (we'll talk about this)



## The good news!!{ .smaller auto-animate="true"}

In many cases complicated spatio-temporal models are _just_ special cases of the same model structure!! :smiley:

- **Stage 1**: What is the distribution of the responses?

- **Stage 2**: What are the model components? and what is their distribution? 

- **Stage 3**: What are our prior beliefs about the parameters controlling the components in the model?


## The good news!!{ .smaller auto-animate="true"}

In many cases complicated spatio-temporal models are _just_ special cases of the same model structure!! :smiley:

- **Stage 1**: What is the distribution of the responses?

  - Gaussian response? (temperature, rainfall, fish weight …)
  - Count data? (people infected with a disease in each area)
  - Point pattern? (locations of trees in a forest)
  - Binary data? (yes/no response, binary image)
  - Survival data? (recovery time, time to death)

- **Stage 2**: What are the model components? and what is their distribution? 

- **Stage 3**: What are our prior beliefs about the parameters controlling the components in the model?


## The good news!!{ .smaller auto-animate="true"}

In many cases complicated spatio-temporal models are _just_ special cases of the same model structure!! :smiley:

- **Stage 1**: What is the distribution of the responses?

  - We assume data to be _conditionally_ independent given the model components and some hyperparameters
  - This means that all dependencies in data are explained in Stage 2.

- **Stage 2**: What are the model components? and what is their distribution? 

- **Stage 3**: What are our prior beliefs about the parameters controlling the components in the model?


## The good news!!{ .smaller auto-animate="true"}

In many cases complicated spatio-temporal models are _just_ special cases of the same model structure!! :smiley:

- **Stage 1**: What is the distribution of the responses?


- **Stage 2**: What are the model components? and what is their distribution? 

Here we can have:

  - Fixed effects for covariates
  - Unstructured random effects (individual effects, group effects)
  - Structured random effects (AR(1), regional effects, )

These are linked to the responses in the likelihood through _linear predictors_.

- **Stage 3**: What are our prior beliefs about the parameters controlling the components in the model?


## The good news!!{ .smaller auto-animate="true"}

In many cases complicated spatio-temporal models are _just_ special cases of the same model structure!! :smiley:

- **Stage 1**: What is the distribution of the responses?


- **Stage 2**: What are the model components? and what is their distribution? 



- **Stage 3**: What are our prior beliefs about the parameters controlling the components in the model?

  The likelihood and the latent model typically have hyperparameters that control their behavior.

  They can include:

  - Variance of observation noise
  - Dispersion parameter in the negative binomial model
  - Variance of unstructured effects
  - ...





## The second good news! 

No matter how complicated is your model, the `inlabru` workflow is _always_ the same :smiley:


```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "1-3|5-7|9-13|15-16"


# Define model components
comps <- component_1(...) + 
  component_2(...) + ...

# Define the model predictor
pred <- non_linear_function(component_1, 
                            component_2, ...)

# Build the observation model
lik <- bru_obs(formula = pred,
               family = ... ,
               data = ... ,
                ...)

# Fit the model
fit <- bru(comps, lik, ...)
```




## The Tokyo rainfall data { auto-animate="true"}

One example with time series: Rainfall over 1 mm in the Tokyo area for each calendar day during two
years (1983-84) are registered.

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

data("Tokyo")
pTokyo = ggplot() + geom_point(data = Tokyo, aes(time, y)) +
  ylab("") + xlab("")
pTokyo

cmp= ~ -1 + Intercept(1) + time(time, model ="rw2")
formula = y ~ Intercept + time
lik = bru_obs(formula = formula,
              data = Tokyo,
              Ntrials = n,
              family = "binomial")
fit = bru(cmp, lik)

```

## The Tokyo rainfall data { auto-animate="true"}

One example with time series: Rainfall over 1 mm in the Tokyo area for each calendar day during two
years (1983-84) are registered.


```{r}

dd = data.frame(ii = 1:366,fit$summary.fitted.values[1:366,c(1,3,5)])
pTokyo +
   # Custom the Y scales:
  scale_y_continuous(
    # Features of the first axis
    name = "",
    # Add a second axis and specify its features
    sec.axis = sec_axis( trans=~./2, name="Probability")
  )  + geom_line(data = dd, aes(ii, mean*2)) +
  geom_ribbon(data = dd, aes(ii, ymin = X0.025quant*2, 
                             ymax = 2 *X0.975quant), alpha = 0.5)
```


## The model { auto-animate="true"}

**Stage 1**  The observation model



$$
y_t|\eta_t\sim\text{Bin}(n_t, p_t),\qquad \eta_t = \text{logit}(p_t),\qquad i = 1,\dots,366
$$
$$
n_{i} = \left\{
 \begin{array}{lr}
1, & \text{for}\; 29\; \text{February}\\
2, & \text{other days}
\end{array}\right.
$$
$$
y_{i} =
\begin{cases}
\{0,1\}, & \text{for}\; 29\; \text{February}\\
\{0,1,2\}, & \text{other days}
 \end{cases}
$$

- the likelihood has no hyperparameters 

      
      
## The model { auto-animate="true"}

**Stage 1**  The observation model

$$
y_t|\eta_t\sim\text{Bin}(n_t, p_t),\qquad \eta_t = \text{logit}(p_t),\qquad i = 1,\dots,366
$$

**Stage 2** The latent field
$$
\eta_t = \beta_0 + f(\text{time}_t)
$$

  - probability of rain on day $i$ depends on $x_i$
    
  - $\beta_0$ is an intercept
    
  - $f(\text{time}_t)$ is a RW2 model (this is just a smoother). The smoothness is controlled by a hyperparameter $\tau_f$



## The model { auto-animate="true"}

**Stage 1**  The observation model

$$
y_t|\eta_t\sim\text{Bin}(n_t, p_t),\qquad \eta_t = \text{logit}(p_t),\qquad i = 1,\dots,366
$$

**Stage 2** The latent field
$$
\eta_t = \beta_0 + f(\text{time}_t)
$$


**Stage 3** The hyperparameters

- The structured time effect is controlled by one  parameter $\tau_f$.

- We assign a prior to $\tau_f$ to finalize the model.




## `inlabru` for time series { auto-animate="true"}

::: columns
::: {.column width="50%"}

**The Model**

$$
\begin{aligned}
y_t|\eta_t & \sim \text{Binomial}(n_t,p_t)\\
\text{logit}(p_t) = \eta_i & = \beta_0 + f(\text{time}_t)
\end{aligned}
$$
:::

::: {.column width="50%"}
```{r}
#| echo: true
Tokyo[1:3,]
```
:::
:::

**The code**
```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "4-6"

# define model component
cmp =  ~ -1 + beta0(1) + time_effect(time, model = "rw2", cyclic = TRUE)

# define model predictor
eta = y ~ beta0 + time_effect

# build the observation model
lik = bru_obs(formula = eta,
              family = "binomial",
              Ntrials = n,
              data = Tokyo)

# fit the model
fit = bru(cmp, lik)

```














## `inlabru` for time series { auto-animate="true"}

::: columns
::: {.column width="50%"}

**The Model**

$$
\begin{aligned}
y_t|\eta_t & \sim \text{Binomial}(n_t,p_t)\\
\text{logit}(p_t) = \eta_i & = \color{red}{\boxed{\beta_0}} + \color{red}{\boxed{f(\text{time}_t)}}
\end{aligned}
$$
:::

::: {.column width="50%"}
```{r}
#| echo: true
Tokyo[1:3,]
```
:::
:::

**The code**
```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "1-2"

# define model component
cmp =  ~ -1 + beta0(1) + time_effect(time, model = "rw2", cyclic = TRUE)

# define model predictor
eta = y ~ beta0 + time_effect

# build the observation model
lik = bru_obs(formula = eta,
              family = "binomial",
              Ntrials = n,
              data = Tokyo)

# fit the model
fit = bru(cmp, lik)

```













## `inlabru` for time series { auto-animate="true"}

::: columns
::: {.column width="50%"}

**The Model**

$$
\begin{aligned}
y_t|\eta_t & \sim \text{Binomial}(n_t,p_t)\\
\text{logit}(p_t) = \color{red}{\boxed{\eta_i}} & = \color{red}{\boxed{\beta_0 + f(\text{time}_t)}}
\end{aligned}
$$
:::

::: {.column width="50%"}
```{r}
#| echo: true
Tokyo[1:3,]
```
:::
:::

**The code**
```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "4-5"

# define model component
cmp =  ~ -1 + beta0(1) + time_effect(time, model = "rw2", cyclic = TRUE)

# define model predictor
eta = y ~ beta0 + time_effect

# build the observation model
lik = bru_obs(formula = eta,
              family = "binomial",
              Ntrials = n,
              data = Tokyo)

# fit the model
fit = bru(cmp, lik)

```














## `inlabru` for time series { auto-animate="true"}

::: columns
::: {.column width="50%"}

**The Model**

$$
\begin{aligned}
\color{red}{\boxed{y_t|\eta_t}} & \color{red}{\boxed{\sim \text{Binomial}(n_t,p_t)}}\\
\text{logit}(p_t) = \eta_i & = \beta_0 + f(\text{time}_t)
\end{aligned}
$$
:::

::: {.column width="50%"}
```{r}
#| echo: true
Tokyo[1:3,]
```
:::
:::

**The code**
```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "7-11"

# define model component
cmp =  ~ -1 + beta0(1) + time_effect(time, model = "rw2", cyclic = TRUE)

# define model predictor
eta = y ~ beta0 + time_effect

# build the observation model
lik = bru_obs(formula = eta,
              family = "binomial",
              Ntrials = n,
              data = Tokyo)

# fit the model
fit = bru(cmp, lik)

```














## `inlabru` for time series { auto-animate="true"}

::: columns
::: {.column width="50%"}

**The Model**

$$
\begin{aligned}
y_t|\eta_t & \sim \text{Binomial}(n_t,p_t)\\
\text{logit}(p_t) = \eta_i & = \beta_0 + f(\text{time}_t)
\end{aligned}
$$
:::

::: {.column width="50%"}
```{r}
#| echo: true
Tokyo[1:3,]
```
:::
:::

**The code**
```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "13-14"

# define model component
cmp =  ~ -1 + beta0(1) + time_effect(time, model = "rw2", cyclic = TRUE)

# define model predictor
eta = y ~ beta0 + time_effect

# build the observation model
lik = bru_obs(formula = eta,
              family = "binomial",
              Ntrials = n,
              data = Tokyo)

# fit the model
fit = bru(cmp, lik)

```
















## Example: disease mapping

We observed larynx cancer mortality counts for males
in 544 district of Germany from 1986 to 1990
and want to make a model. 

::: columns
::: {.column width="50%"}

- $y_i$:	The count at location $i$.

- $E_i$: An offset; expected number of cases in district $i$.

- $c_i$: A covariate (level of smoking consumption) at  $i$

- $\boldsymbol{s}_i$:	spatial location $i$ .
:::

::: {.column width="50%"}


```{r, out.width = "110%"}

#| echo: false
#| eval: true
#| warning: false
#| message: false
#|
my.germany.map = function(data, cutpoints=seq(min(data),max(data),length=256), autoscale=FALSE, legend=TRUE, append=FALSE)
{
  if (autoscale)
  {
    data = (data-min(data))/(max(data)-min(data)+1e-8)
  }
  #cutpoints = c(-1e9,cutpoints, 1e9)
  
  # farben <- rainbow(as.numeric(cut(data,cutpoints,include.lowest=T))/length(cutpoints))
  cols =  hcl.colors(256)
  farben <- cols[as.numeric(cut(data,cutpoints,include.lowest=T))]
  
  xmin <- 1:length(germany)
  xmax <- 1:length(germany)
  ymin <- 1:length(germany)
  ymax <- 1:length(germany)
  
  for(i in 1:length(germany))
  {
    xmin[i] <- min(germany[[i]][,2],na.rm=T)
    xmax[i] <- max(germany[[i]][,2],na.rm=T)
    ymin[i] <- min(germany[[i]][,3],na.rm=T)
    ymax[i] <- max(germany[[i]][,3],na.rm=T)
  }
  
  breite <- c(min(xmin),max(xmax))
  hoehe <- c(min(ymin),max(ymax))
  
  if (!append) plot(breite,hoehe,type="n",axes=F, xlab=" ", ylab=" ", asp = 1)
  
  
  for(k in length(germany):1)
  {
    polygon(germany[[k]][,2],germany[[k]][,3],col=farben[k])
  }
  
  
}
source(system.file("demodata/Bym-map.R", package="INLA"))
data(Germany)
# Load data
Germany$region.struct = Germany$region
g <- system.file("demodata/germany.graph" , package = "INLA")
my.germany.map(Germany$Y/Germany$E, autoscale = F)
```

:::
:::


## Bayesian disease mapping{ .small}

>- **Stage 1:**
			We assume the responses are Poisson distributed:
$$			
				y_i \mid \eta_i \sim \text{Poisson}(E_i\exp(\eta_i)))
$$

>-	 **Stage 2:**  $\eta_i$ is a linear function of three 
                        components: an intercept, a covariate $c_i$, a spatially
                        structured effect $\omega$
			likelihood by
$$
				\eta_i = \beta_0 + \beta_1\ c_i + \omega_i
$$

>- **Stage 3:**
>   - $\tau_{\omega}$:	Precisions parameter for the random effects

. . . 

The latent field is $\boldsymbol{u} = (\beta_0, \beta_1, \omega_1, \omega_2,\ldots, \omega_n)$, the hyperparameters are  $\boldsymbol{\theta} = (\tau_{\omega})$, and must be given a prior.



## `inlabru` for disease mapping { auto-animate="true" .smaller}

::: columns
::: {.column width="50%"}

**The Model**

$$
\begin{aligned}
y_i|\eta_t & \sim \text{Poisson}(E_i\lambda_i)\\
\text{log}(\lambda_i) = \eta_i & = \color{red}{\boxed{\beta_0}} + \color{red}{\boxed{\beta_1\ c_i}} + \color{red}{\boxed{\omega_i}}
\end{aligned}
$$
:::

::: {.column width="50%"}
```{r}
#| echo: true
g = system.file("demodata/germany.graph",
                package="INLA")
Germany[1:3,]
```
:::
:::

**The code**
```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "1-3"

# define model component
cmp =  ~ -1 + beta0(1) + beta1(x, model = "linear") +
  space(region, model = "bym2", graph = g)

# define model predictor
eta = Y ~ beta0 + beta1 + space

# build the observation model
lik = bru_obs(formula = eta,
              family = "poisson",
              E = E,
              data = Germany)

# fit the model
fit = bru(cmp, lik)

```















## `inlabru` for disease mapping { auto-animate="true" .smaller}

::: columns
::: {.column width="50%"}

**The Model**

$$
\begin{aligned}
y_i|\eta_t & \sim \text{Poisson}(E_i\lambda_i)\\
\text{log}(\lambda_i) = \color{red}{\boxed{\eta_i}} & = \color{red}{\boxed{\beta_0 + \beta_1\ c_i + \omega_i}}
\end{aligned}
$$
:::

::: {.column width="50%"}
```{r}
#| echo: true
g = system.file("demodata/germany.graph",
                package="INLA")
Germany[1:3,]
```
:::
:::

**The code**
```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "5-6"

# define model component
cmp =  ~ -1 + beta0(1) + beta1(x, model = "linear") +
  space(region, model = "bym2", graph = g)

# define model predictor
eta = Y ~ beta0 + beta1 + space

# build the observation model
lik = bru_obs(formula = eta,
              family = "poisson",
              E = E,
              data = Germany)

# fit the model
fit = bru(cmp, lik)

```


## `inlabru` for disease mapping { auto-animate="true" .smaller}

::: columns
::: {.column width="50%"}

**The Model**

$$
\begin{aligned}
\color{red}{\boxed{y_i|\eta_t}} & \color{red}{\boxed{\sim \text{Poisson}(E_i\lambda_i)}}\\
\text{log}(\lambda_i) = \eta_i & = \beta_0 + \beta_1\ c_i + \omega_i
\end{aligned}
$$
:::

::: {.column width="50%"}
```{r}
#| echo: true
g = system.file("demodata/germany.graph",
                package="INLA")
Germany[1:3,]
```
:::
:::

**The code**
```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "8-12|14-15"

# define model component
cmp =  ~ -1 + beta0(1) + beta1(x, model = "linear") +
  space(region, model = "bym2", graph = g)

# define model predictor
eta = Y ~ beta0 + beta1 + space

# build the observation model
lik = bru_obs(formula = eta,
              family = "poisson",
              E = E,
              data = Germany)

# fit the model
fit = bru(cmp, lik)

```














## `inlabru` for disease mapping { auto-animate="true" .smaller}

::: columns
::: {.column width="50%"}

**The Model**

$$
\begin{aligned}
y_i|\eta_t & \sim \text{Poisson}(E_i\lambda_i)\\
\text{log}(\lambda_i) = \eta_i & = \beta_0 + \beta_1\ c_i + \omega_i
\end{aligned}
$$
:::

::: {.column width="50%"}
```{r}
#| echo: true
g = system.file("demodata/germany.graph",
                package="INLA")
Germany[1:3,]
```
:::
:::

**The code**
```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "4-6"

# define model component
cmp =  ~ -1 + beta0(1) + beta1(x, model = "linear") +
  space(region, model = "bym2", graph = g)

# define model predictor
eta = Y ~ beta0 + beta1 + space

# build the observation model
lik = bru_obs(formula = eta,
              family = "poisson",
              E = E,
              data = Germany)

# fit the model
fit = bru(cmp, lik)

```




























## Take home message!