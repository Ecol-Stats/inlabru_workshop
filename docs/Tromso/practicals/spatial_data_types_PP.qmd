---
title: ""
format: 
  html:
    theme:
      light: flatly
      dark: darkly
  PrettyPDF-pdf:
    keep-tex: true
    number-sections: true
embed-resources: true
editor_options: 
  chunk_output_type: console
---

```{r test}
#| message: false
#| warning: false
#| echo: false
#| purl: false
library(webexercises)
# For plotting
library(mapview)
library(ggplot2)
library(scico) # for colouring palettes
library(patchwork)
library(terra)
library(sf)
# Data manipulation
library(dplyr)
library(lubridate)
library(tidyr)
library(fmesher)
library(INLA)
library(inlabru)
library(viridis)
library(gt)
library(spatstat)
```

## Point process data

In this practical we are going to fit a log Gaussian Cox Proces (LGCP) model to point-referenced data. We will:

-   Learn how to fit a LGCP model in `inlabru`

-   Learn how to add spatial covariates to the model

-   Learn how to do predictions

In **point processes** we measure the locations where events occur (e.g. trees in a forest, earthquakes) and the coordinates of such occurrences are our data. A spatial point process is a random variable operating in continuous space, and we observe realisations of this variable as point patterns across space.

Consider a fixed geographical region $A$. The set of locations at which events occur are denoted $\mathbf{s} = s_1,\ldots,s_n$. We let $N(A)$ be the random variable which represents the number of events in region $A$.

We typically assume that a spatial point pattern is generated by an unique point process over the whole study area. This means that the delimitation of the study area will affect the observed point patters.

We can define the intensity of a point process as the expected number of events per unit area. This can also be thought of as a measure of the density of our points. In some cases, the intensity will be constant over space (homogeneous), while in other cases it can vary by location (inhomogeneous or heterogenous).

In the next example we will be looking at the location where forest fires occurred in the Castilla-La Mancha region of Spain between 1998 and 2007.

### Point-referenced data visualization

In this practical we consider the data `clmfires` in the `spatstat` library.

This dataset is a record of forest fires in the Castilla-La Mancha region of Spain between 1998 and 2007. This region is approximately 400 by 400 kilometres. The coordinates are recorded in kilometres. For more info about the data you can type:

```{r}
#| echo: true
#| eval: false
#| purl: false
?clmfires
```

We first read the data and transform them into an `sf` object. We also create a polygon that represents the border of the Castilla-La Mancha region. We select the data for year 2004 and only those fires caused by lightning.

```{r}
#| fig-cap: "Distribution of the observed forest fires caused by lightning in Castilla-La Mancha in 2004"
#| 
data("clmfires")
pp = st_as_sf(as.data.frame(clmfires) %>%
                dplyr::mutate(x = x, 
                       y = y),
              coords = c("x","y"),
              crs = NA) %>%
  dplyr::filter(cause == "lightning",
         year(date) == 2004)

poly = as.data.frame(clmfires$window$bdry[[1]]) %>%
  mutate(ID = 1)

region = poly %>% 
  st_as_sf(coords = c("x", "y"), crs = NA) %>% 
  dplyr::group_by(ID) %>% 
  summarise(geometry = st_combine(geometry)) %>%
  st_cast("POLYGON") 
  
ggplot() + geom_sf(data = region, alpha = 0) + geom_sf(data = pp)  
```

The library `spatstat` contains also some covariates that can help explain the fires distribution. We can a raster for the scaled values of elevation using the following code:

```{r}

elev_raster = rast(clmfires.extra[[2]]$elevation)
elev_raster = scale(elev_raster)

```

::: {.callout-warning icon="false"}
## {{< bi pencil-square color=#c8793c >}} Task

Using `tidyterra` and `ggplot`, produce a map of the elevation profile in La Mancha region and overlay the spatial point pattern of the fire locations. Use an appropriate colouring scheme for the elevation values. Do you see any pattern?

`r hide("Take hint")`

You can use the `geom_spatraster()` to add a raster layer to a ggplot object. Furthermore the `scico` library contains a nice range of coloring palettes you can choose, type `scico_palette_show()` to see the color palettes that are available.

`r unhide()`

```{r}
#| fig-width: 5
#| fig-height: 5
#| fig-align: center
#| webex.hide: "Click here to see the solution"
#| code-fold: show
#| purl: false
#| message: false
#| warning: false

library(ggplot2)
library(tidyterra)
library(scico)

ggplot() + 
  geom_spatraster(data = elev_raster) + 
  geom_sf(data = pp) + scale_fill_scico()

```
:::

### Workflow for Fitting a LGCP model

The procedure for fitting a point process model in `inlabru`, specifically a log-Gaussian Cox process, follows a similar workflow to that of a geostatistical model, these are:

1.  Build the mesh
2.  Define the SPDE representation of the spatial GF. This includes defining the priors for the range and sd of the spatial GF
3.  Define the *components* of the linear predictor. This includes the spatial GF and all eventual covariates.
4.  Define the observational model
5.  Run the Model

#### Step 1. Building the mesh for a LGCP

First, we need to create the mesh used to approximate the random field. When analyzing point patterns, mesh nodes (integration points) are not typically placed at point locations. Instead, a mesh is created using the `fm_mesh_2d()`function from the `fmesher` library with boundary being our study area.

Key parameters in mesh construction include: `max.edge` for maximum triangle edge lengths, `offset` for inner and outer extensions (to prevent edge effects), and cutoff to avoid overly small triangles in clustered areas.

::: callout-note
**General guidelines for creating the mesh**

1.  Create triangulation meshes with `fm_mesh_2d()`
2.  Move undesired boundary effects away from the domain of interest by extending to a smooth external boundary
3.  Use a coarser resolution in the extension to reduce computational cost (`max.edge=c(inner, outer)`)
4.  Use a fine resolution (subject to available computational resources) for the domain of interest (inner correlation range) and filter out small input point clusters (0 \< `cutoff` \< inner)
5.  Coastlines and similar can be added to the domain specification in `fm_mesh_2d()` through the `boundary` argument.
:::

```{r}
#| fig-width: 4
#| fig-height: 4
#| fig-align: center
# mesh options

mesh <-  fm_mesh_2d(boundary = region,
                  max.edge = c(5, 10),
                  cutoff = 4, crs = NA)

ggplot() + gg(mesh) + geom_sf(data=pp)
```

#### Step 2. Defining the SPDE model

We can now define our SPDE model using the `inla.spde2.pcmatern` function. To help us chose some sensible model parameters it is often useful to consider the spatial extension of our study.

```{r}
st_area(region)
```

We can use PC-priors for the range $\rho$ and the standard deviation $\sigma$ of the MatÃ©rn process

-   Define the prior for the range `prior.range  = (range0,Prange)` $\text{Prob}(\rho<\rho_0) = p_{\rho}$

-   Define the prior for the range `prior.sigma  = (sigma0,Psigma)` $\text{Prob}(\sigma>\sigma_0) = p_{\sigma}$

```{r}
spde_model =  inla.spde2.pcmatern(mesh,
                       prior.sigma = c(1, 0.5), # P(sigma > 1) = 0.5
                       prior.range = c(100, 0.5)) # P(range < 100) = 0.5

```

#### Step 3. Defining model components

**Stage 1** Model for the response

The total number of points in the study region is a Poisson random variable with a spatially varying intensity and log-likelihood given by :

$$
l(\beta;s) = \sum_{i=1}^m \log [\lambda(s_i)] - \int_A \lambda(s)ds.
$$

The integral in this expression can be interpreted as the expected number of points in the whole study region. However, the integral of the intensity function has no close form solution and thus we need to approximate it using numerical integration. `inlabru` has implemented the `fm_int` function to create integration schemes that are especially well suited to integrating the intensity in models with an SPDE effect. We strongly recommend that users use these integration schemes in this context. See `?fm_int` for more information.

```{r}
# build integration scheme
ips = fm_int(mesh,
             samplers = region)

```

Now, for a point process models, the spatial covariates (i.e., the elevation raster) have to be also available at both data-points and quadrature locations. We can check this using the `eval_spatial` function from `inlabru` :

```{r}
eval_spatial(elev_raster,pp) %>% is.na() %>% any()
eval_spatial(elev_raster,ips) %>% is.na() %>% any()
```

Here, we notice that there is a single point that for which elevation values are missing (see @fig-points the red point that lies outside the raster extension ).

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 4.5
#| fig-height: 4.5
#| label: fig-points
#| fig-cap: "Integration scheme for numerical approximation of the stochastic integral in La Mancha Region"
ggplot()+tidyterra::geom_spatraster(data=elev_raster)+
  geom_sf(data=ips,alpha=0.25,col=1)+
   geom_sf(data=ips[3525,],col=2)
```

To solve this, we can increase the raster extension so it covers all both data-points and quadrature locations as well. Then, we can use the `bru_fill_missing()` function to input the missing values with the nearest-available-value/ We can achieve this using the following code:

```{r}
# Extend raster ext by 5 % of the original raster
re <- extend(elev_raster, ext(elev_raster)*1.05)
# Convert to an sf spatial object
re_df <- re %>% stars::st_as_stars() %>%  st_as_sf(na.rm=F)
# fill in missing values using the original raster 
re_df$lyr.1 <- bru_fill_missing(elev_raster,re_df,re_df$lyr.1)
# rasterize
elev_rast_p <- stars::st_rasterize(re_df) %>% rast()

```

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 4.5
#| fig-height: 4.5
ggplot()+tidyterra::geom_spatraster(data=elev_rast_p)+
  geom_sf(data=ips,alpha=0.25,col=1)+
   geom_sf(data=ips[3525,],col=2)
```

**Stage 2** Latent field model

We will model the fire locations as a point process whose intensity function $\lambda(s)$ is additive on the log-scale:

$$
\eta(s) = \log ~ \lambda(s)= \beta_0 +  \beta_1 ~ \text{elevation}(s) + \omega(s),
$$

Here, $\omega(s)$ is the MatÃ©rn Gaussian field capturing the spatial structure of all the locations where fires have occurred (these locations are assumed to be independent given the Gaussian field).

**Stage 3** Hyperparameters

The hyperparameters of the model are $\rho$ and $\sigma$ corresponding to

$$
\omega(s)\sim \text{  GF with range } \rho\  \text{ and maginal variance }\ \sigma^2
$$

**NOTE** In this case the linear predictor $\eta(s)$ consists of three components.

After the mesh and a SPDE representation of the spatial GF have been defined, the model components can be specified using the formula syntax (recall that this allows users to choose meaningful names for model components).

```{r}
cmp_lgcp <-  geometry ~  Intercept(1)  + 
  elev(elev_rast_p, model = "linear") +
  space(geometry, model = spde_model)
```

Recall that the labels `Intercept`, `elev` (elevation effect) and `space` are used to name the components of the model but they equally well could be something else.

Now, notice that we have called the `elev_rast_p` raster data within the `elevation` component. Recall that `inlabru`provides support for `sf` and `terra` data structures, allowing it to extract information from spatial data objects. This is particularly relevant for LGCP, as spatial covariates (e.g., the elevation raster) must be available across the whole study area.

```{r}
formula = geometry ~ Intercept  + elev + space
```

Recall that in an `sf` object, the geo-referenced information of our points is stored in the `geometry` column, and hence we specify this as our response

#### Step 4. Defining th observational model

`inlabru` has support for latent Gaussian Cox processes through the `cp` likelihood family. We just need to supply the `sf` object as our data and the integration scheme `ips`:

```{r}
lik = bru_obs(formula = formula, 
              data = pp, 
              family = "cp",
              ips = ips)

```

::: callout-note
`inlabru` supports a shortcut for defining the integration points using the `domain` and `samplers` argument of `like()`. This `domain` argument expects a list of named domains with inputs that are then internally passed to `fm_int()` to build the integration scheme. The `samplers` argument is used to define subsets of the domain over which the integral should be computed. An equivalent way to define the same model as above is:

```{r}
#| eval: false
#| purl: false

lik = bru_obs(formula = formula, 
              data = pp, 
              family = "cp",
              domain = list(geometry = mesh),
              samplers = region)

```
:::

### Step 5. Run the model

Finally, we can fit the model as usual

```{r}
fit_lgcp = bru(cmp_lgcp,lik)
```

Posterior summaries of fixed effects and hyper parameters can be obtained using the `summary()` function.

```{r}
#| eval: false

summary(fit_lgcp)
```

```{r}
#| echo: false
#| purl: false

rbind(fit_lgcp$summary.fixed[,1:6] ,
fit_lgcp$summary.hyperpar )%>% gt() %>% gt::fmt_number(decimals = 2)
```

### Model predictions

Model predictions can be computed using the `predict` function by supplying the coordinates where the covariate is defined. We can do this by defining a `sf` object using coordinates in our original raster data (we will crop the extension to that of *La Mancha* Region).

```{r}
elev_crop <- terra::crop(x = elev_raster,y = region,mask=TRUE)


pxl1 = data.frame(crds(elev_crop), 
                  as.data.frame(elev_crop$lyr.1)) %>% 
       filter(!is.na(lyr.1)) %>%
st_as_sf(coords = c("x","y")) %>%
  dplyr::select(-lyr.1)
```

The formula object for the prediction can be a generic R expression that references model components using the user-defined names.

The `predict()` method returns an object in the same data format as was used in the predict call which, in this case, is an `sf` points object.

Support for plotting `sf` data objects is available in the `ggplot2` package.

::: panel-tabset
# Model predictions

```{r}
#| echo: false
#| purl: false

lgcp_pred <- predict(
  fit_lgcp,
  pxl1,
  ~ data.frame(
    lambda = exp(Intercept + elev + space), # intensity
    loglambda = Intercept + elev +space,  #log-intensity
    GF = space # matern field
  )
)

ggplot() +
  gg(lgcp_pred$loglambda, geom = "tile") + 
  scale_fill_viridis(name=expression(log(lambda)))+
  ggplot() +
  gg(lgcp_pred$loglambda, geom = "tile",aes(fill=sd)) + 
  scale_fill_viridis(name=expression(stdev-log(lambda)),option = "B")+
  ggplot() +
  gg(lgcp_pred$lambda, geom = "tile") + scale_fill_viridis(name=expression(lambda))+
  ggplot() +
  gg(lgcp_pred$GF, geom = "tile") + scico::scale_fill_scico(name="Spatial effect")+
  plot_layout(ncol=2)


```

# R Code

```{r}
#| eval: false

lgcp_pred <- predict(
  fit_lgcp,
  pxl1,
  ~ data.frame(
    lambda = exp(Intercept + elev + space), # intensity
    loglambda = Intercept + elev +space,  #log-intensity
    GF = space # matern field
  )
)

# predicted log intensity
ggplot() + gg(lgcp_pred$loglambda, geom = "tile") 
# standard deviation of the predicted log intensity
ggplot() + gg(lgcp_pred$loglambda, geom = "tile",aes(fill=sd)) 
# predicted intensity
ggplot() +  gg(lgcp_pred$lambda, geom = "tile") 
# spatial field
ggplot() +  gg(lgcp_pred$GF, geom = "tile") 
```
:::
