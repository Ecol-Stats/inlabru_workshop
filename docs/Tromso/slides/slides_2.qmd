---
title: "Lecture 2"
subtitle: Spatial Modelling with `inlabru`
author:
  - name: Jafet Belmont 
    email: Jafet.BelmontOsuna@glasgow.ac.uk
    affiliations: School of Mathematics and Statistics, University of Glasgow
  - name: Janine Illian
format:
  revealjs:
    margin: 0
    logo:  UofG.png
    theme: uofg_theme.scss
    header-includes: |
      <script src="custom.js" type="application/javascript"></script>
title-slide-attributes: 
  data-background-color: "#f1f1f1"
slide-number: true
date: 12/15/2025
date-format: long
execute: 
  eval: true
  echo: true
embed-resources: true
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| include: false

knitr::opts_chunk$set(echo = FALSE,  
                      message=FALSE, 
                      warning=FALSE, 
                      strip.white=TRUE, 
                      prompt=FALSE,
                      fig.align="center",
                       out.width = "60%")

library(knitr)    # For knitting document and include_graphics function
library(ggplot2)  # For plotting
library(png) 
library(tidyverse)
library(INLA)
library(BAS)
library(patchwork)
library(inlabru)

```

##  {background-image="river.png"}

::: {style="height: 50px;"}
:::

::: {.blockquote style="color: #FFFFFF; background-color:rgb(38, 38, 38,0.9); font-size: 1.25em; padding: 20px; border-radius: 5px;"}
> Space is inherent to all ecological processes, influencing dynamics such as migration, dispersal, and species interactions. A primary goal in ecology is to understand how these processes shape species distributions and dynamics across space.
:::

## Statistics in space

-   Many natural processes take place in space

-   Large amounts of data collected in space;

-   Recent technologies facilitate access to complex spatial data sets.

::: incremental
-   Spatial statistical analysis is often not complex enough

-   Inaccessible to practitioners

-   Literature written for statisticians development of methodology often not linked to applications (unrealistic assumptions)

-   Difficult to apply (unless you are an expert statistician and programmer)

-   we shall see that `inlabru` can help with this...
:::

## Spatial data structures {.smaller auto-animate="true" background-color="#FFFFFF"}

We can distinguish three types of spatial data structures

:::::: columns
::: {.column width="33.33%"}
**Areal data**

![Map of bird conservation regions (BCRs) showing the proportion of bird species within each region showing a declining trend](figures/BCRtrends.jpg){fig-align="center" width="442"}
:::

::: {.column width="33.33%"}
**Geostatistical data**

![Scotland river temperature monitoring network](figures/srtmn-png.png){fig-align="center" width="605"}
:::

::: {.column width="33.33%"}
**Point-referenced data**

![Occurrence records of four ungulate species in the Tibet,](figures/10980_2021_1202_Fig1_HTML.png){fig-align="center" width="421"}
:::
::::::

## Spatial data structures {.smaller auto-animate="true" background-color="#FFFFFF"}

We can distinguish three types of spatial data structures

:::::: columns
::: {.column width="60%"}
**Areal data**

In areal data our measurements are summarised across a set of **discrete**, non-overlapping spatial units.

![Map of bird conservation regions (BCRs) showing the proportion of bird species within each region showing a declining trend](figures/BCRtrends.jpg){fig-align="center" width="442"}
:::

::: {.column width="20%"}
**Geostatistical data**

![Scotland river temperature monitoring network](figures/srtmn-png.png){fig-align="center" width="605"}
:::

::: {.column width="20%"}
**Point-referenced data**

![Occurrence records of four ungulate species in the Tibet](figures/10980_2021_1202_Fig1_HTML.png){fig-align="center" width="421"}
:::
::::::

## Spatial data structures {.smaller auto-animate="true" background-color="#FFFFFF"}

We can distinguish three types of spatial data structures

:::::: columns
::: {.column width="20%"}
**Areal data**

![Map of bird conservation regions (BCRs) showing the proportion of bird species within each region showing a declining trend](figures/BCRtrends.jpg){fig-align="center" width="442"}
:::

::: {.column width="60%"}
**Geostatistical data**

In geostatistical data, measurements a **continuous** process are taken at a set of fixed locations.

![Scotland river temperature monitoring network](figures/srtmn-png.png){fig-align="center" width="605"}
:::

::: {.column width="20%"}
**Point-referenced data**

![Occurrence records of four ungulate species in the Tibet,](figures/10980_2021_1202_Fig1_HTML.png){fig-align="center" width="421"}
:::
::::::

## Spatial data structures {.smaller auto-animate="true" background-color="#FFFFFF"}

We can distinguish three types of spatial data structures

:::::: columns
::: {.column width="20%"}
**Areal data**

![Map of bird conservation regions (BCRs) showing the proportion of bird species within each region showing a declining trend](figures/BCRtrends.jpg){fig-align="center" width="442"}
:::

::: {.column width="20%"}
**Geostatistical data**

![Scotland river temperature monitoring network](figures/srtmn-png.png){fig-align="center" width="605"}
:::

::: {.column width="60%"}
**Point-referenced data**

In point-referenced data we measure the locations where events occur (e.g. trees in a forest, earthquakes) and the coordinates of such occurrences are our data.

![Occurrence records of four ungulate species in the Tibet,](figures/10980_2021_1202_Fig1_HTML.png){fig-align="center" width="421"}
:::
::::::

# Areal Data

## How do we model this? {auto-animate="\"true"}

Imagine we have animal counts in each region. We can model them as Poisson

$$
y_i \sim \mathrm{Poisson}(e^{\eta_i})
$$ How do we model the linear predictor $\eta_i$?

::: {.fragment .fade-in}
-   We could model the number of animals in each region independently

$$
 \eta_i = \beta_0 + \mathbf{x}'\beta + u_i ~~~ u_i \overset{iid}{\sim} N(0,\sigma^2_i)
$$

-   where regional differences are accounted through a *random effect*
:::

## How do we model this? {auto-animate= "true"}

Imagine we have animal counts in each region. We can model them as Poisson

$$
y_i \sim \mathrm{Poisson}(e^{\eta_i})
$$ How do we model the linear predictor $\eta_i$?

::: {.fragment .fade-in}
-   But... what if the distribution is *inhomogeneous*?
-   If there’s an area where the animal is rare, we’ll get lots of zero counts
:::

## How do we model this? {auto-animate="true"}

Imagine we have animal counts in each region. We can model them as Poisson

$$
y_i \sim \mathrm{Poisson}(e^{\eta_i})
$$ How do we model the linear predictor $\eta_i$?

::: {.fragment .fade-in}
-   We could model some dependence across regions:
    -   *Nearby regions* should have similar counts

$$
\eta_i = \beta_0 + \mathbf{x}'\beta + u_i ~~~ u_i \overset{iid}{\sim} N(0,Q^{-1})
$$

-   Now the random effect $u_i \sim N(0, Q^{-1})$ is correlated.
:::

## Overview of Areal processes

::: {style="font-size: 0.8em;"}
An **areal process** (or lattice process) is a stochastic process defined on a set of regions that form a partition of our region of interest $D$.

-   Let $B_1, \ldots B_m$ be our set of $m$ distinct regions such that: $$\bigcup\limits_{i=1}^m \hspace{1mm}B_i = D.$$

-   Here we require that our regions are non-overlapping, with $$B_i \cap B_j = \emptyset.$$

-   Then our areal process is simply the stochastic process $$\{Z(B_i); i=1,\ldots,m\}.$$
:::

## Neighbourhood structures

::: incremental
-   Each of our regions $B_i$ has a set of other nearby which can be considered **neighbours**

-   We might expect that areas have more in common with their neighbours.

-   Therefore, we can construct dependence structures based on the principle that neighbours are correlated and non-neighbours are uncorrelated.

-   However, we need to come up with a sensible way of defining what a neighbour is in this context.
:::

## Defining a Neighbourhood {.smaller background-color="#FFFFFF"}

-   There are many different ways to define a region's neighbours.

-   The most common ones fall into *two main categories* - those based on borders, and those based on distance.

::::::: columns
:::: {.column width="50%"}
**Common borders**

::: {style="font-size: 0.7em;"}
Assume that regions which share a border on a map are neighbours.

-   Simple and easy to implement

-   Treats all borders the same, regardless of length, which can be unrealistic.

-   Areas very close together are not neighbours if there is even a small gap between them.
:::

![](figures/USAborders.png){fig-align="center" width="366"}
::::

:::: {.column width="50%"}
**Distance-based**

::: {style="font-size: 0.7em;"}
Assume that regions which are a within a certain distance of each other area neighbours.

-   What distance do you choose? How do you decide that?

-   Where do you measure from? (e.g., nearest border or a central point).
:::

![](figures/USAdistance.png){fig-align="center" width="362"}
::::
:::::::

## Neighbourhood matrix

::: incremental
-   Once we have identified a set of neighbours using our chosen method, we can use this to account for spatial correlation.

-   We construct a **neighbourhood matrix** (or proximity matrix), which defines how each of our $m$ regions relate to each other.

-   Let $W$ denote an $m \times m$ matrix where the $(i,j)$th entry, $w_{ij}$ denotes the proximity between regions $B_i$ and $B_j$.

    ::: callout-note
    The values of this matrix can be discrete (which regions are neighbours) or continuous (how far apart are the regions).
    :::
:::

## Binary Neighbourhood matrix

By far the most common approach is to use a binary neighbourhood matrix, $W$, denoted by

$$
\begin{aligned}
w_{ij} &= 1 \hspace{2mm} \mbox{ if areas} (B_i, B_j) \mbox{ are neighbours.}\\
w_{ij} &= 0 \hspace{2mm} \mbox{ otherwise.}
\end{aligned}
$$

::::: columns
::: {.column width="40%"}
-   Dependencies structures are described through this spatial weights matrix

-   Binary matrices are used for their simplicity
:::

::: {.column width="60%"}
![](figures/Wmatrix.png){fig-align="center" width="293"}
:::
:::::

## Modelling spatial similarity {.smaller}

::: {style="height: 50px;"}
:::

One of the most popular CAR approaches to model spatial correlation is the Besag model a.k.a. Intrinsic Conditional Autoregressive (ICAR) model. The conditional distribution for $u_i$ is

$$
u_i|\mathbf{u}_{-i} \sim N\left(\frac{1}{d_i}\sum_{j\sim i}u_j,\frac{1}{d_i\tau_u}\right)
$$

-   $\mathbf{u}_{-i} = (u_i,\ldots,u_{i-1},u_{i+1},\ldots,u_n)^T$

-   $\tau_u$ is the precision parameter (inverse variance).

-   $d_i$ is the number of neighbours

-   The mean of $u_i$ is equivalent to the the mean of the effects over all neighbours, and the precision is proportional to the number of neighbours.

## Modelling spatial similarity {.smaller}

::: {style="height: 50px;"}
:::

The joint distribution is given by:

$$
\mathbf{u}|\tau_u \sim N\left(0,\frac{1}{\tau_u}Q^{-1}\right),
$$

Where $Q$ denotes the precision matrix defined as

$$
Q_{i,j} = \begin{cases}
d_i, & i = j \\
-1, & i \sim j \\
0, &\text{otherwise}
\end{cases}
$$

This structure matrix directly defines the neighbourhood structure and is sparse.

## Modelling spatial similarity {.smaller}

::: {style="height: 50px;"}
:::

The ICAR model accounts only for spatially structured variability and does not include a limiting case where no spatial structure is present.

::: incremental
-   We typically add an unstructured random effect $z_i|\tau_v \sim N(0,\tau_{z}^{-1})$

-   The resulting model $v_i = u_i + z_i$ is known as the Besag-York-Mollié model (BYM)

-   The structured spatial effect is controlled by $\tau_u$ which control the degree of smoothing:

    -   Higher $\tau_u$ values lead to stronger smoothing (less spatial variability).

    -   Lower $\tau_u$ values allow for greater local variation.

-   In the next example we will illustrate how to fit this model using `inlabru`.
:::

## Example: Respiratory hospitalisation in Glasgow

::: {style="font-size: 0.8em;"}
In this example we model the number of respiratory hospitalisations across Intermediate Zones (IZ) that make up the Greater Glasgow and Clyde health board in Scotland.
:::

:::::: columns
:::: {.column width="50%"}
::: {style="font-size: 0.8em;"}
In epidemiology, disease risk is assessed using Standardized Mortality Ratios (SMR):

$$ SMR_i = \dfrac{Y_i}{E_i} $$

-   A value $SMR > 1$ indicates a high risk area.
-   A value $SMR<1$ suggests a low risk area.
:::
::::

::: {.column width="50%"}
```{r}
#| echo: false
#| message: false
#| eval: true
#| out-width: 100%


library(dplyr)
library(INLA)
library(ggplot2)
library(patchwork)
library(inlabru)   
library(mapview)
library(sf)

# load some libraries to generate nice map plots
library(scico)
library(CARBayesdata)
data(pollutionhealthdata)
data(GGHB.IZ)
resp_cases <- merge(GGHB.IZ %>%
                      mutate(space = 1:dim(GGHB.IZ)[1]),
                             pollutionhealthdata, by = "IZ")%>%
  dplyr::filter(year == 2007) 

resp_cases <- resp_cases %>% 
  mutate(SMR = observed/expected )
library(spdep)
W.nb <- poly2nb(GGHB.IZ,queen = TRUE)
R <- nb2mat(W.nb, style = "B", zero.policy = TRUE)
diag = apply(R,1,sum)
Q = -R
diag(Q) = diag


mapview(resp_cases,zcol = "SMR")

```
:::
::::::

## Example: Respiratory hospitalisation in Glasgow {.smaller}

> -   **Stage 1:** We assume the responses are Poisson distributed: $$          
>     \begin{aligned}y_i|\eta_i & \sim \text{Poisson}(E_i\lambda_i)\\\text{log}(\lambda_i) = \color{#FF6B6B}{\boxed{\eta_i}} & = \color{#FF6B6B}{\boxed{\beta_0 + \beta_1 \mathrm{pm10} + u_i + z_i} }\end{aligned}
>     $$

> -   **Stage 2:** $\eta_i$ is a linear function of **four components**: an intercept, pm10 effect , a spatially structured effect $u$ and an unstructured iid random effect $z$:\
>     $$
>     \eta_i = \beta_0 + u_i + z_i
>     $$

> -   **Stage 3:** $\{\tau_{z},\tau_u\}$: Precision parameters for the random effects

The latent field is $\mathbf{x}= (\beta_0, \beta_1, u_1, u_2,\ldots, u_n,z_1,...)$, the hyperparameters are $\boldsymbol{\theta} = (\tau_u,\tau_z)$, and must be given a prior.

## Example: Respiratory hospitalisation in Glasgow {.smaller auto-animate="true" background-color="#FFFFFF"}

::::: columns
::: {.column width="50%"}
**The Model**

$$
\begin{aligned}
y_i|\eta_t & \sim \text{Poisson}(E_i\lambda_i)\\
\text{log}(\lambda_i) = \eta_i & = \color{#FF6B6B}{\boxed{\beta_0}} + \color{#FF6B6B}{\boxed{\beta_1 \mathrm{pm10}}} + \color{#FF6B6B}{\boxed{u_i}} + \color{#FF6B6B}{\boxed{z_i}}
\end{aligned}
$$

**The code**

```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "1-4"

# define model component
cmp = ~ Intercept(1) +  pm10(pm10, model = "linear") +
  space(space, model = "besag", graph = Q) + 
  iid(space, model = "iid") 

# define model predictor
eta =  observed ~ Intercept + space + iid

# build the observation model
lik = bru_obs(formula = formula, 
              family = "poisson",
              E = expected,
              data = resp_cases)

# fit the model
fit = bru(cmp, lik)


```
:::

::: {.column width="50%"}
**neighbourhood structure**

```{r}
#| echo: true
#| message: false
#| warning: false
#| eval: false
library(spdep)
W.nb <- poly2nb(GGHB.IZ,queen = TRUE)
```

```{r}
#| fig-width: 8
#| fig-height: 10
#| echo: false
plot(st_geometry(GGHB.IZ), border = "lightgray")
plot.nb(W.nb, st_geometry(GGHB.IZ), add = TRUE)
```
:::
:::::

## Example: Respiratory hospitalisation in Glasgow {.smaller auto-animate="true" background-color="#FFFFFF"}

::::: columns
::: {.column width="50%"}
**The Model**

$$
\begin{aligned}
y_i|\eta_t & \sim \text{Poisson}(E_i\lambda_i)\\
\text{log}(\lambda_i) =  \color{#FF6B6B}{\boxed{\eta_i}} & = \color{#FF6B6B}{\boxed{\beta_0 + \beta_1 \mathrm{pm10} + u_i + z_i}}
\end{aligned}
$$

**The code**

```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "6-7"

# define model component
cmp = ~ Intercept(1) + pm10(pm10, model = "linear") 
  space(space, model = "besag", graph = Q) + 
  iid(space, model = "iid") 

# define model predictor
eta =  observed ~ Intercept + space + iid

# build the observation model
lik = bru_obs(formula = formula, 
              family = "poisson",
              E = expected,
              data = resp_cases)

# fit the model
fit = bru(cmp, lik)


```
:::

::: {.column width="50%"}
**neighbourhood structure**

```{r}
#| echo: true
#| message: false
#| warning: false
#| eval: false
library(spdep)
W.nb <- poly2nb(GGHB.IZ,queen = TRUE)
```

```{r}
#| fig-width: 8
#| fig-height: 10
#| echo: false
plot(st_geometry(GGHB.IZ), border = "lightgray")
plot.nb(W.nb, st_geometry(GGHB.IZ), add = TRUE)
```
:::
:::::

## Example: Respiratory hospitalisation in Glasgow {.smaller auto-animate="true" background-color="#FFFFFF"}

::::: columns
::: {.column width="50%"}
**The Model**

$$
\begin{aligned}
\color{#FF6B6B}{\boxed{y_i|\eta_t}} & \sim \color{#FF6B6B}{\boxed{\text{Poisson}(E_i\lambda_i)}}\\
\text{log}(\lambda_i) =  \eta_i & = \beta_0 + \beta_1 \mathrm{pm10} + u_i + z_i
\end{aligned}
$$

**The code**

```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "9-13"

# define model component
cmp = ~ Intercept(1) + pm10(pm10, model = "linear")
  space(space, model = "besag", graph = Q) + 
  iid(space, model = "iid") 

# define model predictor
eta =  observed ~ Intercept + space + iid

# build the observation model
lik = bru_obs(formula = formula, 
              family = "poisson",
              E = expected,
              data = resp_cases)

# fit the model
fit = bru(cmp, lik)


```
:::

::: {.column width="50%"}
**neighbourhood structure**

```{r}
#| echo: true
#| message: false
#| warning: false
#| eval: false
library(spdep)
W.nb <- poly2nb(GGHB.IZ,queen = TRUE)
```

```{r}
#| fig-width: 8
#| fig-height: 10
#| echo: false
plot(st_geometry(GGHB.IZ), border = "lightgray")
plot.nb(W.nb, st_geometry(GGHB.IZ), add = TRUE)
```
:::
:::::

## Example: Respiratory hospitalisation in Glasgow {.smaller auto-animate="true" background-color="#FFFFFF"}

::::: columns
::: {.column width="50%"}
**The Model**

$$
\begin{aligned}
y_i|\eta_t & \sim \text{Poisson}(E_i\lambda_i)\\
\text{log}(\lambda_i) =  \eta_i & = \beta_0 + \beta_1 \mathrm{pm10} + u_i + z_i
\end{aligned}
$$

**The code**

```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "15-16"

# define model component
cmp = ~ Intercept(1) + pm10(pm10, model = "linear")
  space(space, model = "besag", graph = Q) + 
  iid(space, model = "iid") 

# define model predictor
eta =  observed ~ Intercept + space + iid

# build the observation model
lik = bru_obs(formula = formula, 
              family = "poisson",
              E = expected,
              data = resp_cases)

# fit the model
fit = bru(cmp, lik)


```
:::

::: {.column width="50%"}
**neighbourhood structure**

```{r}
#| echo: true
#| message: false
#| warning: false
#| eval: false
library(spdep)
W.nb <- poly2nb(GGHB.IZ,queen = TRUE)
```

```{r}
#| fig-width: 8
#| fig-height: 10
#| echo: false
plot(st_geometry(GGHB.IZ), border = "lightgray")
plot.nb(W.nb, st_geometry(GGHB.IZ), add = TRUE)
```
:::
:::::

## Example: Respiratory hospitalisation in Glasgow {background-color="#FFFFFF"}

::: {style="height: 50px;"}
:::

```{r}
#| echo: false
cmp = ~ Intercept(1) + space(space, model = "besag", graph = Q) + iid(space, model = "iid") + pm10(pm10, model = "linear")

formula = observed ~ Intercept + space + iid + pm10  

lik = bru_obs(formula = formula, 
              family = "poisson",
              E = expected,
              data = resp_cases)

fit = bru(cmp, lik)
```

::::: columns
::: {.column width="35%"}
**Posterior summaries**

```{r}
#| echo: false
#| message: false
#| warning: false

library(gt)

rbind(fit$summary.fixed[,c(1,3,5)] ,
fit$summary.hyperpar[,c(1,3,5)] )%>%
  mutate(par = c(rownames(fit$summary.fixed),
                 rownames(fit$summary.hyperpar))) %>%
  gt(rowname_col = "par") %>% gt::fmt_number(decimals = 2)
```
:::

::: {.column width="65%"}
**Estimated relative risks**

```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 10
pred = predict(fit, 
               resp_cases,
               ~data.frame(log_risk = Intercept + space + pm10,
                           risk = exp(Intercept + space + pm10),
                           cases = expected * exp(Intercept + space + + pm10)),
               n.samples = 1000)

ggplot() +
  geom_sf(data = pred$risk, aes(fill = mean)) + scale_fill_scico(direction = -1) +
  ggtitle("Relative Risk") +

pred$cases %>% ggplot() + geom_point(aes(observed, mean)) + 
  geom_errorbar(aes(observed, ymin = q0.025, ymax = q0.975)) +
  geom_abline(intercept = 0, slope = 1)+
labs(y=expression(E[i]~exp(lambda[i]))) + plot_layout(ncol=1)

```
:::
:::::

# Geostatistical modelling

## Geostatistical Data

-   Geostatistical data are the most common form of spatial data found in environmental setting.

-   We regularly take measurements of an environmental variable of interest at a set of fixed locations.

-   This could be data from samples taken across a region (e.g., water depth in a lake) or from monitoring stations as part of a network (e.g., air pollution).

-   In each of these cases,our goal is to estimate the value of our variable across the entire space

## Understanding our region

-   Let $D$ be our two-dimensional region of interest.
-   In principle, there are infinite locations within $D$, each of which can be represented by mathematical coordinates (eg latitude and longitude).
-   We can identify any individual location as $\mathbf{s}_i = (x_i, y_i)$, where $x_i$ and $y_i$ are their coordinates.
-   We can treat our variable of interest as a random variable, $Z$ which can be observed at any location as $Z(\mathbf{s}_i)$.

## Geostatistical process

-   Our geostatistical process can therefore be written as: $$\{Z(\mathbf{s}); \mathbf{s} \in D\}$$
-   In practice, our data are observed at a finite number of locations, $m$, and can be denoted as: $$z = \{z(\mathbf{s}_1), \ldots z(\mathbf{s}_m) \}$$
-   We have observed our data at $m$ locations, but often want to predict this process at a set of unknown locations.
-   For example, what is the value of $z(\mathbf{s}_0)$, where $\mathbf{s}_0$ is an unobserved site?

## Gaussian Random Fields {auto-animate="true"}

If we have a process that is occurring everywhere in space, it is natural to try to model it using some sort of function.

-   If $z$ is a vector of observations of $z(\mathbf{s})$ at different locations, we want this to be normally distributed:

$$
\mathbf{z} = (z(\mathbf{s}_1),\ldots,z(\mathbf{s}_m)) \sim \mathcal{N}(0,\Sigma)
$$ where $\Sigma_{ij} = \mathrm{Cov}(z(\mathbf{s}_i),z(\mathbf{s}_j))$ is a dense $m \times m$ matrix.

## Gaussian Random Fields {auto-animate="true"}

-   A Gaussian random field (GRF) is a collection of random variables where observations occur in a continuous domain, and where every finite collection of random variables has a multivariate normal distribution

::: {.callout-note icon="false"}
## Stationary random fields

A GRF is **stationary** if:

-   has mean zero.

-   the covariance between two points depends only on the distance and direction between those points.

It is **isotropic** if the covariance only depends on the distance between the points.
:::

## The SPDE approach

**The goal**: approximate the GRF using a triangulated mesh via the so-called SPDE approach.

The SPDE approach represents the continuous spatial process as a discretely indexed Gaussian Markov Random Field (GMRF)

-   We construct an appropriate lower-resolution approximation of the surface by sampling it in a set of well designed points and constructing a piecewise linear interpolant.

![](figures/spde.png){fig-align="center" width="520"}

## The SPDE approach

-   A GF with Matérn covariance $c_{\nu}(d;\sigma,\rho)$ is a solution to a particular PDE.

$$
c_{\nu}(d;\sigma,\rho) = \sigma^2\frac{2^{1-\nu}}{\Gamma(\nu)}\left(\sqrt{8\nu}\frac{d}{\rho}\right)^{\nu}K_{\nu}\left(\sqrt{8\nu}\frac{d}{\rho}\right)
$$

-   This solution is then approximated using a finite combination of piecewise linear basis functions defined on a triangulation .
-   The solution is completely defined by a Gaussian vector of weights (defined on the triangulation vertices) with zero mean and a sparse precision matrix.
-   How do we choose sensible priors for $\sigma,\rho$?

## Penalized Complexity (PC) priors

Penalized Complexity (PC) priors proposed by Simpson et al. ([2017](https://projecteuclid.org/journals/statistical-science/volume-32/issue-1/Penalising-Model-Component-Complexity--A-Principled-Practical-Approach-to/10.1214/16-STS576.full)) allow us to control the amount of spatial smoothing and avoid overfitting.

-   PC priors shrink the model towards a simpler baseline unless the data provide strong evidence for a more complex structure.
-   To define the prior for the marginal precision $\sigma^{-2}$ and the range parameter $\rho$, we use the probability statements:
    -   Define the prior for the range $\text{Prob}(\rho<\rho_0) = p_{\rho}$
    -   Define the prior for the range $\text{Prob}(\sigma>\sigma_0) = p_{\sigma}$

## Learning about the SPDE approach {.smaller}

::: {style="height: 50px;"}
:::

-   F. Lindgren, H. Rue, and J. Lindström. An explicit link between Gaussian fields and Gaussian Markov random fields: The SPDE approach (with discussion). In: *Journal of the Royal Statistical Society, Series B* 73.4 (2011), pp. 423–498.

-   H. Bakka, H. Rue, G. A. Fuglstad, A. Riebler, D. Bolin, J. Illian, E. Krainski, D. Simpson, and F. Lindgren. Spatial modelling with R-INLA: A review. In: *WIREs Computational Statistics* 10:e1443.6 (2018). (Invited extended review). DOI: 10.1002/wics.1443.

-   E. T. Krainski, V. Gómez-Rubio, H. Bakka, A. Lenzi, D. Castro-Camilio, D. Simpson, F. Lindgren, and H. Rue. *Advanced Spatial Modeling with Stochastic Partial Differential Equations using R and INLA*. Github version \url{www.r-inla.org/spde-book}. CRC press, Dec. 20

## SPDE models

::: {style="height: 50px;"}
:::

We call spatial Markov models defined on a mesh SPDE models.

SPDE models have 3 parts

1.  A mesh

2.  A range parameter $\kappa$

3.  A precision parameter $\tau$

Lets see hoe this is done in practice!

## Example: Species Distribution Model {background-color="#FFFFFF"}

In the next example, we will explore data on the Pacific Cod (*Gadus macrocephalus*) from a trawl survey in Queen Charlotte Sound. The

-   The dataset contains presence/absence data in 2003 as well as depth covariate infromation.

```{r}
#| echo: false
#| message: false
#| eval: true
#| out-width: 100%

library(sdmTMB)

pcod_df = sdmTMB::pcod %>% filter(year==2003)
qcs_grid = sdmTMB::qcs_grid
pcod_sf =   st_as_sf(pcod_df, coords = c("lon","lat"), crs = 4326)
pcod_sf = st_transform(pcod_sf,
                       crs = "+proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km" )
library(terra)
depth_r <- rast(qcs_grid, type = "xyz")
crs(depth_r) <- crs(pcod_sf)
library(tidyterra)
ggplot()+ 
  geom_spatraster(data=depth_r$depth)+
  geom_sf(data=pcod_sf,aes(color=factor(present))) +
    scale_color_manual(name="Occupancy status for the Pacific Cod",
                     values = c("black","orange"),
                     labels= c("Absence","Presence"))+
  scale_fill_scico(name = "Depth",
                   palette = "nuuk",
                   na.value = "transparent" ) + xlab("") + ylab("")
```

## Example: Species Distribution Model

-   [**Stage 1** Model for the response]{style="color:#FF6B6B;"} $$
    y(s)|\eta(s)\sim\text{Binom}(1, p(s))
    $$

-   **Stage 2** Latent field model $$
    \eta(s) = \text{logit}(p(s)) = \beta_0 + f( x(s)) + \omega(s)
    $$

-   **Stage 3** Hyperparameters

## Bayesian Geostatistics {auto-animate="true"}

-   **Stage 1** Model for the response $$
    y(s)|\eta(s)\sim\text{Binom}(1, p(s))
    $$
-   [**Stage 2** Latent field model]{style="color:#FF6B6B;"} $$
    \eta(s) = \text{logit}(p(s)) = \beta_0 + f( x(s)) + \omega(s)
    $$
    -   A global intercept $\beta_0$
    -   A smooth effect of covariate $x(s)$ (depth)
    -   A Gaussian field $\omega(s)$ (will discuss this later..)
-   **Stage 3** Hyperparameters

## Example: Species Distribution Model {auto-animate="true"}

-   **Stage 1** Model for the response $$
    y(s)|\eta(s)\sim\text{Binom}(1, p(s))
    $$

-   **Stage 2** Latent field model $$
    \eta(s) = \text{logit}(p(s)) = \beta_0 + \beta_1 x(s) + \omega(s)
    $$

-   [**Stage 3** Hyperparameters]{style="color:#FF6B6B;"}

    -   Precision for the smooth function $f(\cdot)$
    -   Range and sd in the Gaussian field $\sigma_{\omega}, \tau_{\omega}$

## Step 1: Define the SPDE representation: The mesh

First, we need to create the mesh used to approximate the random field.

```{r}
library(fmesher)
mesh = fm_mesh_2d(loc = pcod_sf,           # Build the mesh
                  cutoff = 2,
                  max.edge = c(7,20),     # The largest allowed triangle edge length.
                  offset = c(5,50))       # The automatic extension distance

```

```{r}
#| echo: false
ggplot() + gg(mesh) +
  geom_sf(data= pcod_sf, aes(color = factor(present))) + 
  xlab("") + ylab("")
```

-   `max.edge` for maximum triangle edge lengths
-   `offset` for inner and outer extensions (to prevent *edge effects*)
-   `cutoff` to avoid overly small triangles in clustered areas

## Step 1: Define the SPDE representation: The mesh

::: incremental
-   All random field models need to be discretised for practical calculations.

-   The SPDE models were developed to provide a consistent model definition across a range of discretisations.

-   We use finite element methods with local, piecewise linear basis functions defined on a triangulation of a region of space containing the domain of interest.

-   Deviation from stationarity is generated near the boundary of the region.

-   The choice of region and choice of triangulation affects the numerical accuracy.
:::

## Step 1: Define the SPDE representation: The mesh

-   Too fine meshes $\rightarrow$ heavy computation

-   Too coarse mesh $\rightarrow$ not accurate enough

![](figures/mesh_res.png){fig-align="center"}

## Step 1: Define the SPDE representation: The mesh

**Some guidelines**

-   Create triangulation meshes with `fm_mesh_2d()`:

-   edge length should be around a third to a tenth of the spatial range

-   Move undesired boundary effects away from the domain of interest by extending to a smooth external boundary:

-   Use a coarser resolution in the extension to reduce computational cost (`max.edge=c(inner, outer)`), i.e., add extra, larger triangles around the border

## Step 1: Define the SPDE representation: The mesh

-   Use a fine resolution (subject to available computational resources) for the domain of interest (inner correlation range) and avoid small edges ,i.e., filter out small input point clusters (0 $<$ \`cutoff $<$ inner)

-   Coastlines and similar can be added to the domain specification in `fm_mesh_2d()` through the `boundary` argument.

-   simplify the border

## Step 1: Define the SPDE representation: The SPDE

We use the `inla.spde2.pcmatern` to define the SPDE model using PC priors through the following probability statements

-   $P(\rho < 100) = 0.5$

-   $P(\sigma > 1) = 0.5$

```{r}
spde_model =  inla.spde2.pcmatern(mesh,
                                  prior.sigma = c(1, 0.5),
                                  prior.range = c(100, 0.5))
```

![](figures/priors.png){fig-align="center" width="425"}

## Step 2: Define the model components {.smaller auto-animate="true"}

::::: columns
::: {.column width="50%"}
**The Model**

\begin{aligned}
y(s)|\eta(s) & \sim\text{Binom}(1, p(s))\\
\eta(s) &  = \color{#FF6B6B}{\boxed{\beta_0}} + \color{#FF6B6B}{\boxed{ f(x(s))}} + \color{#FF6B6B}{\boxed{ \omega(s)}}\\

\end{aligned}
:::

::: {.column width="50%"}
```{r}
#| echo: true
pcod_sf %>% select(depth, present) %>% print(n = 3)
```
:::
:::::

**The code**

```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "1-3"

# define model component
cmp = ~ -1 + Intercept(1) +  depth_smooth(log(depth), model='rw2') + 
  space(geometry, model = spde_model)

# define model predictor
eta = present ~ Intercept + depth_smooth + space

# build the observation model
lik = bru_obs(formula = eta,
              data = pcod_sf,
              family = "binomial")

# fit the model
fit = bru(cmp, lik)
```

## Step 3: Define the linear predictor {.smaller auto-animate="true"}

::::: columns
::: {.column width="50%"}
**The Model**

\begin{aligned}
y(s)|\eta(s) & \sim\text{Binom}(1, p(s))\\
\color{#FF6B6B}{\boxed{\eta(s)}} &  = \color{#FF6B6B}{\boxed{\beta_0 +  f(x(s)) +  \omega(s)}}\\

\end{aligned}
:::

::: {.column width="50%"}
```{r}
#| echo: true
pcod_sf %>% select(depth, present) %>% print(n = 3)
```
:::
:::::

**The code**

```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "5-6"

# define model component
cmp = ~ -1 + Intercept(1) +  depth_smooth(log(depth), model='rw2') + 
  space(geometry, model = spde_model)

# define model predictor
eta = present ~ Intercept + depth_smooth + space

# build the observation model
lik = bru_obs(formula = eta,
              data = pcod_sf,
              family = "binomial")

# fit the model
fit = bru(cmp, lik)
```

## Step 4: Define the observational model {.smaller auto-animate="true"}

::::: columns
::: {.column width="50%"}
**The Model**

\begin{aligned}
\color{#FF6B6B}{\boxed{y(s)|\eta(s)}} & \sim \color{#FF6B6B}{\boxed{\text{Binom}(1, p(s))}}\\
\eta(s) &  = \beta_0 +  f(x(s)) +  \omega(s)\\

\end{aligned}
:::

::: {.column width="50%"}
```{r}
#| echo: true
pcod_sf %>% select(depth, present) %>% print(n = 3)
```
:::
:::::

**The code**

```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "8-11|13-14"

# define model component
cmp = ~ -1 + Intercept(1) +  depth_smooth(log(depth), model='rw2') + 
  space(geometry, model = spde_model)

# define model predictor
eta = present ~ Intercept + depth_smooth + space

# build the observation model
lik = bru_obs(formula = eta,
              data = pcod_sf,
              family = "binomial")

# fit the model
fit = bru(cmp, lik)
```

## Model Results

```{r}
#| echo: false

# create the grouped variable
depth_r$depth_group = inla.group(values(depth_r$depth_scaled))

# run the model
cmp = ~ Intercept(1) + space(geometry, model = spde_model) +
        covariate(depth_r$depth_group, model = "rw2")

formula = present ~ Intercept  + space + covariate

lik = bru_obs(formula = formula, 
              data = pcod_sf, 
              family = "binomial")


fit3 = bru(cmp, lik)
```

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 8
#| fig-height: 6
#| 
inv_logit = function(x) (1+exp(-x))^(-1)

pxl1 = data.frame(crds(depth_r), 
                  as.data.frame(depth_r$depth)) %>% 
       filter(!is.na(depth)) %>%
st_as_sf(coords = c("x","y"))

pred3  = predict(fit3, pxl1, ~inv_logit(Intercept + space + covariate) )


p1 = fit3$summary.random$covariate %>% ggplot() + geom_line(aes(ID,mean)) + 
                                  geom_ribbon(aes(ID, ymin = `0.025quant`, 
                                                      ymax = `0.975quant`), alpha = 0.5)

p2 = pred3 %>% ggplot() + 
      geom_sf(aes(color = mean)) +
        scale_color_scico(direction = -1)
p1 +p2
```

# Point Processes

## Point process data {.smaller background-color="#FFFFFF"}

::: {style="height: 50px;"}
:::

Many of the ecological and environmental processes of interest can be represented by a spatial point process or can be view as an aggregation of one.

![](figures/frompontsto.png){fig-align="center" width="600"}

-   Many contemporary data sources collect georeferenced information about the location where an event has occur (e.g., species occurrence, wildfire, flood events).
-   This point-based information provides valuable insights into ecosystem dynamics.

## Defining a Point Process

-   Consider a fixed geographical region $A$.

-   The set of locations at which events occur are denoted by $\mathbf{s} = (\mathbf{s}_1, \ldots, \mathbf{s}_n)$.

-   We let $N(A)$ be the random variable which represents the total number of events in region $A$.

-   Our primary interest is in measuring where events occur, so the **locations are our data**.

## Homogeneous Poisson Process {.smaller}

::: {style="height: 50px;"}
:::

-   The simplest version of a point process model is the homogeneous Poisson process (HPP).

-   The likelihood of a point pattern $\mathbf{y} = \left[ \mathbf{s}_1, \ldots, \mathbf{s}_n \right]^\intercal$ distributed as a HPP with intensity $\lambda$ and observation window $\Omega$ is

    $$
    p(\mathbf{y} | \lambda) \propto \lambda^n e^{ \left( - |\Omega| \lambda \right)} ,
    $$

    -   $|\Omega|$ is the size of the observation window.

    -   $\lambda$ is the expected number of points per unit area.

    -   $|\Omega|\lambda$ the total expected number of points in the observation window.

::: {style="height: 50px;"}
:::

-   A key property of a Poisson process is that the number of points within a region $A$ is Poisson distributed with constant rate $|A|\lambda$.

## Inhomogeneous Poisson process

The *inhomogeneous Poisson process* has a spatially varying intensity $\lambda(\mathbf{s})$.

The likelihood in this case is

$$
p(\mathbf{y} | \lambda) \propto \exp \left( -\int_\Omega \lambda(\mathbf{s}) \mathrm{d}\mathbf{s} \right) \prod_{i=1}^n \lambda(\mathbf{s}_i).
$$

-   If the case of an HPP the integral in the likelihood can easily be computed as $\int_\Omega \lambda(\mathbf{s}) \mathrm{d}\mathbf{s} =|\Omega|\lambda$

-   For IPP, integral in the likelihood has to be approximated numerically as a weighted sum.

## Inhomogeneous Poisson process

The integral in is approximated as $\sum_{j=1}^J w_j \lambda(\mathbf{s}_j)$

-   $w_j$ are the integration weights

-   $\mathbf{s}_j$ are the quadrature locations.

This serves two purposes:

1.  Approximates the integral

2.  re-write the inhomogeneous Poisson process likelihood as a regular Poisson likelihood.

## Inhomogeneous Poisson process {.smaller}

The idea behind the trick to rewrite the approximate likelihood is to introduce a dummy vector $\mathbf{z}$ and an integration weights vector $\mathbf{w}$ of length $J + n$

::::: columns
::: {.column width="50%"}
$$\mathbf{z} = \left[\underbrace{0_1, \ldots,0_J}_\text{quadrature locations}, \underbrace{1_1, \ldots ,1_n}_{\text{data points}} \right]^\intercal$$
:::

::: {.column width="50%"}
$$\mathbf{w} = \left[ \underbrace{w_1, \ldots, w_J}_\text{quadrature locations}, \underbrace{0_1, \ldots, 0_n}_\text{data points} \right]^\intercal$$
:::
:::::

Then the approximate likelihood can be written as

$$
\begin{aligned}
p(\mathbf{z} | \lambda) &\propto \prod_{i=1}^{J + n} \eta_i^{z_i} \exp\left(-w_i \eta_i \right) \\
\eta_i &= \log\lambda(\mathbf{s}_i) = \mathbf{x}(s)'\beta
\end{aligned}
$$

-   This is similar to a product of Poisson distributions with means $\eta_i$, exposures $w_i$ and observations $z_i$.

-   This is the basis for the implementation of Cox process models in `inlabru`, which can be specified using `family = "cp"`.

## Limitations with IPP

::: {style="height: 50px;"}
:::

::: incremental
-   IPP models assume that data points are conditionally independent given the covariates, meaning that any spatial variation is fully explained by environmental and sampling factors.
-   Unmeasured endogenous and exogenous factors can create spatial
-    Ignoring them can lead to bias in our conclusions.
:::

## The Log-Gaussian Cox Process {.smaller}

::: {style="height: 50px;"}
:::

-   Log-Gaussian Cox processes (LGCP) extends the IPP by allowing the intensity function to vary spatially according to a structured spatial random effect.

$$
\log~\lambda(s)= \mathbf{x}(s)'\beta + \xi(s)
$$

-   The events are then assumed to be independent given $\xi(s)$ - a GMRF with Matérn covariance.

-   `inlabru` has implemented some integration schemes that are especially well suited to integrating the intensity in models with an SPDE effect.

::: aside
See for further reference: Simpson, Daniel, Janine B. Illian, Finn Lindgren, Sigrunn H. Sørbye, and Håvard Rue. 2016. "Going off grid: computationally efficient inference for log-Gaussian Cox processes." *Biometrika* 103 (1): 49–70.
:::

## Example: Forest fires in Castilla-La Mancha {.smaller background-color="#FFFFFF"}

-   In this example we model the location of forest fires in the Castilla-La Mancha region of Spain between 1998 and 2007.

-   We are now going to use the elevation as a covariate to explain the variability of the intensity $\lambda(s)$ over the domain of interest and a spatially structured SPDE model.

$$
\log\lambda(s) = \beta_0 + \beta_1 \text{elevation}(s) + \xi(s)
$$

```{r}
#| fig-align: center
#| fig-width: 6
#| fig-height: 6
#| message: false
#| warning: false

library(spatstat)
data("clmfires")
pp = st_as_sf(as.data.frame(clmfires) %>%
                mutate(x = x, 
                       y = y),
              coords = c("x","y"),
              crs = NA) %>%
  filter(cause == "lightning",
         year(date) == 2004)

poly = as.data.frame(clmfires$window$bdry[[1]]) %>%
  mutate(ID = 1)

region = poly %>% 
  st_as_sf(coords = c("x", "y"), crs = NA) %>% 
  dplyr::group_by(ID) %>% 
  summarise(geometry = st_combine(geometry)) %>%
  st_cast("POLYGON") 
  
elev_raster = rast(clmfires.extra[[2]]$elevation)
elev_raster = scale(elev_raster)

ggplot() + geom_spatraster(data = elev_raster) +  geom_sf(data = region, col = "black", alpha = 0,linewidth =0.75) + geom_sf(data = pp) + scale_fill_scico(name= "Elevation")

```

## Example: Forest fires in Castilla-La Mancha {.smaller background-color="#FFFFFF"}

::::: columns
::: {.column width="50%"}
**The IPP Model** $$
\begin{aligned}
p(\mathbf{y} | \lambda)  & \propto \exp \left( -\int_\Omega \lambda(\mathbf{s}) \mathrm{d}\mathbf{s} \right) \prod_{i=1}^n \lambda(\mathbf{s}_i) \\
\eta(s) &  = \log ~\lambda(s) = \beta_0 +  x(s) 
\end{aligned}
$$ **The code**

```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false

# define model component
cmp = ~ Intercept(1) + elev(elev_raster, model = "linear") 

# define model predictor
eta  = geometry ~ Intercept +  elev 

# build the observation model
lik = bru_obs("cp",
              formula = eta,
              data = pp,
              ips = ips)

# fit the model
fit = bru(cmp, lik)

```
:::

::: {.column width="50%"}


::: {.panel-tabset}

## Regular Grid

```{r}
#| echo: true
n.int = 1000
ips = st_sf(geometry = st_sample(region,
            size = n.int,
            type = "regular"))

ips$weight = st_area(region) / n.int

```

```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
ggplot() + geom_sf(data = ips, aes(color = weight)) + geom_sf(data= region, alpha = 0)
```


## `fm_int` 


```{r}
#| echo: true

# The mesh
mesh = fm_mesh_2d(boundary = region,
                  max.edge = c(5, 10),
                  cutoff = 4, crs = NA)

# build integration scheme
ips = fm_int(mesh,
             samplers = region)

```

```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5


ggplot() + geom_sf(data = ips, aes(color = weight),size=0.5) + 
   scale_color_scico(palette = "roma") + gg(mesh)


```

:::

:::
:::::



## Example: Forest fires in Castilla-La Mancha {.smaller background-color="#FFFFFF"}

::::: columns
::: {.column width="50%"}
**The LGCP Model** 

$$
\begin{aligned}
p(\mathbf{y} | \lambda)  & \propto \exp \left( -\int_\Omega \lambda(\mathbf{s}) \mathrm{d}\mathbf{s} \right) \prod_{i=1}^n \lambda(\mathbf{s}_i) \\
\eta(s) &  = \log(\lambda(s)) = \color{#FF6B6B}{\boxed{\beta_0}} + \color{#FF6B6B}{\boxed{ x(s)}} + \color{#FF6B6B}{\boxed{ \omega(s)}}\\
\end{aligned}
$$ 

**The code**

```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "1-3"

# define model component
cmp = ~ Intercept(1) + elev(elev_raster, model = "linear") +
  space(geometry, model = spde_model) 


# define model predictor
eta  = geometry ~ Intercept +  elev + space


# build the observation model
lik = bru_obs("cp",
              formula = eta,
              data = pp,
              ips = ips)


# fit the model
fit = bru(cmp, lik)

```
:::

::: {.column width="50%"}
```{r}
#| echo: true

# The mesh
mesh = fm_mesh_2d(boundary = region,
                  max.edge = c(5, 10),
                  cutoff = 4, crs = NA)

# The SPDE model
spde_model =  inla.spde2.pcmatern(mesh,
                                  prior.sigma = c(1, 0.5),
                                  prior.range = c(100, 0.5))

# build integration scheme
ips = fm_int(mesh,
             samplers = region)

```

```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 6


ggplot() + geom_sf(data = ips, aes(color = weight),size=0.5) + 
   scale_color_scico(palette = "roma") + gg(mesh)


```
:::
:::::

## Example: Forest fires in Castilla-La Mancha {.smaller background-color="#FFFFFF"}

::::: columns
::: {.column width="50%"}
**The LGCP Model** 

$$
\begin{aligned}
p(\mathbf{y} | \lambda)  & \propto \exp \left( -\int_\Omega \lambda(\mathbf{s}) \mathrm{d}\mathbf{s} \right) \prod_{i=1}^n \lambda(\mathbf{s}_i) \\
\color{#FF6B6B}{\boxed{\eta(s)}} &  = \log(\lambda(s)) = \color{#FF6B6B}{\boxed{\beta_0 +  x(s) + \omega(s)}}\\
\end{aligned}
$$ 

**The code**

```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "6-7"

# define model component
cmp = ~ Intercept(1) + elev(elev_raster, model = "linear") +
  space(geometry, model = spde_model) 


# define model predictor
eta  = geometry ~ Intercept +  elev + space


# build the observation model
lik = bru_obs("cp",
              formula = eta,
              data = pp,
              ips = ips)


# fit the model
fit = bru(cmp, lik)

```
:::

::: {.column width="50%"}
```{r}
#| echo: true

# The mesh
mesh = fm_mesh_2d(boundary = region,
                  max.edge = c(5, 10),
                  cutoff = 4, crs = NA)

# The SPDE model
spde_model =  inla.spde2.pcmatern(mesh,
                                  prior.sigma = c(1, 0.5),
                                  prior.range = c(100, 0.5))

# build integration scheme
ips = fm_int(mesh,
             samplers = region)

```

```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 6


ggplot() + geom_sf(data = ips, aes(color = weight),size=0.5) + 
   scale_color_scico(palette = "roma") + gg(mesh)


```
:::
:::::





## Example: Forest fires in Castilla-La Mancha {.smaller background-color="#FFFFFF"}

::::: columns
::: {.column width="50%"}
**The LGCP Model** 

$$
\begin{aligned}
\color{#FF6B6B}{\boxed{p(\mathbf{y} | \lambda)}}  & \propto \exp \left( -\int_\Omega \lambda(\mathbf{s}) \mathrm{d}\mathbf{s} \right) \prod_{i=1}^n \lambda(\mathbf{s}_i) \\
\eta(s) &  = \log(\lambda(s)) = \beta_0 +  x(s) + \omega(s)\\
\end{aligned}
$$ 

**The code**

```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "10-14"

# define model component
cmp = ~ Intercept(1) + elev(elev_raster, model = "linear") +
  space(geometry, model = spde_model) 


# define model predictor
eta  = geometry ~ Intercept +  elev + space


# build the observation model
lik = bru_obs("cp",
              formula = eta,
              data = pp,
              ips = ips)


# fit the model
fit = bru(cmp, lik)

```
:::

::: {.column width="50%"}
```{r}
#| echo: true

# The mesh
mesh = fm_mesh_2d(boundary = region,
                  max.edge = c(5, 10),
                  cutoff = 4, crs = NA)

# The SPDE model
spde_model =  inla.spde2.pcmatern(mesh,
                                  prior.sigma = c(1, 0.5),
                                  prior.range = c(100, 0.5))

# build integration scheme
ips = fm_int(mesh,
             samplers = region)

```

```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 6


ggplot() + geom_sf(data = ips, aes(color = weight),size=0.5) + 
   scale_color_scico(palette = "roma") + gg(mesh)


```
:::
:::::


## Example: Forest fires in Castilla-La Mancha {.smaller background-color="#FFFFFF"}

::::: columns
::: {.column width="50%"}
**The LGCP Model** 

$$
\begin{aligned}
p(\mathbf{y} | \lambda)  & \propto \exp \left( -\int_\Omega \lambda(\mathbf{s}) \mathrm{d}\mathbf{s} \right) \prod_{i=1}^n \lambda(\mathbf{s}_i) \\
\eta(s) &  = \log(\lambda(s)) = \beta_0 +  x(s) + \omega(s)\\
\end{aligned}
$$ 

**The code**

```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "17-18"

# define model component
cmp = ~ Intercept(1) + elev(elev_raster, model = "linear") +
  space(geometry, model = spde_model) 


# define model predictor
eta  = geometry ~ Intercept +  elev + space


# build the observation model
lik = bru_obs("cp",
              formula = eta,
              data = pp,
              ips = ips)


# fit the model
fit = bru(cmp, lik)

```
:::

::: {.column width="50%"}
```{r}
#| echo: true

# The mesh
mesh = fm_mesh_2d(boundary = region,
                  max.edge = c(5, 10),
                  cutoff = 4, crs = NA)

# The SPDE model
spde_model =  inla.spde2.pcmatern(mesh,
                                  prior.sigma = c(1, 0.5),
                                  prior.range = c(100, 0.5))

# build integration scheme
ips = fm_int(mesh,
             samplers = region)

```

```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 6


ggplot() + geom_sf(data = ips, aes(color = weight),size=0.5) + 
   scale_color_scico(palette = "roma") + gg(mesh)


```
:::
:::::

## Example: Forest fires in Castilla-La Mancha {.smaller background-color="#FFFFFF"}

**Model Predictions**


```{r}
#| echo: false
#| message: false
#| warning: false
library(viridis)

#eval_spatial(elev_raster,pp) %>% is.na() %>% any()
#eval_spatial(elev_raster,ips) %>% is.na() %>% any()
#eval_spatial(elev_raster,ips[3525,]) %>% is.na() %>% which()

# ggplot()+tidyterra::geom_spatraster(data=elev_raster)+
#   geom_sf(data=ips[3525,])

# Extend raster ext by 5 % of the original raster
re <- extend(elev_raster, ext(elev_raster)*1.05)
# Convert to an sf spatial object
re_df <- re %>% stars::st_as_stars() %>%  st_as_sf(na.rm=F)
# fill in missing values using the original raster 
re_df$lyr.1 <- bru_fill_missing(elev_raster,re_df,re_df$lyr.1)
# rasterize
elev_rast_p <- stars::st_rasterize(re_df) %>% rast()
# visualize

# ggplot()+tidyterra::geom_spatraster(data=elev_rast_p)+
#   geom_sf(data=ips[3525,])


# define model component
cmp = ~ Intercept(1) + elev(elev_rast_p, model = "linear") +
  space(geometry, model = spde_model) 


# define model predictor
eta  = geometry ~ Intercept +  elev + space


# build the observation model
lik = bru_obs("cp",
              formula = eta,
              data = pp,
              ips = ips)

# fit the model
fit = bru(cmp, lik)

# Predictions
elev_crop <- terra::crop(x = elev_raster,y = region,mask=TRUE)

pxl1 = data.frame(crds(elev_crop), 
                  as.data.frame(elev_crop$lyr.1)) %>% 
       filter(!is.na(lyr.1)) %>%
st_as_sf(coords = c("x","y")) %>%
  dplyr::select(-lyr.1)


lgcp_pred <- predict(
  fit,
  pxl1,
  ~ data.frame(
    lambda = exp(Intercept + elev + space), # intensity
    loglambda = Intercept + elev +space,  #log-intensity
    GF = space # matern field
  )
)

ggplot() +
  gg(lgcp_pred$loglambda, geom = "tile") + 
  scale_fill_viridis(name=expression(log(lambda)))+
  ggplot() +
  gg(lgcp_pred$loglambda, geom = "tile",aes(fill=sd)) + 
  scale_fill_viridis(name=expression(stdev-log(lambda)),option = "B")+
  ggplot() +
  gg(lgcp_pred$lambda, geom = "tile") + scale_fill_viridis(name=expression(lambda))+
  ggplot() +
  gg(lgcp_pred$GF, geom = "tile") + scico::scale_fill_scico(name="Spatial effect")+
  plot_layout(ncol=2)



```