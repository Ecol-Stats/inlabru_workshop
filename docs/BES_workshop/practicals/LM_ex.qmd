---
title: "Practical - `inlabru` workflow"
execute: 
  freeze: auto
format: 
  html:
    theme:
      light: flatly
      dark: darkly
  PrettyPDF-pdf:
    keep-tex: true
    number-sections: true
embed-resources: true
editor_options: 
  chunk_output_type: console
---

## Linear Model {#sec-linmodel}

In this practical we will:


- Fit a [simple linear regression](#SLR) with `inlabru`
- Fit a [linear regression with discrete covariates and interactions](#LM_int)

Start by loading useful libraries:

```{r}
#| warning: false
#| message: false
#| code-summary: "Load libraries"

library(dplyr)
library(INLA)
library(ggplot2)
library(patchwork)
library(inlabru)     
# load some libraries to generate nice plots
library(scico)
```

As our first example we consider a simple linear regression model with Gaussian observations 

$$
y_i\sim\mathcal{N}(\mu_i, \sigma^2), \qquad i = 1,\dots,N
$$

where $\sigma^2$ is the observation error, and the mean parameter $\mu_i$ is linked to the **linear predictor** ($\eta_i$) through an identity function: $$
\eta_i = \mu_i = \beta_0 + \beta_1 x_i
$$ where $x_i$ is a covariate and $\beta_0, \beta_1$ are parameters to be estimated. We assign $\beta_0$ and $\beta_1$ a vague Gaussian prior.

To finalize the Bayesian model we assign a $\text{Gamma}(a,b)$ prior to the precision parameter $\tau = 1/\sigma^2$ and two independent Gaussian priors with mean $0$ and precision $\tau_{\beta}$ to the regression parameters $\beta_0$ and $\beta_1$ (we will use the default prior settings in INLA for now).

::: {.callout-tip icon="false"}
## {{< bi question-octagon color=#6dc83c >}} Question

What is the dimension of the hyperparameter vector and latent Gaussian field?

`r hide("Answer")`

The hyperparameter vector has dimension 1, $\pmb{\theta} = (\tau)$ while the latent Gaussian field $\pmb{u} = (\beta_0, \beta_1)$ has dimension 2, $0$ mean, and sparse precision matrix:

$$
\pmb{Q} = \begin{bmatrix}
\tau_{\beta_0} & 0\\
0 & \tau_{\beta_1}
\end{bmatrix}
$$ Note that, since $\beta_0$ and $\beta_1$ are fixed effects, the precision parameters $\tau_{\beta_0}$ and $\tau_{\beta_1}$ are fixed.

`r unhide()`
:::

::: callout-note
We can write the linear predictor vector $\pmb{\eta} = (\eta_1,\dots,\eta_N)$ as

$$
\pmb{\eta} = \pmb{A}\pmb{u} = \pmb{A}_1\pmb{u}_1 + \pmb{A}_2\pmb{u}_2 = \begin{bmatrix}
1 \\
1\\
\vdots\\
1
\end{bmatrix} \beta_0 + \begin{bmatrix}
x_1 \\
x_2\\
\vdots\\
x_N
\end{bmatrix} \beta_1
$$

Our linear predictor consists then of two components: an intercept and a slope.
:::

### Simulate example data

We fix the model parameters $\beta_0$, $\beta_1$ and the hyperparameter $\tau_y$ to a given value and simulate the data accordingly using the code below. The simulated response and covariate data are then saved in a `data.frame` object.

```{r}
#| code-fold: show
#| code-summary: "Simulate Data from a LM"

# set seed for reproducibility
set.seed(1234) 

beta = c(2,0.5)
sd_error = 0.1

n = 100
x = rnorm(n)
y = beta[1] + beta[2] * x + rnorm(n, sd = sd_error)

df = data.frame(y = y, x = x)  

```

### Fitting a linear regression model with `inlabru`

------------------------------------------------------------------------

**Step1: Defining model components**


The first step is to define the two model components: The intercept and the linear covariate effect.


::: {.callout-warning icon="false"}
## {{< bi pencil-square color=#c8793c >}} Task

Define an object called `cmp` that includes and (i) intercept `beta_0` and (ii) a covariate `x` linear effect `beta_1`.

`r hide("Take hint")`

The `cmp` object is here used to define model components. We can give them any useful names we like, in this case, `beta_0` and `beta_1`. You can remove the automatic intercept construction by adding a `-1` in the components

`r unhide()`

```{r}
#| webex.hide: "Click here to see the solution"
#| code-fold: show
#| purl: false
cmp =  ~ -1 + beta_0(1) + beta_1(x, model = "linear")

```
:::

::: callout-note
`inlabru` has automatic intercept that can be called by typing `Intercept()` , which is one of `inlabru` special names and it is used to define a global intercept, e.g.

```{r}
#| eval: false
#| purl: false

cmp =  ~  Intercept(1) + beta_1(x, model = "linear")

```
:::

**Step 2: Build the observation model**

The next step is to construct the observation model by defining the model likelihood. The most important inputs here are the `formula`, the `family` and the `data`.

::: {.callout-warning icon="false"}
## {{< bi pencil-square color=#c8793c >}} Task

Define a linear predictor `eta` using the component labels you have defined on the previous task.

`r hide("Take hint")`

The `eta` object defines how the components should be combined in order to define the model predictor.

`r unhide()`

```{r}
#| webex.hide: "Click here to see the solution"
#| code-fold: show
#| purl: false
eta = y ~ beta_0 + beta_1

```
:::

The likelihood for the observational model is defined using the `bru_obs()` function.

::: {.callout-warning icon="false"}
## {{< bi pencil-square color=#c8793c >}} Task

Define the observational model likelihood in an object called `lik` using the `bru_obs()` function.

`r hide("Take hint")`

The `bru_obs` is expecting three arguments:

-   The linear predictor `eta` we defined in the previous task
-   The data likelihood (this can be specified by setting `family = "gaussian"`)
-   The data set `df`

`r unhide()`

```{r}
#| webex.hide: "Click here to see the solution"
#| code-fold: show
#| purl: false
lik =  bru_obs(formula = eta,
            family = "gaussian",
            data = df)

```
:::

**Step 3: Fit the model**

We fit the model using the `bru()` functions which takes as input the components and the observation model:

```{r}
#| code-summary: "Fit LM in `inlabru`"
fit.lm = bru(cmp, lik)
```

**Step 4: Extract results**

There are several ways to extract and examine the results of a fitted `inlabru` object. 

The most natural place to start is to use the 
 `summary()` which gives access to some basic information about model fit and estimates

```{r }
#| code-summary: "Model summaries"
#| collapse: true
summary(fit.lm)
```

We can see that both the intercept and slope and the error precision are correctly estimated.

Another way, which gives access to more complicated (and useful) output is to use the `predict()` function. 
Below we  take the fitted `bru` object and use the `predict()` function to produce predictions for $\mu$ given a new set of values for the model covariates or the original values used for the model fit


```{r}
new_data = data.frame(x = c(df$x, runif(10)),
                      y = c(df$y, rep(NA,10)))
pred = predict(fit.lm, new_data, ~ beta_0 + beta_1,
               n.samples = 1000)



```

The `predict` function generate samples from the fitted model. In this case we set the number of samples to 1000.

::: panel-tabset
## Plot

```{r}
#| code-fold: true
#| fig-cap: Data and 95% credible intervals
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| fig-width: 4
#| fig-height: 4

pred %>% ggplot() + 
  geom_point(aes(x,y), alpha = 0.3) +
  geom_line(aes(x,mean)) +
  geom_line(aes(x, q0.025), linetype = "dashed")+
  geom_line(aes(x, q0.975), linetype = "dashed")+
  xlab("Covariate") + ylab("Observations")
```

## R Code

```{r}
#| code-fold: show
#| eval: false
#| purl: false


pred %>% ggplot() + 
  geom_point(aes(x,y), alpha = 0.3) +
  geom_line(aes(x,mean)) +
  geom_line(aes(x, q0.025), linetype = "dashed")+
  geom_line(aes(x, q0.975), linetype = "dashed")+
  xlab("Covariate") + ylab("Observations")
```
:::

::: {.callout-warning icon="false"}
## {{< bi pencil-square color=#c8793c >}} Task

Generate predictions for the linear predictor $\mu$ when the covariate has value $x_0 = 0.45$.

What is the predicted value for $\mu$? And what is the uncertainty?


`r hide("Take hint")`

You can create a new data frame containing the new observation $x_0$ and then use the `predict` function.

`r unhide()`

```{r}
#| webex.hide: "Click here to see the solution"
#| code-fold: show
#| purl: false

new_data = data.frame(x = 0.45)
pred = predict(fit.lm, new_data, ~ beta_0 + beta_1,
               n.samples = 1000)
pred
```

You can see the predicted mean and sd by examining the produced `pred` object. In this case the mean is `r round(pred$mean,2)` with sd ca `r round(pred$sd,2)`. This gives a 95% CI ca [`r round(pred$q0.025,2)`,  `r round(pred$q0.975,2)`].
:::

::: {.callout-note}

Before we have produced a credible interval for the expected mean $\mu$ if we want to produce a _prediction_ interval for  a new observation $y$ we need to add the uncertainty that comes from the likelihood with precision $\tau_y$. To do this we can again use the `predict()` function to compute  a 95% prediction interval for $y$. 

```{r}

pred2 = predict(fit.lm, new_data, 
               formula = ~ {
                 mu = beta_0 + beta_1
                 sigma = sqrt(1/Precision_for_the_Gaussian_observations)
                 list(q1 = qnorm(0.025, mean = mu, sd = sigma),
                      q2 =  qnorm(0.975, mean = mu, sd = sigma))},
               n.samples = 1000)
round(c(pred2$q1$mean, pred2$q2$mean),2)
```

Notice that now the interval we obtain is larger.
:::

### Setting Priors

In `R-INLA`, the default choice of priors for each $\beta$ is

$$
\beta \sim \mathcal{N}(0,10^3).
$$

and the prior for the variance parameter in terms of the log precision is

$$ \log(\tau) \sim \mathrm{logGamma}(1,5 \times 10^{-5}) $$

::: callout-note
If your model uses the default intercept construction (i.e., `Intercept(1)` in the linear predictor) `inlabru` will assign a default $\mathcal{N} (0,0)$ prior to it.
:::

To check which priors are used in a fitted model one can use the function `inla.prior.used()`

```{r}
#| eval: true
#| purl: true 

inla.priors.used(fit.lm)
```

From the output we see that the precision for the observation $\tau\sim\text{Gamma}(1e+00,5e-05)$ while $\beta_0$ and $\beta_1$ have precision 0.001, that is variance $1/0.001$.

**Changing the precision for the linear effects**

The precision for linear effects is set in the component definition. For example, if we want to increase the precision to 0.01 for $\beta_0$ we define the respective components as:

```{r }
#| eval: false 
#| purl: false

cmp1 =  ~-1 +  beta_0(1, prec.linear = 0.01) + beta_1(x, model = "linear")
```

::: {.callout-warning icon="false"}
## {{< bi pencil-square color=#c8793c >}} Task

Run the model again using 0.1 as default precision for both the intercept and the slope parameter.

```{r}
#| webex.hide: "Click here to see the solution"
#| code-fold: false
#| fig-width: 6 
#| fig-height: 4
#| fig-align: center 
#| purl: false
#| eval: true  
 
cmp2 =  ~ -1 + 
          beta_0(1, prec.linear = 0.1) + 
          beta_1(x, model = "linear", prec.linear = 0.1)

lm.fit2 = bru(cmp2, lik) 
```

Note that we can use the same observation model as before since both the formula and the dataset are unchanged.
:::

**Changing the prior for the precision of the observation error** $\tau$

Priors on the hyperparameters of the observation model must be passed by defining argument `hyper` within `control.family` in the call to the `bru_obs()` function.

```{r}

# First we define the logGamma (0.01,0.01) prior 

prec.tau <- list(prec = list(prior = "loggamma",   # prior name
                             param = c(0.01, 0.01))) # prior values

lik2 =  bru_obs(formula = y ~.,
                family = "gaussian",
                data = df,
                control.family = list(hyper = prec.tau))

fit.lm2 = bru(cmp2, lik2) 

```

The names of the priors available in `inlabru` can be seen with `names(inla.models()$prior)`

### Visualizing the posterior marginals

Posterior marginal distributions of the ï¬xed effects parameters and the hyperparameters can be visualized using the `plot()` function by calling the name of the component. For example, if want to visualize the posterior density of the intercept $\beta_0$ we can type:

```{r}
#| fig-width: 4
#| fig-align: center
#| fig-height: 4
#| code-fold: show

plot(fit.lm, "beta_0")
```

::: {.callout-warning icon="false"}
## {{< bi pencil-square color=#c8793c >}} Task

Plot the posterior marginals for $\beta_1$ and for the precision of the observation error $\pi(\tau|y)$

`r hide("Take hint")`

You can use the `bru_names(fit.lm)` function to check the names for the different model components.

`r unhide()`

```{r}
#| fig-width: 6
#| fig-height: 4
#| fig-align: center
#| webex.hide: "Click here to see the solution"
#| code-fold: show
#| purl: false

plot(fit.lm, "beta_1") +
plot(fit.lm, "Precision for the Gaussian observations")
```
:::

::: callout-note
## Supplementary Material

The marginal densities for the hyper parameters can be also found by calling`inlabru_model$marginals.hyperpar`.

```{r}
#| purl: false
#| code-fold: false
#| 
tau_e <- fit.lm$marginals.hyperpar$`Precision for the Gaussian observations` 

```

We can then apply a transformation using the `inla.tmarginal` function to transform the precision posterior distributions to a SD scale.

```{r}
#| purl: false
#| code-fold: false

sigma_e <- tau_e %>%
  inla.tmarginal(function(x) 1/x,.) 

```

Then, we can compute posterior summaries using `inla.zmarginal` function as follows:

```{r}
#| purl: false
post_sigma_summaries <- inla.zmarginal(sigma_e,silent = T)
cbind(post_sigma_summaries)
```
:::



## Linear model with discrete variables and  interactions{#LM_int}

We consider now the dataset `iris`. Here data are recorded about 150 different iris flowers belonging to 3 different species (50 for each species).

You can get more information about these data by typing `?iris` 


We want to model the `Petal.length` as a function of `Sepal.length` and species.

```{r}
data("iris")
iris %>% ggplot() +
  geom_point(aes(Sepal.Length, Petal.Length, color= Species)) +
  facet_wrap(.~Species)
```

**Model 1 - Only `Species` effect**
Our first model assumes that the Sepal length only depends on the species, which is a categoriacal variable.

$$
\begin{aligned}
Y_i & \sim\mathcal{N}(\mu_i,\sigma_y),\ &  i = 1,\dots,150 \\
\mu_{i} & = \eta_{i} = \beta_1\ I(\text{obs }i\text{belongs to species } 1  ) + \beta_2\ I(\text{obs }i\text{belongs to species } 2  ) + \beta_3\ I(\text{obs }i\text{belongs to species } 3  )
\end{aligned}
$$

Using `lm()` we can fit the model as:

```{r}
mod1 = lm(Petal.Length ~ Species, data  = iris)
summary(mod1)
```

Notice that `lm()` uses `setosa` as reference category, the parameter `Speciesversicolor` is then interpreted as the difference between the effect of the reference species and effect of `versicolor` species.

`inlabru` features a `model = 'fixed'` component type that allows users to specify linear fixed effects using a formula as input. The basic component input is a model matrix. Alternatively, one can supply a formula specification, which is then used to generate a model matrix automatically

```{r}
cmp = ~   spp(
  main = ~ Species,
  model = "fixed"
)
```

::: {.callout-warning icon="false"}
## {{< bi pencil-square color=#c8793c >}} Task

Use the component defined above to fit the linear model using `inlabru` and compare the output with that form the `lm` function.

`r hide("Take hint")`


Realizing that fixed effects are treated as random effects with fixed precision, the fitted values can then be inspected via `fit1a$summary.random`

`r unhide()`

```{r}
#| webex.hide: "Click here to see the solution"
#| code-fold: show
#| purl: false

lik = bru_obs(formula =Petal.Length ~ spp,
              data = iris)
fit1a = bru(cmp, lik)

fit1a$summary.random$spp

```

:::

**Model 2 - Interaction between `Species` and `Sepal.Length`**

Our second model is defined as
$$
\begin{aligned}
Y_i & \sim\mathcal{N}(\mu_i,\sigma_y),\ &  i = 1,\dots,150 \\
\mu_{i} & = \eta_{i} = \beta_0 +  \beta_1 x_i\ I(\text{obs }i\text{belongs to species } 1  ) + \beta_2x_i\ I(\text{obs }i\text{belongs to species } 2  ) + \beta_3x_i\ I(\text{obs }i\text{belongs to species } 3  )
\end{aligned}
$$

that is, we have a common intercept $\beta_0$ while the linear effect of the Sepal length depends on the Species. Using `lm()` we have:
```{r}
mod2 = lm(Petal.Length ~ Species:Sepal.Length, data = iris)
mod2
```



::: {.callout-warning icon="false"}
## {{< bi pencil-square color=#c8793c >}} Task

Fit the same model in `inlabru`.

```{r}
#| webex.hide: "Click here to see the solution"
#| code-fold: show
#| purl: false

cmp = ~   spp_sepal(
  main = ~ Species:Sepal.Length,
  model = "fixed"
)

lik = bru_obs(formula =Petal.Length ~ spp_sepal,
              data = iris)

fit1b = bru(cmp, lik)

fit1b$summary.random$spp_sepal


```
:::


::: {.callout-warning icon="false"}
## {{< bi pencil-square color=#c8793c >}} Task

Plot the estimated regression lines for the three species using model `fit1b`


```{r}
#| webex.hide: "Click here to see the solution"
#| code-fold: show
#| purl: false
#| fig-align: center
#| fig-width: 5
#| fig-height: 5

preds = predict(fit1b, iris, ~ spp_sepal)

pp = preds %>% ggplot() + geom_line(aes(Sepal.Length, mean, group = Species, color = Species)) +
  geom_ribbon(aes(Sepal.Length, ymin = q0.025, ymax = q0.975, 
                  group = Species, fill = Species), alpha = 0.5) +
  geom_point(aes(Sepal.Length, Petal.Length,color = Species))

pp
```
:::




